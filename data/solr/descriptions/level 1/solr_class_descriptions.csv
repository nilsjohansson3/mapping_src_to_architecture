Module,Text
cloud,public class ActionThrottle extends Object
cloud,"Additionally, the provided SolrCloseableLatch instance can be used to await for all listed replicas to become active."
cloud,public class CloudDescriptor extends Object SolrCloud metadata attached to a CoreDescriptor.
cloud,public class CloudUtil extends Object
cloud,public class ConfigSetApiLockFactory extends Object This class implements a higher level locking abstraction for the Config Set API using lower level read and write locks.
cloud,"public class DistributedApiAsyncTracker extends Object Class holding the implementation required for tracking asynchronous Collection API (or other) tasks when the Collection API is distributed. This replaces the features provided by the distributed maps on ZK paths /overseer/collection-map-completed, /overseer/collection-map-failure and /overseer/async_ids when the Collection API commands are handled by the Overseer. It works by using two Zookeeper directories, one for persistent nodes for each new async id and one for ephemeral nodes for each async id currently being processed (in flight). A persistent async node has either no data, or has a serialized OverseerSolrResponse as content. An ephemeral async node has two possible states (content): 'S' or 'R'. The actual state of an async task is built from a combination of the two nodes: +===================+=========================================+=================================================+====================+ | | persistent=success OverseerSolrResponse | persistent=null or failed OverseerSolrResponse | No persistent node | +===================+=========================================+=================================================+====================+ | ephemeral=""S"" | Task completed successfully | Task submitted | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+ | ephemeral=""R"" | Task completed successfully | Task running | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+ | No ephemeral node | Task completed successfully | Task failed (see response or null=node failure) | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+"
cloud,"This replaces the features provided by the distributed maps on ZK paths /overseer/collection-map-completed, /overseer/collection-map-failure and /overseer/async_ids when the Collection API commands are handled by the Overseer. It works by using two Zookeeper directories, one for persistent nodes for each new async id and one for ephemeral nodes for each async id currently being processed (in flight). A persistent async node has either no data, or has a serialized OverseerSolrResponse as content. An ephemeral async node has two possible states (content): 'S' or 'R'. The actual state of an async task is built from a combination of the two nodes: +===================+=========================================+=================================================+====================+ | | persistent=success OverseerSolrResponse | persistent=null or failed OverseerSolrResponse | No persistent node | +===================+=========================================+=================================================+====================+ | ephemeral=""S"" | Task completed successfully | Task submitted | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+ | ephemeral=""R"" | Task completed successfully | Task running | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+ | No ephemeral node | Task completed successfully | Task failed (see response or null=node failure) | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+"
cloud,"It works by using two Zookeeper directories, one for persistent nodes for each new async id and one for ephemeral nodes for each async id currently being processed (in flight). A persistent async node has either no data, or has a serialized OverseerSolrResponse as content. An ephemeral async node has two possible states (content): 'S' or 'R'. The actual state of an async task is built from a combination of the two nodes: +===================+=========================================+=================================================+====================+ | | persistent=success OverseerSolrResponse | persistent=null or failed OverseerSolrResponse | No persistent node | +===================+=========================================+=================================================+====================+ | ephemeral=""S"" | Task completed successfully | Task submitted | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+ | ephemeral=""R"" | Task completed successfully | Task running | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+ | No ephemeral node | Task completed successfully | Task failed (see response or null=node failure) | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+"
cloud,"The actual state of an async task is built from a combination of the two nodes: +===================+=========================================+=================================================+====================+ | | persistent=success OverseerSolrResponse | persistent=null or failed OverseerSolrResponse | No persistent node | +===================+=========================================+=================================================+====================+ | ephemeral=""S"" | Task completed successfully | Task submitted | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+ | ephemeral=""R"" | Task completed successfully | Task running | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+ | No ephemeral node | Task completed successfully | Task failed (see response or null=node failure) | Unknown task | +-------------------+-----------------------------------------+-------------------------------------------------+--------------------+"
cloud,public class DistributedClusterStateUpdater extends Object Gives access to distributed cluster state update methods and allows code to inquire whether distributed state update is enabled.
cloud,"Records desired changes to state.json files in Zookeeper (as are done by the family of mutator classes such as ClusterStateMutator, CollectionMutator etc.) in order to be able to later execute them on the actual content of the state.json files using optimistic locking (and retry a few times if the optimistic locking failed). Instances are not thread safe."
cloud,Instances are not thread safe.
cloud,"Direct Known Subclasses: SizeLimitedDistributedMap public class DistributedMap extends Object A distributed map. This supports basic map functions e.g. get, put, contains for interaction with zk which don't have to be ordered i.e. DistributedQueue."
cloud,public class DistributedMultiLock extends Object A lock as acquired for running a single API command (Collection or Config Set or anything else in the future). Internally it is composed of multiple DistributedLock's.
cloud,"public class LeaderElector extends Object Leader Election process. This class contains the logic by which a leader is chosen. First call setup(ElectionContext) to ensure the election process is init'd. Next call joinElection(ElectionContext, boolean) to start the leader election. The implementation follows the classic ZooKeeper recipe of creating an ephemeral, sequential node for each candidate and then looking at the set of such nodes - if the created node is the lowest sequential node, the candidate that created the node is the leader. If not, the candidate puts a watch on the next lowest node it finds, and if that node goes down, starts the whole process over by checking if it's the lowest sequential node, etc."
cloud,"The implementation follows the classic ZooKeeper recipe of creating an ephemeral, sequential node for each candidate and then looking at the set of such nodes - if the created node is the lowest sequential node, the candidate that created the node is the leader. If not, the candidate puts a watch on the next lowest node it finds, and if that node goes down, starts the whole process over by checking if it's the lowest sequential node, etc."
cloud,public class LockTree extends Object This is a utility class that offers fine grained locking for various Collection Operations This class is designed for single threaded operation. It's safe for multiple threads to use it but internally it is synchronized so that only one thread can perform any operation.
cloud,"Enclosing class: LockTree public class LockTree.Session extends Object This class is used to mark nodes for which acquiring a lock was attempted but didn't succeed. Lock acquisition failure needs to be ""remembered"" to trigger failures to acquire a competing lock until the Session is replaced, to prevent tasks enqueued later (and dequeued later once the busy lock got released) from being executed before earlier tasks that failed to execute because the lock wasn't available earlier when they attempted to acquire it. A new Session is created each time the iteration over the queue tasks is restarted starting at the oldest non running or completed tasks."
cloud,A new Session is created each time the iteration over the queue tasks is restarted starting at the oldest non running or completed tasks.
cloud,"Cluster State updates, i.e. updating Collections' state.json files in ZooKeeper, see Overseer.ClusterStateUpdater, Collection API implementation, see OverseerCollectionConfigSetProcessor and OverseerCollectionMessageHandler (and the example below), Updating Config Sets, see OverseerCollectionConfigSetProcessor and OverseerConfigSetMessageHandler,"
cloud,"Collection API implementation, see OverseerCollectionConfigSetProcessor and OverseerCollectionMessageHandler (and the example below), Updating Config Sets, see OverseerCollectionConfigSetProcessor and OverseerConfigSetMessageHandler,"
cloud,"Updating Config Sets, see OverseerCollectionConfigSetProcessor and OverseerConfigSetMessageHandler,"
cloud,"The state update queue, through which nodes request the Overseer to update the state.json file of a Collection in ZooKeeper. This queue is in Zookeeper at /overseer/queue, A queue shared between Collection API and Config Set API requests. This queue is in Zookeeper at /overseer/collection-queue-work."
cloud,A queue shared between Collection API and Config Set API requests. This queue is in Zookeeper at /overseer/collection-queue-work.
cloud,"Client uses the Collection API with CREATE action and reaches a node of the cluster, The node (via CollectionsHandler) enqueues the request into the /overseer/collection-queue-work queue in ZooKeepeer, The OverseerCollectionConfigSetProcessor running on the Overseer node dequeues the message and using an executor service with a maximum pool size of OverseerTaskProcessor.MAX_PARALLEL_TASKS hands it for processing to OverseerCollectionMessageHandler, Command CreateCollectionCmd then executes and does: Update some state directly in ZooKeeper (creating collection znode), Compute replica placement on available nodes in the cluster, Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper... The Overseer.ClusterStateUpdater (also running on the Overseer node) dequeues the state change message and creates the state.json file in ZooKeeper for the Collection. All the work of the cluster state updater (creations, updates, deletes) is done sequentially for the whole cluster by a single thread. The CreateCollectionCmd sees the state change in ZooKeeper and: Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE. The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,"The node (via CollectionsHandler) enqueues the request into the /overseer/collection-queue-work queue in ZooKeepeer, The OverseerCollectionConfigSetProcessor running on the Overseer node dequeues the message and using an executor service with a maximum pool size of OverseerTaskProcessor.MAX_PARALLEL_TASKS hands it for processing to OverseerCollectionMessageHandler, Command CreateCollectionCmd then executes and does: Update some state directly in ZooKeeper (creating collection znode), Compute replica placement on available nodes in the cluster, Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper... The Overseer.ClusterStateUpdater (also running on the Overseer node) dequeues the state change message and creates the state.json file in ZooKeeper for the Collection. All the work of the cluster state updater (creations, updates, deletes) is done sequentially for the whole cluster by a single thread. The CreateCollectionCmd sees the state change in ZooKeeper and: Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE. The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,"The OverseerCollectionConfigSetProcessor running on the Overseer node dequeues the message and using an executor service with a maximum pool size of OverseerTaskProcessor.MAX_PARALLEL_TASKS hands it for processing to OverseerCollectionMessageHandler, Command CreateCollectionCmd then executes and does: Update some state directly in ZooKeeper (creating collection znode), Compute replica placement on available nodes in the cluster, Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper... The Overseer.ClusterStateUpdater (also running on the Overseer node) dequeues the state change message and creates the state.json file in ZooKeeper for the Collection. All the work of the cluster state updater (creations, updates, deletes) is done sequentially for the whole cluster by a single thread. The CreateCollectionCmd sees the state change in ZooKeeper and: Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE. The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,"Command CreateCollectionCmd then executes and does: Update some state directly in ZooKeeper (creating collection znode), Compute replica placement on available nodes in the cluster, Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper... The Overseer.ClusterStateUpdater (also running on the Overseer node) dequeues the state change message and creates the state.json file in ZooKeeper for the Collection. All the work of the cluster state updater (creations, updates, deletes) is done sequentially for the whole cluster by a single thread. The CreateCollectionCmd sees the state change in ZooKeeper and: Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE. The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,"Update some state directly in ZooKeeper (creating collection znode), Compute replica placement on available nodes in the cluster, Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper..."
cloud,"Compute replica placement on available nodes in the cluster, Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper..."
cloud,"Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper..."
cloud,The command then waits for the update to be seen in ZooKeeper...
cloud,"The Overseer.ClusterStateUpdater (also running on the Overseer node) dequeues the state change message and creates the state.json file in ZooKeeper for the Collection. All the work of the cluster state updater (creations, updates, deletes) is done sequentially for the whole cluster by a single thread. The CreateCollectionCmd sees the state change in ZooKeeper and: Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE. The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,"The CreateCollectionCmd sees the state change in ZooKeeper and: Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE. The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE.
cloud,"The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,"CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,The client receives a success return.
cloud,"The Overseer is a single elected node in the SolrCloud cluster that is in charge of interactions with ZooKeeper that require global synchronization. The Overseer deals with: Cluster State updates, i.e. updating Collections' state.json files in ZooKeeper, see Overseer.ClusterStateUpdater, Collection API implementation, see OverseerCollectionConfigSetProcessor and OverseerCollectionMessageHandler (and the example below), Updating Config Sets, see OverseerCollectionConfigSetProcessor and OverseerConfigSetMessageHandler, The nodes in the cluster communicate with the Overseer over queues implemented in ZooKeeper. There are essentially two queues: The state update queue, through which nodes request the Overseer to update the state.json file of a Collection in ZooKeeper. This queue is in Zookeeper at /overseer/queue, A queue shared between Collection API and Config Set API requests. This queue is in Zookeeper at /overseer/collection-queue-work. An example of the steps involved in the Overseer processing a Collection creation API call: Client uses the Collection API with CREATE action and reaches a node of the cluster, The node (via CollectionsHandler) enqueues the request into the /overseer/collection-queue-work queue in ZooKeepeer, The OverseerCollectionConfigSetProcessor running on the Overseer node dequeues the message and using an executor service with a maximum pool size of OverseerTaskProcessor.MAX_PARALLEL_TASKS hands it for processing to OverseerCollectionMessageHandler, Command CreateCollectionCmd then executes and does: Update some state directly in ZooKeeper (creating collection znode), Compute replica placement on available nodes in the cluster, Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper... The Overseer.ClusterStateUpdater (also running on the Overseer node) dequeues the state change message and creates the state.json file in ZooKeeper for the Collection. All the work of the cluster state updater (creations, updates, deletes) is done sequentially for the whole cluster by a single thread. The CreateCollectionCmd sees the state change in ZooKeeper and: Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE. The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,"The Overseer deals with: Cluster State updates, i.e. updating Collections' state.json files in ZooKeeper, see Overseer.ClusterStateUpdater, Collection API implementation, see OverseerCollectionConfigSetProcessor and OverseerCollectionMessageHandler (and the example below), Updating Config Sets, see OverseerCollectionConfigSetProcessor and OverseerConfigSetMessageHandler, The nodes in the cluster communicate with the Overseer over queues implemented in ZooKeeper. There are essentially two queues: The state update queue, through which nodes request the Overseer to update the state.json file of a Collection in ZooKeeper. This queue is in Zookeeper at /overseer/queue, A queue shared between Collection API and Config Set API requests. This queue is in Zookeeper at /overseer/collection-queue-work. An example of the steps involved in the Overseer processing a Collection creation API call: Client uses the Collection API with CREATE action and reaches a node of the cluster, The node (via CollectionsHandler) enqueues the request into the /overseer/collection-queue-work queue in ZooKeepeer, The OverseerCollectionConfigSetProcessor running on the Overseer node dequeues the message and using an executor service with a maximum pool size of OverseerTaskProcessor.MAX_PARALLEL_TASKS hands it for processing to OverseerCollectionMessageHandler, Command CreateCollectionCmd then executes and does: Update some state directly in ZooKeeper (creating collection znode), Compute replica placement on available nodes in the cluster, Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper... The Overseer.ClusterStateUpdater (also running on the Overseer node) dequeues the state change message and creates the state.json file in ZooKeeper for the Collection. All the work of the cluster state updater (creations, updates, deletes) is done sequentially for the whole cluster by a single thread. The CreateCollectionCmd sees the state change in ZooKeeper and: Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE. The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,"Cluster State updates, i.e. updating Collections' state.json files in ZooKeeper, see Overseer.ClusterStateUpdater, Collection API implementation, see OverseerCollectionConfigSetProcessor and OverseerCollectionMessageHandler (and the example below), Updating Config Sets, see OverseerCollectionConfigSetProcessor and OverseerConfigSetMessageHandler,"
cloud,"Collection API implementation, see OverseerCollectionConfigSetProcessor and OverseerCollectionMessageHandler (and the example below), Updating Config Sets, see OverseerCollectionConfigSetProcessor and OverseerConfigSetMessageHandler,"
cloud,"Updating Config Sets, see OverseerCollectionConfigSetProcessor and OverseerConfigSetMessageHandler,"
cloud,"The nodes in the cluster communicate with the Overseer over queues implemented in ZooKeeper. There are essentially two queues: The state update queue, through which nodes request the Overseer to update the state.json file of a Collection in ZooKeeper. This queue is in Zookeeper at /overseer/queue, A queue shared between Collection API and Config Set API requests. This queue is in Zookeeper at /overseer/collection-queue-work. An example of the steps involved in the Overseer processing a Collection creation API call: Client uses the Collection API with CREATE action and reaches a node of the cluster, The node (via CollectionsHandler) enqueues the request into the /overseer/collection-queue-work queue in ZooKeepeer, The OverseerCollectionConfigSetProcessor running on the Overseer node dequeues the message and using an executor service with a maximum pool size of OverseerTaskProcessor.MAX_PARALLEL_TASKS hands it for processing to OverseerCollectionMessageHandler, Command CreateCollectionCmd then executes and does: Update some state directly in ZooKeeper (creating collection znode), Compute replica placement on available nodes in the cluster, Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper... The Overseer.ClusterStateUpdater (also running on the Overseer node) dequeues the state change message and creates the state.json file in ZooKeeper for the Collection. All the work of the cluster state updater (creations, updates, deletes) is done sequentially for the whole cluster by a single thread. The CreateCollectionCmd sees the state change in ZooKeeper and: Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE. The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,"An example of the steps involved in the Overseer processing a Collection creation API call: Client uses the Collection API with CREATE action and reaches a node of the cluster, The node (via CollectionsHandler) enqueues the request into the /overseer/collection-queue-work queue in ZooKeepeer, The OverseerCollectionConfigSetProcessor running on the Overseer node dequeues the message and using an executor service with a maximum pool size of OverseerTaskProcessor.MAX_PARALLEL_TASKS hands it for processing to OverseerCollectionMessageHandler, Command CreateCollectionCmd then executes and does: Update some state directly in ZooKeeper (creating collection znode), Compute replica placement on available nodes in the cluster, Enqueue a state change request for creating the state.json file for the collection in ZooKeeper. This is done by enqueuing a message in /overseer/queue , The command then waits for the update to be seen in ZooKeeper... The Overseer.ClusterStateUpdater (also running on the Overseer node) dequeues the state change message and creates the state.json file in ZooKeeper for the Collection. All the work of the cluster state updater (creations, updates, deletes) is done sequentially for the whole cluster by a single thread. The CreateCollectionCmd sees the state change in ZooKeeper and: Builds and sends requests to each node to create the appropriate cores for all the replicas of all shards of the collection. Nodes create the replicas and set them to Replica.State.ACTIVE. The collection creation command has succeeded from the Overseer perspective, CollectionsHandler checks the replicas in Zookeeper and verifies they are all Replica.State.ACTIVE, The client receives a success return."
cloud,"public class OverseerNodePrioritizer extends Object Responsible for prioritization of Overseer nodes, for example with the ADDROLE collection command."
cloud,public class OverseerSolrResponseSerializer extends Object
cloud,An OverseerTaskProcessor.OverseerMessageHandlerSelector determines which OverseerMessageHandler handles specific messages in the queue.
cloud,Enclosing class: OverseerTaskQueue public static class OverseerTaskQueue.QueueEvent extends Object
cloud,public class RecoveringCoreTermWatcher extends Object Start recovery of a core if its term is less than leader's term
cloud,public class ReplicateFromLeader extends Object
cloud,"public class SizeLimitedDistributedMap extends DistributedMap A size limited distributed map maintained in zk. Oldest znodes (as per modification time) are evicted as newer ones come in. When the map hits the specified maximum size, the oldest maxSize / 10 items are evicted on the next put(String, byte[]) invocation."
cloud,"When the map hits the specified maximum size, the oldest maxSize / 10 items are evicted on the next put(String, byte[]) invocation."
cloud,public class SolrZkServer extends Object
cloud,Enclosing class: Stats public static class Stats.FailedOp extends Object
cloud,public class Stats extends Object Used to hold statistics about some SolrCloud operations. This is experimental API and subject to change.
cloud,This is experimental API and subject to change.
cloud,Enclosing class: Stats public static class Stats.Stat extends Object
cloud,public class SyncStrategy extends Object
cloud,public class ZkConfigSetService extends ConfigSetService SolrCloud Zookeeper ConfigSetService impl.
cloud,"notes: loads everything on init, creates what's not there - further updates are prompted with Watches. TODO: exceptions during close on attempts to update cloud state"
cloud,TODO: exceptions during close on attempts to update cloud state
cloud,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
cloud,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
cloud,"This implementation (with help from subclass OverseerTaskQueue) is used for the /overseer/collection-queue-work queue used for Collection and Config Set API calls to the Overseer. Implementation note: In order to enqueue a message into this queue, a CreateMode.EPHEMERAL_SEQUENTIAL response node is created and watched at /overseer/collection-queue-work/qnr-monotonically_increasng_id, then a corresponding CreateMode.PERSISTENT request node reusing the same id is created at /overseer/collection-queue-work/qn-response_id."
cloud,"Implementation note: In order to enqueue a message into this queue, a CreateMode.EPHEMERAL_SEQUENTIAL response node is created and watched at /overseer/collection-queue-work/qnr-monotonically_increasng_id, then a corresponding CreateMode.PERSISTENT request node reusing the same id is created at /overseer/collection-queue-work/qn-response_id."
cloud,A replica sets its term equals to leader's term The leader increase its term and some other replicas by 1
cloud,The leader increase its term and some other replicas by 1
cloud,"{ ""replicaNodeName1"" : 1, ""replicaNodeName2"" : 2, .. } The values correspond to replicas are called terms. Only replicas with highest term value are considered up to date and be able to become leader and serve queries. Terms can only updated in two strict ways: A replica sets its term equals to leader's term The leader increase its term and some other replicas by 1 This class should not be reused after Watcher.Event.KeeperState.Expired event"
cloud,The values correspond to replicas are called terms. Only replicas with highest term value are considered up to date and be able to become leader and serve queries. Terms can only updated in two strict ways: A replica sets its term equals to leader's term The leader increase its term and some other replicas by 1 This class should not be reused after Watcher.Event.KeeperState.Expired event
cloud,Terms can only updated in two strict ways: A replica sets its term equals to leader's term The leader increase its term and some other replicas by 1 This class should not be reused after Watcher.Event.KeeperState.Expired event
cloud,A replica sets its term equals to leader's term The leader increase its term and some other replicas by 1
cloud,The leader increase its term and some other replicas by 1
cloud,"Fields inherited from classjava.io.ByteArrayInputStream buf, count, mark, pos"
cloud,"Methods inherited from classjava.io.ByteArrayInputStream available, close, mark, markSupported, read, read, readAllBytes, readNBytes, reset, skip, transferTo"
cloud,"Methods inherited from classjava.io.InputStream nullInputStream, read, readNBytes"
cloud,"Fields inherited from classjava.io.ByteArrayInputStream buf, count, mark, pos"
cloud,Enclosing class: AddReplicaCmd public static class AddReplicaCmd.CreateReplica extends Object A data structure to keep all information required to create a new replica in one place. Think of it as a typed ZkNodeProps for replica creation. This is not a public API and can be changed at any time without notice.
cloud,This is not a public API and can be changed at any time without notice.
cloud,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
cloud,Enclosing class: Assign public static class Assign.AssignRequest extends Object
cloud,Enclosing class: Assign public static class Assign.AssignRequestBuilder extends Object
cloud,public class Assign extends Object
cloud,public class CategoryRoutedAlias extends RoutedAlias
cloud,Enclosing class: CollApiCmds protected static class CollApiCmds.CommandMap extends Object Map CollectionParams.CollectionAction to instances of CollApiCmds.CollectionApiCommand and being usable by both OverseerCollectionMessageHandler and DistributedCollectionConfigSetCommandRunner so that the mappings do not have to be maintained in two places.
cloud,"public class CollApiCmds extends Object This class contains ""smaller"" Collection API commands implementation, the interface implemented by all commands and the class mapping a collection action to the actual command. Previously the ""smaller"" command implementations in OverseerCollectionMessageHandler were relying on methods implementing the functional interface."
cloud,public class CollectionApiLockFactory extends Object This class implements a higher level locking abstraction for the Collection API using lower level read and write locks.
cloud,public class CollectionHandlingUtils extends Object This class contains helper methods used by commands of the Collection API. Previously these methods were in OverseerCollectionMessageHandler and were refactored out to (eventually) allow Collection API commands to be executed outside the context of the Overseer.
cloud,Enclosing class: CollectionHandlingUtils public static class CollectionHandlingUtils.ShardRequestTracker extends Object
cloud,"This assumes use of the incremental backup format, and not the (now deprecated) traditional 'full-snapshot' format. The deletion can either delete a specific BackupId, delete everything except the most recent N backup points, or can be used to trigger a ""garbage collection"" of unused index files in the backup repository."
cloud,public class DimensionalRoutedAlias extends RoutedAlias
cloud,"public class DistributedCollectionConfigSetCommandRunner extends Object Class for execution Collection API and Config Set API commands in a distributed way, without going through Overseer and OverseerCollectionMessageHandler or OverseerConfigSetMessageHandler. This class is only called when Collection and Config Set API calls are configured to be distributed, which implies cluster state updates are distributed as well."
cloud,"This class is only called when Collection and Config Set API calls are configured to be distributed, which implies cluster state updates are distributed as well."
cloud,"Largely this overseer processing consists of ensuring that read-only mode is enabled for the specified collection, identifying the core hosting the shard leader, and sending it a core- admin 'install' request."
cloud,A lot of the content that was in this class got moved to CollectionHandlingUtils and CollApiCmds. The equivalent of this class for distributed Collection API command execution is DistributedCollectionConfigSetCommandRunner.
cloud,The equivalent of this class for distributed Collection API command execution is DistributedCollectionConfigSetCommandRunner.
cloud,"leader: ID of the current overseer leader node overseer_queue_size: count of entries in the /overseer/queue Zookeeper queue/directory overseer_work_queue_size: count of entries in the /overseer/queue-work Zookeeper queue/directory overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,"overseer_queue_size: count of entries in the /overseer/queue Zookeeper queue/directory overseer_work_queue_size: count of entries in the /overseer/queue-work Zookeeper queue/directory overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,"overseer_work_queue_size: count of entries in the /overseer/queue-work Zookeeper queue/directory overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,"overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,"overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,"am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper)"
cloud,"configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper)"
cloud,"Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper)"
cloud,update_state (when Overseer cluster state updater persists changes in Zookeeper)
cloud,requests: success count of the given operation errors: error count of the operation More metrics (see below)
cloud,errors: error count of the operation More metrics (see below)
cloud,More metrics (see below)
cloud,"collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String))
cloud,Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String))
cloud,"requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below)"
cloud,"errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below)"
cloud,"recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below)"
cloud,More metrics (see below)
cloud,overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work
cloud,offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peekTopN_wait_forever poll remove remove_event take
cloud,poll remove remove_event take
cloud,remove remove_event take
cloud,remove_event take
cloud,overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work
cloud,collection_queue: same as above but for queue /overseer/collection-queue-work
cloud,avgRequestsPerSecond 5minRateRequestsPerSecond 15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,5minRateRequestsPerSecond 15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,99thPcRequestTime 999thPcRequestTime
cloud,"More fundamentally, when the Collection API command execution is distributed, this specific command is not being run on the Overseer anyway (but then not much is running on the overseer as cluster state updates are distributed as well) so Overseer stats and status can't be returned and actually do not even make sense. Zookeeper based queue metrics do not make sense either because Zookeeper queues are then not used. The Stats instance returned by CollectionCommandContext.getOverseerStats() when running in the Overseer is created in Overseer.start() and passed to the cluster state updater from where it is also propagated to the various Zookeeper queues to register various events. This class is the only place where it is used in the Collection API implementation, and only to return results. TODO: create a new command returning node specific Collection API/Config set API/cluster state updates stats such as success and failures? The structure of the returned results is as follows: leader: ID of the current overseer leader node overseer_queue_size: count of entries in the /overseer/queue Zookeeper queue/directory overseer_work_queue_size: count of entries in the /overseer/queue-work Zookeeper queue/directory overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work Maps returned as values of keys in overseer_operations, collection_operations, overseer_queue, overseer_internal_queue and collection_queue include additional stats. These stats are provided by MetricUtils, and represent metrics on each type of operation execution (be it failed or successful), see calls to Stats.time(String). The metric keys are: avgRequestsPerSecond 5minRateRequestsPerSecond 15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime"
cloud,"The Stats instance returned by CollectionCommandContext.getOverseerStats() when running in the Overseer is created in Overseer.start() and passed to the cluster state updater from where it is also propagated to the various Zookeeper queues to register various events. This class is the only place where it is used in the Collection API implementation, and only to return results. TODO: create a new command returning node specific Collection API/Config set API/cluster state updates stats such as success and failures? The structure of the returned results is as follows: leader: ID of the current overseer leader node overseer_queue_size: count of entries in the /overseer/queue Zookeeper queue/directory overseer_work_queue_size: count of entries in the /overseer/queue-work Zookeeper queue/directory overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work Maps returned as values of keys in overseer_operations, collection_operations, overseer_queue, overseer_internal_queue and collection_queue include additional stats. These stats are provided by MetricUtils, and represent metrics on each type of operation execution (be it failed or successful), see calls to Stats.time(String). The metric keys are: avgRequestsPerSecond 5minRateRequestsPerSecond 15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime"
cloud,"TODO: create a new command returning node specific Collection API/Config set API/cluster state updates stats such as success and failures? The structure of the returned results is as follows: leader: ID of the current overseer leader node overseer_queue_size: count of entries in the /overseer/queue Zookeeper queue/directory overseer_work_queue_size: count of entries in the /overseer/queue-work Zookeeper queue/directory overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work Maps returned as values of keys in overseer_operations, collection_operations, overseer_queue, overseer_internal_queue and collection_queue include additional stats. These stats are provided by MetricUtils, and represent metrics on each type of operation execution (be it failed or successful), see calls to Stats.time(String). The metric keys are: avgRequestsPerSecond 5minRateRequestsPerSecond 15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime"
cloud,"The structure of the returned results is as follows: leader: ID of the current overseer leader node overseer_queue_size: count of entries in the /overseer/queue Zookeeper queue/directory overseer_work_queue_size: count of entries in the /overseer/queue-work Zookeeper queue/directory overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work Maps returned as values of keys in overseer_operations, collection_operations, overseer_queue, overseer_internal_queue and collection_queue include additional stats. These stats are provided by MetricUtils, and represent metrics on each type of operation execution (be it failed or successful), see calls to Stats.time(String). The metric keys are: avgRequestsPerSecond 5minRateRequestsPerSecond 15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime"
cloud,"leader: ID of the current overseer leader node overseer_queue_size: count of entries in the /overseer/queue Zookeeper queue/directory overseer_work_queue_size: count of entries in the /overseer/queue-work Zookeeper queue/directory overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,"overseer_queue_size: count of entries in the /overseer/queue Zookeeper queue/directory overseer_work_queue_size: count of entries in the /overseer/queue-work Zookeeper queue/directory overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,"overseer_work_queue_size: count of entries in the /overseer/queue-work Zookeeper queue/directory overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,"overseer_collection_queue_size: count of entries in the /overseer/collection-queue-work Zookeeper queue/directory overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,"overseer_operations: map (of maps) of success and error counts for operations. The operations (keys) tracked in this map are: am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation More metrics (see below) collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,"am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper)"
cloud,"configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper)"
cloud,"Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper)"
cloud,update_state (when Overseer cluster state updater persists changes in Zookeeper)
cloud,requests: success count of the given operation errors: error count of the operation More metrics (see below)
cloud,errors: error count of the operation More metrics (see below)
cloud,More metrics (see below)
cloud,"collection_operations: map (of maps) of success and error counts for collection related operations. The operations(keys) tracked in this map are all operations that start with collection_, but the collection_ prefix is stripped of the returned value. Possible keys are therefore: am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String)) For each key, the value is a map composed of: requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below) overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work"
cloud,am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String))
cloud,Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String))
cloud,"requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below)"
cloud,"errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below)"
cloud,"recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below)"
cloud,More metrics (see below)
cloud,overseer_queue: metrics on operations done on the Zookeeper queue /overseer/queue (see metrics below). The operations that can be done on the queue and that can be keys whose values are a metrics map are: offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work
cloud,offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peekTopN_wait_forever poll remove remove_event take
cloud,poll remove remove_event take
cloud,remove remove_event take
cloud,remove_event take
cloud,overseer_internal_queue: same as above but for queue /overseer/queue-work collection_queue: same as above but for queue /overseer/collection-queue-work
cloud,collection_queue: same as above but for queue /overseer/collection-queue-work
cloud,"am_i_leader (Overseer checking it is still the elected Overseer as it processes cluster state update messages) configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper)"
cloud,"configset_<config set operation> Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper)"
cloud,"Cluster state change operation names from CollectionParams.CollectionAction (not all of them!) and OverseerAction (the complete list: create, delete, createshard, deleteshard, addreplica, addreplicaprop, deletereplicaprop, balanceshardunique, modifycollection, state, leader, deletecore, addroutingrule, removeroutingrule, updateshardstate, downnode and quit with this last one unlikely to be observed since the Overseer is exiting right away) update_state (when Overseer cluster state updater persists changes in Zookeeper)"
cloud,update_state (when Overseer cluster state updater persists changes in Zookeeper)
cloud,requests: success count of the given operation errors: error count of the operation More metrics (see below)
cloud,errors: error count of the operation More metrics (see below)
cloud,More metrics (see below)
cloud,am_i_leader: originating in a stat called collection_am_i_leader representing Overseer checking it is still the elected Overseer as it processes Collection API and Config Set API messages. Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String))
cloud,Collection API operation names from CollectionParams.CollectionAction (the stripped collection_ prefix gets added in OverseerCollectionMessageHandler.getTimerName(String))
cloud,"requests: success count of the given operation errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below)"
cloud,"errors: error count of the operation recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below)"
cloud,"recent_failures: an optional entry containing a list of maps, each map having two entries, one with key request with a failed request properties (a ZkNodeProps) and the other with key response with the corresponding response properties (a SolrResponse). More metrics (see below)"
cloud,More metrics (see below)
cloud,offer peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peek peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peek_wait peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peek_wait_forever peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peekTopN_wait peekTopN_wait_forever poll remove remove_event take
cloud,peekTopN_wait_forever poll remove remove_event take
cloud,poll remove remove_event take
cloud,remove remove_event take
cloud,remove_event take
cloud,"Maps returned as values of keys in overseer_operations, collection_operations, overseer_queue, overseer_internal_queue and collection_queue include additional stats. These stats are provided by MetricUtils, and represent metrics on each type of operation execution (be it failed or successful), see calls to Stats.time(String). The metric keys are: avgRequestsPerSecond 5minRateRequestsPerSecond 15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime"
cloud,avgRequestsPerSecond 5minRateRequestsPerSecond 15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,5minRateRequestsPerSecond 15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,15minRateRequestsPerSecond avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,avgTimePerRequest medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,medianRequestTime 75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,75thPcRequestTime 95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,95thPcRequestTime 99thPcRequestTime 999thPcRequestTime
cloud,99thPcRequestTime 999thPcRequestTime
cloud,"creates a temporary collection using the most recent schema of the source collection (or the one specified in the parameters, which must already exist), and the shape of the original collection, unless overridden by parameters. copy the source documents to the temporary collection, using their stored fields and reindexing them using the specified schema. NOTE: some data loss may occur if the original stored field data is not available! create the target collection from scratch with the specified name (or the same as source if not specified) and the specified parameters. NOTE: if the target name was not specified or is the same as the source collection then a unique sequential collection name will be used. copy the documents from the source collection to the target collection. if the source and target collection name was the same then set up an alias pointing from the source collection name to the actual (sequentially named) target collection optionally delete the source collection."
cloud,"copy the source documents to the temporary collection, using their stored fields and reindexing them using the specified schema. NOTE: some data loss may occur if the original stored field data is not available! create the target collection from scratch with the specified name (or the same as source if not specified) and the specified parameters. NOTE: if the target name was not specified or is the same as the source collection then a unique sequential collection name will be used. copy the documents from the source collection to the target collection. if the source and target collection name was the same then set up an alias pointing from the source collection name to the actual (sequentially named) target collection optionally delete the source collection."
cloud,create the target collection from scratch with the specified name (or the same as source if not specified) and the specified parameters. NOTE: if the target name was not specified or is the same as the source collection then a unique sequential collection name will be used. copy the documents from the source collection to the target collection. if the source and target collection name was the same then set up an alias pointing from the source collection name to the actual (sequentially named) target collection optionally delete the source collection.
cloud,copy the documents from the source collection to the target collection. if the source and target collection name was the same then set up an alias pointing from the source collection name to the actual (sequentially named) target collection optionally delete the source collection.
cloud,if the source and target collection name was the same then set up an alias pointing from the source collection name to the actual (sequentially named) target collection optionally delete the source collection.
cloud,optionally delete the source collection.
cloud,"WARNING: Reindexing is potentially a lossy operation - some indexed data that is not available as stored fields may be irretrievably lost, so users should use this command with caution, evaluating the potential impact by using different source and target collection names first, and preserving the source collection until the evaluation is complete. Reindexing follows these steps: creates a temporary collection using the most recent schema of the source collection (or the one specified in the parameters, which must already exist), and the shape of the original collection, unless overridden by parameters. copy the source documents to the temporary collection, using their stored fields and reindexing them using the specified schema. NOTE: some data loss may occur if the original stored field data is not available! create the target collection from scratch with the specified name (or the same as source if not specified) and the specified parameters. NOTE: if the target name was not specified or is the same as the source collection then a unique sequential collection name will be used. copy the documents from the source collection to the target collection. if the source and target collection name was the same then set up an alias pointing from the source collection name to the actual (sequentially named) target collection optionally delete the source collection."
cloud,"Reindexing follows these steps: creates a temporary collection using the most recent schema of the source collection (or the one specified in the parameters, which must already exist), and the shape of the original collection, unless overridden by parameters. copy the source documents to the temporary collection, using their stored fields and reindexing them using the specified schema. NOTE: some data loss may occur if the original stored field data is not available! create the target collection from scratch with the specified name (or the same as source if not specified) and the specified parameters. NOTE: if the target name was not specified or is the same as the source collection then a unique sequential collection name will be used. copy the documents from the source collection to the target collection. if the source and target collection name was the same then set up an alias pointing from the source collection name to the actual (sequentially named) target collection optionally delete the source collection."
cloud,public class ReplicaMigrationUtils extends Object
cloud,Enclosing class: RoutedAlias protected static class RoutedAlias.Action extends Object
cloud,"Direct Known Subclasses: CategoryRoutedAlias, DimensionalRoutedAlias, TimeRoutedAlias public abstract class RoutedAlias extends Object"
cloud,"public class TimeRoutedAlias extends RoutedAlias Holds configuration for a routed alias, and some common code and constants. See Also: CreateAliasCmd, MaintainRoutedAliasCmd, RoutedAliasUpdateProcessor"
cloud,public class ClusterStateMutator extends Object
cloud,public class CollectionMutator extends Object
cloud,public class NodeMutator extends Object
cloud,public class ReplicaMutator extends Object
cloud,public class SliceMutator extends Object
cloud,"public class ZkStateWriter extends Object ZkStateWriter is responsible for writing updates to the cluster state stored in ZooKeeper for collections each of which gets their own individual state.json in ZK. Updates to the cluster state are specified using the enqueueUpdate(ClusterState, List, ZkWriteCallback) method. The class buffers updates to reduce the number of writes to ZK. The buffered updates are flushed during enqueueUpdate automatically if necessary. The writePendingUpdates() can be used to force flush any pending updates. If either enqueueUpdate(ClusterState, List, ZkWriteCallback) or writePendingUpdates() throws a KeeperException.BadVersionException then the internal buffered state of the class is suspect and the current instance of the class should be discarded and a new instance should be created and used for any future updates."
cloud,"Updates to the cluster state are specified using the enqueueUpdate(ClusterState, List, ZkWriteCallback) method. The class buffers updates to reduce the number of writes to ZK. The buffered updates are flushed during enqueueUpdate automatically if necessary. The writePendingUpdates() can be used to force flush any pending updates. If either enqueueUpdate(ClusterState, List, ZkWriteCallback) or writePendingUpdates() throws a KeeperException.BadVersionException then the internal buffered state of the class is suspect and the current instance of the class should be discarded and a new instance should be created and used for any future updates."
cloud,"If either enqueueUpdate(ClusterState, List, ZkWriteCallback) or writePendingUpdates() throws a KeeperException.BadVersionException then the internal buffered state of the class is suspect and the current instance of the class should be discarded and a new instance should be created and used for any future updates."
cloud,public class ZkWriteCommand extends Object
cluster,"The notion of waitFor delay between detection and repair action is implemented as a scheduled execution of the repair method, which is called every 1 sec to check whether there are any lost nodes that exceeded their waitFor period. NOTE: this functionality would be probably more reliable when executed also as a periodically scheduled check - both as a reactive (listener) and proactive (scheduled) measure."
cluster,NOTE: this functionality would be probably more reliable when executed also as a periodically scheduled check - both as a reactive (listener) and proactive (scheduled) measure.
cluster,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
cluster,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
cluster,public class CollectionMetricsBuilder extends Object Builder class for constructing instances of CollectionMetrics.
cluster,Enclosing class: CollectionMetricsBuilder public static class CollectionMetricsBuilder.ReplicaMetricsBuilder extends Object
cluster,Enclosing class: CollectionMetricsBuilder public static class CollectionMetricsBuilder.ShardMetricsBuilder extends Object
cluster,public class ModificationRequestImpl extends Object Helper class to create modification request instances.
cluster,public class PlacementPluginFactoryLoader extends Object Utility class to work with PlacementPluginFactory plugins.
cluster,"In order to delete the placement-plugin section (and to fallback to either Legacy or rule based placement if configured for a collection), execute: curl -X POST -H 'Content-type:application/json' -d '{ ""remove"" : "".placement-plugin"" }' http://localhost:8983/api/cluster/plugin AffinityPlacementFactory.AffinityPlacementPlugin implements placing replicas in a way that replicate past Autoscaling config defined here. This specification is doing the following: Spread replicas per shard as evenly as possible across multiple availability zones (given by a sys prop), assign replicas based on replica type to specific kinds of nodes (another sys prop), and avoid having more than one replica per shard on the same node. Only after these constraints are satisfied do minimize cores per node or disk usage. This plugin achieves this by creating a AffinityPlacementFactory.AffinityPlacementPlugin.AffinityNode that weights nodes very high if they are unbalanced with respect to AvailabilityZone and SpreadDomain. See AffinityPlacementFactory.AffinityPlacementPlugin.AffinityNode for more information on how this weighting helps the plugin correctly place and balance replicas. This code is a realistic placement computation, based on a few assumptions. The code is written in such a way to make it relatively easy to adapt it to (somewhat) different assumptions. Additional configuration options could be introduced to allow configuration base option selection as well..."
cluster,"AffinityPlacementFactory.AffinityPlacementPlugin implements placing replicas in a way that replicate past Autoscaling config defined here. This specification is doing the following: Spread replicas per shard as evenly as possible across multiple availability zones (given by a sys prop), assign replicas based on replica type to specific kinds of nodes (another sys prop), and avoid having more than one replica per shard on the same node. Only after these constraints are satisfied do minimize cores per node or disk usage. This plugin achieves this by creating a AffinityPlacementFactory.AffinityPlacementPlugin.AffinityNode that weights nodes very high if they are unbalanced with respect to AvailabilityZone and SpreadDomain. See AffinityPlacementFactory.AffinityPlacementPlugin.AffinityNode for more information on how this weighting helps the plugin correctly place and balance replicas. This code is a realistic placement computation, based on a few assumptions. The code is written in such a way to make it relatively easy to adapt it to (somewhat) different assumptions. Additional configuration options could be introduced to allow configuration base option selection as well..."
cluster,"This specification is doing the following: Spread replicas per shard as evenly as possible across multiple availability zones (given by a sys prop), assign replicas based on replica type to specific kinds of nodes (another sys prop), and avoid having more than one replica per shard on the same node. Only after these constraints are satisfied do minimize cores per node or disk usage. This plugin achieves this by creating a AffinityPlacementFactory.AffinityPlacementPlugin.AffinityNode that weights nodes very high if they are unbalanced with respect to AvailabilityZone and SpreadDomain. See AffinityPlacementFactory.AffinityPlacementPlugin.AffinityNode for more information on how this weighting helps the plugin correctly place and balance replicas. This code is a realistic placement computation, based on a few assumptions. The code is written in such a way to make it relatively easy to adapt it to (somewhat) different assumptions. Additional configuration options could be introduced to allow configuration base option selection as well..."
cluster,"Spread replicas per shard as evenly as possible across multiple availability zones (given by a sys prop), assign replicas based on replica type to specific kinds of nodes (another sys prop), and avoid having more than one replica per shard on the same node. Only after these constraints are satisfied do minimize cores per node or disk usage. This plugin achieves this by creating a AffinityPlacementFactory.AffinityPlacementPlugin.AffinityNode that weights nodes very high if they are unbalanced with respect to AvailabilityZone and SpreadDomain. See AffinityPlacementFactory.AffinityPlacementPlugin.AffinityNode for more information on how this weighting helps the plugin correctly place and balance replicas. This code is a realistic placement computation, based on a few assumptions. The code is written in such a way to make it relatively easy to adapt it to (somewhat) different assumptions. Additional configuration options could be introduced to allow configuration base option selection as well..."
cluster,"This plugin achieves this by creating a AffinityPlacementFactory.AffinityPlacementPlugin.AffinityNode that weights nodes very high if they are unbalanced with respect to AvailabilityZone and SpreadDomain. See AffinityPlacementFactory.AffinityPlacementPlugin.AffinityNode for more information on how this weighting helps the plugin correctly place and balance replicas. This code is a realistic placement computation, based on a few assumptions. The code is written in such a way to make it relatively easy to adapt it to (somewhat) different assumptions. Additional configuration options could be introduced to allow configuration base option selection as well..."
cluster,"This code is a realistic placement computation, based on a few assumptions. The code is written in such a way to make it relatively easy to adapt it to (somewhat) different assumptions. Additional configuration options could be introduced to allow configuration base option selection as well..."
cluster,See MinimizeCoresPlacementFactory.NodeWithCoreCount for information on how this PlacementFactory weights nodes. See AffinityPlacementFactory for a more realistic example and documentation.
cluster,See AffinityPlacementFactory for a more realistic example and documentation.
cluster,"The OrderedNodePlacementPlugin uses the weights determined here to place and balance replicas across the cluster. Replicas will be placed onto WeightedNodes with lower weights, and be taken off of WeightedNodes with higher weights."
cluster,See RandomPlacementFactory.RandomNode for information on how this PlacementFactory weights nodes. See AffinityPlacementFactory for a more realistic example and documentation.
cluster,See AffinityPlacementFactory for a more realistic example and documentation.
cluster,See SimplePlacementFactory.SameCollWeightedNode for information on how this PlacementFactory weights nodes. See AffinityPlacementFactory for a more realistic example and documentation.
cluster,See AffinityPlacementFactory for a more realistic example and documentation.
core,Enclosing class: BlobRepository public static class BlobRepository.BlobContent<T> extends Object
core,Enclosing class: BlobRepository public static class BlobRepository.BlobContentRef<T> extends Object
core,public class BlobRepository extends Object The purpose of this class is to store the Jars loaded in memory and to keep only one copy of the Jar in a single node.
core,Enclosing class: CachingDirectoryFactory protected static class CachingDirectoryFactory.CacheValue extends Object
core,This is an expert class and these API's are subject to change.
core,public class CancellableQueryTracker extends Object Tracks metadata for active queries and provides methods for access
core,Enclosing class: CloudConfig public static class CloudConfig.CloudConfigBuilder extends Object
core,public class CloudConfig extends Object
core,public class ClusterSingletons extends Object Helper class to manage the initial registration of ClusterSingleton plugins and to track the changes in loaded plugins in ContainerPluginsRegistry.
core,public class ConfigSet extends Object Stores a core's configuration in the form of a SolrConfig and IndexSchema. Immutable. See Also: ConfigSetService
core,public class ConfigSetProperties extends Object Utility methods for reading configSet properties. One purpose of this notion is to express immutability. The contents are not used within the config itself; do not confuse this with config user-defined properties. The properties are stored as a JSON file within the configSet that we read into a NamedList. It's optional; there is no file if there are no properties. Note that this logic is also used to load configSet flags; see ConfigSetService.
core,"Direct Known Subclasses: FileSystemConfigSetService, ZkConfigSetService public abstract class ConfigSetService extends Object Service class used by the CoreContainer to load ConfigSets for use in SolrCore creation."
core,Enclosing class: CoreContainer public static class CoreContainer.CoreLoadFailure extends Object
core,public class CoreContainer extends Object Since: solr 1.3
core,public class CoreDescriptor extends Object Metadata about a SolrCore. It's mostly loaded from a file on disk at the very beginning of loading a core. It's mostly but not completely immutable; we should fix this! Since: solr 1.3
core,It's mostly but not completely immutable; we should fix this!
core,public class CoreSorter extends Object This is a utility class that sorts cores in such a way as to minimize other cores waiting for replicas in the current node. This helps in avoiding leaderVote timeouts happening in other nodes of the cluster
core,public class Diagnostics extends Object
core,"public class FileSystemConfigSetService extends ConfigSetService FileSystem ConfigSetService impl. Loads a ConfigSet defined by the core's configSet property, looking for a directory named for the configSet property value underneath a base directory. If no configSet property is set, loads the ConfigSet instead from the core's instance directory."
core,"Loads a ConfigSet defined by the core's configSet property, looking for a directory named for the configSet property value underneath a base directory. If no configSet property is set, loads the ConfigSet instead from the core's instance directory."
core,public final class IndexDeletionPolicyWrapper extends org.apache.lucene.index.IndexDeletionPolicy A wrapper for an IndexDeletionPolicy instance. Provides features for looking up IndexCommit given a version. Allows reserving index commit points for certain amounts of time to support features such as index replication or snapshotting directly out of a live index directory. NOTE: The Object.clone() method returns this in order to make this IndexDeletionPolicy instance trackable across IndexWriter instantiations. This is correct because each core has its own IndexDeletionPolicy and never has more than one open IndexWriter. See Also: IndexDeletionPolicy
core,Provides features for looking up IndexCommit given a version. Allows reserving index commit points for certain amounts of time to support features such as index replication or snapshotting directly out of a live index directory. NOTE: The Object.clone() method returns this in order to make this IndexDeletionPolicy instance trackable across IndexWriter instantiations. This is correct because each core has its own IndexDeletionPolicy and never has more than one open IndexWriter.
core,NOTE: The Object.clone() method returns this in order to make this IndexDeletionPolicy instance trackable across IndexWriter instantiations. This is correct because each core has its own IndexDeletionPolicy and never has more than one open IndexWriter.
core,public class InitParams extends Object An Object which represents a <initParams> tag
core,Enclosing class: MetricsConfig public static class MetricsConfig.CacheConfig extends Object
core,public class MetricsConfig extends Object
core,Enclosing class: MetricsConfig public static class MetricsConfig.MetricsConfigBuilder extends Object
core,"unmap -- See MMapDirectory.setUseUnmap(boolean) preload -- See MMapDirectory.setPreload(boolean) maxChunkSize -- The Max chunk size. See MMapDirectory(Path, LockFactory, long)"
core,"preload -- See MMapDirectory.setPreload(boolean) maxChunkSize -- The Max chunk size. See MMapDirectory(Path, LockFactory, long)"
core,"maxChunkSize -- The Max chunk size. See MMapDirectory(Path, LockFactory, long)"
core,"Can set the following parameters: unmap -- See MMapDirectory.setUseUnmap(boolean) preload -- See MMapDirectory.setPreload(boolean) maxChunkSize -- The Max chunk size. See MMapDirectory(Path, LockFactory, long)"
core,"unmap -- See MMapDirectory.setUseUnmap(boolean) preload -- See MMapDirectory.setPreload(boolean) maxChunkSize -- The Max chunk size. See MMapDirectory(Path, LockFactory, long)"
core,"preload -- See MMapDirectory.setPreload(boolean) maxChunkSize -- The Max chunk size. See MMapDirectory(Path, LockFactory, long)"
core,"maxChunkSize -- The Max chunk size. See MMapDirectory(Path, LockFactory, long)"
core,public class NodeConfig extends Object
core,Enclosing class: NodeConfig public static class NodeConfig.NodeConfigBuilder extends Object
core,public class NodeRoles extends Object
core,"java.util.HashMap<Class<? extends JerseyResource>,RequestHandlerBase>"
core,"java.util.HashMap<Class<? extends JerseyResource>,RequestHandlerBase>"
core,"java.util.HashMap<Class<? extends JerseyResource>,RequestHandlerBase>"
core,"Used primarily by JAX-RS when recording per-request metrics, which requires a RequestHandlerBase.HandlerMetrics object from the relevant requestHandler."
core,"Nested classes/interfaces inherited from classjava.util.AbstractMap AbstractMap.SimpleEntry<K extends Object,V extends Object>, AbstractMap.SimpleImmutableEntry<K extends Object,V extends Object>"
core,"Nested classes/interfaces inherited from interfacejava.util.Map Map.Entry<K extends Object,V extends Object>"
core,"Methods inherited from classjava.util.HashMap clear, clone, compute, computeIfAbsent, computeIfPresent, containsKey, containsValue, entrySet, forEach, get, getOrDefault, isEmpty, keySet, merge, put, putAll, putIfAbsent, remove, remove, replace, replace, replaceAll, size, values"
core,"Methods inherited from classjava.util.AbstractMap equals, hashCode, toString"
core,"Methods inherited from interfacejava.util.Map equals, hashCode"
core,"Nested classes/interfaces inherited from classjava.util.AbstractMap AbstractMap.SimpleEntry<K extends Object,V extends Object>, AbstractMap.SimpleImmutableEntry<K extends Object,V extends Object>"
core,"Nested classes/interfaces inherited from interfacejava.util.Map Map.Entry<K extends Object,V extends Object>"
core,Enclosing class: PluginInfo public static class PluginInfo.ClassName extends Object
core,public class RateLimiterConfig extends Object
core,public final class RequestHandlers extends Object
core,"Methods inherited from interfacejava.lang.Iterable forEach, spliterator"
core,Enclosing class: SolrConfig public static class SolrConfig.SolrPluginInfo extends Object
core,Enclosing class: SolrCore public static class SolrCore.Provider extends Object Provides the core instance if the core instance is still alive. This helps to not hold on to a live SolrCore instance even after it's unloaded
core,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
core,Direct Known Subclasses: TransientSolrCores public class SolrCores extends Object AKA CoreManager: Holds/manages SolrCores within CoreContainer.
core,"Enclosing class: SolrPaths public static class SolrPaths.AllowPathBuilder extends Object Builds a set of allowed Path. Detects special paths ""*"" and ""_ALL_"" that mean all paths are allowed."
core,public final class SolrPaths extends Object Utility methods about paths in Solr.
core,Fields inherited from classjava.io.FilterInputStream in
core,"Methods inherited from classjava.io.FilterInputStream available, close, mark, markSupported, read, read, read, reset, skip"
core,"Methods inherited from classjava.io.InputStream nullInputStream, readAllBytes, readNBytes, readNBytes, transferTo"
core,Fields inherited from classjava.io.FilterInputStream in
core,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
core,public class SolrXmlConfig extends Object Loads solr.xml.
core,File based DirectoryFactory implementations generally extend this class.
core,This is only used in Coordinator node to support a subset of SolrCore functionalities required by Coordinator operations such as aggregating and writing out response and providing configset info. There should only be one instance of SyntheticSolrCore per configset
core,There should only be one instance of SyntheticSolrCore per configset
core,"Direct Known Subclasses: TransientSolrCoreCacheDefault @Deprecated(since=""9.2"") public abstract class TransientSolrCoreCache extends Object Deprecated. The base class for custom transient core maintenance. Any custom plugin that wants to take control of transient caches (i.e. any core defined with transient=true) should override this class. WARNING: There is quite a bit of higher-level locking done by the CoreContainer to avoid various race conditions etc. You should _only_ manipulate them within the method calls designed to change them. E.g. only add to the transient core descriptors in addTransientDescriptor etc. Trust the higher-level code (mainly SolrCores and CoreContainer) to call the appropriate operations when necessary and to coordinate shutting down cores, manipulating the internal structures and the like. The only real action you should _initiate_ is to close a core for whatever reason, and do that by calling notifyCoreCloseListener(coreToClose); The observer will call back to removeCore(name) at the appropriate time. There is no need to directly remove the core _at that time_ from the transientCores list, a call will come back to this class when CoreContainer is closing this core. CoreDescriptors are read-once. During ""core discovery"" all valid descriptors are enumerated and added to the appropriate list. Thereafter, they are NOT re-read from disk. In those situations where you want to re-define the coreDescriptor, maintain a ""side list"" of changed core descriptors. Then override getTransientDescriptor to return your new core descriptor. NOTE: assuming you've already closed the core, the _next_ time that core is required getTransientDescriptor will be called and if you return the new core descriptor your re-definition should be honored. You'll have to maintain this list for the duration of this Solr instance running. If you persist the coreDescriptor, then next time Solr starts up the new definition will be read. If you need to manipulate the return, for instance block a core from being loaded for some period of time, override say getTransientDescriptor and return null. In particular, DO NOT reach into the transientCores structure from a method called to manipulate core descriptors or vice-versa."
core,"WARNING: There is quite a bit of higher-level locking done by the CoreContainer to avoid various race conditions etc. You should _only_ manipulate them within the method calls designed to change them. E.g. only add to the transient core descriptors in addTransientDescriptor etc. Trust the higher-level code (mainly SolrCores and CoreContainer) to call the appropriate operations when necessary and to coordinate shutting down cores, manipulating the internal structures and the like. The only real action you should _initiate_ is to close a core for whatever reason, and do that by calling notifyCoreCloseListener(coreToClose); The observer will call back to removeCore(name) at the appropriate time. There is no need to directly remove the core _at that time_ from the transientCores list, a call will come back to this class when CoreContainer is closing this core. CoreDescriptors are read-once. During ""core discovery"" all valid descriptors are enumerated and added to the appropriate list. Thereafter, they are NOT re-read from disk. In those situations where you want to re-define the coreDescriptor, maintain a ""side list"" of changed core descriptors. Then override getTransientDescriptor to return your new core descriptor. NOTE: assuming you've already closed the core, the _next_ time that core is required getTransientDescriptor will be called and if you return the new core descriptor your re-definition should be honored. You'll have to maintain this list for the duration of this Solr instance running. If you persist the coreDescriptor, then next time Solr starts up the new definition will be read. If you need to manipulate the return, for instance block a core from being loaded for some period of time, override say getTransientDescriptor and return null. In particular, DO NOT reach into the transientCores structure from a method called to manipulate core descriptors or vice-versa."
core,"Trust the higher-level code (mainly SolrCores and CoreContainer) to call the appropriate operations when necessary and to coordinate shutting down cores, manipulating the internal structures and the like. The only real action you should _initiate_ is to close a core for whatever reason, and do that by calling notifyCoreCloseListener(coreToClose); The observer will call back to removeCore(name) at the appropriate time. There is no need to directly remove the core _at that time_ from the transientCores list, a call will come back to this class when CoreContainer is closing this core. CoreDescriptors are read-once. During ""core discovery"" all valid descriptors are enumerated and added to the appropriate list. Thereafter, they are NOT re-read from disk. In those situations where you want to re-define the coreDescriptor, maintain a ""side list"" of changed core descriptors. Then override getTransientDescriptor to return your new core descriptor. NOTE: assuming you've already closed the core, the _next_ time that core is required getTransientDescriptor will be called and if you return the new core descriptor your re-definition should be honored. You'll have to maintain this list for the duration of this Solr instance running. If you persist the coreDescriptor, then next time Solr starts up the new definition will be read. If you need to manipulate the return, for instance block a core from being loaded for some period of time, override say getTransientDescriptor and return null. In particular, DO NOT reach into the transientCores structure from a method called to manipulate core descriptors or vice-versa."
core,"The only real action you should _initiate_ is to close a core for whatever reason, and do that by calling notifyCoreCloseListener(coreToClose); The observer will call back to removeCore(name) at the appropriate time. There is no need to directly remove the core _at that time_ from the transientCores list, a call will come back to this class when CoreContainer is closing this core. CoreDescriptors are read-once. During ""core discovery"" all valid descriptors are enumerated and added to the appropriate list. Thereafter, they are NOT re-read from disk. In those situations where you want to re-define the coreDescriptor, maintain a ""side list"" of changed core descriptors. Then override getTransientDescriptor to return your new core descriptor. NOTE: assuming you've already closed the core, the _next_ time that core is required getTransientDescriptor will be called and if you return the new core descriptor your re-definition should be honored. You'll have to maintain this list for the duration of this Solr instance running. If you persist the coreDescriptor, then next time Solr starts up the new definition will be read. If you need to manipulate the return, for instance block a core from being loaded for some period of time, override say getTransientDescriptor and return null. In particular, DO NOT reach into the transientCores structure from a method called to manipulate core descriptors or vice-versa."
core,"CoreDescriptors are read-once. During ""core discovery"" all valid descriptors are enumerated and added to the appropriate list. Thereafter, they are NOT re-read from disk. In those situations where you want to re-define the coreDescriptor, maintain a ""side list"" of changed core descriptors. Then override getTransientDescriptor to return your new core descriptor. NOTE: assuming you've already closed the core, the _next_ time that core is required getTransientDescriptor will be called and if you return the new core descriptor your re-definition should be honored. You'll have to maintain this list for the duration of this Solr instance running. If you persist the coreDescriptor, then next time Solr starts up the new definition will be read. If you need to manipulate the return, for instance block a core from being loaded for some period of time, override say getTransientDescriptor and return null. In particular, DO NOT reach into the transientCores structure from a method called to manipulate core descriptors or vice-versa."
core,"If you need to manipulate the return, for instance block a core from being loaded for some period of time, override say getTransientDescriptor and return null. In particular, DO NOT reach into the transientCores structure from a method called to manipulate core descriptors or vice-versa."
core,"In particular, DO NOT reach into the transientCores structure from a method called to manipulate core descriptors or vice-versa."
core,"@Deprecated(since=""9.2"") public class TransientSolrCoreCacheDefault extends TransientSolrCoreCache Deprecated. Cache of the most frequently accessed transient cores. Keeps track of all the registered transient cores descriptors, including the cores in the cache as well as all the others."
core,"@Deprecated(since=""9.2"") public class TransientSolrCores extends SolrCores Deprecated. A SolrCores that supports CoreDescriptor.isTransient()."
core,public class XmlConfigFile extends Object Wrapper around an XML DOM object to provide convenient accessors to it. Intended for XML config files.
core,"public class ZkContainer extends Object Used by CoreContainer to hold ZooKeeper / SolrCloud info, especially ZkController. Mainly it does some ZK initialization, and ensures a loading core registers in ZK. Even when in standalone mode, perhaps surprisingly, an instance of this class exists. If getZkController() returns null then we're in standalone mode."
core,"public class AggregateBackupStats extends Object Aggregate stats from multiple ShardBackupMetadata Counted stats may represent multiple shards within a given BackupId, or span multiple different BackupIds."
core,"Counted stats may represent multiple shards within a given BackupId, or span multiple different BackupIds."
core,"public class BackupFilePaths extends Object Utility class for getting paths related to backups, or parsing information out of those paths."
core,"Backup IDs are used to track different backup points stored at the same backup-location under the same backup-name. Incremental backups can have any non-negative integer as an ID, and ID's are expected to increase sequentially. Traditional (now-deprecated) 'full-snapshot' backups only support a single backup point per name per location. So these all have the same ID value of TRADITIONAL_BACKUP"
core,"Incremental backups can have any non-negative integer as an ID, and ID's are expected to increase sequentially. Traditional (now-deprecated) 'full-snapshot' backups only support a single backup point per name per location. So these all have the same ID value of TRADITIONAL_BACKUP"
core,Traditional (now-deprecated) 'full-snapshot' backups only support a single backup point per name per location. So these all have the same ID value of TRADITIONAL_BACKUP
core,public class BackupManager extends Object This class implements functionality to create a backup with extension points provided to integrate with different types of file-systems.
core,"public class BackupProperties extends Object Represents a backup[-*].properties file, responsible for holding whole-collection and whole-backup metadata. These files live in a different location and hold different metadata depending on the backup format used. The (now deprecated) traditional 'full-snapshot' backup format places this file at $LOCATION/$NAME, while the preferred incremental backup format stores these files in $LOCATION/$NAME/$COLLECTION."
core,"These files live in a different location and hold different metadata depending on the backup format used. The (now deprecated) traditional 'full-snapshot' backup format places this file at $LOCATION/$NAME, while the preferred incremental backup format stores these files in $LOCATION/$NAME/$COLLECTION."
core,public class Checksum extends Object Represents checksum information for an index file being backed up.
core,"public class ShardBackupId extends Object Represents the ID of a particular backup point for a particular shard. ShardBackupId's only need be unique within a given collection and backup location/name, so in practice they're formed by combining the shard name with the BackupId in the form: ""md_$SHARDNAME_BACKUPID"". ShardBackupId's are most often used as a filename to store shard-level metadata for the backup. See ShardBackupMetadata for more information. See Also: ShardBackupMetadata"
core,"ShardBackupId's only need be unique within a given collection and backup location/name, so in practice they're formed by combining the shard name with the BackupId in the form: ""md_$SHARDNAME_BACKUPID"". ShardBackupId's are most often used as a filename to store shard-level metadata for the backup. See ShardBackupMetadata for more information."
core,ShardBackupId's are most often used as a filename to store shard-level metadata for the backup. See ShardBackupMetadata for more information.
core,Enclosing class: ShardBackupMetadata public static class ShardBackupMetadata.BackedFile extends Object
core,"public class ShardBackupMetadata extends Object Represents the shard-backup metadata file. The shard-backup metadata file is responsible for holding information about a specific backup-point for a specific shard. This includes the full list of index files required to restore this shard to the backup-point, with pointers to where each lives in the repository. Shard backup metadata files have names derived from an associated ShardBackupId, to avoid conflicts between shards and backupIds. Not used by the (now deprecated) traditional 'full-snapshot' backup format."
core,"The shard-backup metadata file is responsible for holding information about a specific backup-point for a specific shard. This includes the full list of index files required to restore this shard to the backup-point, with pointers to where each lives in the repository. Shard backup metadata files have names derived from an associated ShardBackupId, to avoid conflicts between shards and backupIds. Not used by the (now deprecated) traditional 'full-snapshot' backup format."
core,"Shard backup metadata files have names derived from an associated ShardBackupId, to avoid conflicts between shards and backupIds. Not used by the (now deprecated) traditional 'full-snapshot' backup format."
core,Not used by the (now deprecated) traditional 'full-snapshot' backup format.
core,Methods inherited from interfacejava.io.Closeable close
core,public class BackupRepositoryFactory extends Object
core,public class SolrSnapshotManager extends Object This class provides functionality required to handle the data files corresponding to Solr snapshots.
core,public class SolrSnapshotMetaDataManager extends Object This class is responsible to manage the persistent snapshots meta-data for the Solr indexes. The persistent snapshots are implemented by relying on Lucene IndexDeletionPolicy abstraction to configure a specific IndexCommit to be retained. The IndexDeletionPolicyWrapper in Solr uses this class to create/delete the Solr index snapshots.
core,Enclosing class: SolrSnapshotMetaDataManager public static class SolrSnapshotMetaDataManager.SnapshotMetaData extends Object A class defining the meta-data for a specific snapshot.
filestore,Enclosing interface: FileStore public static class FileStore.FileEntry extends Object
filestore,public class FileStoreAPI extends Object
filestore,public class FileStoreUtils extends Object Common utilities used by filestore-related code.
handler,Enclosing class: AnalysisRequestHandlerBase protected static class AnalysisRequestHandlerBase.AnalysisContext extends Object Serves as the context of an analysis process. This context contains the following constructs
handler,Nested classes/interfaces inherited from classorg.apache.lucene.util.AttributeSource org.apache.lucene.util.AttributeSource.State
handler,Fields inherited from classorg.apache.lucene.analysis.TokenStream DEFAULT_TOKEN_ATTRIBUTE_FACTORY
handler,"Methods inherited from classorg.apache.lucene.analysis.TokenStream close, end"
handler,"Methods inherited from classorg.apache.lucene.util.AttributeSource addAttribute, addAttributeImpl, captureState, clearAttributes, cloneAttributes, copyTo, endAttributes, equals, getAttribute, getAttributeClassesIterator, getAttributeFactory, getAttributeImplsIterator, hasAttribute, hasAttributes, hashCode, reflectAsString, reflectWith, removeAllAttributes, restoreState, toString"
handler,Nested classes/interfaces inherited from classorg.apache.lucene.util.AttributeSource org.apache.lucene.util.AttributeSource.State
handler,"Methods inherited from classorg.apache.lucene.util.AttributeImpl clone, end, reflectAsString"
handler,Enclosing class: CatStream public static class CatStream.CrawlFile extends Object
handler,Enclosing class: ClusterAPI public class ClusterAPI.Commands extends Object
handler,public class ClusterAPI extends Object All V2 APIs that have a prefix of /api/cluster/
handler,"Like the FieldAnalysisRequestHandler, this handler also supports query analysis by sending either an ""analysis.query"" or ""q"" request parameter that holds the query text to be analyzed. It also supports the ""analysis.showmatch"" parameter which when set to true, all field tokens that match the query tokens will be marked as a ""match""."
handler,"When present, the text will be analyzed based on the type of this field name."
handler,"Yes, this parameter may hold a comma-separated list of values and the analysis will be performed for each of the specified fields"
handler,"When present, the text will be analyzed based on the specified type"
handler,"Yes, this parameter may hold a comma-separated list of values and the analysis will be performed for each of the specified field types"
handler,The text that will be analyzed. The analysis will mimic the index-time analysis.
handler,analysis.query OR q
handler,"When present, the text that will be analyzed. The analysis will mimic the query-time analysis. Note that the analysis.query parameter as precedes the q parameters."
handler,"When set to true and when query analysis is performed, the produced tokens of the field value analysis will be marked as ""matched"" for every token that is produces by the query analysis"
handler,"Note that if neither analysis.fieldname and analysis.fieldtype is specified, then the default search field's analyzer is used. Note that if one of analysis.value or analysis.query or q must be specified"
handler,Note that if one of analysis.value or analysis.query or q must be specified
handler,"public class IncrementalShardBackup extends Object Responsible for orchestrating the actual incremental backup process. If this is the first backup for a collection, all files are uploaded. But if previous backups exist, uses the most recent ShardBackupMetadata file to determine which files already exist in the repository and can be skipped."
handler,"If this is the first backup for a collection, all files are uploaded. But if previous backups exist, uses the most recent ShardBackupMetadata file to determine which files already exist in the repository and can be skipped."
handler,Enclosing class: IndexFetcher protected static class IndexFetcher.CompareResult extends Object
handler,Enclosing class: IndexFetcher protected class IndexFetcher.DirectoryFileFetcher extends Object
handler,Enclosing class: IndexFetcher public static class IndexFetcher.IndexFetchResult extends Object
handler,public class IndexFetcher extends Object Provides functionality of downloading changed index files as well as config files and a timer for scheduling fetches from the leader. Since: solr 1.4
handler,Enclosing class: IndexFetcher protected class IndexFetcher.LocalFsFileFetcher extends Object
handler,Enclosing class: MoreLikeThisHandler public static class MoreLikeThisHandler.InterestingTerm extends Object
handler,Return similar documents either based on a single document or based on posted text.
handler,Enclosing class: MoreLikeThisHandler @NotThreadSafe public static class MoreLikeThisHandler.MoreLikeThisHelper extends Object Helper class for MoreLikeThis that can be called from other request handlers
handler,"If the health check file exists, the handler will execute the delegated query and return status as described above. If the health check file does not exist, the handler will return an HTTP error even if the server is working fine and the delegated query would have succeeded"
handler,"If the health check file does not exist, the handler will return an HTTP error even if the server is working fine and the delegated query would have succeeded"
handler,"http://.../ping?action=enable - creates the health check file if it does not already exist http://.../ping?action=disable - deletes the health check file if it exists http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"http://.../ping?action=disable - deletes the health check file if it exists http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"This handler is designed to be used as the endpoint for an HTTP Load-Balancer to use when checking the ""health"" or ""up status"" of a Solr server. In its simplest form, the PingRequestHandler should be configured with some defaults indicating a request that should be executed. If the request succeeds, then the PingRequestHandler will respond back with a simple ""OK"" status. If the request fails, then the PingRequestHandler will respond back with the corresponding HTTP Error code. Clients (such as load balancers) can be configured to poll the PingRequestHandler monitoring for these types of responses (or for a simple connection failure) to know if there is a problem with the Solr server. Note in case isShard=true, PingRequestHandler respond back with what the delegated handler returns (by default it's /select handler). <requestHandler name=""/admin/ping"" class=""solr.PingRequestHandler""> <lst name=""invariants""> <str name=""qt"">/search</str><!-- handler to delegate to --> <str name=""q"">some test query</str> </lst> </requestHandler> A more advanced option available, is to configure the handler with a ""healthcheckFile"" which can be used to enable/disable the PingRequestHandler. <requestHandler name=""/admin/ping"" class=""solr.PingRequestHandler""> <!-- relative paths are resolved against the data dir --> <str name=""healthcheckFile"">server-enabled.txt</str> <lst name=""invariants""> <str name=""qt"">/search</str><!-- handler to delegate to --> <str name=""q"">some test query</str> </lst> </requestHandler> If the health check file exists, the handler will execute the delegated query and return status as described above. If the health check file does not exist, the handler will return an HTTP error even if the server is working fine and the delegated query would have succeeded This health check file feature can be used as a way to indicate to some Load Balancers that the server should be ""removed from rotation"" for maintenance, or upgrades, or whatever reason you may wish. The health check file may be created/deleted by any external system, or the PingRequestHandler itself can be used to create/delete the file by specifying an ""action"" param in a request: http://.../ping?action=enable - creates the health check file if it does not already exist http://.../ping?action=disable - deletes the health check file if it exists http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"In its simplest form, the PingRequestHandler should be configured with some defaults indicating a request that should be executed. If the request succeeds, then the PingRequestHandler will respond back with a simple ""OK"" status. If the request fails, then the PingRequestHandler will respond back with the corresponding HTTP Error code. Clients (such as load balancers) can be configured to poll the PingRequestHandler monitoring for these types of responses (or for a simple connection failure) to know if there is a problem with the Solr server. Note in case isShard=true, PingRequestHandler respond back with what the delegated handler returns (by default it's /select handler). <requestHandler name=""/admin/ping"" class=""solr.PingRequestHandler""> <lst name=""invariants""> <str name=""qt"">/search</str><!-- handler to delegate to --> <str name=""q"">some test query</str> </lst> </requestHandler> A more advanced option available, is to configure the handler with a ""healthcheckFile"" which can be used to enable/disable the PingRequestHandler. <requestHandler name=""/admin/ping"" class=""solr.PingRequestHandler""> <!-- relative paths are resolved against the data dir --> <str name=""healthcheckFile"">server-enabled.txt</str> <lst name=""invariants""> <str name=""qt"">/search</str><!-- handler to delegate to --> <str name=""q"">some test query</str> </lst> </requestHandler> If the health check file exists, the handler will execute the delegated query and return status as described above. If the health check file does not exist, the handler will return an HTTP error even if the server is working fine and the delegated query would have succeeded This health check file feature can be used as a way to indicate to some Load Balancers that the server should be ""removed from rotation"" for maintenance, or upgrades, or whatever reason you may wish. The health check file may be created/deleted by any external system, or the PingRequestHandler itself can be used to create/delete the file by specifying an ""action"" param in a request: http://.../ping?action=enable - creates the health check file if it does not already exist http://.../ping?action=disable - deletes the health check file if it exists http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"Note in case isShard=true, PingRequestHandler respond back with what the delegated handler returns (by default it's /select handler). <requestHandler name=""/admin/ping"" class=""solr.PingRequestHandler""> <lst name=""invariants""> <str name=""qt"">/search</str><!-- handler to delegate to --> <str name=""q"">some test query</str> </lst> </requestHandler> A more advanced option available, is to configure the handler with a ""healthcheckFile"" which can be used to enable/disable the PingRequestHandler. <requestHandler name=""/admin/ping"" class=""solr.PingRequestHandler""> <!-- relative paths are resolved against the data dir --> <str name=""healthcheckFile"">server-enabled.txt</str> <lst name=""invariants""> <str name=""qt"">/search</str><!-- handler to delegate to --> <str name=""q"">some test query</str> </lst> </requestHandler> If the health check file exists, the handler will execute the delegated query and return status as described above. If the health check file does not exist, the handler will return an HTTP error even if the server is working fine and the delegated query would have succeeded This health check file feature can be used as a way to indicate to some Load Balancers that the server should be ""removed from rotation"" for maintenance, or upgrades, or whatever reason you may wish. The health check file may be created/deleted by any external system, or the PingRequestHandler itself can be used to create/delete the file by specifying an ""action"" param in a request: http://.../ping?action=enable - creates the health check file if it does not already exist http://.../ping?action=disable - deletes the health check file if it exists http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"A more advanced option available, is to configure the handler with a ""healthcheckFile"" which can be used to enable/disable the PingRequestHandler. <requestHandler name=""/admin/ping"" class=""solr.PingRequestHandler""> <!-- relative paths are resolved against the data dir --> <str name=""healthcheckFile"">server-enabled.txt</str> <lst name=""invariants""> <str name=""qt"">/search</str><!-- handler to delegate to --> <str name=""q"">some test query</str> </lst> </requestHandler> If the health check file exists, the handler will execute the delegated query and return status as described above. If the health check file does not exist, the handler will return an HTTP error even if the server is working fine and the delegated query would have succeeded This health check file feature can be used as a way to indicate to some Load Balancers that the server should be ""removed from rotation"" for maintenance, or upgrades, or whatever reason you may wish. The health check file may be created/deleted by any external system, or the PingRequestHandler itself can be used to create/delete the file by specifying an ""action"" param in a request: http://.../ping?action=enable - creates the health check file if it does not already exist http://.../ping?action=disable - deletes the health check file if it exists http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"If the health check file exists, the handler will execute the delegated query and return status as described above. If the health check file does not exist, the handler will return an HTTP error even if the server is working fine and the delegated query would have succeeded"
handler,"If the health check file does not exist, the handler will return an HTTP error even if the server is working fine and the delegated query would have succeeded"
handler,"This health check file feature can be used as a way to indicate to some Load Balancers that the server should be ""removed from rotation"" for maintenance, or upgrades, or whatever reason you may wish. The health check file may be created/deleted by any external system, or the PingRequestHandler itself can be used to create/delete the file by specifying an ""action"" param in a request: http://.../ping?action=enable - creates the health check file if it does not already exist http://.../ping?action=disable - deletes the health check file if it exists http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"The health check file may be created/deleted by any external system, or the PingRequestHandler itself can be used to create/delete the file by specifying an ""action"" param in a request: http://.../ping?action=enable - creates the health check file if it does not already exist http://.../ping?action=disable - deletes the health check file if it exists http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"http://.../ping?action=enable - creates the health check file if it does not already exist http://.../ping?action=disable - deletes the health check file if it exists http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"http://.../ping?action=disable - deletes the health check file if it exists http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"http://.../ping?action=status - returns a status code indicating if the healthcheck file exists (""enabled"") or not (""disabled"")"
handler,"Get the current replicable index version (command=indexversion) Get the list of files for a given index version (command=filelist&indexversion=<VERSION>) Get full or a part (chunk) of a given index or a config file (command=filecontent&file=<FILE_NAME>) You can optionally specify an offset and length to get that chunk of the file. You can request a configuration file by using ""cf"" parameter instead of the ""file"" parameter. Get status/statistics (command=details)"
handler,"Get the list of files for a given index version (command=filelist&indexversion=<VERSION>) Get full or a part (chunk) of a given index or a config file (command=filecontent&file=<FILE_NAME>) You can optionally specify an offset and length to get that chunk of the file. You can request a configuration file by using ""cf"" parameter instead of the ""file"" parameter. Get status/statistics (command=details)"
handler,"Get full or a part (chunk) of a given index or a config file (command=filecontent&file=<FILE_NAME>) You can optionally specify an offset and length to get that chunk of the file. You can request a configuration file by using ""cf"" parameter instead of the ""file"" parameter. Get status/statistics (command=details)"
handler,Get status/statistics (command=details)
handler,Perform an index fetch now (command=snappull) Get status/statistics (command=details) Abort an index fetch (command=abort) Enable/Disable polling the leader for new versions (command=enablepoll or command=disablepoll)
handler,Get status/statistics (command=details) Abort an index fetch (command=abort) Enable/Disable polling the leader for new versions (command=enablepoll or command=disablepoll)
handler,Abort an index fetch (command=abort) Enable/Disable polling the leader for new versions (command=enablepoll or command=disablepoll)
handler,Enable/Disable polling the leader for new versions (command=enablepoll or command=disablepoll)
handler,"When running on the leader, it provides the following commands Get the current replicable index version (command=indexversion) Get the list of files for a given index version (command=filelist&indexversion=<VERSION>) Get full or a part (chunk) of a given index or a config file (command=filecontent&file=<FILE_NAME>) You can optionally specify an offset and length to get that chunk of the file. You can request a configuration file by using ""cf"" parameter instead of the ""file"" parameter. Get status/statistics (command=details) When running on the follower, it provides the following commands Perform an index fetch now (command=snappull) Get status/statistics (command=details) Abort an index fetch (command=abort) Enable/Disable polling the leader for new versions (command=enablepoll or command=disablepoll)"
handler,"When running on the follower, it provides the following commands Perform an index fetch now (command=snappull) Get status/statistics (command=details) Abort an index fetch (command=abort) Enable/Disable polling the leader for new versions (command=enablepoll or command=disablepoll)"
handler,Enclosing class: RequestHandlerBase public static class RequestHandlerBase.HandlerMetrics extends Object Metrics for this handler.
handler,public class RequestHandlerUtils extends Object Common helper functions for RequestHandlers Since: solr 1.2
handler,public class SnapShooter extends Object Provides functionality equivalent to the snapshooter script This is no longer used in standard replication. Since: solr 1.4
handler,"At core-load time, Solr looks at each 'plugin' in ImplicitPlugins.json, fetches the v2 Api implementations associated with each RequestHandler, and registers them in an ApiBag. Since UpdateRequestHandler is mentioned multiple times in ImplicitPlugins.json (once for each update API: /update, /update/json, etc.), this would cause the v2 APIs to be registered in duplicate. To avoid this, Solr has this RequestHandler, whose only purpose is to register the v2 APIs that conceptually should be associated with UpdateRequestHandler."
handler,public class AdminHandlersProxy extends Object Static methods to proxy calls to an Admin (GET) API to other nodes in the cluster and return a combined response
handler,public class ClusterStatus extends Object
handler,public class ColStatus extends Object Report low-level details of collection.
handler,Enclosing class: ContainerPluginsApi public class ContainerPluginsApi.Edit extends Object API for editing the plugin configurations.
handler,public class ContainerPluginsApi extends Object API to maintain container-level plugin configurations.
handler,Enclosing class: ContainerPluginsApi public class ContainerPluginsApi.Read extends Object API for reading the current plugin configurations.
handler,Enclosing class: CoreAdminHandler public static class CoreAdminHandler.CallInfo extends Object
handler,Enclosing class: CoreAdminHandler public static class CoreAdminHandler.CoreAdminAsyncTracker extends Object
handler,"Enclosing class: CoreAdminHandler.CoreAdminAsyncTracker public static class CoreAdminHandler.CoreAdminAsyncTracker.TaskObject extends Object Helper class to manage the tasks to be tracked. This contains the taskId, request and the response (if available)."
handler,Cores container is active. Node connected to zookeeper. Node listed in live_nodes in zookeeper.
handler,Node connected to zookeeper. Node listed in live_nodes in zookeeper.
handler,Node listed in live_nodes in zookeeper.
handler,"For the Solr Cloud mode by default the handler returns status 200 OK if all checks succeed, else it returns status 503 UNAVAILABLE: Cores container is active. Node connected to zookeeper. Node listed in live_nodes in zookeeper. The handler takes an optional request parameter requireHealthyCores=true which will also require that all local cores that are part of an active shard are done initializing, i.e. not in states RECOVERING or DOWN. This parameter is designed to help during rolling restarts, to make sure each node is fully initialized and stable before proceeding with restarting the next node, and thus reduce the risk of restarting the last live replica of a shard. For the legacy mode the handler returns status 200 OK if all the cores configured as follower have successfully replicated index from their respective leader after startup. Note that this is a weak check i.e. once a follower has caught up with the leader the health check will keep reporting 200 OK even if the follower starts lagging behind. You should specify the acceptable generation lag follower should be with respect to its leader using the maxGenerationLag=<max_generation_lag> request parameter. If maxGenerationLag is not provided then health check would simply return OK."
handler,"The handler takes an optional request parameter requireHealthyCores=true which will also require that all local cores that are part of an active shard are done initializing, i.e. not in states RECOVERING or DOWN. This parameter is designed to help during rolling restarts, to make sure each node is fully initialized and stable before proceeding with restarting the next node, and thus reduce the risk of restarting the last live replica of a shard. For the legacy mode the handler returns status 200 OK if all the cores configured as follower have successfully replicated index from their respective leader after startup. Note that this is a weak check i.e. once a follower has caught up with the leader the health check will keep reporting 200 OK even if the follower starts lagging behind. You should specify the acceptable generation lag follower should be with respect to its leader using the maxGenerationLag=<max_generation_lag> request parameter. If maxGenerationLag is not provided then health check would simply return OK."
handler,For the legacy mode the handler returns status 200 OK if all the cores configured as follower have successfully replicated index from their respective leader after startup. Note that this is a weak check i.e. once a follower has caught up with the leader the health check will keep reporting 200 OK even if the follower starts lagging behind. You should specify the acceptable generation lag follower should be with respect to its leader using the maxGenerationLag=<max_generation_lag> request parameter. If maxGenerationLag is not provided then health check would simply return OK.
handler,Enclosing class: IndexSizeEstimator public static class IndexSizeEstimator.Item extends Object
handler,"Methods inherited from classorg.apache.lucene.util.PriorityQueue add, addAll, clear, getHeapArray, insertWithOverflow, iterator, pop, remove, size, top, updateTop, updateTop"
handler,"Methods inherited from interfacejava.lang.Iterable forEach, spliterator"
handler,"public class IndexSizeEstimator extends Object Estimates the raw size of all uncompressed indexed data by scanning term, docValues and stored fields data. This utility also provides detailed statistics about term, docValues, postings and stored fields distributions."
handler,"Methods inherited from classorg.apache.commons.math3.stat.descriptive.SummaryStatistics addValue, clear, copy, copy, equals, getGeoMeanImpl, getGeometricMean, getMax, getMaxImpl, getMean, getMeanImpl, getMin, getMinImpl, getN, getPopulationVariance, getQuadraticMean, getSecondMoment, getStandardDeviation, getSum, getSumImpl, getSumLogImpl, getSummary, getSumOfLogs, getSumsq, getSumsqImpl, getVariance, getVarianceImpl, hashCode, setGeoMeanImpl, setMaxImpl, setMeanImpl, setMinImpl, setSumImpl, setSumLogImpl, setSumsqImpl, setVarianceImpl, toString"
handler,Converts v1-style query parameters into a v2-style request body and delegating to InstallCoreData.
handler,"It is inspired by and modeled on Luke, the Lucene Index Browser that is currently a Lucene module: https://github.com/apache/lucene/tree/main/lucene/luke"
handler,SolrReporter.GROUP_ID - (required) specifies target registry name where metrics will be grouped. SolrReporter.REPORTER_ID - (required) id of the reporter that sent this update. This can be eg. node name or replica name or other id that uniquely identifies the source of metrics values. MetricUtils.METRIC_NAME - (required) metric name (in the source registry) SolrReporter.LABEL_ID - (optional) label to prepend to metric names in the target registry. SolrReporter.REGISTRY_ID - (optional) name of the source registry.
handler,SolrReporter.REPORTER_ID - (required) id of the reporter that sent this update. This can be eg. node name or replica name or other id that uniquely identifies the source of metrics values. MetricUtils.METRIC_NAME - (required) metric name (in the source registry) SolrReporter.LABEL_ID - (optional) label to prepend to metric names in the target registry. SolrReporter.REGISTRY_ID - (optional) name of the source registry.
handler,MetricUtils.METRIC_NAME - (required) metric name (in the source registry) SolrReporter.LABEL_ID - (optional) label to prepend to metric names in the target registry. SolrReporter.REGISTRY_ID - (optional) name of the source registry.
handler,SolrReporter.LABEL_ID - (optional) label to prepend to metric names in the target registry. SolrReporter.REGISTRY_ID - (optional) name of the source registry.
handler,SolrReporter.REGISTRY_ID - (optional) name of the source registry.
handler,"Each report consists of SolrInputDocument-s that are expected to contain the following fields: SolrReporter.GROUP_ID - (required) specifies target registry name where metrics will be grouped. SolrReporter.REPORTER_ID - (required) id of the reporter that sent this update. This can be eg. node name or replica name or other id that uniquely identifies the source of metrics values. MetricUtils.METRIC_NAME - (required) metric name (in the source registry) SolrReporter.LABEL_ID - (optional) label to prepend to metric names in the target registry. SolrReporter.REGISTRY_ID - (optional) name of the source registry. Remaining fields are assumed to be single-valued, and to contain metric attributes and their values. Example: <doc> <field name=""_group_"">solr.core.collection1.shard1.leader</field> <field name=""_reporter_"">core_node3</field> <field name=""metric"">INDEX.merge.errors</field> <field name=""value"">0</field> </doc>"
handler,SolrReporter.GROUP_ID - (required) specifies target registry name where metrics will be grouped. SolrReporter.REPORTER_ID - (required) id of the reporter that sent this update. This can be eg. node name or replica name or other id that uniquely identifies the source of metrics values. MetricUtils.METRIC_NAME - (required) metric name (in the source registry) SolrReporter.LABEL_ID - (optional) label to prepend to metric names in the target registry. SolrReporter.REGISTRY_ID - (optional) name of the source registry.
handler,SolrReporter.REPORTER_ID - (required) id of the reporter that sent this update. This can be eg. node name or replica name or other id that uniquely identifies the source of metrics values. MetricUtils.METRIC_NAME - (required) metric name (in the source registry) SolrReporter.LABEL_ID - (optional) label to prepend to metric names in the target registry. SolrReporter.REGISTRY_ID - (optional) name of the source registry.
handler,MetricUtils.METRIC_NAME - (required) metric name (in the source registry) SolrReporter.LABEL_ID - (optional) label to prepend to metric names in the target registry. SolrReporter.REGISTRY_ID - (optional) name of the source registry.
handler,SolrReporter.LABEL_ID - (optional) label to prepend to metric names in the target registry. SolrReporter.REGISTRY_ID - (optional) name of the source registry.
handler,SolrReporter.REGISTRY_ID - (optional) name of the source registry.
handler,"Enclosing class: SecurityConfHandler public static class SecurityConfHandler.SecurityConfig extends Object Object to hold security.json as nested Map<String,Object> and optionally its version. The version property is optional and defaults to -1 if not initialized. The data object defaults to EMPTY_MAP if not set"
handler,"The ShowFileRequestHandler uses the RawResponseWriter (wt=raw) to return file contents. If you need to use a different writer, you will need to change the registered invariant param for wt. If you want to override the contentType header returned for a given file, you can set it directly using: USE_CONTENT_TYPE. For example, to get a plain text version of schema.xml, try: http://localhost:8983/solr/admin/file?file=schema.xml&contentType=text/plain"
handler,"If you want to override the contentType header returned for a given file, you can set it directly using: USE_CONTENT_TYPE. For example, to get a plain text version of schema.xml, try: http://localhost:8983/solr/admin/file?file=schema.xml&contentType=text/plain"
handler,"public class SolrEnvironment extends Object It is possible to define an environment code when starting Solr, through -Dsolr.environment=prod|stage|test|dev or by setting the cluster property ""environment"". This class checks if any of these are defined, and parses the string, which may also contain custom overrides for environment name (label) and color to be shown in Admin UI"
handler,"This is an expert feature that exposes the data inside the back end zookeeper.This API may change or be removed in future versions. This is not a public API. The data that is returned is not guaranteed to remain same across releases, as the data stored in Zookeeper may change from time to time."
handler,This API is analogous to the v1 /admin/collections?action=ADDREPLICAPROP command.
handler,"Direct Known Subclasses: AddReplicaProperty, AliasProperty, BackupAPIBase, BalanceReplicas, BalanceShardUnique, ClusterProperty, CollectionProperty, CreateAlias, CreateCollection, CreateCollectionSnapshot, CreateReplica, CreateShard, DeleteAlias, DeleteCollection, DeleteCollectionSnapshot, DeleteNode, DeleteReplica, DeleteReplicaProperty, DeleteShard, ForceLeader, InstallShardData, ListAliases, ListCollections, ListCollectionSnapshots, MigrateReplicas, ReloadCollectionAPI, RenameCollection, ReplaceNode, SyncShard, ZookeeperRead public abstract class AdminAPIBase extends JerseyResource A common parent for ""admin"" (i.e. container-level) APIs."
handler,public class AllCoresStatusAPI extends Object V2 API for retrieving status information for all cores on the receiving node. This API (GET /v2/cores is analogous to the v1 /admin/cores?action=status command. See Also: SingleCoreStatusAPI
handler,This API (GET /v2/cores is analogous to the v1 /admin/cores?action=status command.
handler,"Direct Known Subclasses: CreateCollectionBackup, DeleteCollectionBackup, ListCollectionBackups, RestoreCollection public abstract class BackupAPIBase extends AdminAPIBase Base class that facilitates reuse of common validation logic for collection-backup APIs."
handler,"public class CancelTaskAPI extends Object V2 API for cancelling a currently running ""task"". This API (GET /v2/collections/collectionName/tasks/cancel) is analogous to the v1 /solr/collectionName/tasks/cancel API."
handler,This API (GET /v2/collections/collectionName/tasks/cancel) is analogous to the v1 /solr/collectionName/tasks/cancel API.
handler,These APIs (PUT and DELETE /api/collections/collName/properties/propName) are analogous to the v1 /admin/collections?action=COLLECTIONPROP command.
handler,public class CollectionStatusAPI extends Object V2 API for displaying basic information about a single collection. This API (GET /v2/collections/collectionName) is analogous to the v1 /admin/collections?action=CLUSTERSTATUS&collection=collectionName command.
handler,This API (GET /v2/collections/collectionName) is analogous to the v1 /admin/collections?action=CLUSTERSTATUS&collection=collectionName command.
handler,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
handler,"Direct Known Subclasses: CoreSnapshot, CreateCoreBackup, GetNodeCommandStatus, InstallCoreData, MergeIndexes, ReloadCore, RenameCore, RestoreCore, SwapCores, UnloadCore public abstract class CoreAdminAPIBase extends JerseyResource A common parent for admin Core Jersey-based APIs. This base class is used when creating Core APIs to allow extra bookkeeping tasks such as async requests handling."
handler,This base class is used when creating Core APIs to allow extra bookkeeping tasks such as async requests handling.
handler,These APIs are analogous to the v1 /coreName/replication APIs.
handler,Enclosing class: CreateAlias public static class CreateAlias.CategoryRoutedAliasValidationHelper extends Object
handler,This API is analogous to the v1 /admin/collections?action=CREATEALIAS command.
handler,Enclosing class: CreateAlias public static class CreateAlias.TimeRoutedAliasValidationHelper extends Object
handler,This API is analogous to the v1 /admin/collections?action=CREATE command.
handler,public class CreateCoreAPI extends Object V2 API for creating a new core on the receiving node. This API (POST /v2/cores {'create': {...}}) is analogous to the v1 /admin/cores?action=CREATE command. See Also: CreateCorePayload
handler,This API (POST /v2/cores {'create': {...}}) is analogous to the v1 /admin/cores?action=CREATE command.
handler,This API (POST /v2/collections/cName/shards/sName/replicas {...}) is analogous to the v1 /admin/collections?action=ADDREPLICA command.
handler,This API (POST /v2/collections/collectionName/shards {...}) is analogous to the v1 /admin/collections?action=CREATESHARD command.
handler,This API (DELETE /v2/collections/collectionName) is equivalent to the v1 /admin/collections?action=DELETE command.
handler,These APIs are equivalent to the v1 '/admin/collections?action=DELETEBACKUP' command.
handler,This API is analogous to the V1 /admin/collections?action=DELETENODE
handler,These APIs are analogous to the v1 /admin/collections?action=DELETEREPLICA command.
handler,This API (DELETE /v2/collections/collectionName/shards/shardName) is analogous to the v1 /admin/collections?action=DELETESHARD command.
handler,This API (POST /v2/collections/collectionName/shards/shardName/force-leader) is analogous to the v1 /admin/collections?action=FORCELEADER command.
handler,public class GetAuthenticationConfigAPI extends Object V2 API for fetching the authentication section from Solr's security.json configuration. This API (GET /v2/cluster/security/authentication) is analogous to the v1 `GET /solr/admin/authentication` API.
handler,This API (GET /v2/cluster/security/authentication) is analogous to the v1 `GET /solr/admin/authentication` API.
handler,public class GetAuthorizationConfigAPI extends Object V2 API for fetching the authorization section of Solr's security.json configuration. This API (GET /v2/cluster/security/authorization) is analogous to the v1 `GET /solr/admin/authorization` API.
handler,This API (GET /v2/cluster/security/authorization) is analogous to the v1 `GET /solr/admin/authorization` API.
handler,public class GetBlobInfoAPI extends Object V2 APIs for fetching blob(s) and their metadata These APIs (GET /v2/collections/.system/blob/*) is analogous to the v1 GET /solr/.system/blob/* APIs.
handler,These APIs (GET /v2/collections/.system/blob/*) is analogous to the v1 GET /solr/.system/blob/* APIs.
handler,"public class GetConfigAPI extends Object V2 APIs for retrieving some or all configuration relevant to a particular collection (or core). This class covers a handful of distinct paths under GET /v2/c/collectionName/config: /, /overlay, /params, /znodeVersion, etc."
handler,"This class covers a handful of distinct paths under GET /v2/c/collectionName/config: /, /overlay, /params, /znodeVersion, etc."
handler,/fields /fields/{fieldName} /copyfields /dynamicfields /dynamicfields/{fieldName} /fieldtypes /fieldtypes/{fieldTypeName}
handler,/fields/{fieldName} /copyfields /dynamicfields /dynamicfields/{fieldName} /fieldtypes /fieldtypes/{fieldTypeName}
handler,/copyfields /dynamicfields /dynamicfields/{fieldName} /fieldtypes /fieldtypes/{fieldTypeName}
handler,/dynamicfields /dynamicfields/{fieldName} /fieldtypes /fieldtypes/{fieldTypeName}
handler,/dynamicfields/{fieldName} /fieldtypes /fieldtypes/{fieldTypeName}
handler,/fieldtypes /fieldtypes/{fieldTypeName}
handler,/fields /fields/{fieldName} /copyfields /dynamicfields /dynamicfields/{fieldName} /fieldtypes /fieldtypes/{fieldTypeName}
handler,/fields/{fieldName} /copyfields /dynamicfields /dynamicfields/{fieldName} /fieldtypes /fieldtypes/{fieldTypeName}
handler,/copyfields /dynamicfields /dynamicfields/{fieldName} /fieldtypes /fieldtypes/{fieldTypeName}
handler,/dynamicfields /dynamicfields/{fieldName} /fieldtypes /fieldtypes/{fieldTypeName}
handler,/dynamicfields/{fieldName} /fieldtypes /fieldtypes/{fieldTypeName}
handler,/fieldtypes /fieldtypes/{fieldTypeName}
handler,"This is an internal API intended for use only by the Collection Admin ""Install Shard Data"" API."
handler,"Particularly useful for installing (per-shard) indices constructed offline into a SolrCloud deployment. Callers are required to put the collection into read-only mode prior to installing data into any shards of that collection, and should exit read only mode when completed."
handler,"public class ListActiveTasksAPI extends Object V2 API for listing any currently running ""tasks"". This API (GET /v2/collections/collectionName/tasks/list) is analogous to the v1 /solr/collectionName/tasks/list API."
handler,This API (GET /v2/collections/collectionName/tasks/list) is analogous to the v1 /solr/collectionName/tasks/list API.
handler,These APIs are equivalent to the v1 '/admin/collections?action=LISTBACKUP' command.
handler,This API (GET /v2/collections) is equivalent to the v1 /admin/collections?action=LIST command
handler,public class MigrateDocsAPI extends Object V2 API for migrating docs from one collection to another. The new API (POST /v2/collections/collectionName {'migrate-docs': {...}}) is analogous to the v1 /admin/collections?action=MIGRATE command. See Also: MigrateDocsPayload
handler,The new API (POST /v2/collections/collectionName {'migrate-docs': {...}}) is analogous to the v1 /admin/collections?action=MIGRATE command.
handler,public class ModifyBasicAuthConfigAPI extends Object V2 API to modify configuration for Solr's BasicAuthPlugin
handler,public class ModifyCollectionAPI extends Object V2 API for modifying collections. The new API (POST /v2/collections/collectionName {'modify-collection': {...}}) is equivalent to the v1 /admin/collections?action=MODIFYCOLLECTION command. See Also: ModifyCollectionPayload
handler,The new API (POST /v2/collections/collectionName {'modify-collection': {...}}) is equivalent to the v1 /admin/collections?action=MODIFYCOLLECTION command.
handler,"public class ModifyConfigComponentAPI extends Object V2 APIs for creating, updating, or deleting individual components in a collection configuration. This API (POST /v2/collections/collectionName/config {...}) is analogous to the commands supported by the v1 POST /collectionName/config API. Typically v2 ""POST"" API implementations separate each ""command"" into a different method. This is not done here because the v1 API code in SolrConfigHandler expects to consume the request body itself (meaning that nothing _before_ SolrConfigHandler can consume the request body). As a result the single method below handles all ""commands"" supported by the POST /config API: 'set-property, 'unset-property', 'add-requesthandler', 'update-requesthandler', 'delete-requesthandler', 'add-searchcomponent', 'update-searchcomponent', 'delete-searchcomponent', 'add-initparams', 'update-initparams', 'delete-initparams', 'add-queryresponsewriter', 'update-queryresponsewriter', 'delete-queryresponsewriter', 'add-queryparser', 'update-queryparser', 'delete-queryparser', 'add-valuesourceparser', 'update-valuesourceparser', 'delete-valuesourceparser', 'add-transformer', 'update-transformer', 'delete-transformer', 'add-updateprocessor', 'update-updateprocessor', 'delete-updateprocessor', 'add-queryconverter', 'update-queryconverter', 'delete-queryconverter', 'add-listener', 'update-listener', and 'delete-listener'."
handler,"This API (POST /v2/collections/collectionName/config {...}) is analogous to the commands supported by the v1 POST /collectionName/config API. Typically v2 ""POST"" API implementations separate each ""command"" into a different method. This is not done here because the v1 API code in SolrConfigHandler expects to consume the request body itself (meaning that nothing _before_ SolrConfigHandler can consume the request body). As a result the single method below handles all ""commands"" supported by the POST /config API: 'set-property, 'unset-property', 'add-requesthandler', 'update-requesthandler', 'delete-requesthandler', 'add-searchcomponent', 'update-searchcomponent', 'delete-searchcomponent', 'add-initparams', 'update-initparams', 'delete-initparams', 'add-queryresponsewriter', 'update-queryresponsewriter', 'delete-queryresponsewriter', 'add-queryparser', 'update-queryparser', 'delete-queryparser', 'add-valuesourceparser', 'update-valuesourceparser', 'delete-valuesourceparser', 'add-transformer', 'update-transformer', 'delete-transformer', 'add-updateprocessor', 'update-updateprocessor', 'delete-updateprocessor', 'add-queryconverter', 'update-queryconverter', 'delete-queryconverter', 'add-listener', 'update-listener', and 'delete-listener'."
handler,"Typically v2 ""POST"" API implementations separate each ""command"" into a different method. This is not done here because the v1 API code in SolrConfigHandler expects to consume the request body itself (meaning that nothing _before_ SolrConfigHandler can consume the request body). As a result the single method below handles all ""commands"" supported by the POST /config API: 'set-property, 'unset-property', 'add-requesthandler', 'update-requesthandler', 'delete-requesthandler', 'add-searchcomponent', 'update-searchcomponent', 'delete-searchcomponent', 'add-initparams', 'update-initparams', 'delete-initparams', 'add-queryresponsewriter', 'update-queryresponsewriter', 'delete-queryresponsewriter', 'add-queryparser', 'update-queryparser', 'delete-queryparser', 'add-valuesourceparser', 'update-valuesourceparser', 'delete-valuesourceparser', 'add-transformer', 'update-transformer', 'delete-transformer', 'add-updateprocessor', 'update-updateprocessor', 'delete-updateprocessor', 'add-queryconverter', 'update-queryconverter', 'delete-queryconverter', 'add-listener', 'update-listener', and 'delete-listener'."
handler,"As a result the single method below handles all ""commands"" supported by the POST /config API: 'set-property, 'unset-property', 'add-requesthandler', 'update-requesthandler', 'delete-requesthandler', 'add-searchcomponent', 'update-searchcomponent', 'delete-searchcomponent', 'add-initparams', 'update-initparams', 'delete-initparams', 'add-queryresponsewriter', 'update-queryresponsewriter', 'delete-queryresponsewriter', 'add-queryparser', 'update-queryparser', 'delete-queryparser', 'add-valuesourceparser', 'update-valuesourceparser', 'delete-valuesourceparser', 'add-transformer', 'update-transformer', 'delete-transformer', 'add-updateprocessor', 'update-updateprocessor', 'delete-updateprocessor', 'add-queryconverter', 'update-queryconverter', 'delete-queryconverter', 'add-listener', 'update-listener', and 'delete-listener'."
handler,public class ModifyMultiPluginAuthConfigAPI extends Object V2 API to modify configuration options for the MultiAuthPlugin.
handler,public class ModifyNoAuthPluginSecurityConfigAPI extends Object V2 API for POST requests received when no authentication plugin is active. Solr's security APIs only supports authc config modifications once an Authentication plugin is in place. So this API serves solely as a placeholder that allows SecurityConfHandler to return a helpful error message (instead of the opaque 404 that users would get without this API).
handler,Solr's security APIs only supports authc config modifications once an Authentication plugin is in place. So this API serves solely as a placeholder that allows SecurityConfHandler to return a helpful error message (instead of the opaque 404 that users would get without this API).
handler,public class ModifyNoAuthzPluginSecurityConfigAPI extends Object V2 API for POST requests received when no authorization plugin is active. Solr's security APIs only supports authz config modifications once an Authorization plugin is in place. So this API serves solely as a placeholder that allows SecurityConfHandler to return a helpful error message (instead of the opaque 404 that users would get without this API).
handler,Solr's security APIs only supports authz config modifications once an Authorization plugin is in place. So this API serves solely as a placeholder that allows SecurityConfHandler to return a helpful error message (instead of the opaque 404 that users would get without this API).
handler,"public class ModifyParamSetAPI extends Object V2 APIs for creating, modifying, or deleting paramsets. This API (POST /v2/collections/collectionName/config/params {...}) is analogous to the commands supported by the v1 /techproducts/config/params Typically v2 ""POST"" API implementations separate each ""command"" into a different method. This is not done here because the v1 API code in SolrConfigHandler expects to consume the request body itself (meaning that nothing _before_ SolrConfigHandler can consume the request body). As a result the single method below handles all three ""commands"" supported by the POST /config/params API: 'set', 'delete', and 'update'."
handler,"This API (POST /v2/collections/collectionName/config/params {...}) is analogous to the commands supported by the v1 /techproducts/config/params Typically v2 ""POST"" API implementations separate each ""command"" into a different method. This is not done here because the v1 API code in SolrConfigHandler expects to consume the request body itself (meaning that nothing _before_ SolrConfigHandler can consume the request body). As a result the single method below handles all three ""commands"" supported by the POST /config/params API: 'set', 'delete', and 'update'."
handler,"Typically v2 ""POST"" API implementations separate each ""command"" into a different method. This is not done here because the v1 API code in SolrConfigHandler expects to consume the request body itself (meaning that nothing _before_ SolrConfigHandler can consume the request body). As a result the single method below handles all three ""commands"" supported by the POST /config/params API: 'set', 'delete', and 'update'."
handler,"As a result the single method below handles all three ""commands"" supported by the POST /config/params API: 'set', 'delete', and 'update'."
handler,public class ModifyRuleBasedAuthConfigAPI extends Object V2 API to modify configuration options for the RuleBasedAuthorizationPlugin.
handler,public class MoreLikeThisAPI extends Object V2 API for performing a more like this request to a solr collection. This API (GET /v2/collections/collectionName/mlt) is analogous to the v1 /solr/collName/mlt API.
handler,This API (GET /v2/collections/collectionName/mlt) is analogous to the v1 /solr/collName/mlt API.
handler,public class MoveReplicaAPI extends Object V2 API for moving a collection replica to a different physical node. The new API (POST /v2/collections/collectionName {'move-replica': {...}}) is analogous to the v1 /admin/collections?action=MOVEREPLICA command. See Also: MoveReplicaPayload
handler,The new API (POST /v2/collections/collectionName {'move-replica': {...}}) is analogous to the v1 /admin/collections?action=MOVEREPLICA command.
handler,public class NodeHealthAPI extends Object V2 API for checking the health of the receiving node. This API (GET /v2/node/health) is analogous to the v1 /admin/info/health.
handler,This API (GET /v2/node/health) is analogous to the v1 /admin/info/health.
handler,These APIs ('/api/node/logging' and descendants) are analogous to the v1 /admin/info/logging.
handler,public class NodePropertiesAPI extends Object V2 API for listing system properties for each node. This API (GET /v2/node/properties) is analogous to the v1 /admin/info/properties.
handler,This API (GET /v2/node/properties) is analogous to the v1 /admin/info/properties.
handler,"public class NodeSystemInfoAPI extends Object V2 API for getting ""system"" information from the receiving node. This includes current resource utilization, information about the installation (location, version, etc.), and JVM settings. This API (GET /v2/node/system) is analogous to the v1 /admin/info/system."
handler,"This includes current resource utilization, information about the installation (location, version, etc.), and JVM settings. This API (GET /v2/node/system) is analogous to the v1 /admin/info/system."
handler,This API (GET /v2/node/system) is analogous to the v1 /admin/info/system.
handler,public class NodeThreadsAPI extends Object V2 API for triggering a thread dump on the receiving node. This API (GET /v2/node/threads) is analogous to the v1 /admin/info/threads.
handler,This API (GET /v2/node/threads) is analogous to the v1 /admin/info/threads.
handler,public class OverseerOperationAPI extends Object V2 API for triggering a node to rejoin leader election for the 'overseer' role. This API (POST /v2/node {'overseer-op': {...}}) is analogous to the v1 /admin/cores?action=overseerop command. See Also: OverseerOperationPayload
handler,This API (POST /v2/node {'overseer-op': {...}}) is analogous to the v1 /admin/cores?action=overseerop command.
handler,public class PrepareCoreRecoveryAPI extends Object Internal V2 API used to prepare a core for recovery. Only valid in SolrCloud mode. This API (POST /v2/cores/coreName {'prep-recovery': {...}}) is analogous to the v1 /admin/cores?action=PREPRECOVERY command. See Also: PrepareCoreRecoveryPayload
handler,Only valid in SolrCloud mode. This API (POST /v2/cores/coreName {'prep-recovery': {...}}) is analogous to the v1 /admin/cores?action=PREPRECOVERY command.
handler,public class RealTimeGetAPI extends Object V2 API for fetching the latest (possibly uncommitted) version of one or more documents. This API (GET /v2/collections/collectionName/get) is analogous to the v1 /solr/collectionName/get API.
handler,This API (GET /v2/collections/collectionName/get) is analogous to the v1 /solr/collectionName/get API.
handler,public class RebalanceLeadersAPI extends Object V2 API for balancing shard leaders in a collection across nodes. This API (POST /v2/collections/collectionName {'rebalance-leaders': {...}}) is analogous to the v1 /admin/collections?action=REBALANCELEADERS command. See Also: RebalanceLeadersPayload
handler,This API (POST /v2/collections/collectionName {'rebalance-leaders': {...}}) is analogous to the v1 /admin/collections?action=REBALANCELEADERS command.
handler,public class RejoinLeaderElectionAPI extends Object V2 API for triggering a core to rejoin leader election for the shard it constitutes. This API (POST /v2/node {'rejoin-leader-election': {...}}) is analogous to the v1 /admin/cores?action=REJOINLEADERELECTION command.
handler,This API (POST /v2/node {'rejoin-leader-election': {...}}) is analogous to the v1 /admin/cores?action=REJOINLEADERELECTION command.
handler,The new API (POST /v2/collections/collectionName/reload {...}) is analogous to the v1 /admin/collections?action=RELOAD command.
handler,The new API (POST /v2/cores/coreName/reload is analogous to the v1 /admin/cores?action=RELOAD command.
handler,This API is analogous to the v1 /admin/collections?action=RENAME command.
handler,The new API (POST /v2/cores/coreName/rename is equivalent to the v1 /admin/cores?action=rename command.
handler,public class RenameCoreAPI extends Object V2 API for renaming an existing Solr core. The new API (POST /v2/cores/coreName {'rename': {...}}) is equivalent to the v1 /admin/cores?action=rename command.
handler,The new API (POST /v2/cores/coreName {'rename': {...}}) is equivalent to the v1 /admin/cores?action=rename command.
handler,This API is analogous to the v1 /admin/collections?action=REPLACENODE command.
handler,"Direct Known Subclasses: CoreReplication public abstract class ReplicationAPIBase extends JerseyResource A common parent for ""replication"" (i.e. replication-level) APIs."
handler,public class RequestApplyCoreUpdatesAPI extends Object Internal V2 API used to apply updates to a core. Only valid in SolrCloud mode. This API (POST /v2/cores/coreName {'request-apply-updates': {}}) is analogous to the v1 /admin/cores?action=REQUESTAPPLYUPDATES command. See Also: RequestApplyCoreUpdatesPayload
handler,Only valid in SolrCloud mode. This API (POST /v2/cores/coreName {'request-apply-updates': {}}) is analogous to the v1 /admin/cores?action=REQUESTAPPLYUPDATES command.
handler,public class RequestBufferUpdatesAPI extends Object Internal V2 API used to start update-buffering on the specified core. Only valid in SolrCloud mode. This API (POST /v2/cores/coreName {'request-buffer-updates': {}}) is analogous to the v1 /admin/cores?action=REQUESTBUFFERUPDATES command. See Also: RequestSyncShardPayload
handler,Only valid in SolrCloud mode. This API (POST /v2/cores/coreName {'request-buffer-updates': {}}) is analogous to the v1 /admin/cores?action=REQUESTBUFFERUPDATES command.
handler,public class RequestCoreRecoveryAPI extends Object Internal V2 API triggering recovery on a core. Only valid in SolrCloud mode. This API (POST /v2/cores/coreName {'request-recovery': {}}) is analogous to the v1 /admin/cores?action=REQUESTRECOVERY command. See Also: RequestCoreRecoveryPayload
handler,Only valid in SolrCloud mode. This API (POST /v2/cores/coreName {'request-recovery': {}}) is analogous to the v1 /admin/cores?action=REQUESTRECOVERY command.
handler,public class RequestSyncShardAPI extends Object Internal V2 API used to request a core sync with its shard leader. Only valid in SolrCloud mode. This API (POST /v2/cores/coreName {'request-sync-shard': {}}) is analogous to the v1 /admin/cores?action=REQUESTSYNCSHARD command. See Also: RequestSyncShardPayload
handler,Only valid in SolrCloud mode. This API (POST /v2/cores/coreName {'request-sync-shard': {}}) is analogous to the v1 /admin/cores?action=REQUESTSYNCSHARD command.
handler,This API is analogous to the v1 /admin/collections?action=RESTORE command.
handler,Only valid in SolrCloud mode. This API (POST /api/cores/coreName/restore {}) is analogous to the v1 GET /solr/admin/cores?action=RESTORECORE command.
handler,"public class SchemaBulkModifyAPI extends Object V2 APIs for creating, updating, or deleting individual components in a collection's schema. This API (POST /v2/collections/collectionName/schema {...}) is analogous to the commands supported by the v1 POST /collectionName/schema API. Typically v2 ""POST"" API implementations separate each ""command"" into a different method. This is not done here because the v1 API code in SchemaHandler expects to consume the request body itself (meaning that nothing _before_ SchemaHandler can consume the request body). As a result the single method below handles all ""commands"" supported by the POST /schema API: 'add-field', 'delete-field', 'replace-field', 'add-dynamic-field', 'delete-dynamic-field', 'replace-dynamic-field', 'add-field-type', 'delete-field-type', 'replace-field-type', 'add-copy-field', and 'delete-copy-field'."
handler,"This API (POST /v2/collections/collectionName/schema {...}) is analogous to the commands supported by the v1 POST /collectionName/schema API. Typically v2 ""POST"" API implementations separate each ""command"" into a different method. This is not done here because the v1 API code in SchemaHandler expects to consume the request body itself (meaning that nothing _before_ SchemaHandler can consume the request body). As a result the single method below handles all ""commands"" supported by the POST /schema API: 'add-field', 'delete-field', 'replace-field', 'add-dynamic-field', 'delete-dynamic-field', 'replace-dynamic-field', 'add-field-type', 'delete-field-type', 'replace-field-type', 'add-copy-field', and 'delete-copy-field'."
handler,"Typically v2 ""POST"" API implementations separate each ""command"" into a different method. This is not done here because the v1 API code in SchemaHandler expects to consume the request body itself (meaning that nothing _before_ SchemaHandler can consume the request body). As a result the single method below handles all ""commands"" supported by the POST /schema API: 'add-field', 'delete-field', 'replace-field', 'add-dynamic-field', 'delete-dynamic-field', 'replace-dynamic-field', 'add-field-type', 'delete-field-type', 'replace-field-type', 'add-copy-field', and 'delete-copy-field'."
handler,"As a result the single method below handles all ""commands"" supported by the POST /schema API: 'add-field', 'delete-field', 'replace-field', 'add-dynamic-field', 'delete-dynamic-field', 'replace-dynamic-field', 'add-field-type', 'delete-field-type', 'replace-field-type', 'add-copy-field', and 'delete-copy-field'."
handler,public class SingleCoreStatusAPI extends Object V2 API for checking the status of a specific core. This API (GET /v2/cores/coreName is analogous to the v1 /admin/cores?action=status&core=coreName command. See Also: AllCoresStatusAPI
handler,This API (GET /v2/cores/coreName is analogous to the v1 /admin/cores?action=status&core=coreName command.
handler,This is the main backup functionality available to 'standalone' users.
handler,public class SplitCoreAPI extends Object V2 API for splitting a single core into multiple pieces The new API (POST /v2/cores/coreName {'split': {...}}) is equivalent to the v1 /admin/cores?action=split command.
handler,The new API (POST /v2/cores/coreName {'split': {...}}) is equivalent to the v1 /admin/cores?action=split command.
handler,public class SplitShardAPI extends Object V2 API for splitting an existing shard up into multiple pieces. This API (POST /v2/collections/collectionName/shards {'split': {...}}) is analogous to the v1 /admin/collections?action=SPLITSHARD command. See Also: SplitShardPayload
handler,This API (POST /v2/collections/collectionName/shards {'split': {...}}) is analogous to the v1 /admin/collections?action=SPLITSHARD command.
handler,This API (POST /v2/collections/cName/shards/sName/sync {...}) is analogous to the v1 /admin/collections?action=SYNCSHARD command.
handler,The API (POST /v2/cores/coreName/unload is equivalent to the v1 /admin/cores?action=unload command.
handler,"public class UpdateAPI extends Object All v2 APIs that share a prefix of /update Most of these v2 APIs are implemented as pure ""pass-throughs"" to the v1 code paths, but there are a few exceptions: /update and /update/json are both rewritten to /update/json/docs."
handler,"Most of these v2 APIs are implemented as pure ""pass-throughs"" to the v1 code paths, but there are a few exceptions: /update and /update/json are both rewritten to /update/json/docs."
handler,public class UploadBlobAPI extends Object V2 API for uploading blobs into Solr's .system blobstore This API (POST /v2/collections/.system/blob/blobName) is analogous to the v1 POST /solr/.system/blob/blobName API.
handler,This API (POST /v2/collections/.system/blob/blobName) is analogous to the v1 POST /solr/.system/blob/blobName API.
handler,public class V2ApiUtils extends Object Utilities helpful for common V2 API declaration tasks.
handler,"The ExpandComponent expands the collapsed groups for a single page. When multiple collapse groups are specified then, the field is chosen from collapse group with min cost. If the cost are equal then, the field is chosen from first collapse group. http parameters: expand=true expand.rows=5 expand.sort=field asc|desc expand.q=*:* (optional, overrides the main query) expand.fq=type:child (optional, overrides the main filter queries) expand.field=field (mandatory, if the not used with the CollapsingQParserPlugin. This is given higher priority when both are present)"
handler,"http parameters: expand=true expand.rows=5 expand.sort=field asc|desc expand.q=*:* (optional, overrides the main query) expand.fq=type:child (optional, overrides the main filter queries) expand.field=field (mandatory, if the not used with the CollapsingQParserPlugin. This is given higher priority when both are present)"
handler,"expand=true expand.rows=5 expand.sort=field asc|desc expand.q=*:* (optional, overrides the main query) expand.fq=type:child (optional, overrides the main filter queries) expand.field=field (mandatory, if the not used with the CollapsingQParserPlugin. This is given higher priority when both are present)"
handler,Enclosing class: FacetComponent public static class FacetComponent.DistribFieldFacet extends FacetComponent.FieldFacet This API is experimental and subject to change
handler,"Direct Known Subclasses: FacetComponent.FieldFacet, FacetComponent.QueryFacet, PivotFacet, RangeFacetRequest, SpatialHeatmapFacets.HeatmapFacet Enclosing class: FacetComponent public static class FacetComponent.FacetBase extends Object This API is experimental and subject to change"
handler,Enclosing class: FacetComponent public static class FacetComponent.FacetContext extends Object Encapsulates facet ranges and facet queries such that their parameters are parsed and cached for efficient re-use. An instance of this class is initialized and kept in the request context via the static method initContext(ResponseBuilder) and can be retrieved via getFacetContext(SolrQueryRequest) This class is used exclusively in a single-node context (i.e. non distributed requests or an individual shard request). Also see FacetComponent.FacetInfo which is dedicated exclusively for merging responses from multiple shards and plays no role during computation of facet counts in a single node request. This API is experimental and subject to change See Also: FacetComponent.FacetInfo
handler,An instance of this class is initialized and kept in the request context via the static method initContext(ResponseBuilder) and can be retrieved via getFacetContext(SolrQueryRequest) This class is used exclusively in a single-node context (i.e. non distributed requests or an individual shard request). Also see FacetComponent.FacetInfo which is dedicated exclusively for merging responses from multiple shards and plays no role during computation of facet counts in a single node request. This API is experimental and subject to change
handler,This class is used exclusively in a single-node context (i.e. non distributed requests or an individual shard request). Also see FacetComponent.FacetInfo which is dedicated exclusively for merging responses from multiple shards and plays no role during computation of facet counts in a single node request. This API is experimental and subject to change
handler,This API is experimental and subject to change
handler,Enclosing class: FacetComponent public static class FacetComponent.FacetInfo extends Object This class is used exclusively for merging results from each shard in a distributed facet request. It plays no role in the computation of facet counts inside a single node. A related class FacetComponent.FacetContext exists for assisting computation inside a single node. This API is experimental and subject to change See Also: FacetComponent.FacetContext
handler,A related class FacetComponent.FacetContext exists for assisting computation inside a single node. This API is experimental and subject to change
handler,This API is experimental and subject to change
handler,Direct Known Subclasses: FacetComponent.DistribFieldFacet Enclosing class: FacetComponent public static class FacetComponent.FieldFacet extends FacetComponent.FacetBase This API is experimental and subject to change
handler,Enclosing class: FacetComponent public static class FacetComponent.QueryFacet extends FacetComponent.FacetBase This API is experimental and subject to change
handler,Enclosing class: FacetComponent public static class FacetComponent.ShardFacetCount extends Object This API is experimental and subject to change
handler,"public class FieldFacetStats extends Object FieldFacetStats is a utility to accumulate statistics on a set of values in one field, for facet values present in another field. 9/10/2009 - Moved out of StatsComponent to allow open access to UnInvertedField See Also: StatsComponent"
handler,9/10/2009 - Moved out of StatsComponent to allow open access to UnInvertedField
handler,"Direct Known Subclasses: ParallelHttpShardHandler @NotThreadSafe public class HttpShardHandler extends ShardHandler Solr's default ShardHandler implementation; uses Jetty's async HTTP Client APIs for sending requests. Shard-requests triggered by submit(ShardRequest, String, ModifiableSolrParams) will be sent synchronously (i.e. before 'submit' returns to the caller). Response waiting and parsing happens asynchronously via HttpShardHandlerFactory.commExecutor. See HttpShardHandlerFactory for details on configuring this executor. The ideal choice for collections with modest or moderate sharding."
handler,"Shard-requests triggered by submit(ShardRequest, String, ModifiableSolrParams) will be sent synchronously (i.e. before 'submit' returns to the caller). Response waiting and parsing happens asynchronously via HttpShardHandlerFactory.commExecutor. See HttpShardHandlerFactory for details on configuring this executor. The ideal choice for collections with modest or moderate sharding."
handler,The ideal choice for collections with modest or moderate sharding.
handler,"@NotThreadSafe public class ParallelHttpShardHandler extends HttpShardHandler A version of HttpShardHandler optimized for massively-sharded collections. Uses a HttpShardHandlerFactory.commExecutor thread for all work related to outgoing requests, allowing HttpShardHandler.submit(ShardRequest, String, ModifiableSolrParams) to return more quickly. (See HttpShardHandler for comparison.) The additional focus on parallelization makes this an ideal implementation for collections with many shards."
handler,"Uses a HttpShardHandlerFactory.commExecutor thread for all work related to outgoing requests, allowing HttpShardHandler.submit(ShardRequest, String, ModifiableSolrParams) to return more quickly. (See HttpShardHandler for comparison.) The additional focus on parallelization makes this an ideal implementation for collections with many shards."
handler,The additional focus on parallelization makes this an ideal implementation for collections with many shards.
handler,"The most common way to use this component is in conjunction with field that use ShingleFilterFactory on both the index and query analyzers. An example field type configuration would be something like this... <fieldType name=""phrases"" class=""solr.TextField"" positionIncrementGap=""100""> <analyzer type=""index""> <tokenizer class=""solr.StandardTokenizerFactory""/> <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.ShingleFilterFactory"" minShingleSize=""2"" maxShingleSize=""3"" outputUnigrams=""true""/> </analyzer> <analyzer type=""query""> <tokenizer class=""solr.StandardTokenizerFactory""/> <filter class=""solr.LowerCaseFilterFactory""/> <filter class=""solr.ShingleFilterFactory"" minShingleSize=""2"" maxShingleSize=""7"" outputUnigramsIfNoShingles=""true"" outputUnigrams=""true""/> </analyzer> </fieldType> ...where the query analyzer's maxShingleSize=""7"" determines the maximum possible phrase length that can be hueristically deduced, the index analyzer's maxShingleSize=""3"" determines the accuracy of phrases identified. The large the indexed maxShingleSize the higher the accuracy. Both analyzers must include minShingleSize=""2"" outputUnigrams=""true"". With a field type like this, one or more fields can be specified (with weights) via a phrases.fields param to request that this component identify possible phrases in the input q param, or an alternative phrases.q override param. The identified phrases will include their scores relative each field specified, as well an overal weighted score based on the field weights provided by the client. Higher score values indicate a greater confidence in the Phrase. NOTE: In a distributed request, this component uses a single phase (piggy backing on the ShardRequest.PURPOSE_GET_TOP_IDS generated by QueryComponent if it is in use) to collect all field & shingle stats. No ""refinement"" requests are used."
handler,"...where the query analyzer's maxShingleSize=""7"" determines the maximum possible phrase length that can be hueristically deduced, the index analyzer's maxShingleSize=""3"" determines the accuracy of phrases identified. The large the indexed maxShingleSize the higher the accuracy. Both analyzers must include minShingleSize=""2"" outputUnigrams=""true"". With a field type like this, one or more fields can be specified (with weights) via a phrases.fields param to request that this component identify possible phrases in the input q param, or an alternative phrases.q override param. The identified phrases will include their scores relative each field specified, as well an overal weighted score based on the field weights provided by the client. Higher score values indicate a greater confidence in the Phrase. NOTE: In a distributed request, this component uses a single phase (piggy backing on the ShardRequest.PURPOSE_GET_TOP_IDS generated by QueryComponent if it is in use) to collect all field & shingle stats. No ""refinement"" requests are used."
handler,"With a field type like this, one or more fields can be specified (with weights) via a phrases.fields param to request that this component identify possible phrases in the input q param, or an alternative phrases.q override param. The identified phrases will include their scores relative each field specified, as well an overal weighted score based on the field weights provided by the client. Higher score values indicate a greater confidence in the Phrase. NOTE: In a distributed request, this component uses a single phase (piggy backing on the ShardRequest.PURPOSE_GET_TOP_IDS generated by QueryComponent if it is in use) to collect all field & shingle stats. No ""refinement"" requests are used."
handler,"NOTE: In a distributed request, this component uses a single phase (piggy backing on the ShardRequest.PURPOSE_GET_TOP_IDS generated by QueryComponent if it is in use) to collect all field & shingle stats. No ""refinement"" requests are used."
handler,Enclosing class: PhrasesIdentificationComponent public static final class PhrasesIdentificationComponent.Phrase extends Object Model the data known about a single (candidate) Phrase -- which may or may not be indexed NOTE: This API is for internal purposes only and might change in incompatible ways in the next release.
handler,Enclosing class: PhrasesIdentificationComponent public static final class PhrasesIdentificationComponent.PhrasesContextData extends Object Simple container for all request options and data this component needs to store in the Request Context NOTE: This API is for internal purposes only and might change in incompatible ways in the next release.
handler,"public class PivotFacet extends FacetComponent.FacetBase Models a single instance of a ""pivot"" specified by a FacetParams.FACET_PIVOT param, which may contain multiple nested fields. This class is also used to coordinate the refinement requests needed from various shards when doing processing a distributed request"
handler,This class is also used to coordinate the refinement requests needed from various shards when doing processing a distributed request
handler,"public class PivotFacetField extends Object Models a single field somewhere in a hierarchy of fields as part of a pivot facet. This pivot field contains PivotFacetValues which may each contain a nested PivotFacetField child. This PivotFacetField may itself be a child of a PivotFacetValue parent. See Also: PivotFacetValue, PivotFacetFieldValueCollection"
handler,"Methods inherited from interfacejava.lang.Iterable forEach, spliterator"
handler,"Methods inherited from interfacejava.util.Comparator equals, reversed, thenComparing, thenComparing, thenComparing, thenComparingDouble, thenComparingInt, thenComparingLong"
handler,"Methods inherited from interfacejava.util.Comparator equals, reversed, thenComparing, thenComparing, thenComparing, thenComparingDouble, thenComparingInt, thenComparingLong"
handler,public class PivotFacetHelper extends Object
handler,"public class PivotFacetProcessor extends SimpleFacets Processes all Pivot facet logic for a single node -- both non-distrib, and per-shard"
handler,"public class PivotFacetValue extends Object Models a single (value, count) pair that will exist in the collection of values for a PivotFacetField parent. This PivotFacetValue may itself have a nested PivotFacetField child See Also: PivotFacetField, PivotFacetFieldValueCollection"
handler,"Enclosing class: QueryComponent protected static class QueryComponent.ScoreAndDoc extends org.apache.lucene.search.Scorable Fake scorer for a single document TODO: when SOLR-5595 is fixed, this wont be needed, as we dont need to recompute sort values here from the comparator"
handler,"TODO: when SOLR-5595 is fixed, this wont be needed, as we dont need to recompute sort values here from the comparator"
handler,Nested classes/interfaces inherited from classorg.apache.lucene.search.Scorable org.apache.lucene.search.Scorable.ChildScorable
handler,"Methods inherited from classorg.apache.lucene.search.Scorable getChildren, setMinCompetitiveScore, smoothingScore"
handler,Nested classes/interfaces inherited from classorg.apache.lucene.search.Scorable org.apache.lucene.search.Scorable.ChildScorable
handler,"subset match - all the elevating terms are matched in the search query, in any order. exact match - the elevating query matches fully (all terms in same order) the search query."
handler,exact match - the elevating query matches fully (all terms in same order) the search query.
handler,"subset match - all the elevating terms are matched in the search query, in any order. exact match - the elevating query matches fully (all terms in same order) the search query."
handler,exact match - the elevating query matches fully (all terms in same order) the search query.
handler,Enclosing class: QueryElevationComponent protected static class QueryElevationComponent.ElevatingQuery extends Object Query triggering elevation.
handler,"Enclosing class: QueryElevationComponent protected static class QueryElevationComponent.Elevation extends Object Elevation of some documents in search results, with potential exclusion of others. Immutable."
handler,"Enclosing class: QueryElevationComponent public class QueryElevationComponent.ElevationBuilder extends Object Builds an QueryElevationComponent.Elevation. This class is used to start defining query elevations, but allowing the merge of multiple elevations for the same query."
handler,"Enclosing class: QueryElevationComponent.TrieSubsetMatcher<E extends Comparable<? super E>,M> public static class QueryElevationComponent.TrieSubsetMatcher.Builder<E extends Comparable<? super E>,M> extends Object"
handler,"Type Parameters: E - Subset element type. M - Subset match value type. Enclosing class: QueryElevationComponent protected static class QueryElevationComponent.TrieSubsetMatcher<E extends Comparable<? super E>,M> extends Object Matches a potentially large collection of subsets with a trie implementation. Given a collection of subsets N, finds all the subsets that are contained (ignoring duplicate elements) by a provided set s. That is, finds all subsets n in N for which s.containsAll(n) (s contains all the elements of n, in any order). Associates a match value of type <M> to each subset and provides it each time the subset matches (i.e. is contained by the provided set). This matcher imposes the elements are Comparable. It does not keep the subset insertion order. Duplicate subsets stack their match values. The time complexity of adding a subset is O(n.log(n)), where n is the size of the subset. The worst case time complexity of the subset matching is O(2^s), however a more typical case time complexity is O(s^3) where s is the size of the set to partially match. Note it does not depend on N, the size of the collection of subsets, nor on n, the size of a subset."
handler,"Given a collection of subsets N, finds all the subsets that are contained (ignoring duplicate elements) by a provided set s. That is, finds all subsets n in N for which s.containsAll(n) (s contains all the elements of n, in any order). Associates a match value of type <M> to each subset and provides it each time the subset matches (i.e. is contained by the provided set). This matcher imposes the elements are Comparable. It does not keep the subset insertion order. Duplicate subsets stack their match values. The time complexity of adding a subset is O(n.log(n)), where n is the size of the subset. The worst case time complexity of the subset matching is O(2^s), however a more typical case time complexity is O(s^3) where s is the size of the set to partially match. Note it does not depend on N, the size of the collection of subsets, nor on n, the size of a subset."
handler,"Associates a match value of type <M> to each subset and provides it each time the subset matches (i.e. is contained by the provided set). This matcher imposes the elements are Comparable. It does not keep the subset insertion order. Duplicate subsets stack their match values. The time complexity of adding a subset is O(n.log(n)), where n is the size of the subset. The worst case time complexity of the subset matching is O(2^s), however a more typical case time complexity is O(s^3) where s is the size of the set to partially match. Note it does not depend on N, the size of the collection of subsets, nor on n, the size of a subset."
handler,"This matcher imposes the elements are Comparable. It does not keep the subset insertion order. Duplicate subsets stack their match values. The time complexity of adding a subset is O(n.log(n)), where n is the size of the subset. The worst case time complexity of the subset matching is O(2^s), however a more typical case time complexity is O(s^3) where s is the size of the set to partially match. Note it does not depend on N, the size of the collection of subsets, nor on n, the size of a subset."
handler,"The time complexity of adding a subset is O(n.log(n)), where n is the size of the subset. The worst case time complexity of the subset matching is O(2^s), however a more typical case time complexity is O(s^3) where s is the size of the set to partially match. Note it does not depend on N, the size of the collection of subsets, nor on n, the size of a subset."
handler,"The worst case time complexity of the subset matching is O(2^s), however a more typical case time complexity is O(s^3) where s is the size of the set to partially match. Note it does not depend on N, the size of the collection of subsets, nor on n, the size of a subset."
handler,public class RangeFacetProcessor extends SimpleFacets Processor for Range Facets
handler,Enclosing class: RangeFacetRequest public static class RangeFacetRequest.FacetRange extends Object Represents a single facet range (or gap) for which the count is to be calculated
handler,public class RangeFacetRequest extends FacetComponent.FacetBase Encapsulates a single facet.range request along with all its parameters. This class calculates all the ranges (gaps) required to be counted.
handler,public class ResponseBuilder extends Object This class is experimental and will be changing in the future. Since: solr 1.3
handler,public class ShardDoc extends org.apache.lucene.search.FieldDoc
handler,Fields inherited from classorg.apache.lucene.search.FieldDoc fields
handler,"Fields inherited from classorg.apache.lucene.search.ScoreDoc doc, score, shardIndex"
handler,Fields inherited from classorg.apache.lucene.search.FieldDoc fields
handler,"Fields inherited from classorg.apache.lucene.search.ScoreDoc doc, score, shardIndex"
handler,"Methods inherited from classorg.apache.lucene.util.PriorityQueue add, addAll, clear, getHeapArray, insertWithOverflow, iterator, pop, remove, size, top, updateTop, updateTop"
handler,"Methods inherited from interfacejava.lang.Iterable forEach, spliterator"
handler,"Direct Known Subclasses: HttpShardHandler public abstract class ShardHandler extends Object Executes, tracks, and awaits all shard-requests made in the course of a distributed request. New ShardHandler instances are created for each individual distributed request, and should not be assumed to be thread-safe."
handler,"New ShardHandler instances are created for each individual distributed request, and should not be assumed to be thread-safe."
handler,Direct Known Subclasses: HttpShardHandlerFactory public abstract class ShardHandlerFactory extends Object
handler,public class ShardRequest extends Object
handler,public final class ShardResponse extends Object
handler,public class SpatialHeatmapFacets extends Object A 2D spatial faceting summary of a rectangular region. Used by FacetComponent and SimpleFacets. See Also: FacetHeatmap
handler,Refer to https://solr.apache.org/guide/solr/latest/query-guide/spell-checking.html for more details
handler,public class SpellCheckMergeData extends Object
handler,Enclosing class: StatsField public static final class StatsField.HllOptions extends Object Helper Struct for parsing and encapsulating all of the options relaed to building a HLL See Also: StatsField.Stat.cardinality NOTE: This API is for internal purposes only and might change in incompatible ways in the next release.
handler,public class StatsField extends Object Models all of the information associated with a single StatsParams.STATS_FIELD instance. See Also: StatsComponent
handler,public class StatsValuesFactory extends Object Factory class for creating instance of StatsValues
handler,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
handler,"Methods inherited from interfacejava.util.Comparator equals, reversed, thenComparing, thenComparing, thenComparing, thenComparingDouble, thenComparingInt, thenComparingLong"
handler,Enclosing class: TermsComponent public static class TermsComponent.TermsHelper extends Object
handler,"Info available: term, frequency, position, offset, payloads, IDF. Note Returning IDF can be expensive. <searchComponent name=""tvComponent"" class=""solr.TermVectorComponent""/> <requestHandler name=""/terms"" class=""solr.SearchHandler""> <lst name=""defaults""> <bool name=""tv"">true</bool> </lst> <arr name=""last-component""> <str>tvComponent</str> </arr> </requestHandler>"
handler,"Note Returning IDF can be expensive. <searchComponent name=""tvComponent"" class=""solr.TermVectorComponent""/> <requestHandler name=""/terms"" class=""solr.SearchHandler""> <lst name=""defaults""> <bool name=""tv"">true</bool> </lst> <arr name=""last-component""> <str>tvComponent</str> </arr> </requestHandler>"
handler,"Direct Known Subclasses: CreateConfigSetAPI, DeleteConfigSetAPI, UploadConfigSetAPI, UploadConfigSetFileAPI public class ConfigSetAPIBase extends Object Parent class for all APIs that manipulate configsets Contains utilities for tasks common in configset manipulation, including running configset ""commands"" and checking configset ""trusted-ness""."
handler,"Contains utilities for tasks common in configset manipulation, including running configset ""commands"" and checking configset ""trusted-ness""."
handler,"public class CreateConfigSetAPI extends ConfigSetAPIBase V2 API for creating a new configset as a copy of an existing one. This API (POST /v2/cluster/configs {""create"": {...}}) is analogous to the v1 /admin/configs?action=CREATE command."
handler,"This API (POST /v2/cluster/configs {""create"": {...}}) is analogous to the v1 /admin/configs?action=CREATE command."
handler,public class DeleteConfigSetAPI extends ConfigSetAPIBase V2 API for deleting an existing configset This API (DELETE /v2/cluster/configs/configsetName) is analogous to the v1 /admin/configs?action=DELETE command.
handler,This API (DELETE /v2/cluster/configs/configsetName) is analogous to the v1 /admin/configs?action=DELETE command.
handler,This API (GET /v2/cluster/configs) is analogous to the v1 /admin/configs?action=LIST command.
handler,public class UploadConfigSetAPI extends ConfigSetAPIBase V2 API for uploading a new configset (or overwriting an existing one). This API (PUT /v2/cluster/configs/configsetName) is analogous to the v1 /admin/configs?action=UPLOAD command.
handler,This API (PUT /v2/cluster/configs/configsetName) is analogous to the v1 /admin/configs?action=UPLOAD command.
handler,public class UploadConfigSetFileAPI extends ConfigSetAPIBase V2 API for adding or updating a single file within a configset. This API (PUT /v2/cluster/configs/configsetName/someFilePath) is analogous to the v1 /admin/configs?action=UPLOAD&filePath=someFilePath command.
handler,This API (PUT /v2/cluster/configs/configsetName/someFilePath) is analogous to the v1 /admin/configs?action=UPLOAD&filePath=someFilePath command.
handler,public class ManagedSchemaDiff extends Object Utility methods for comparing managed index schemas
handler,public class SampleDocuments extends Object
handler,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, setStackTrace, toString"
handler,"ExportWriter gathers and sorts the documents for a core using ""stream sorting"". Stream sorting works by repeatedly processing and modifying a bitmap of matching documents. Each pass over the bitmap identifies the smallest docs (default is DEFAULT_BATCH_SIZE) that haven't been sent yet and stores them in a Priority Queue. They are then exported (written across the wire) and marked as sent (unset in the bitmap). This process repeats until all matching documents have been sent."
handler,Stream sorting works by repeatedly processing and modifying a bitmap of matching documents. Each pass over the bitmap identifies the smallest docs (default is DEFAULT_BATCH_SIZE) that haven't been sent yet and stores them in a Priority Queue. They are then exported (written across the wire) and marked as sent (unset in the bitmap). This process repeats until all matching documents have been sent.
handler,"Note: this class is made public only to allow access from ExportHandler, it should be treated as an internal detail of implementation."
handler,public class CborLoader extends Object This class can load a single document or a stream of documents in CBOR format this is equivalent of loading a single json documet or an array of json documents
handler,"Direct Known Subclasses: CSVLoader, CSVLoaderBase, JavabinLoader, JsonLoader, XMLLoader public abstract class ContentStreamLoader extends Object Load a ContentStream into Solr This should be thread safe and can be called from multiple threads"
handler,This should be thread safe and can be called from multiple threads
handler,public class CSVLoader extends ContentStreamLoader
handler,public abstract class CSVLoaderBase extends ContentStreamLoader
handler,"public class JavabinLoader extends ContentStreamLoader Update handler which uses the JavaBin format See Also: JavaBinUpdateRequestCodec, JavaBinCodec"
handler,public class JsonLoader extends ContentStreamLoader Since: solr 4.0
handler,public class XMLLoader extends ContentStreamLoader
handler,Direct Known Subclasses: XmlOffsetCorrector public abstract class OffsetCorrector extends Object
handler,"public abstract class Tagger extends Object Tags maximum string of words in a corpus. This is a callback-style API in which you implement tagCallback(int, int, Object). This class should be independently usable outside Solr."
handler,This class should be independently usable outside Solr.
handler,"Methods inherited from classorg.apache.lucene.util.AttributeImpl clone, end, reflectAsString"
handler,"public class TagLL extends Object This is a Tag -- a startOffset, endOffset and value. A Tag starts without a value in an ""advancing"" state. advance(org.apache.lucene.util.BytesRef, int) is called with subsequent words and then eventually it won't advance any more, and value is set (could be null). A Tag is also a doubly-linked-list (hence the LL in the name). All tags share a reference to the head via a 1-element array, which is potentially modified if any of the linked-list methods are called. Tags in the list should have equal or increasing start offsets."
handler,"A Tag starts without a value in an ""advancing"" state. advance(org.apache.lucene.util.BytesRef, int) is called with subsequent words and then eventually it won't advance any more, and value is set (could be null). A Tag is also a doubly-linked-list (hence the LL in the name). All tags share a reference to the head via a 1-element array, which is potentially modified if any of the linked-list methods are called. Tags in the list should have equal or increasing start offsets."
handler,"A Tag is also a doubly-linked-list (hence the LL in the name). All tags share a reference to the head via a 1-element array, which is potentially modified if any of the linked-list methods are called. Tags in the list should have equal or increasing start offsets."
handler,"public class XmlOffsetCorrector extends OffsetCorrector Corrects offsets to adjust for XML formatted data. The goal is such that the caller should be able to insert a start XML tag at the start offset and a corresponding end XML tag at the end offset of the tagger, and have it be valid XML. See OffsetCorrector.correctPair(int, int). This will not work on invalid XML. Not thread-safe."
handler,This will not work on invalid XML. Not thread-safe.
handler,Not thread-safe.
highlight,Enclosing class: DefaultSolrHighlighter public static class DefaultSolrHighlighter.FvhContainer extends Object
highlight,"hl.regex.pattern: regular expression corresponding to ""nice"" fragments. hl.regex.slop: how far the fragmenter can stray from the ideal fragment size. A slop of 0.2 means that the fragmenter can go over or under by 20%. hl.regex.maxAnalyzedChars: how many characters to apply the regular expression to (independent from the global highlighter setting)."
highlight,hl.regex.slop: how far the fragmenter can stray from the ideal fragment size. A slop of 0.2 means that the fragmenter can go over or under by 20%. hl.regex.maxAnalyzedChars: how many characters to apply the regular expression to (independent from the global highlighter setting).
highlight,hl.regex.maxAnalyzedChars: how many characters to apply the regular expression to (independent from the global highlighter setting).
highlight,"hl.regex.pattern: regular expression corresponding to ""nice"" fragments. hl.regex.slop: how far the fragmenter can stray from the ideal fragment size. A slop of 0.2 means that the fragmenter can go over or under by 20%. hl.regex.maxAnalyzedChars: how many characters to apply the regular expression to (independent from the global highlighter setting)."
highlight,hl.regex.slop: how far the fragmenter can stray from the ideal fragment size. A slop of 0.2 means that the fragmenter can go over or under by 20%. hl.regex.maxAnalyzedChars: how many characters to apply the regular expression to (independent from the global highlighter setting).
highlight,hl.regex.maxAnalyzedChars: how many characters to apply the regular expression to (independent from the global highlighter setting).
highlight,"Direct Known Subclasses: DefaultSolrHighlighter, UnifiedSolrHighlighter public abstract class SolrHighlighter extends Object"
highlight,"hl.q (string) can specify the query hl.fl (string) specifies the field list. hl.snippets (int) specifies how many snippets to return. hl.tag.pre (string) specifies text which appears before a highlighted term. hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.fl (string) specifies the field list. hl.snippets (int) specifies how many snippets to return. hl.tag.pre (string) specifies text which appears before a highlighted term. hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.snippets (int) specifies how many snippets to return. hl.tag.pre (string) specifies text which appears before a highlighted term. hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.tag.pre (string) specifies text which appears before a highlighted term. hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,hl.weightMatches (bool) enables Lucene Weight Matches mode
highlight,"Example configuration with default values: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""defaults""> <str name=""hl.method"">unified</str> <int name=""hl.snippets"">1</int> <str name=""hl.tag.pre"">&lt;em&gt;</str> <str name=""hl.tag.post"">&lt;/em&gt;</str> <str name=""hl.simple.pre"">&lt;em&gt;</str> <str name=""hl.simple.post"">&lt;/em&gt;</str> <str name=""hl.tag.ellipsis"">(internal/unspecified)</str> <bool name=""hl.defaultSummary"">false</bool> <str name=""hl.encoder"">simple</str> <float name=""hl.score.k1"">1.2</float> <float name=""hl.score.b"">0.75</float> <float name=""hl.score.pivot"">87</float> <str name=""hl.bs.language""></str> <str name=""hl.bs.country""></str> <str name=""hl.bs.variant""></str> <str name=""hl.bs.type"">SENTENCE</str> <int name=""hl.maxAnalyzedChars"">51200</int> <bool name=""hl.highlightMultiTerm"">true</bool> <bool name=""hl.usePhraseHighlighter"">true</bool> <int name=""hl.cacheFieldValCharsThreshold"">524288</int> <str name=""hl.offsetSource""></str> <bool name=""hl.weightMatches"">true</bool> </lst> </requestHandler> Notes: hl.q (string) can specify the query hl.fl (string) specifies the field list. hl.snippets (int) specifies how many snippets to return. hl.tag.pre (string) specifies text which appears before a highlighted term. hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"Notes: hl.q (string) can specify the query hl.fl (string) specifies the field list. hl.snippets (int) specifies how many snippets to return. hl.tag.pre (string) specifies text which appears before a highlighted term. hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.q (string) can specify the query hl.fl (string) specifies the field list. hl.snippets (int) specifies how many snippets to return. hl.tag.pre (string) specifies text which appears before a highlighted term. hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.fl (string) specifies the field list. hl.snippets (int) specifies how many snippets to return. hl.tag.pre (string) specifies text which appears before a highlighted term. hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.snippets (int) specifies how many snippets to return. hl.tag.pre (string) specifies text which appears before a highlighted term. hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.tag.pre (string) specifies text which appears before a highlighted term. hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.tag.post (string) specifies text which appears after a highlighted term. hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.simple.pre (string) specifies text which appears before a highlighted term. (prefer hl.tag.pre) hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.simple.post (string) specifies text which appears before a highlighted term. (prefer hl.tag.post) hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.tag.ellipsis (string) specifies text which joins non-adjacent passages. The default is to retain each value in a list without joining them. hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.defaultSummary (bool) specifies if a field should have a default summary of the leading text. hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.encoder (string) can be 'html' (html escapes content) or 'simple' (no escaping). hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.score.k1 (float) specifies bm25 scoring parameter 'k1' hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.score.b (float) specifies bm25 scoring parameter 'b' hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.score.pivot (float) specifies bm25 scoring parameter 'avgdl' hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.bs.type (string) specifies how to divide text into passages: [SENTENCE, LINE, WORD, CHAR, WHOLE] hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.bs.language (string) specifies language code for BreakIterator. default is empty string (root locale) hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.bs.country (string) specifies country code for BreakIterator. default is empty string (root locale) hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.bs.variant (string) specifies country code for BreakIterator. default is empty string (root locale) hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.maxAnalyzedChars (int) specifies how many characters at most will be processed in a document for any one field. hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.highlightMultiTerm (bool) enables highlighting for range/wildcard/fuzzy/prefix queries at some cost. default is true hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.usePhraseHighlighter (bool) enables phrase highlighting. default is true hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.cacheFieldValCharsThreshold (int) controls how many characters from a field are cached. default is 524288 (1MB in 2 byte chars) hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,"hl.offsetSource (string) specifies which offset source to use, prefers postings, but will use what's available if not specified hl.weightMatches (bool) enables Lucene Weight Matches mode"
highlight,hl.weightMatches (bool) enables Lucene Weight Matches mode
highlight,"Nested classes/interfaces inherited from classorg.apache.lucene.search.uhighlight.UnifiedHighlighter org.apache.lucene.search.uhighlight.UnifiedHighlighter.Builder, org.apache.lucene.search.uhighlight.UnifiedHighlighter.HighlightFlag, org.apache.lucene.search.uhighlight.UnifiedHighlighter.LimitedStoredFieldVisitor, org.apache.lucene.search.uhighlight.UnifiedHighlighter.OffsetSource"
highlight,"Fields inherited from classorg.apache.lucene.search.uhighlight.UnifiedHighlighter DEFAULT_CACHE_CHARS_THRESHOLD, DEFAULT_MAX_LENGTH, fieldInfos, indexAnalyzer, MULTIVAL_SEP_CHAR, searcher, ZERO_LEN_AUTOMATA_ARRAY"
highlight,"Methods inherited from classorg.apache.lucene.search.uhighlight.UnifiedHighlighter builder, builderWithoutSearcher, evaluateFlags, evaluateFlags, evaluateFlags, extractTerms, filterExtractedTerms, getAutomata, getCacheFieldValCharsThreshold, getFieldHighlighter, getHighlightComponents, getIndexAnalyzer, getIndexSearcher, getMaskedFields, getMaxLength, getOffsetStrategy, getOptimizedOffsetSource, getPassageSortComparator, getPhraseHelper, hasUnrecognizedQuery, highlight, highlight, highlightFields, highlightFields, highlightFields, highlightFieldsAsObjects, highlightWithoutSearcher, newFieldHighlighter, newLimitedStoredFieldsVisitor, preSpanQueryRewrite, requiresRewrite, setBreakIterator, setCacheFieldValCharsThreshold, setFieldMatcher, setFormatter, setHandleMultiTermQuery, setHighlightPhrasesStrictly, setMaxLength, setMaxNoHighlightPassages, setPassageRelevancyOverSpeed, setScorer, setWeightMatches, shouldHandleMultiTermQuery, shouldHighlightPhrasesStrictly, shouldPreferPassageRelevancyOverSpeed"
highlight,"Nested classes/interfaces inherited from classorg.apache.lucene.search.uhighlight.UnifiedHighlighter org.apache.lucene.search.uhighlight.UnifiedHighlighter.Builder, org.apache.lucene.search.uhighlight.UnifiedHighlighter.HighlightFlag, org.apache.lucene.search.uhighlight.UnifiedHighlighter.LimitedStoredFieldVisitor, org.apache.lucene.search.uhighlight.UnifiedHighlighter.OffsetSource"
index,public class DefaultMergePolicyFactory extends MergePolicyFactory A MergePolicyFactory for the default MergePolicy.
index,public class LogByteSizeMergePolicyFactory extends SimpleMergePolicyFactory A MergePolicyFactory for LogByteSizeMergePolicy objects.
index,public class LogDocMergePolicyFactory extends SimpleMergePolicyFactory A MergePolicyFactory for LogDocMergePolicy objects.
index,"Direct Known Subclasses: DefaultMergePolicyFactory, SimpleMergePolicyFactory, WrapperMergePolicyFactory public abstract class MergePolicyFactory extends Object A factory for creating a MergePolicy."
index,public class MergePolicyFactoryArgs extends Object
index,public class NoMergePolicyFactory extends SimpleMergePolicyFactory
index,"Direct Known Subclasses: LogByteSizeMergePolicyFactory, LogDocMergePolicyFactory, NoMergePolicyFactory, TieredMergePolicyFactory public abstract class SimpleMergePolicyFactory extends MergePolicyFactory A MergePolicyFactory for simple MergePolicy objects. Implementations need only create the policy instance and this class will then configure it with all set properties."
index,"NOTE: this class almost always results in a performance hit. If this is important to your use case, you'll get better performance by gathering the sub readers using IndexReader.getContext() to get the leaves and then operate per-LeafReader, instead of using this class."
index,"Nested classes/interfaces inherited from classorg.apache.lucene.index.IndexReader org.apache.lucene.index.IndexReader.CacheHelper, org.apache.lucene.index.IndexReader.CacheKey, org.apache.lucene.index.IndexReader.ClosedListener"
index,"Methods inherited from classorg.apache.lucene.index.LeafReader docFreq, getContext, getDocCount, getSumDocFreq, getSumTotalTermFreq, postings, postings, searchNearestVectors, searchNearestVectors, totalTermFreq"
index,"Methods inherited from classorg.apache.lucene.index.IndexReader close, decRef, document, document, ensureOpen, equals, getRefCount, getTermVector, hasDeletions, hashCode, incRef, leaves, notifyReaderClosedListeners, numDeletedDocs, registerParentReader, tryIncRef"
index,"Nested classes/interfaces inherited from classorg.apache.lucene.index.IndexReader org.apache.lucene.index.IndexReader.CacheHelper, org.apache.lucene.index.IndexReader.CacheKey, org.apache.lucene.index.IndexReader.ClosedListener"
index,"Nested classes/interfaces inherited from classorg.apache.lucene.index.MergePolicy org.apache.lucene.index.MergePolicy.MergeAbortedException, org.apache.lucene.index.MergePolicy.MergeContext, org.apache.lucene.index.MergePolicy.MergeException, org.apache.lucene.index.MergePolicy.MergeSpecification, org.apache.lucene.index.MergePolicy.OneMerge, org.apache.lucene.index.MergePolicy.OneMergeProgress"
index,Fields inherited from classorg.apache.lucene.index.FilterMergePolicy in
index,"Fields inherited from classorg.apache.lucene.index.MergePolicy DEFAULT_MAX_CFS_SEGMENT_SIZE, DEFAULT_NO_CFS_RATIO, maxCFSSegmentSize, noCFSRatio"
index,"Methods inherited from classorg.apache.lucene.index.FilterMergePolicy findForcedDeletesMerges, findForcedMerges, findFullFlushMerges, findMerges, findMerges, getMaxCFSSegmentSizeMB, getNoCFSRatio, keepFullyDeletedSegment, maxFullFlushMergeSize, numDeletesToMerge, setMaxCFSSegmentSizeMB, setNoCFSRatio, size, unwrap, useCompoundFile"
index,"Methods inherited from classorg.apache.lucene.index.MergePolicy assertDelCount, isMerged, message, segString, verbose"
index,"Nested classes/interfaces inherited from classorg.apache.lucene.index.MergePolicy org.apache.lucene.index.MergePolicy.MergeAbortedException, org.apache.lucene.index.MergePolicy.MergeContext, org.apache.lucene.index.MergePolicy.MergeException, org.apache.lucene.index.MergePolicy.MergeSpecification, org.apache.lucene.index.MergePolicy.OneMerge, org.apache.lucene.index.MergePolicy.OneMergeProgress"
index,public class SortingMergePolicyFactory extends WrapperMergePolicyFactory A MergePolicyFactory for SortingMergePolicy objects.
index,public class TieredMergePolicyFactory extends SimpleMergePolicyFactory A MergePolicyFactory for TieredMergePolicy objects.
index,public class UpgradeIndexMergePolicyFactory extends WrapperMergePolicyFactory A MergePolicyFactory for UpgradeIndexMergePolicy objects.
index,"Direct Known Subclasses: SortingMergePolicyFactory, UpgradeIndexMergePolicyFactory public abstract class WrapperMergePolicyFactory extends MergePolicyFactory A MergePolicyFactory for wrapping additional factories."
internal,public class CharBuffer extends Object A simple StringBuffer replacement that aims to reduce copying as much as possible. The buffer grows as necessary. This class is not thread safe.
internal,"public class CSVParser extends Object Parses CSV files according to the specified configuration. Because CSV appears in many different dialects, the parser supports many configuration settings by allowing the specification of a CSVStrategy. Parsing of a csv-string having tabs as separators, '""' as an optional value encapsulator, and comments starting with '#': String[][] data = (new CSVParser(new StringReader(""a\tb\nc\td""), new CSVStrategy('\t','""','#'))).getAllValues(); Parsing of a csv-string in Excel CSV format String[][] data = (new CSVParser(new StringReader(""a;b\nc;d""), CSVStrategy.EXCEL_STRATEGY)).getAllValues(); Internal parser state is completely covered by the strategy and the reader-state. see package documentation for more details"
internal,"Because CSV appears in many different dialects, the parser supports many configuration settings by allowing the specification of a CSVStrategy. Parsing of a csv-string having tabs as separators, '""' as an optional value encapsulator, and comments starting with '#': String[][] data = (new CSVParser(new StringReader(""a\tb\nc\td""), new CSVStrategy('\t','""','#'))).getAllValues(); Parsing of a csv-string in Excel CSV format String[][] data = (new CSVParser(new StringReader(""a;b\nc;d""), CSVStrategy.EXCEL_STRATEGY)).getAllValues(); Internal parser state is completely covered by the strategy and the reader-state. see package documentation for more details"
internal,"Parsing of a csv-string having tabs as separators, '""' as an optional value encapsulator, and comments starting with '#': String[][] data = (new CSVParser(new StringReader(""a\tb\nc\td""), new CSVStrategy('\t','""','#'))).getAllValues(); Parsing of a csv-string in Excel CSV format String[][] data = (new CSVParser(new StringReader(""a;b\nc;d""), CSVStrategy.EXCEL_STRATEGY)).getAllValues(); Internal parser state is completely covered by the strategy and the reader-state. see package documentation for more details"
internal,"Parsing of a csv-string in Excel CSV format String[][] data = (new CSVParser(new StringReader(""a;b\nc;d""), CSVStrategy.EXCEL_STRATEGY)).getAllValues(); Internal parser state is completely covered by the strategy and the reader-state. see package documentation for more details"
internal,Internal parser state is completely covered by the strategy and the reader-state. see package documentation for more details
internal,see package documentation for more details
internal,public class CSVPrinter extends Object Print values as a comma separated list.
internal,Represents the strategy for a CSV.
internal,public class CSVUtils extends Object Utility methods for dealing with CSV files
jersey,"Methods inherited from classorg.glassfish.hk2.utilities.binding.AbstractBinder addActiveDescriptor, addActiveDescriptor, addActiveDescriptor, addActiveFactoryDescriptor, addIdempotentFilter, addUnbindFilter, bind, bind, bind, bind, bind, bind, bind, bindAsContract, bindAsContract, bindAsContract, bindFactory, bindFactory, bindFactory, commit, install, registerTwoPhaseResources"
jersey,Format and behavior based on the exception handling in Solr's v1 requestHandler's. Also sets metrics if present on the request context.
jersey,public class InjectionFactories extends Object
jersey,"Direct Known Subclasses: InjectionFactories.ReuseFromContextSolrCoreFactory, InjectionFactories.ReuseFromContextSolrParamsFactory, InjectionFactories.SolrQueryRequestFactory, InjectionFactories.SolrQueryResponseFactory Enclosing class: InjectionFactories public static class InjectionFactories.RequestContextBasedFactory extends Object Allows access to a ContainerRequestContext via a ServiceLocator ServiceLocator must be used util https://github.com/eclipse-ee4j/jersey/issues/3503 is resolved."
jersey,ServiceLocator must be used util https://github.com/eclipse-ee4j/jersey/issues/3503 is resolved.
jersey,public class JerseyAppHandlerCache extends Object Stores Jersey 'ApplicationHandler' instances by an ID or hash derived from their ConfigSet. ApplicationHandler creation is expensive; caching these objects allows them to be shared by multiple cores with the same configuration.
jersey,ApplicationHandler creation is expensive; caching these objects allows them to be shared by multiple cores with the same configuration.
jersey,"Methods inherited from classorg.glassfish.jersey.server.ResourceConfig addProperties, files, files, forApplication, forApplicationClass, forApplicationClass, getApplication, getApplicationName, getApplicationPath, getClasses, getClassLoader, getConfiguration, getContracts, getInstances, getProperties, getProperty, getPropertyNames, getResources, getRuntimeType, getSingletons, hasProperty, isEnabled, isEnabled, isProperty, isRegistered, isRegistered, packages, packages, packages, property, register, register, register, register, register, register, register, register, registerClasses, registerClasses, registerFinder, registerInstances, registerInstances, registerResources, registerResources, setApplicationName, setClassLoader, setProperties"
jersey,"public class JerseyApplications extends Object JAX-RS ""application"" configurations for Solr's CoreContainer and SolrCore instances"
jersey,"Methods inherited from classorg.glassfish.jersey.server.ResourceConfig addProperties, files, files, forApplication, forApplicationClass, forApplicationClass, getApplication, getApplicationName, getApplicationPath, getClasses, getClassLoader, getConfiguration, getContracts, getInstances, getProperties, getProperty, getPropertyNames, getResources, getRuntimeType, getSingletons, hasProperty, isEnabled, isEnabled, isProperty, isRegistered, isRegistered, packages, packages, packages, property, register, register, register, register, register, register, register, register, registerClasses, registerClasses, registerFinder, registerInstances, registerInstances, registerResources, registerResources, setApplicationName, setClassLoader, setProperties"
jersey,This makes the request body accessible to any Jersey response filters or interceptors.
jersey,"Methods inherited from interfacejakarta.ws.rs.ext.MessageBodyReader isReadable, readFrom"
jersey,public class MessageBodyReaders extends Object A collection point for various MessageBodyReader implementations.
jersey,Methods inherited from interfacejakarta.ws.rs.ext.MessageBodyWriter getSize
jersey,"Methods inherited from interfacejakarta.ws.rs.ext.MessageBodyWriter getSize, isWriteable, writeTo"
jersey,"Methods inherited from interfacejakarta.ws.rs.ext.MessageBodyWriter getSize, isWriteable, writeTo"
jersey,public class MessageBodyWriters extends Object A collection of thin Jersey shims around Solr's existing QueryResponseWriter interface
jersey,"Methods inherited from interfacejakarta.ws.rs.ext.MessageBodyWriter getSize, isWriteable, writeTo"
jersey,"Methods inherited from interfacejakarta.ws.rs.ext.MessageBodyWriter getSize, isWriteable, writeTo"
jersey,"If no special flags are set, this mapper returns an error response similar to that produced by other V2 codepaths. If the RequestContextKeys.SUPPRESS_ERROR_ON_NOT_FOUND_EXCEPTION flag is present, we suppress the error response and merely set a flag indicating the lack of matching resource. This is done in service of fielding requests whose resource might be registered in one of multiple JAX-RS applications. See V2HttpCall's ""executeCoreRequest"" method for more details."
jersey,"If the RequestContextKeys.SUPPRESS_ERROR_ON_NOT_FOUND_EXCEPTION flag is present, we suppress the error response and merely set a flag indicating the lack of matching resource. This is done in service of fielding requests whose resource might be registered in one of multiple JAX-RS applications. See V2HttpCall's ""executeCoreRequest"" method for more details."
jersey,"public class RequestMetricHandling extends Object A request and response filter used to initialize and report per-request metrics. Currently, Jersey resources that have a corresponding v1 API produce the same metrics as their v1 equivalent and rely on the v1 requestHandler instance to do so. Solr facilitates this by building a map of the JAX-RS resources to requestHandler mapping (a PluginBag.JaxrsResourceToHandlerMappings), and using that to look up the associated request handler (if one exists) in pre- and post- filters This isn't ideal, as requestHandler's don't really ""fit"" conceptually here. But it's unavoidable while we want our v2 APIs to exactly match the metrics produced by v1 calls, and while metrics are bundled in with requestHandlers as they are currently. See Also: RequestMetricHandling.PreRequestMetricsFilter, RequestMetricHandling.PostRequestMetricsFilter"
jersey,"Currently, Jersey resources that have a corresponding v1 API produce the same metrics as their v1 equivalent and rely on the v1 requestHandler instance to do so. Solr facilitates this by building a map of the JAX-RS resources to requestHandler mapping (a PluginBag.JaxrsResourceToHandlerMappings), and using that to look up the associated request handler (if one exists) in pre- and post- filters This isn't ideal, as requestHandler's don't really ""fit"" conceptually here. But it's unavoidable while we want our v2 APIs to exactly match the metrics produced by v1 calls, and while metrics are bundled in with requestHandlers as they are currently."
jersey,"This isn't ideal, as requestHandler's don't really ""fit"" conceptually here. But it's unavoidable while we want our v2 APIs to exactly match the metrics produced by v1 calls, and while metrics are bundled in with requestHandlers as they are currently."
jersey,Looks up the requestHandler associated with the particular Jersey request and attaches its RequestHandlerBase.HandlerMetrics to the request context to be manipulated by other pre- and post-request filters in this chain.
jersey,Nested classes/interfaces inherited from classcom.fasterxml.jackson.databind.JsonSerializer com.fasterxml.jackson.databind.JsonSerializer.None
jersey,Fields inherited from classcom.fasterxml.jackson.databind.ser.std.StdSerializer _handledType
jersey,"Methods inherited from classcom.fasterxml.jackson.databind.ser.std.StdSerializer _neitherNull, _nonEmpty, acceptJsonFormatVisitor, createSchemaNode, createSchemaNode, findAnnotatedContentSerializer, findContextualConvertingSerializer, findConvertingContentSerializer, findFormatFeature, findFormatOverrides, findIncludeOverrides, findPropertyFilter, getSchema, getSchema, handledType, isDefaultSerializer, visitArrayFormat, visitArrayFormat, visitFloatFormat, visitIntFormat, visitIntFormat, visitStringFormat, visitStringFormat, wrapAndThrow, wrapAndThrow"
jersey,"Methods inherited from classcom.fasterxml.jackson.databind.JsonSerializer getDelegatee, isEmpty, isEmpty, isUnwrappingSerializer, properties, replaceDelegatee, serializeWithType, unwrappingSerializer, usesObjectId, withFilterId, withIgnoredProperties"
jersey,Nested classes/interfaces inherited from classcom.fasterxml.jackson.databind.JsonSerializer com.fasterxml.jackson.databind.JsonSerializer.None
jersey,public class ContainerRequestUtils extends Object Utility methods for creating and populating a ContainerRequest for use with Jersey ApplicationHandlers
jersey,"Since we're not running Jersey as a full server or as a servlet that Jetty knows about, we need to connect the response Jersey outputs with the actual objects that Jetty sends back to clients. Inspired and partially copied from the JettyHttpContainer.ResponseWriter class available in the jersey-container-jetty-http artifact."
jersey,Inspired and partially copied from the JettyHttpContainer.ResponseWriter class available in the jersey-container-jetty-http artifact.
jersey,Nested classes/interfaces inherited from interfaceorg.glassfish.jersey.server.spi.ContainerResponseWriter org.glassfish.jersey.server.spi.ContainerResponseWriter.TimeoutHandler
jersey,Nested classes/interfaces inherited from interfaceorg.glassfish.jersey.server.spi.ContainerResponseWriter org.glassfish.jersey.server.spi.ContainerResponseWriter.TimeoutHandler
legacy,"@Deprecated public class BBoxStrategy extends org.apache.lucene.spatial.SpatialStrategy Deprecated. A SpatialStrategy for indexing and searching Rectangles by storing its coordinates in numeric fields. It supports all SpatialOperations and has a custom overlap relevancy. It is based on GeoPortal's SpatialClauseAdapter. Characteristics: Only indexes Rectangles; just one per field value. Other shapes can be provided and the bounding box will be used. Can query only by a Rectangle. Providing other shapes is an error. Supports most SpatialOperations but not Overlaps. Uses the DocValues API for any sorting / relevancy. Implementation: This uses 4 double fields for minX, maxX, minY, maxY and a boolean to mark a dateline cross. Depending on the particular SpatialOperations, there are a variety of range queries on DoublePoints to be done. The makeOverlapRatioValueSource(org.locationtech.spatial4j.shape.Rectangle, double) works by calculating the query bbox overlap percentage against the indexed shape overlap percentage. The indexed shape's coordinates are retrieved from LeafReader.getNumericDocValues(java.lang.String). WARNING: This API is experimental and might change in incompatible ways in the next release."
legacy,Only indexes Rectangles; just one per field value. Other shapes can be provided and the bounding box will be used. Can query only by a Rectangle. Providing other shapes is an error. Supports most SpatialOperations but not Overlaps. Uses the DocValues API for any sorting / relevancy.
legacy,Can query only by a Rectangle. Providing other shapes is an error. Supports most SpatialOperations but not Overlaps. Uses the DocValues API for any sorting / relevancy.
legacy,Supports most SpatialOperations but not Overlaps. Uses the DocValues API for any sorting / relevancy.
legacy,Uses the DocValues API for any sorting / relevancy.
legacy,"Characteristics: Only indexes Rectangles; just one per field value. Other shapes can be provided and the bounding box will be used. Can query only by a Rectangle. Providing other shapes is an error. Supports most SpatialOperations but not Overlaps. Uses the DocValues API for any sorting / relevancy. Implementation: This uses 4 double fields for minX, maxX, minY, maxY and a boolean to mark a dateline cross. Depending on the particular SpatialOperations, there are a variety of range queries on DoublePoints to be done. The makeOverlapRatioValueSource(org.locationtech.spatial4j.shape.Rectangle, double) works by calculating the query bbox overlap percentage against the indexed shape overlap percentage. The indexed shape's coordinates are retrieved from LeafReader.getNumericDocValues(java.lang.String)."
legacy,Only indexes Rectangles; just one per field value. Other shapes can be provided and the bounding box will be used. Can query only by a Rectangle. Providing other shapes is an error. Supports most SpatialOperations but not Overlaps. Uses the DocValues API for any sorting / relevancy.
legacy,Can query only by a Rectangle. Providing other shapes is an error. Supports most SpatialOperations but not Overlaps. Uses the DocValues API for any sorting / relevancy.
legacy,Supports most SpatialOperations but not Overlaps. Uses the DocValues API for any sorting / relevancy.
legacy,Uses the DocValues API for any sorting / relevancy.
legacy,"Implementation: This uses 4 double fields for minX, maxX, minY, maxY and a boolean to mark a dateline cross. Depending on the particular SpatialOperations, there are a variety of range queries on DoublePoints to be done. The makeOverlapRatioValueSource(org.locationtech.spatial4j.shape.Rectangle, double) works by calculating the query bbox overlap percentage against the indexed shape overlap percentage. The indexed shape's coordinates are retrieved from LeafReader.getNumericDocValues(java.lang.String)."
legacy,"This uses 4 double fields for minX, maxX, minY, maxY and a boolean to mark a dateline cross. Depending on the particular SpatialOperations, there are a variety of range queries on DoublePoints to be done. The makeOverlapRatioValueSource(org.locationtech.spatial4j.shape.Rectangle, double) works by calculating the query bbox overlap percentage against the indexed shape overlap percentage. The indexed shape's coordinates are retrieved from LeafReader.getNumericDocValues(java.lang.String)."
legacy,Fields inherited from classorg.apache.lucene.spatial.SpatialStrategy ctx
legacy,"Methods inherited from classorg.apache.lucene.spatial.SpatialStrategy getFieldName, getSpatialContext, makeDistanceValueSource, makeRecipDistanceValueSource, toString"
legacy,Fields inherited from classorg.apache.lucene.spatial.SpatialStrategy ctx
legacy,"To perform range querying or filtering against a LegacyDoubleField, use LegacyNumericRangeQuery. To sort according to a LegacyDoubleField , use the normal numeric sort types, eg SortField.Type.DOUBLE. LegacyDoubleField values can also be loaded directly from LeafReader.getNumericDocValues(java.lang.String). You may add the same field name as an LegacyDoubleField to the same document more than once. Range querying and filtering will be the logical OR of all values; so a range query will hit all documents that have at least one value in the range. However sort behavior is not defined. If you need to sort, you should separately index a single-valued LegacyDoubleField . A LegacyDoubleField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 16, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"You may add the same field name as an LegacyDoubleField to the same document more than once. Range querying and filtering will be the logical OR of all values; so a range query will hit all documents that have at least one value in the range. However sort behavior is not defined. If you need to sort, you should separately index a single-valued LegacyDoubleField . A LegacyDoubleField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 16, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"A LegacyDoubleField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 16, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 16, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.document.Field org.apache.lucene.document.Field.Store
legacy,"Fields inherited from classorg.apache.lucene.document.Field fieldsData, name, tokenStream, type"
legacy,"Methods inherited from classorg.apache.lucene.document.Field binaryValue, fieldType, getCharSequenceValue, invertableType, name, numericValue, readerValue, setBytesValue, setBytesValue, setByteValue, setDoubleValue, setFloatValue, setIntValue, setLongValue, setReaderValue, setShortValue, setStringValue, storedValue, stringValue, tokenStreamValue, toString"
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.document.Field org.apache.lucene.document.Field.Store
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.document.Field org.apache.lucene.document.Field.Store
legacy,"Fields inherited from classorg.apache.lucene.document.Field fieldsData, name, tokenStream, type"
legacy,"Methods inherited from classorg.apache.lucene.document.Field binaryValue, fieldType, getCharSequenceValue, invertableType, name, numericValue, readerValue, setBytesValue, setBytesValue, setByteValue, setDoubleValue, setFloatValue, setIntValue, setLongValue, setReaderValue, setShortValue, setStringValue, storedValue, stringValue, tokenStreamValue, toString"
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.document.Field org.apache.lucene.document.Field.Store
legacy,"Methods inherited from classorg.apache.lucene.document.FieldType checkIfFrozen, docValuesType, freeze, getAttributes, indexOptions, omitNorms, pointDimensionCount, pointIndexDimensionCount, pointNumBytes, putAttribute, setDimensions, setDimensions, setDocValuesType, setIndexOptions, setOmitNorms, setStored, setStoreTermVectorOffsets, setStoreTermVectorPayloads, setStoreTermVectorPositions, setStoreTermVectors, setTokenized, setVectorAttributes, stored, storeTermVectorOffsets, storeTermVectorPayloads, storeTermVectorPositions, storeTermVectors, tokenized, vectorDimension, vectorEncoding, vectorSimilarityFunction"
legacy,"To perform range querying or filtering against a LegacyFloatField, use LegacyNumericRangeQuery. To sort according to a LegacyFloatField , use the normal numeric sort types, eg SortField.Type.FLOAT. LegacyFloatField values can also be loaded directly from LeafReader.getNumericDocValues(java.lang.String). You may add the same field name as an LegacyFloatField to the same document more than once. Range querying and filtering will be the logical OR of all values; so a range query will hit all documents that have at least one value in the range. However sort behavior is not defined. If you need to sort, you should separately index a single-valued LegacyFloatField . A LegacyFloatField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 8, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"You may add the same field name as an LegacyFloatField to the same document more than once. Range querying and filtering will be the logical OR of all values; so a range query will hit all documents that have at least one value in the range. However sort behavior is not defined. If you need to sort, you should separately index a single-valued LegacyFloatField . A LegacyFloatField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 8, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"A LegacyFloatField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 8, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 8, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.document.Field org.apache.lucene.document.Field.Store
legacy,"Fields inherited from classorg.apache.lucene.document.Field fieldsData, name, tokenStream, type"
legacy,"Methods inherited from classorg.apache.lucene.document.Field binaryValue, fieldType, getCharSequenceValue, invertableType, name, numericValue, readerValue, setBytesValue, setBytesValue, setByteValue, setDoubleValue, setFloatValue, setIntValue, setLongValue, setReaderValue, setShortValue, setStringValue, storedValue, stringValue, tokenStreamValue, toString"
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.document.Field org.apache.lucene.document.Field.Store
legacy,"To perform range querying or filtering against a LegacyIntField, use LegacyNumericRangeQuery. To sort according to a LegacyIntField , use the normal numeric sort types, eg SortField.Type.INT. LegacyIntField values can also be loaded directly from LeafReader.getNumericDocValues(java.lang.String). You may add the same field name as an LegacyIntField to the same document more than once. Range querying and filtering will be the logical OR of all values; so a range query will hit all documents that have at least one value in the range. However sort behavior is not defined. If you need to sort, you should separately index a single-valued LegacyIntField . An LegacyIntField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 8, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"You may add the same field name as an LegacyIntField to the same document more than once. Range querying and filtering will be the logical OR of all values; so a range query will hit all documents that have at least one value in the range. However sort behavior is not defined. If you need to sort, you should separately index a single-valued LegacyIntField . An LegacyIntField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 8, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"An LegacyIntField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 8, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 8, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.document.Field org.apache.lucene.document.Field.Store
legacy,"Fields inherited from classorg.apache.lucene.document.Field fieldsData, name, tokenStream, type"
legacy,"Methods inherited from classorg.apache.lucene.document.Field binaryValue, fieldType, getCharSequenceValue, invertableType, name, numericValue, readerValue, setBytesValue, setBytesValue, setByteValue, setDoubleValue, setFloatValue, setIntValue, setLongValue, setReaderValue, setShortValue, setStringValue, storedValue, stringValue, tokenStreamValue, toString"
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.document.Field org.apache.lucene.document.Field.Store
legacy,"Any type that can be converted to long can also be indexed. For example, date/time values represented by a Date can be translated into a long value using the Date.getTime() method. If you don't need millisecond precision, you can quantize the value, either by dividing the result of Date.getTime() or using the separate getters (for year, month, etc.) to construct an int or long value. To perform range querying or filtering against a LegacyLongField, use LegacyNumericRangeQuery. To sort according to a LegacyLongField , use the normal numeric sort types, eg SortField.Type.LONG. LegacyLongField values can also be loaded directly from LeafReader.getNumericDocValues(java.lang.String). You may add the same field name as an LegacyLongField to the same document more than once. Range querying and filtering will be the logical OR of all values; so a range query will hit all documents that have at least one value in the range. However sort behavior is not defined. If you need to sort, you should separately index a single-valued LegacyLongField . A LegacyLongField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 16, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"To perform range querying or filtering against a LegacyLongField, use LegacyNumericRangeQuery. To sort according to a LegacyLongField , use the normal numeric sort types, eg SortField.Type.LONG. LegacyLongField values can also be loaded directly from LeafReader.getNumericDocValues(java.lang.String). You may add the same field name as an LegacyLongField to the same document more than once. Range querying and filtering will be the logical OR of all values; so a range query will hit all documents that have at least one value in the range. However sort behavior is not defined. If you need to sort, you should separately index a single-valued LegacyLongField . A LegacyLongField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 16, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"You may add the same field name as an LegacyLongField to the same document more than once. Range querying and filtering will be the logical OR of all values; so a range query will hit all documents that have at least one value in the range. However sort behavior is not defined. If you need to sort, you should separately index a single-valued LegacyLongField . A LegacyLongField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 16, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"A LegacyLongField will consume somewhat more disk space in the index than an ordinary single-valued field. However, for a typical index that includes substantial textual content per document, this increase will likely be in the noise. Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 16, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"Within Lucene, each numeric value is indexed as a trie structure, where each term is logically assigned to larger and larger pre-defined brackets (which are simply lower-precision representations of the value). The step size between each successive bracket is called the precisionStep, measured in bits. Smaller precisionStep values result in larger number of brackets, which consumes more disk space in the index but may result in faster range search performance. The default value, 16, was selected for a reasonable tradeoff of disk space consumption versus performance. You can create a custom LegacyFieldType and invoke the LegacyFieldType.setNumericPrecisionStep(int) method if you'd like to change the value. Note that you must also specify a congruent value when creating LegacyNumericRangeQuery. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE, which produces one term per value. For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"For more information on the internals of numeric trie indexing, including the precisionStep configuration, see LegacyNumericRangeQuery. The format of indexed values is described in LegacyNumericUtils. If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"If you only need to sort by numeric value, and never run range querying/filtering, you can index using a precisionStep of Integer.MAX_VALUE. This will minimize disk space consumed. More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,"More advanced users can instead use LegacyNumericTokenStream directly, when indexing numbers. This class is a wrapper around this token stream type for easier, more intuitive usage."
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.document.Field org.apache.lucene.document.Field.Store
legacy,"Fields inherited from classorg.apache.lucene.document.Field fieldsData, name, tokenStream, type"
legacy,"Methods inherited from classorg.apache.lucene.document.Field binaryValue, fieldType, getCharSequenceValue, invertableType, name, numericValue, readerValue, setBytesValue, setBytesValue, setByteValue, setDoubleValue, setFloatValue, setIntValue, setLongValue, setReaderValue, setShortValue, setStringValue, storedValue, stringValue, tokenStreamValue, toString"
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.document.Field org.apache.lucene.document.Field.Store
legacy,"@Deprecated public final class LegacyNumericRangeQuery<T extends Number> extends org.apache.lucene.search.MultiTermQuery Deprecated. Instead index with IntPoint, LongPoint, FloatPoint, DoublePoint, and create range queries with IntPoint.newRangeQuery(), LongPoint.newRangeQuery(), FloatPoint.newRangeQuery(), DoublePoint.newRangeQuery() respectively. See PointValues for background information on Points. A Query that matches numeric values within a specified range. To use this, you must first index the numeric values using LegacyIntField, LegacyFloatField, LegacyLongField or LegacyDoubleField (expert: LegacyNumericTokenStream). If your terms are instead textual, you should use TermRangeQuery. You create a new LegacyNumericRangeQuery with the static factory methods, eg: Query q = LegacyNumericRangeQuery.newFloatRange(""weight"", 0.03f, 0.10f, true, true); matches all documents whose float valued ""weight"" field ranges from 0.03 to 0.10, inclusive. The performance of LegacyNumericRangeQuery is much better than the corresponding TermRangeQuery because the number of terms that must be searched is usually far fewer, thanks to trie indexing, described below. You can optionally specify a precisionStep when creating this query. This is necessary if you've changed this configuration from its default (4) during indexing. Lower values consume more disk space but speed up searching. Suitable values are between 1 and 8. A good starting point to test is 4, which is the default value for all Numeric* classes. See below for details. This query defaults to MultiTermQuery.CONSTANT_SCORE_REWRITE. With precision steps of 4, this query can be run with one of the BooleanQuery rewrite methods without changing BooleanQuery's default max clause count. How it works See the publication about panFMP, where this algorithm was described (referred to as TrieRangeQuery): Schindler, U, Diepenbroek, M, 2008. Generic XML-based Framework for Metadata Portals. Computers & Geosciences 34 (12), 1947-1955. doi:10.1016/j.cageo.2008.02.023 A quote from this paper: Because Apache Lucene is a full-text search engine and not a conventional database, it cannot handle numerical ranges (e.g., field value is inside user defined bounds, even dates are numerical values). We have developed an extension to Apache Lucene that stores the numerical values in a special string-encoded format with variable precision (all numerical values like doubles, longs, floats, and ints are converted to lexicographic sortable string representations and stored with different precisions (for a more detailed description of how the values are stored, see LegacyNumericUtils). A range is then divided recursively into multiple intervals for searching: The center of the range is searched only with the lowest possible precision in the trie, while the boundaries are matched more exactly. This reduces the number of terms dramatically. For the variant that stores long values in 8 different precisions (each reduced by 8 bits) that uses a lowest precision of 1 byte, the index contains only a maximum of 256 distinct values in the lowest precision. Overall, a range could consist of a theoretical maximum of 7*255*2 + 255 = 3825 distinct terms (when there is a term for every distinct value of an 8-byte-number in the index and the range covers almost all of them; a maximum of 255 distinct values is used because it would always be possible to reduce the full 256 values to one term with degraded precision). In practice, we have seen up to 300 terms in most cases (index with 500,000 metadata records and a uniform value distribution). Precision Step You can choose any precisionStep when encoding values. Lower step values mean more precisions and so more terms in index (and index gets larger). The number of indexed terms per value is (those are generated by LegacyNumericTokenStream): indexedTermsPerValue = ceil(bitsPerValue / precisionStep) As the lower precision terms are shared by many values, the additional terms only slightly grow the term dictionary (approx. 7% for precisionStep=4), but have a larger impact on the postings (the postings file will have more entries, as every document is linked to indexedTermsPerValue terms instead of one). The formula to estimate the growth of the term dictionary in comparison to one term per value: On the other hand, if the precisionStep is smaller, the maximum number of terms to match reduces, which optimizes query speed. The formula to calculate the maximum number of terms that will be visited while executing the query is: For longs stored using a precision step of 4, maxQueryTerms = 15*15*2 + 15 = 465, and for a precision step of 2, maxQueryTerms = 31*3*2 + 3 = 189. But the faster search speed is reduced by more seeking in the term enum of the index. Because of this, the ideal precisionStep value can only be found out by testing. Important: You can index with a lower precision step value and test search speed using a multiple of the original step value. Good values for precisionStep are depending on usage and data type: The default for all data types is 4, which is used, when no precisionStep is given. Ideal value in most cases for 64 bit data types (long, double) is 6 or 8. Ideal value in most cases for 32 bit data types (int, float) is 4. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE (see below). Steps 64 for long/double and 32 for int/float produces one token per value in the index and querying is as slow as a conventional TermRangeQuery. But it can be used to produce fields, that are solely used for sorting (in this case simply use Integer.MAX_VALUE as precisionStep). Using LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField for sorting is ideal, because building the field cache is much faster than with text-only numbers. These fields have one term per value and therefore also work with term enumeration for building distinct lists (e.g. facets / preselected values to search for). Sorting is also possible with range query optimized fields using one of the above precisionSteps. Comparisons of the different types of RangeQueries on an index with about 500,000 docs showed that TermRangeQuery in boolean rewrite mode (with raised BooleanQuery clause count) took about 30-40 secs to complete, TermRangeQuery in constant score filter rewrite mode took 5 secs and executing this class took <100ms to complete (on an Opteron64 machine, Java 1.5, 8 bit precision step). This query type was developed for a geographic portal, where the performance for e.g. bounding boxes or exact date/time stamps is important. Since: 2.9"
legacy,"The default for all data types is 4, which is used, when no precisionStep is given. Ideal value in most cases for 64 bit data types (long, double) is 6 or 8. Ideal value in most cases for 32 bit data types (int, float) is 4. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE (see below). Steps 64 for long/double and 32 for int/float produces one token per value in the index and querying is as slow as a conventional TermRangeQuery. But it can be used to produce fields, that are solely used for sorting (in this case simply use Integer.MAX_VALUE as precisionStep). Using LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField for sorting is ideal, because building the field cache is much faster than with text-only numbers. These fields have one term per value and therefore also work with term enumeration for building distinct lists (e.g. facets / preselected values to search for). Sorting is also possible with range query optimized fields using one of the above precisionSteps."
legacy,"Ideal value in most cases for 64 bit data types (long, double) is 6 or 8. Ideal value in most cases for 32 bit data types (int, float) is 4. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE (see below). Steps 64 for long/double and 32 for int/float produces one token per value in the index and querying is as slow as a conventional TermRangeQuery. But it can be used to produce fields, that are solely used for sorting (in this case simply use Integer.MAX_VALUE as precisionStep). Using LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField for sorting is ideal, because building the field cache is much faster than with text-only numbers. These fields have one term per value and therefore also work with term enumeration for building distinct lists (e.g. facets / preselected values to search for). Sorting is also possible with range query optimized fields using one of the above precisionSteps."
legacy,"Ideal value in most cases for 32 bit data types (int, float) is 4. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE (see below). Steps 64 for long/double and 32 for int/float produces one token per value in the index and querying is as slow as a conventional TermRangeQuery. But it can be used to produce fields, that are solely used for sorting (in this case simply use Integer.MAX_VALUE as precisionStep). Using LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField for sorting is ideal, because building the field cache is much faster than with text-only numbers. These fields have one term per value and therefore also work with term enumeration for building distinct lists (e.g. facets / preselected values to search for). Sorting is also possible with range query optimized fields using one of the above precisionSteps."
legacy,"For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE (see below). Steps 64 for long/double and 32 for int/float produces one token per value in the index and querying is as slow as a conventional TermRangeQuery. But it can be used to produce fields, that are solely used for sorting (in this case simply use Integer.MAX_VALUE as precisionStep). Using LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField for sorting is ideal, because building the field cache is much faster than with text-only numbers. These fields have one term per value and therefore also work with term enumeration for building distinct lists (e.g. facets / preselected values to search for). Sorting is also possible with range query optimized fields using one of the above precisionSteps."
legacy,"Steps 64 for long/double and 32 for int/float produces one token per value in the index and querying is as slow as a conventional TermRangeQuery. But it can be used to produce fields, that are solely used for sorting (in this case simply use Integer.MAX_VALUE as precisionStep). Using LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField for sorting is ideal, because building the field cache is much faster than with text-only numbers. These fields have one term per value and therefore also work with term enumeration for building distinct lists (e.g. facets / preselected values to search for). Sorting is also possible with range query optimized fields using one of the above precisionSteps."
legacy,"You create a new LegacyNumericRangeQuery with the static factory methods, eg: Query q = LegacyNumericRangeQuery.newFloatRange(""weight"", 0.03f, 0.10f, true, true); matches all documents whose float valued ""weight"" field ranges from 0.03 to 0.10, inclusive. The performance of LegacyNumericRangeQuery is much better than the corresponding TermRangeQuery because the number of terms that must be searched is usually far fewer, thanks to trie indexing, described below. You can optionally specify a precisionStep when creating this query. This is necessary if you've changed this configuration from its default (4) during indexing. Lower values consume more disk space but speed up searching. Suitable values are between 1 and 8. A good starting point to test is 4, which is the default value for all Numeric* classes. See below for details. This query defaults to MultiTermQuery.CONSTANT_SCORE_REWRITE. With precision steps of 4, this query can be run with one of the BooleanQuery rewrite methods without changing BooleanQuery's default max clause count. How it works See the publication about panFMP, where this algorithm was described (referred to as TrieRangeQuery): Schindler, U, Diepenbroek, M, 2008. Generic XML-based Framework for Metadata Portals. Computers & Geosciences 34 (12), 1947-1955. doi:10.1016/j.cageo.2008.02.023 A quote from this paper: Because Apache Lucene is a full-text search engine and not a conventional database, it cannot handle numerical ranges (e.g., field value is inside user defined bounds, even dates are numerical values). We have developed an extension to Apache Lucene that stores the numerical values in a special string-encoded format with variable precision (all numerical values like doubles, longs, floats, and ints are converted to lexicographic sortable string representations and stored with different precisions (for a more detailed description of how the values are stored, see LegacyNumericUtils). A range is then divided recursively into multiple intervals for searching: The center of the range is searched only with the lowest possible precision in the trie, while the boundaries are matched more exactly. This reduces the number of terms dramatically. For the variant that stores long values in 8 different precisions (each reduced by 8 bits) that uses a lowest precision of 1 byte, the index contains only a maximum of 256 distinct values in the lowest precision. Overall, a range could consist of a theoretical maximum of 7*255*2 + 255 = 3825 distinct terms (when there is a term for every distinct value of an 8-byte-number in the index and the range covers almost all of them; a maximum of 255 distinct values is used because it would always be possible to reduce the full 256 values to one term with degraded precision). In practice, we have seen up to 300 terms in most cases (index with 500,000 metadata records and a uniform value distribution). Precision Step You can choose any precisionStep when encoding values. Lower step values mean more precisions and so more terms in index (and index gets larger). The number of indexed terms per value is (those are generated by LegacyNumericTokenStream): indexedTermsPerValue = ceil(bitsPerValue / precisionStep) As the lower precision terms are shared by many values, the additional terms only slightly grow the term dictionary (approx. 7% for precisionStep=4), but have a larger impact on the postings (the postings file will have more entries, as every document is linked to indexedTermsPerValue terms instead of one). The formula to estimate the growth of the term dictionary in comparison to one term per value: On the other hand, if the precisionStep is smaller, the maximum number of terms to match reduces, which optimizes query speed. The formula to calculate the maximum number of terms that will be visited while executing the query is: For longs stored using a precision step of 4, maxQueryTerms = 15*15*2 + 15 = 465, and for a precision step of 2, maxQueryTerms = 31*3*2 + 3 = 189. But the faster search speed is reduced by more seeking in the term enum of the index. Because of this, the ideal precisionStep value can only be found out by testing. Important: You can index with a lower precision step value and test search speed using a multiple of the original step value. Good values for precisionStep are depending on usage and data type: The default for all data types is 4, which is used, when no precisionStep is given. Ideal value in most cases for 64 bit data types (long, double) is 6 or 8. Ideal value in most cases for 32 bit data types (int, float) is 4. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE (see below). Steps 64 for long/double and 32 for int/float produces one token per value in the index and querying is as slow as a conventional TermRangeQuery. But it can be used to produce fields, that are solely used for sorting (in this case simply use Integer.MAX_VALUE as precisionStep). Using LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField for sorting is ideal, because building the field cache is much faster than with text-only numbers. These fields have one term per value and therefore also work with term enumeration for building distinct lists (e.g. facets / preselected values to search for). Sorting is also possible with range query optimized fields using one of the above precisionSteps. Comparisons of the different types of RangeQueries on an index with about 500,000 docs showed that TermRangeQuery in boolean rewrite mode (with raised BooleanQuery clause count) took about 30-40 secs to complete, TermRangeQuery in constant score filter rewrite mode took 5 secs and executing this class took <100ms to complete (on an Opteron64 machine, Java 1.5, 8 bit precision step). This query type was developed for a geographic portal, where the performance for e.g. bounding boxes or exact date/time stamps is important."
legacy,"The performance of LegacyNumericRangeQuery is much better than the corresponding TermRangeQuery because the number of terms that must be searched is usually far fewer, thanks to trie indexing, described below. You can optionally specify a precisionStep when creating this query. This is necessary if you've changed this configuration from its default (4) during indexing. Lower values consume more disk space but speed up searching. Suitable values are between 1 and 8. A good starting point to test is 4, which is the default value for all Numeric* classes. See below for details. This query defaults to MultiTermQuery.CONSTANT_SCORE_REWRITE. With precision steps of 4, this query can be run with one of the BooleanQuery rewrite methods without changing BooleanQuery's default max clause count. How it works See the publication about panFMP, where this algorithm was described (referred to as TrieRangeQuery): Schindler, U, Diepenbroek, M, 2008. Generic XML-based Framework for Metadata Portals. Computers & Geosciences 34 (12), 1947-1955. doi:10.1016/j.cageo.2008.02.023 A quote from this paper: Because Apache Lucene is a full-text search engine and not a conventional database, it cannot handle numerical ranges (e.g., field value is inside user defined bounds, even dates are numerical values). We have developed an extension to Apache Lucene that stores the numerical values in a special string-encoded format with variable precision (all numerical values like doubles, longs, floats, and ints are converted to lexicographic sortable string representations and stored with different precisions (for a more detailed description of how the values are stored, see LegacyNumericUtils). A range is then divided recursively into multiple intervals for searching: The center of the range is searched only with the lowest possible precision in the trie, while the boundaries are matched more exactly. This reduces the number of terms dramatically. For the variant that stores long values in 8 different precisions (each reduced by 8 bits) that uses a lowest precision of 1 byte, the index contains only a maximum of 256 distinct values in the lowest precision. Overall, a range could consist of a theoretical maximum of 7*255*2 + 255 = 3825 distinct terms (when there is a term for every distinct value of an 8-byte-number in the index and the range covers almost all of them; a maximum of 255 distinct values is used because it would always be possible to reduce the full 256 values to one term with degraded precision). In practice, we have seen up to 300 terms in most cases (index with 500,000 metadata records and a uniform value distribution). Precision Step You can choose any precisionStep when encoding values. Lower step values mean more precisions and so more terms in index (and index gets larger). The number of indexed terms per value is (those are generated by LegacyNumericTokenStream): indexedTermsPerValue = ceil(bitsPerValue / precisionStep) As the lower precision terms are shared by many values, the additional terms only slightly grow the term dictionary (approx. 7% for precisionStep=4), but have a larger impact on the postings (the postings file will have more entries, as every document is linked to indexedTermsPerValue terms instead of one). The formula to estimate the growth of the term dictionary in comparison to one term per value: On the other hand, if the precisionStep is smaller, the maximum number of terms to match reduces, which optimizes query speed. The formula to calculate the maximum number of terms that will be visited while executing the query is: For longs stored using a precision step of 4, maxQueryTerms = 15*15*2 + 15 = 465, and for a precision step of 2, maxQueryTerms = 31*3*2 + 3 = 189. But the faster search speed is reduced by more seeking in the term enum of the index. Because of this, the ideal precisionStep value can only be found out by testing. Important: You can index with a lower precision step value and test search speed using a multiple of the original step value. Good values for precisionStep are depending on usage and data type: The default for all data types is 4, which is used, when no precisionStep is given. Ideal value in most cases for 64 bit data types (long, double) is 6 or 8. Ideal value in most cases for 32 bit data types (int, float) is 4. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE (see below). Steps 64 for long/double and 32 for int/float produces one token per value in the index and querying is as slow as a conventional TermRangeQuery. But it can be used to produce fields, that are solely used for sorting (in this case simply use Integer.MAX_VALUE as precisionStep). Using LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField for sorting is ideal, because building the field cache is much faster than with text-only numbers. These fields have one term per value and therefore also work with term enumeration for building distinct lists (e.g. facets / preselected values to search for). Sorting is also possible with range query optimized fields using one of the above precisionSteps. Comparisons of the different types of RangeQueries on an index with about 500,000 docs showed that TermRangeQuery in boolean rewrite mode (with raised BooleanQuery clause count) took about 30-40 secs to complete, TermRangeQuery in constant score filter rewrite mode took 5 secs and executing this class took <100ms to complete (on an Opteron64 machine, Java 1.5, 8 bit precision step). This query type was developed for a geographic portal, where the performance for e.g. bounding boxes or exact date/time stamps is important."
legacy,"You can optionally specify a precisionStep when creating this query. This is necessary if you've changed this configuration from its default (4) during indexing. Lower values consume more disk space but speed up searching. Suitable values are between 1 and 8. A good starting point to test is 4, which is the default value for all Numeric* classes. See below for details. This query defaults to MultiTermQuery.CONSTANT_SCORE_REWRITE. With precision steps of 4, this query can be run with one of the BooleanQuery rewrite methods without changing BooleanQuery's default max clause count. How it works See the publication about panFMP, where this algorithm was described (referred to as TrieRangeQuery): Schindler, U, Diepenbroek, M, 2008. Generic XML-based Framework for Metadata Portals. Computers & Geosciences 34 (12), 1947-1955. doi:10.1016/j.cageo.2008.02.023 A quote from this paper: Because Apache Lucene is a full-text search engine and not a conventional database, it cannot handle numerical ranges (e.g., field value is inside user defined bounds, even dates are numerical values). We have developed an extension to Apache Lucene that stores the numerical values in a special string-encoded format with variable precision (all numerical values like doubles, longs, floats, and ints are converted to lexicographic sortable string representations and stored with different precisions (for a more detailed description of how the values are stored, see LegacyNumericUtils). A range is then divided recursively into multiple intervals for searching: The center of the range is searched only with the lowest possible precision in the trie, while the boundaries are matched more exactly. This reduces the number of terms dramatically. For the variant that stores long values in 8 different precisions (each reduced by 8 bits) that uses a lowest precision of 1 byte, the index contains only a maximum of 256 distinct values in the lowest precision. Overall, a range could consist of a theoretical maximum of 7*255*2 + 255 = 3825 distinct terms (when there is a term for every distinct value of an 8-byte-number in the index and the range covers almost all of them; a maximum of 255 distinct values is used because it would always be possible to reduce the full 256 values to one term with degraded precision). In practice, we have seen up to 300 terms in most cases (index with 500,000 metadata records and a uniform value distribution). Precision Step You can choose any precisionStep when encoding values. Lower step values mean more precisions and so more terms in index (and index gets larger). The number of indexed terms per value is (those are generated by LegacyNumericTokenStream): indexedTermsPerValue = ceil(bitsPerValue / precisionStep) As the lower precision terms are shared by many values, the additional terms only slightly grow the term dictionary (approx. 7% for precisionStep=4), but have a larger impact on the postings (the postings file will have more entries, as every document is linked to indexedTermsPerValue terms instead of one). The formula to estimate the growth of the term dictionary in comparison to one term per value: On the other hand, if the precisionStep is smaller, the maximum number of terms to match reduces, which optimizes query speed. The formula to calculate the maximum number of terms that will be visited while executing the query is: For longs stored using a precision step of 4, maxQueryTerms = 15*15*2 + 15 = 465, and for a precision step of 2, maxQueryTerms = 31*3*2 + 3 = 189. But the faster search speed is reduced by more seeking in the term enum of the index. Because of this, the ideal precisionStep value can only be found out by testing. Important: You can index with a lower precision step value and test search speed using a multiple of the original step value. Good values for precisionStep are depending on usage and data type: The default for all data types is 4, which is used, when no precisionStep is given. Ideal value in most cases for 64 bit data types (long, double) is 6 or 8. Ideal value in most cases for 32 bit data types (int, float) is 4. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE (see below). Steps 64 for long/double and 32 for int/float produces one token per value in the index and querying is as slow as a conventional TermRangeQuery. But it can be used to produce fields, that are solely used for sorting (in this case simply use Integer.MAX_VALUE as precisionStep). Using LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField for sorting is ideal, because building the field cache is much faster than with text-only numbers. These fields have one term per value and therefore also work with term enumeration for building distinct lists (e.g. facets / preselected values to search for). Sorting is also possible with range query optimized fields using one of the above precisionSteps. Comparisons of the different types of RangeQueries on an index with about 500,000 docs showed that TermRangeQuery in boolean rewrite mode (with raised BooleanQuery clause count) took about 30-40 secs to complete, TermRangeQuery in constant score filter rewrite mode took 5 secs and executing this class took <100ms to complete (on an Opteron64 machine, Java 1.5, 8 bit precision step). This query type was developed for a geographic portal, where the performance for e.g. bounding boxes or exact date/time stamps is important."
legacy,"This query defaults to MultiTermQuery.CONSTANT_SCORE_REWRITE. With precision steps of 4, this query can be run with one of the BooleanQuery rewrite methods without changing BooleanQuery's default max clause count. How it works See the publication about panFMP, where this algorithm was described (referred to as TrieRangeQuery): Schindler, U, Diepenbroek, M, 2008. Generic XML-based Framework for Metadata Portals. Computers & Geosciences 34 (12), 1947-1955. doi:10.1016/j.cageo.2008.02.023 A quote from this paper: Because Apache Lucene is a full-text search engine and not a conventional database, it cannot handle numerical ranges (e.g., field value is inside user defined bounds, even dates are numerical values). We have developed an extension to Apache Lucene that stores the numerical values in a special string-encoded format with variable precision (all numerical values like doubles, longs, floats, and ints are converted to lexicographic sortable string representations and stored with different precisions (for a more detailed description of how the values are stored, see LegacyNumericUtils). A range is then divided recursively into multiple intervals for searching: The center of the range is searched only with the lowest possible precision in the trie, while the boundaries are matched more exactly. This reduces the number of terms dramatically. For the variant that stores long values in 8 different precisions (each reduced by 8 bits) that uses a lowest precision of 1 byte, the index contains only a maximum of 256 distinct values in the lowest precision. Overall, a range could consist of a theoretical maximum of 7*255*2 + 255 = 3825 distinct terms (when there is a term for every distinct value of an 8-byte-number in the index and the range covers almost all of them; a maximum of 255 distinct values is used because it would always be possible to reduce the full 256 values to one term with degraded precision). In practice, we have seen up to 300 terms in most cases (index with 500,000 metadata records and a uniform value distribution). Precision Step You can choose any precisionStep when encoding values. Lower step values mean more precisions and so more terms in index (and index gets larger). The number of indexed terms per value is (those are generated by LegacyNumericTokenStream): indexedTermsPerValue = ceil(bitsPerValue / precisionStep) As the lower precision terms are shared by many values, the additional terms only slightly grow the term dictionary (approx. 7% for precisionStep=4), but have a larger impact on the postings (the postings file will have more entries, as every document is linked to indexedTermsPerValue terms instead of one). The formula to estimate the growth of the term dictionary in comparison to one term per value: On the other hand, if the precisionStep is smaller, the maximum number of terms to match reduces, which optimizes query speed. The formula to calculate the maximum number of terms that will be visited while executing the query is: For longs stored using a precision step of 4, maxQueryTerms = 15*15*2 + 15 = 465, and for a precision step of 2, maxQueryTerms = 31*3*2 + 3 = 189. But the faster search speed is reduced by more seeking in the term enum of the index. Because of this, the ideal precisionStep value can only be found out by testing. Important: You can index with a lower precision step value and test search speed using a multiple of the original step value. Good values for precisionStep are depending on usage and data type: The default for all data types is 4, which is used, when no precisionStep is given. Ideal value in most cases for 64 bit data types (long, double) is 6 or 8. Ideal value in most cases for 32 bit data types (int, float) is 4. For low cardinality fields larger precision steps are good. If the cardinality is < 100, it is fair to use Integer.MAX_VALUE (see below). Steps 64 for long/double and 32 for int/float produces one token per value in the index and querying is as slow as a conventional TermRangeQuery. But it can be used to produce fields, that are solely used for sorting (in this case simply use Integer.MAX_VALUE as precisionStep). Using LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField for sorting is ideal, because building the field cache is much faster than with text-only numbers. These fields have one term per value and therefore also work with term enumeration for building distinct lists (e.g. facets / preselected values to search for). Sorting is also possible with range query optimized fields using one of the above precisionSteps. Comparisons of the different types of RangeQueries on an index with about 500,000 docs showed that TermRangeQuery in boolean rewrite mode (with raised BooleanQuery clause count) took about 30-40 secs to complete, TermRangeQuery in constant score filter rewrite mode took 5 secs and executing this class took <100ms to complete (on an Opteron64 machine, Java 1.5, 8 bit precision step). This query type was developed for a geographic portal, where the performance for e.g. bounding boxes or exact date/time stamps is important."
legacy,"Note that for simple usage, LegacyIntField, LegacyLongField, LegacyFloatField or LegacyDoubleField is recommended. These fields disable norms and term freqs, as they are not usually needed during searching. If you need to change these settings, you should use this class. Here's an example usage, for an int field: FieldType fieldType = new FieldType(TextField.TYPE_NOT_STORED); fieldType.setOmitNorms(true); fieldType.setIndexOptions(IndexOptions.DOCS_ONLY); Field field = new Field(name, new LegacyNumericTokenStream(precisionStep).setIntValue(value), fieldType); document.add(field); For optimal performance, re-use the TokenStream and Field instance for more than one document: LegacyNumericTokenStream stream = new LegacyNumericTokenStream(precisionStep); FieldType fieldType = new FieldType(TextField.TYPE_NOT_STORED); fieldType.setOmitNorms(true); fieldType.setIndexOptions(IndexOptions.DOCS_ONLY); Field field = new Field(name, stream, fieldType); Document document = new Document(); document.add(field); for(all documents) { stream.setIntValue(value) writer.addDocument(document); } This stream is not intended to be used in analyzers; it's more for iterating the different precisions during indexing a specific numeric value. NOTE: as token streams are only consumed once the document is added to the index, if you index more than one numeric field, use a separate LegacyNumericTokenStream instance for each. See LegacyNumericRangeQuery for more details on the precisionStep parameter as well as how numeric fields work under the hood."
legacy,"Here's an example usage, for an int field: FieldType fieldType = new FieldType(TextField.TYPE_NOT_STORED); fieldType.setOmitNorms(true); fieldType.setIndexOptions(IndexOptions.DOCS_ONLY); Field field = new Field(name, new LegacyNumericTokenStream(precisionStep).setIntValue(value), fieldType); document.add(field); For optimal performance, re-use the TokenStream and Field instance for more than one document: LegacyNumericTokenStream stream = new LegacyNumericTokenStream(precisionStep); FieldType fieldType = new FieldType(TextField.TYPE_NOT_STORED); fieldType.setOmitNorms(true); fieldType.setIndexOptions(IndexOptions.DOCS_ONLY); Field field = new Field(name, stream, fieldType); Document document = new Document(); document.add(field); for(all documents) { stream.setIntValue(value) writer.addDocument(document); } This stream is not intended to be used in analyzers; it's more for iterating the different precisions during indexing a specific numeric value. NOTE: as token streams are only consumed once the document is added to the index, if you index more than one numeric field, use a separate LegacyNumericTokenStream instance for each. See LegacyNumericRangeQuery for more details on the precisionStep parameter as well as how numeric fields work under the hood."
legacy,"For optimal performance, re-use the TokenStream and Field instance for more than one document: LegacyNumericTokenStream stream = new LegacyNumericTokenStream(precisionStep); FieldType fieldType = new FieldType(TextField.TYPE_NOT_STORED); fieldType.setOmitNorms(true); fieldType.setIndexOptions(IndexOptions.DOCS_ONLY); Field field = new Field(name, stream, fieldType); Document document = new Document(); document.add(field); for(all documents) { stream.setIntValue(value) writer.addDocument(document); } This stream is not intended to be used in analyzers; it's more for iterating the different precisions during indexing a specific numeric value. NOTE: as token streams are only consumed once the document is added to the index, if you index more than one numeric field, use a separate LegacyNumericTokenStream instance for each. See LegacyNumericRangeQuery for more details on the precisionStep parameter as well as how numeric fields work under the hood."
legacy,"This stream is not intended to be used in analyzers; it's more for iterating the different precisions during indexing a specific numeric value. NOTE: as token streams are only consumed once the document is added to the index, if you index more than one numeric field, use a separate LegacyNumericTokenStream instance for each. See LegacyNumericRangeQuery for more details on the precisionStep parameter as well as how numeric fields work under the hood."
legacy,"NOTE: as token streams are only consumed once the document is added to the index, if you index more than one numeric field, use a separate LegacyNumericTokenStream instance for each. See LegacyNumericRangeQuery for more details on the precisionStep parameter as well as how numeric fields work under the hood."
legacy,See LegacyNumericRangeQuery for more details on the precisionStep parameter as well as how numeric fields work under the hood.
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.util.AttributeSource org.apache.lucene.util.AttributeSource.State
legacy,Fields inherited from classorg.apache.lucene.analysis.TokenStream DEFAULT_TOKEN_ATTRIBUTE_FACTORY
legacy,"Methods inherited from classorg.apache.lucene.analysis.TokenStream close, end"
legacy,"Methods inherited from classorg.apache.lucene.util.AttributeSource addAttribute, addAttributeImpl, captureState, clearAttributes, cloneAttributes, copyTo, endAttributes, equals, getAttribute, getAttributeClassesIterator, getAttributeFactory, getAttributeImplsIterator, hasAttribute, hasAttributes, hashCode, reflectAsString, reflectWith, removeAllAttributes, restoreState"
legacy,Nested classes/interfaces inherited from classorg.apache.lucene.util.AttributeSource org.apache.lucene.util.AttributeSource.State
legacy,"Methods inherited from classorg.apache.lucene.util.AttributeImpl end, reflectAsString"
legacy,"@Deprecated public final class LegacyNumericUtils extends Object Deprecated. Please use PointValues instead. This is a helper class to generate prefix-encoded representations for numerical values and supplies converters to represent float/double values as sortable integers/longs. To quickly execute range queries in Apache Lucene, a range is divided recursively into multiple intervals for searching: The center of the range is searched only with the lowest possible precision in the trie, while the boundaries are matched more exactly. This reduces the number of terms dramatically. This class generates terms to achieve this: First the numerical integer values need to be converted to bytes. For that integer values (32 bit or 64 bit) are made unsigned and the bits are converted to ASCII chars with each 7 bit. The resulting byte[] is sortable like the original integer value (even using UTF-8 sort order). Each value is also prefixed (in the first char) by the shift value (number of bits removed) used during encoding. For easy usage, the trie algorithm is implemented for indexing inside LegacyNumericTokenStream that can index int, long , float, and double. For querying, LegacyNumericRangeQuery implements the query part for the same data types. Since: 2.9, API changed non backwards-compliant in 4.0 NOTE: This API is for internal purposes only and might change in incompatible ways in the next release."
legacy,"To quickly execute range queries in Apache Lucene, a range is divided recursively into multiple intervals for searching: The center of the range is searched only with the lowest possible precision in the trie, while the boundaries are matched more exactly. This reduces the number of terms dramatically. This class generates terms to achieve this: First the numerical integer values need to be converted to bytes. For that integer values (32 bit or 64 bit) are made unsigned and the bits are converted to ASCII chars with each 7 bit. The resulting byte[] is sortable like the original integer value (even using UTF-8 sort order). Each value is also prefixed (in the first char) by the shift value (number of bits removed) used during encoding. For easy usage, the trie algorithm is implemented for indexing inside LegacyNumericTokenStream that can index int, long , float, and double. For querying, LegacyNumericRangeQuery implements the query part for the same data types."
legacy,"This class generates terms to achieve this: First the numerical integer values need to be converted to bytes. For that integer values (32 bit or 64 bit) are made unsigned and the bits are converted to ASCII chars with each 7 bit. The resulting byte[] is sortable like the original integer value (even using UTF-8 sort order). Each value is also prefixed (in the first char) by the shift value (number of bits removed) used during encoding. For easy usage, the trie algorithm is implemented for indexing inside LegacyNumericTokenStream that can index int, long , float, and double. For querying, LegacyNumericRangeQuery implements the query part for the same data types."
legacy,"For easy usage, the trie algorithm is implemented for indexing inside LegacyNumericTokenStream that can index int, long , float, and double. For querying, LegacyNumericRangeQuery implements the query part for the same data types."
logging,"Once the size is reached, it will overwrite previous entries"
logging,"Methods inherited from interfacejava.lang.Iterable forEach, spliterator"
logging,public class DeprecationLog extends Object Utility to log a deprecation.
logging,public class ListenerConfig extends Object
logging,"Direct Known Subclasses: JulWatcher, Log4j2Watcher public abstract class LogWatcher<E> extends Object A Class to monitor Logging events and hold N events in memory This is abstract so we can support both JUL and Log4j2 (and other logging platforms)"
logging,This is abstract so we can support both JUL and Log4j2 (and other logging platforms)
logging,public class LogWatcherConfig extends Object Defines the configuration of a LogWatcher
logging,"public class MDCLoggingContext extends Object Set's per thread context info for logging. Nested calls will use the top level parent for all context. The first caller always owns the context until it calls clear(). Always call setCore(SolrCore) or setCoreDescriptor(CoreContainer, CoreDescriptor) and then clear() in a finally block."
logging,public class JulWatcher extends LogWatcher<LogRecord>
logging,public final class RecordHandler extends Handler
logging,"Methods inherited from classjava.util.logging.Handler getEncoding, getErrorManager, getFilter, getFormatter, getLevel, isLoggable, reportError, setEncoding, setErrorManager, setFilter, setFormatter, setLevel"
logging,public class Log4j2Watcher extends LogWatcher<org.apache.logging.log4j.core.LogEvent>
logging,Nested classes/interfaces inherited from classorg.apache.logging.log4j.core.appender.AbstractAppender org.apache.logging.log4j.core.appender.AbstractAppender.Builder<B extends org.apache.logging.log4j.core.appender.AbstractAppender.Builder<B>>
logging,Nested classes/interfaces inherited from interfaceorg.apache.logging.log4j.core.LifeCycle org.apache.logging.log4j.core.LifeCycle.State
logging,"Fields inherited from classorg.apache.logging.log4j.core.AbstractLifeCycle DEFAULT_STOP_TIMEOUT, DEFAULT_STOP_TIMEUNIT, LOGGER"
logging,"Fields inherited from interfaceorg.apache.logging.log4j.core.Appender ELEMENT_TYPE, EMPTY_ARRAY"
logging,"Methods inherited from classorg.apache.logging.log4j.core.appender.AbstractAppender error, error, error, getHandler, getLayout, getName, ignoreExceptions, parseInt, requiresLocation, setHandler, toSerializable, toString"
logging,"Methods inherited from classorg.apache.logging.log4j.core.filter.AbstractFilterable addFilter, getFilter, getPropertyArray, hasFilter, isFiltered, removeFilter, start, stop, stop"
logging,"Methods inherited from classorg.apache.logging.log4j.core.AbstractLifeCycle equalsImpl, getState, getStatusLogger, hashCodeImpl, initialize, isInitialized, isStarted, isStarting, isStopped, isStopping, setStarted, setStarting, setState, setStopped, setStopping, stop, stop"
logging,"Methods inherited from interfaceorg.apache.logging.log4j.core.LifeCycle getState, initialize, isStarted, isStopped, start, stop"
logging,Nested classes/interfaces inherited from classorg.apache.logging.log4j.core.appender.AbstractAppender org.apache.logging.log4j.core.appender.AbstractAppender.Builder<B extends org.apache.logging.log4j.core.appender.AbstractAppender.Builder<B>>
logging,Nested classes/interfaces inherited from interfaceorg.apache.logging.log4j.core.LifeCycle org.apache.logging.log4j.core.LifeCycle.State
metrics,Enclosing class: AggregateMetric public static class AggregateMetric.Update extends Object Simple class to represent current value and how many times it was set.
metrics,Nested classes/interfaces inherited from classcom.codahale.metrics.Timer com.codahale.metrics.Timer.Context
metrics,Nested classes/interfaces inherited from classcom.codahale.metrics.Timer com.codahale.metrics.Timer.Context
metrics,Methods inherited from interfacejava.io.Closeable close
metrics,"Note: this awkwardly extends Gauge and not Metric because awkwardly Metric instances are not supported by MetricRegistryListener :( Note 2: values added to this metric map should belong to the list of types supported by JMX: OpenType.ALLOWED_CLASSNAMES_LIST, otherwise only their toString() representation will be shown in JConsole."
metrics,"Note 2: values added to this metric map should belong to the list of types supported by JMX: OpenType.ALLOWED_CLASSNAMES_LIST, otherwise only their toString() representation will be shown in JConsole."
metrics,clock - (string) can be set to MetricSuppliers.CLOCK_USER for Clock.UserTimeClock or MetricSuppliers.CLOCK_CPU for MetricSuppliers.CpuTimeClock. If not set then the value of Clock.defaultClock() will be used.
metrics,clock - (string) can be set to MetricSuppliers.CLOCK_USER for Clock.UserTimeClock or MetricSuppliers.CLOCK_CPU for MetricSuppliers.CpuTimeClock. If not set then the value of Clock.defaultClock() will be used.
metrics,"public class MetricSuppliers extends Object Helper class for constructing instances of MetricRegistry.MetricSupplier based on plugin configuration. This allows us to customize eg. Reservoir implementations and parameters for timers and histograms. Custom supplier implementations must provide a zero-args constructor, and may optionally implement PluginInfoInitialized interface for configuration - if they don't then SolrPluginUtils.invokeSetters(Object, Iterable, boolean) will be used for initialization."
metrics,"Custom supplier implementations must provide a zero-args constructor, and may optionally implement PluginInfoInitialized interface for configuration - if they don't then SolrPluginUtils.invokeSetters(Object, Iterable, boolean) will be used for initialization."
metrics,Methods inherited from interfacejava.io.Closeable close
metrics,Methods inherited from interfacejava.io.Closeable close
metrics,"public class SolrDelegateRegistryMetricsContext extends SolrMetricsContext This class represents a metrics context that is delegate aware in that it is aware of multiple metric registries, a primary and a delegate. This enables creating metrics that are tracked at multiple levels, i.e. core-level and node-level. This class will create instances of new Timer, Meter, Counter, Histogram implementations that hold references to both primary and delegate implementations of corresponding classes. The DelegateRegistry* metric classes are just pass-through to two different implementations. As such the DelegateRegistry* metric classes do not hold any metric data themselves. See Also: SolrMetricsContext"
metrics,public final class SolrMetricInfo extends Object Wraps meta-data for a metric.
metrics,Fields inherited from interfacecom.codahale.metrics.MetricFilter ALL
metrics,Fields inherited from interfacecom.codahale.metrics.MetricFilter ALL
metrics,"public class SolrMetricManager extends Object This class maintains a repository of named MetricRegistry instances, and provides several helper methods for managing various aspects of metrics reporting: registry creation, clearing and removal, creation of most common metric implementations, management of SolrMetricReporter-s specific to a named registry. MetricRegistry instances are automatically created when first referenced by name. Similarly, instances of Metric implementations, such as Meter, Counter, Timer and Histogram are automatically created and registered under hierarchical names, in a specified registry, when meter(SolrMetricsContext, String, String, String...) and other similar methods are called. This class enforces a common prefix (REGISTRY_NAME_PREFIX) in all registry names. Solr uses several different registries for collecting metrics belonging to different groups, using SolrInfoBean.Group as the main name of the registry (plus the above-mentioned prefix). Instances of SolrMetricManager are created for each CoreContainer, and most registries are local to each instance, with the exception of two global registries: solr.jetty and solr.jvm, which are shared between all CoreContainer-s"
metrics,"registry creation, clearing and removal, creation of most common metric implementations, management of SolrMetricReporter-s specific to a named registry."
metrics,"creation of most common metric implementations, management of SolrMetricReporter-s specific to a named registry."
metrics,management of SolrMetricReporter-s specific to a named registry.
metrics,"registry creation, clearing and removal, creation of most common metric implementations, management of SolrMetricReporter-s specific to a named registry."
metrics,"creation of most common metric implementations, management of SolrMetricReporter-s specific to a named registry."
metrics,management of SolrMetricReporter-s specific to a named registry.
metrics,"This class enforces a common prefix (REGISTRY_NAME_PREFIX) in all registry names. Solr uses several different registries for collecting metrics belonging to different groups, using SolrInfoBean.Group as the main name of the registry (plus the above-mentioned prefix). Instances of SolrMetricManager are created for each CoreContainer, and most registries are local to each instance, with the exception of two global registries: solr.jetty and solr.jvm, which are shared between all CoreContainer-s"
metrics,"Solr uses several different registries for collecting metrics belonging to different groups, using SolrInfoBean.Group as the main name of the registry (plus the above-mentioned prefix). Instances of SolrMetricManager are created for each CoreContainer, and most registries are local to each instance, with the exception of two global registries: solr.jetty and solr.jvm, which are shared between all CoreContainer-s"
metrics,Fields inherited from interfacecom.codahale.metrics.MetricFilter ALL
metrics,Fields inherited from interfacecom.codahale.metrics.MetricFilter ALL
metrics,Fields inherited from interfacecom.codahale.metrics.MetricFilter ALL
metrics,Fields inherited from interfacecom.codahale.metrics.MetricFilter ALL
metrics,Fields inherited from interfacecom.codahale.metrics.MetricFilter ALL
metrics,Fields inherited from interfacecom.codahale.metrics.MetricFilter ALL
metrics,Methods inherited from interfacejava.io.Closeable close
metrics,Direct Known Subclasses: SolrDelegateRegistryMetricsContext public class SolrMetricsContext extends Object This class represents a metrics context that ties together components with the same life-cycle and provides convenient access to the metric registry. Additionally it's used for registering and reporting metrics specific to the components that use the same instance of context.
metrics,Additionally it's used for registering and reporting metrics specific to the components that use the same instance of context.
metrics,"Direct Known Subclasses: SolrCoreMetric, SolrJettyMetric, SolrJvmMetric, SolrNodeMetric, SolrNoOpMetric public abstract class SolrMetric extends Object Base class is a wrapper to categorize and export Metric to DataPointSnapshot and register to a SolrPrometheusFormatter. MetricRegistry does not support tags unlike prometheus. Metrics registered to the registry need to be parsed out from the metric name to be exported to DataPointSnapshot"
metrics,public class SolrNoOpMetric extends SolrMetric
metrics,"Direct Known Subclasses: SolrPrometheusCoreFormatter, SolrPrometheusJettyFormatter, SolrPrometheusJvmFormatter, SolrPrometheusNodeFormatter public abstract class SolrPrometheusFormatter extends Object Base class for all SolrPrometheusFormatter holding MetricSnapshots. Can export Metric to MetricSnapshot to be outputted for PrometheusResponseWriter"
metrics,public class SolrCoreCacheMetric extends SolrCoreMetric Dropwizard metrics of name CACHE.*
metrics,public class SolrCoreHandlerMetric extends SolrCoreMetric Dropwizard metrics of name ADMIN/QUERY/UPDATE/REPLICATION.*
metrics,public class SolrCoreHighlighterMetric extends SolrCoreMetric Dropwizard metrics of name HIGHLIGHTER.*
metrics,public class SolrCoreIndexMetric extends SolrCoreMetric Dropwizard metrics of name INDEX.*
metrics,"Direct Known Subclasses: SolrCoreCacheMetric, SolrCoreHandlerMetric, SolrCoreHighlighterMetric, SolrCoreIndexMetric, SolrCoreSearcherMetric, SolrCoreTlogMetric public abstract class SolrCoreMetric extends SolrMetric Base class is a wrapper to export a solr.core Metric"
metrics,public class SolrCoreSearcherMetric extends SolrCoreMetric Dropwizard metrics of name SEARCHER.*
metrics,public class SolrCoreTlogMetric extends SolrCoreMetric Dropwizard metrics of name TLOG.*
metrics,public class SolrJettyDispatchesMetric extends SolrJettyMetric
metrics,"Direct Known Subclasses: SolrJettyDispatchesMetric, SolrJettyReqRespMetric public abstract class SolrJettyMetric extends SolrMetric Base class is a wrapper to export a solr.jetty Metric"
metrics,public class SolrJettyReqRespMetric extends SolrJettyMetric
metrics,public class SolrPrometheusJettyFormatter extends SolrPrometheusFormatter This class maintains a MetricSnapshots exported from solr.jetty MetricRegistry
metrics,public class SolrJvmBuffersMetric extends SolrJvmMetric
metrics,public class SolrJvmGcMetrics extends SolrJvmMetric
metrics,public class SolrJvmMemoryMetric extends SolrJvmMetric
metrics,"Direct Known Subclasses: SolrJvmBuffersMetric, SolrJvmGcMetrics, SolrJvmMemoryMetric, SolrJvmOsMetric public abstract class SolrJvmMetric extends SolrMetric Base class is a wrapper to export a solr.jvm Metric"
metrics,public class SolrJvmOsMetric extends SolrJvmMetric
metrics,public class SolrNodeContainerMetric extends SolrNodeMetric
metrics,public class SolrNodeHandlerMetric extends SolrNodeMetric
metrics,"Direct Known Subclasses: SolrNodeContainerMetric, SolrNodeHandlerMetric public abstract class SolrNodeMetric extends SolrMetric Base class is a wrapper to export a solr.node Metric"
metrics,NOTE: JmxReporter that this class uses exports only newly added metrics (it doesn't process already existing metrics in a registry)
metrics,"period: (optional, int) number of seconds between reports, default is 60, prefix: (optional, str) prefix for metric names, in addition to registry name. Default is none, ie. just registry name. filter: (optional, str) if not empty only metric names that start with this value will be reported, default is all metrics from a registry, logger: (optional, str) logger name to use. Default is the metrics group, eg. solr.jvm, solr.core, etc"
metrics,"prefix: (optional, str) prefix for metric names, in addition to registry name. Default is none, ie. just registry name. filter: (optional, str) if not empty only metric names that start with this value will be reported, default is all metrics from a registry, logger: (optional, str) logger name to use. Default is the metrics group, eg. solr.jvm, solr.core, etc"
metrics,"filter: (optional, str) if not empty only metric names that start with this value will be reported, default is all metrics from a registry, logger: (optional, str) logger name to use. Default is the metrics group, eg. solr.jvm, solr.core, etc"
metrics,"logger: (optional, str) logger name to use. Default is the metrics group, eg. solr.jvm, solr.core, etc"
metrics,"period: (optional, int) number of seconds between reports, default is 60, prefix: (optional, str) prefix for metric names, in addition to registry name. Default is none, ie. just registry name. filter: (optional, str) if not empty only metric names that start with this value will be reported, default is all metrics from a registry, logger: (optional, str) logger name to use. Default is the metrics group, eg. solr.jvm, solr.core, etc"
metrics,"prefix: (optional, str) prefix for metric names, in addition to registry name. Default is none, ie. just registry name. filter: (optional, str) if not empty only metric names that start with this value will be reported, default is all metrics from a registry, logger: (optional, str) logger name to use. Default is the metrics group, eg. solr.jvm, solr.core, etc"
metrics,"filter: (optional, str) if not empty only metric names that start with this value will be reported, default is all metrics from a registry, logger: (optional, str) logger name to use. Default is the metrics group, eg. solr.jvm, solr.core, etc"
metrics,"logger: (optional, str) logger name to use. Default is the metrics group, eg. solr.jvm, solr.core, etc"
metrics,Enclosing class: JmxMetricsReporter public static class JmxMetricsReporter.Builder extends Object Builder for the JmxMetricsReporter class.
metrics,"this class knows that it can directly use MetricsMap as a dynamic MBean. this class allows us to ""tag"" MBean instances so that we can later unregister only instances registered with the same tag. this class processes all metrics already existing in the registry at the time when reporter is started."
metrics,"this class allows us to ""tag"" MBean instances so that we can later unregister only instances registered with the same tag. this class processes all metrics already existing in the registry at the time when reporter is started."
metrics,this class processes all metrics already existing in the registry at the time when reporter is started.
metrics,"this class knows that it can directly use MetricsMap as a dynamic MBean. this class allows us to ""tag"" MBean instances so that we can later unregister only instances registered with the same tag. this class processes all metrics already existing in the registry at the time when reporter is started."
metrics,"this class allows us to ""tag"" MBean instances so that we can later unregister only instances registered with the same tag. this class processes all metrics already existing in the registry at the time when reporter is started."
metrics,this class processes all metrics already existing in the registry at the time when reporter is started.
metrics,Enclosing class: SolrReporter public static class SolrReporter.Builder extends Object Builder for the SolrReporter class.
metrics,"Methods inherited from classcom.codahale.metrics.ScheduledReporter convertDuration, convertRate, getDisabledMetricAttributes, getDurationUnit, getRateUnit, getScheduledFuture, getScheduledFuture, isShutdownExecutorOnStop, start, start, stop"
metrics,Enclosing class: SolrReporter public static final class SolrReporter.Report extends Object Specification of what registries and what metrics to send.
packagemanager,public class DefaultPackageRepository extends PackageRepository This is a serializable bean (for the JSON that is stored in /repository.json) representing a repository of Solr packages. Supports standard repositories based on a webservice.
packagemanager,"Direct Known Subclasses: DefaultPackageRepository public abstract class PackageRepository extends Object Abstract class for a repository, holding SolrPackage items."
packagemanager,public class PackageUtils extends Object
packagemanager,public class RepositoryManager extends Object Handles most of the management of repositories and packages present in external repositories.
parser,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
parser,Nested classes/interfaces inherited from classorg.apache.lucene.util.QueryBuilder org.apache.lucene.util.QueryBuilder.TermAndBoost
parser,"Fields inherited from classorg.apache.lucene.util.QueryBuilder analyzer, autoGenerateMultiTermSynonymsPhraseQuery, enableGraphQueries, enablePositionIncrements"
parser,"Methods inherited from classorg.apache.lucene.util.QueryBuilder add, analyzeBoolean, analyzeGraphBoolean, analyzeGraphPhrase, analyzeMultiBoolean, analyzeMultiPhrase, analyzePhrase, analyzeTerm, createBooleanQuery, createBooleanQuery, createFieldQuery, createFieldQuery, createMinShouldMatchQuery, createPhraseQuery, createPhraseQuery, getAnalyzer, getAutoGenerateMultiTermSynonymsPhraseQuery, getEnableGraphQueries, getEnablePositionIncrements, newBooleanQuery, newMultiPhraseQueryBuilder, newTermQuery, setAnalyzer, setAutoGenerateMultiTermSynonymsPhraseQuery, setEnableGraphQueries, setEnablePositionIncrements"
parser,Nested classes/interfaces inherited from classorg.apache.lucene.util.QueryBuilder org.apache.lucene.util.QueryBuilder.TermAndBoost
parser,Direct Known Subclasses: QueryParser public abstract class SolrQueryParserBase extends org.apache.lucene.util.QueryBuilder This class is overridden by QueryParser in QueryParser.jj and acts to separate the majority of the Java code from the .jj grammar file.
parser,Nested classes/interfaces inherited from classorg.apache.lucene.util.QueryBuilder org.apache.lucene.util.QueryBuilder.TermAndBoost
parser,"Fields inherited from classorg.apache.lucene.util.QueryBuilder analyzer, autoGenerateMultiTermSynonymsPhraseQuery, enableGraphQueries, enablePositionIncrements"
parser,"Methods inherited from classorg.apache.lucene.util.QueryBuilder add, analyzeBoolean, analyzeGraphBoolean, analyzeGraphPhrase, analyzeMultiBoolean, analyzeMultiPhrase, analyzePhrase, analyzeTerm, createBooleanQuery, createBooleanQuery, createFieldQuery, createFieldQuery, createMinShouldMatchQuery, createPhraseQuery, createPhraseQuery, getAnalyzer, getAutoGenerateMultiTermSynonymsPhraseQuery, getEnableGraphQueries, getEnablePositionIncrements, newBooleanQuery, newMultiPhraseQueryBuilder, newTermQuery, setAnalyzer, setAutoGenerateMultiTermSynonymsPhraseQuery, setEnableGraphQueries, setEnablePositionIncrements"
parser,Nested classes/interfaces inherited from classorg.apache.lucene.util.QueryBuilder org.apache.lucene.util.QueryBuilder.TermAndBoost
parser,Enclosing class: SolrQueryParserBase public static class SolrQueryParserBase.RawQuery extends org.apache.lucene.search.Query
parser,"Methods inherited from classorg.apache.lucene.search.Query classHash, createWeight, rewrite, rewrite, sameClassAs, toString"
parser,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
pkg,Enclosing class: PackageAPI public class PackageAPI.Edit extends Object
pkg,public class PackageAPI extends Object This implements the public end points (/api/cluster/package) of package API.
pkg,Enclosing class: PackageAPI public class PackageAPI.Read extends Object
pkg,public class PackageListeners extends Object
pkg,Enclosing interface: PackageListeners.Listener public static class PackageListeners.Listener.Ctx extends Object
query,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, sameClassAs, toString"
query,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, sameClassAs, toString"
request,Used primarily in cases where developers want to customize one or more SolrQueryRequest methods while deferring the remainder to an existing instances.
request,"public class DocValuesFacets extends Object Computes term facets for docvalues field (single or multivalued). This is basically a specialized case of the code in SimpleFacets. Instead of working on a top-level reader view (binary-search per docid), it collects per-segment, but maps ordinals to global ordinal space using MultiDocValues' OrdinalMap. This means the ordinal map is created per-reopen: O(nterms), but this may perform better than PerSegmentSingleValuedFaceting which has to merge O(nterms) per query. Additionally it works for multi-valued fields."
request,"This is basically a specialized case of the code in SimpleFacets. Instead of working on a top-level reader view (binary-search per docid), it collects per-segment, but maps ordinals to global ordinal space using MultiDocValues' OrdinalMap. This means the ordinal map is created per-reopen: O(nterms), but this may perform better than PerSegmentSingleValuedFaceting which has to merge O(nterms) per query. Additionally it works for multi-valued fields."
request,"This means the ordinal map is created per-reopen: O(nterms), but this may perform better than PerSegmentSingleValuedFaceting which has to merge O(nterms) per query. Additionally it works for multi-valued fields."
request,"public class DocValuesStats extends Object Computes term stats for docvalues field (single or multivalued). Instead of working on a top-level reader view (binary-search per docid), it collects per-segment, but maps ordinals to global ordinal space using MultiDocValues' OrdinalMap."
request,"Instead of working on a top-level reader view (binary-search per docid), it collects per-segment, but maps ordinals to global ordinal space using MultiDocValues' OrdinalMap."
request,Enclosing class: IntervalFacets public static class IntervalFacets.FacetInterval extends Object Helper class to match and count of documents in specified intervals
request,"(1,10) -> will include values greater than 1 and lower than 10 [1,10) -> will include values greater or equal to 1 and lower than 10 [1,10] -> will include values greater or equal to 1 and lower or equal to 10"
request,"[1,10) -> will include values greater or equal to 1 and lower than 10 [1,10] -> will include values greater or equal to 1 and lower or equal to 10"
request,"[1,10] -> will include values greater or equal to 1 and lower or equal to 10"
request,"Given a set of intervals for a field and a DocSet, it calculates the number of documents that match each of the intervals provided. The final count for each interval should be exactly the same as the number of results of a range query using the DocSet and the range as filters. This means that the count of facet.query=field:[A TO B] should be the same as the count of f.field.facet.interval.set=[A,B], however, this method will usually be faster in cases where there are a larger number of intervals per field. To use this class, create an instance using IntervalFacets(SchemaField, SolrIndexSearcher, DocSet, String[], SolrParams) and then iterate the IntervalFacets.FacetInterval using iterator() Intervals Format Intervals must begin with either '(' or '[', be followed by the start value, then a comma ',', the end value, and finally ')' or ']'. For example: (1,10) -> will include values greater than 1 and lower than 10 [1,10) -> will include values greater or equal to 1 and lower than 10 [1,10] -> will include values greater or equal to 1 and lower or equal to 10 The initial and end values can't be empty, if the interval needs to be unbounded, the special character '*' can be used for both, start and end limit. When using '*', '(' and '[', and ')' and ']' will be treated equal. [*,*] will include all documents with a value in the field The interval limits may be strings, there is no need to add quotes, all the text until the comma will be treated as the start limit, and the text after that will be the end limit, for example: [Buenos Aires,New York]. Keep in mind that a string-like comparison will be done to match documents in string intervals (case-sensitive). The comparator can't be changed. Commas, brackets and square brackets can be escaped by using '\' in front of them. Whitespaces before and after the values will be omitted. Start limit can't be grater than the end limit. Equal limits are allowed. As with facet.query, the key used to display the result can be set by using local params syntax, for example: {!key='First Half'}[0,5) To use this class: IntervalFacets intervalFacets = new IntervalFacets(schemaField, searcher, docs, intervalStrs, params); for (FacetInterval interval : intervalFacets) { results.add(interval.getKey(), interval.getCount()); }"
request,"To use this class, create an instance using IntervalFacets(SchemaField, SolrIndexSearcher, DocSet, String[], SolrParams) and then iterate the IntervalFacets.FacetInterval using iterator() Intervals Format Intervals must begin with either '(' or '[', be followed by the start value, then a comma ',', the end value, and finally ')' or ']'. For example: (1,10) -> will include values greater than 1 and lower than 10 [1,10) -> will include values greater or equal to 1 and lower than 10 [1,10] -> will include values greater or equal to 1 and lower or equal to 10 The initial and end values can't be empty, if the interval needs to be unbounded, the special character '*' can be used for both, start and end limit. When using '*', '(' and '[', and ')' and ']' will be treated equal. [*,*] will include all documents with a value in the field The interval limits may be strings, there is no need to add quotes, all the text until the comma will be treated as the start limit, and the text after that will be the end limit, for example: [Buenos Aires,New York]. Keep in mind that a string-like comparison will be done to match documents in string intervals (case-sensitive). The comparator can't be changed. Commas, brackets and square brackets can be escaped by using '\' in front of them. Whitespaces before and after the values will be omitted. Start limit can't be grater than the end limit. Equal limits are allowed. As with facet.query, the key used to display the result can be set by using local params syntax, for example: {!key='First Half'}[0,5) To use this class: IntervalFacets intervalFacets = new IntervalFacets(schemaField, searcher, docs, intervalStrs, params); for (FacetInterval interval : intervalFacets) { results.add(interval.getKey(), interval.getCount()); }"
request,"Intervals Format Intervals must begin with either '(' or '[', be followed by the start value, then a comma ',', the end value, and finally ')' or ']'. For example: (1,10) -> will include values greater than 1 and lower than 10 [1,10) -> will include values greater or equal to 1 and lower than 10 [1,10] -> will include values greater or equal to 1 and lower or equal to 10 The initial and end values can't be empty, if the interval needs to be unbounded, the special character '*' can be used for both, start and end limit. When using '*', '(' and '[', and ')' and ']' will be treated equal. [*,*] will include all documents with a value in the field The interval limits may be strings, there is no need to add quotes, all the text until the comma will be treated as the start limit, and the text after that will be the end limit, for example: [Buenos Aires,New York]. Keep in mind that a string-like comparison will be done to match documents in string intervals (case-sensitive). The comparator can't be changed. Commas, brackets and square brackets can be escaped by using '\' in front of them. Whitespaces before and after the values will be omitted. Start limit can't be grater than the end limit. Equal limits are allowed. As with facet.query, the key used to display the result can be set by using local params syntax, for example: {!key='First Half'}[0,5) To use this class: IntervalFacets intervalFacets = new IntervalFacets(schemaField, searcher, docs, intervalStrs, params); for (FacetInterval interval : intervalFacets) { results.add(interval.getKey(), interval.getCount()); }"
request,"(1,10) -> will include values greater than 1 and lower than 10 [1,10) -> will include values greater or equal to 1 and lower than 10 [1,10] -> will include values greater or equal to 1 and lower or equal to 10"
request,"[1,10) -> will include values greater or equal to 1 and lower than 10 [1,10] -> will include values greater or equal to 1 and lower or equal to 10"
request,"[1,10] -> will include values greater or equal to 1 and lower or equal to 10"
request,"The interval limits may be strings, there is no need to add quotes, all the text until the comma will be treated as the start limit, and the text after that will be the end limit, for example: [Buenos Aires,New York]. Keep in mind that a string-like comparison will be done to match documents in string intervals (case-sensitive). The comparator can't be changed. Commas, brackets and square brackets can be escaped by using '\' in front of them. Whitespaces before and after the values will be omitted. Start limit can't be grater than the end limit. Equal limits are allowed. As with facet.query, the key used to display the result can be set by using local params syntax, for example: {!key='First Half'}[0,5) To use this class: IntervalFacets intervalFacets = new IntervalFacets(schemaField, searcher, docs, intervalStrs, params); for (FacetInterval interval : intervalFacets) { results.add(interval.getKey(), interval.getCount()); }"
request,"As with facet.query, the key used to display the result can be set by using local params syntax, for example: {!key='First Half'}[0,5) To use this class: IntervalFacets intervalFacets = new IntervalFacets(schemaField, searcher, docs, intervalStrs, params); for (FacetInterval interval : intervalFacets) { results.add(interval.getKey(), interval.getCount()); }"
request,"{!key='First Half'}[0,5) To use this class: IntervalFacets intervalFacets = new IntervalFacets(schemaField, searcher, docs, intervalStrs, params); for (FacetInterval interval : intervalFacets) { results.add(interval.getKey(), interval.getCount()); }"
request,"To use this class: IntervalFacets intervalFacets = new IntervalFacets(schemaField, searcher, docs, intervalStrs, params); for (FacetInterval interval : intervalFacets) { results.add(interval.getKey(), interval.getCount()); }"
request,"Methods inherited from interfacejava.lang.Iterable forEach, spliterator"
request,"Methods inherited from interfacejava.util.function.Predicate and, negate, or"
request,"Direct Known Subclasses: PivotFacetProcessor, RangeFacetProcessor public class SimpleFacets extends Object A class that generates simple Facet information for a request. More advanced facet implementations may compose or subclass this class to leverage any of its functionality."
request,More advanced facet implementations may compose or subclass this class to leverage any of its functionality.
request,Enclosing class: SimpleFacets protected static final class SimpleFacets.ParsedParams extends Object
request,The close() method must be called on any instance of this class once it is no longer in use.
request,public class SolrRequestInfo extends Object Information about the Solr request/response held in a ThreadLocal.
request,"Methods inherited from interfacejava.util.function.Predicate and, negate, or"
request,public class JSONUtil extends Object
request,Enclosing class: ObjectUtil public static class ObjectUtil.ConflictHandler extends Object
request,public class ObjectUtil extends Object
request,public class RequestUtil extends Object
request,public class MacroExpander extends Object
response,public class BasicResultContext extends ResultContext
response,Methods inherited from interfacejava.util.Iterator forEachRemaining
response,Methods inherited from interfacejava.io.Closeable close
response,public final class QueryResponseWriterUtil extends Object Static utility methods relating to QueryResponseWriters
response,"This writer is a special case that extends and alters the QueryResponseWriter contract. If SolrQueryResponse contains a ContentStream added with the key CONTENT then this writer will output that stream exactly as is (with its Content-Type). if no such ContentStream has been added, then a ""base"" QueryResponseWriter will be used to write the response according to the usual contract. The name of the ""base"" writer can be specified as an initialization param for this writer, or it defaults to the ""standard"" writer."
response,Direct Known Subclasses: BasicResultContext public abstract class ResultContext extends Object A class to hold the QueryResult and the Query
response,"public class SolrQueryResponse extends Object SolrQueryResponse is used by a query handler to return the response to a query request. Note On Returnable Data... A SolrQueryResponse may contain the following types of Objects generated by the SolrRequestHandler that processed the request. String Integer Long Float Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null Other data types may be added to the SolrQueryResponse, but there is no guarantee that QueryResponseWriters will be able to deal with unexpected types. Since: solr 0.9"
response,String Integer Long Float Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Integer Long Float Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Long Float Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Float Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Collection containing any of the items in this list Array containing any of the items in this list null
response,Array containing any of the items in this list null
response,"Note On Returnable Data... A SolrQueryResponse may contain the following types of Objects generated by the SolrRequestHandler that processed the request. String Integer Long Float Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null Other data types may be added to the SolrQueryResponse, but there is no guarantee that QueryResponseWriters will be able to deal with unexpected types."
response,String Integer Long Float Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Integer Long Float Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Long Float Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Float Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Double Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Boolean Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Date DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,DocList SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,SolrDocument (since 1.3) SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,SolrDocumentList (since 1.3) Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Map containing any of the items in this list NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,NamedList containing any of the items in this list Collection containing any of the items in this list Array containing any of the items in this list null
response,Collection containing any of the items in this list Array containing any of the items in this list null
response,Array containing any of the items in this list null
response,"Other data types may be added to the SolrQueryResponse, but there is no guarantee that QueryResponseWriters will be able to deal with unexpected types."
response,public abstract class BaseEditorialTransformer extends DocTransformer
response,"Optionally you can provide a ""parentFilter"" param to designate which documents are the root documents (parent-most documents). Solr can figure this out on its own but you might want to specify it. Optionally you can provide a ""childFilter"" param to filter out which child documents should be returned and a ""limit"" param which provides an option to specify the number of child documents to be returned per parent document. Examples - [child parentFilter=""fieldName:fieldValue""] [child parentFilter=""fieldName:fieldValue"" childFilter=""fieldName:fieldValue""] [child parentFilter=""fieldName:fieldValue"" childFilter=""fieldName:fieldValue"" limit=20]"
response,"Optionally you can provide a ""childFilter"" param to filter out which child documents should be returned and a ""limit"" param which provides an option to specify the number of child documents to be returned per parent document. Examples - [child parentFilter=""fieldName:fieldValue""] [child parentFilter=""fieldName:fieldValue"" childFilter=""fieldName:fieldValue""] [child parentFilter=""fieldName:fieldValue"" childFilter=""fieldName:fieldValue"" limit=20]"
response,"Examples - [child parentFilter=""fieldName:fieldValue""] [child parentFilter=""fieldName:fieldValue"" childFilter=""fieldName:fieldValue""] [child parentFilter=""fieldName:fieldValue"" childFilter=""fieldName:fieldValue"" limit=20]"
response,"Direct Known Subclasses: BaseEditorialTransformer, DocTransformer.NoopFieldTransformer, DocTransformers, RenameFieldTransformer, ScoreAugmenter, ValueAugmenterFactory.ValueAugmenter, ValueSourceAugmenter public abstract class DocTransformer extends Object A DocTransformer can add, remove or alter a Document before it is written out to the Response. For instance, there are implementations that can put explanations inline with a document, add constant values and mark items as being artificially boosted (see QueryElevationComponent) New instance for each request See Also: TransformerFactory"
response,New instance for each request
response,"Enclosing class: DocTransformer public static final class DocTransformer.NoopFieldTransformer extends DocTransformer Trivial Impl that ensure that the specified field is requested as an ""extra"" field, but then does nothing during the transformation phase."
response,public class DocTransformers extends DocTransformer Transform a document before it gets sent out
response,GeoJSON WKT Polyshape
response,WKT Polyshape
response,"You want to return a format different than the stored encoding (WKT vs GeoJSON) The Shape is stored in a ValueSource, not a stored field the value is not stored in a format the output understands (ie, raw GeoJSON)"
response,"The Shape is stored in a ValueSource, not a stored field the value is not stored in a format the output understands (ie, raw GeoJSON)"
response,"the value is not stored in a format the output understands (ie, raw GeoJSON)"
response,GeoJSON WKT Polyshape
response,WKT Polyshape
response,"The shape is either read from a stored field, or a ValueSource. This transformer is useful when: You want to return a format different than the stored encoding (WKT vs GeoJSON) The Shape is stored in a ValueSource, not a stored field the value is not stored in a format the output understands (ie, raw GeoJSON)"
response,"This transformer is useful when: You want to return a format different than the stored encoding (WKT vs GeoJSON) The Shape is stored in a ValueSource, not a stored field the value is not stored in a format the output understands (ie, raw GeoJSON)"
response,"You want to return a format different than the stored encoding (WKT vs GeoJSON) The Shape is stored in a ValueSource, not a stored field the value is not stored in a format the output understands (ie, raw GeoJSON)"
response,"The Shape is stored in a ValueSource, not a stored field the value is not stored in a format the output understands (ie, raw GeoJSON)"
response,"the value is not stored in a format the output understands (ie, raw GeoJSON)"
response,public class RenameFieldTransformer extends DocTransformer Return a field with a name that is different that what is indexed Since: solr 4.0
response,public class ScoreAugmenter extends DocTransformer Simple Augmenter that adds the score Since: solr 4.0
response,Enclosing class: ValueAugmenterFactory public static class ValueAugmenterFactory.ValueAugmenter extends DocTransformer
response,public class ValueSourceAugmenter extends DocTransformer Add values from a ValueSource (function query etc) NOT really sure how or if this could work... Since: solr 4.0
response,NOT really sure how or if this could work...
rest,"Direct Known Subclasses: RestManager.ManagedEndpoint public abstract class BaseSolrResource extends Object Base class for delegating REST-oriented requests to ManagedResources. ManagedResources are heavy-weight and should not be created for every request, so this class serves as a gateway between a REST call and the resource."
rest,"Direct Known Subclasses: ManagedSynonymFilterFactory.SynonymManager, ManagedSynonymGraphFilterFactory.SynonymManager, ManagedWordSetResource public abstract class ManagedResource extends Object Supports Solr components that have external data that needs to be managed using the REST API."
rest,"Direct Known Subclasses: ManagedResourceStorage.JsonStorage public abstract class ManagedResourceStorage extends Object Abstract base class that provides most of the functionality needed to store arbitrary data for managed resources. Concrete implementations need to decide the underlying format that data is stored in, such as JSON. The underlying storage I/O layer will be determined by the environment Solr is running in, e.g. in cloud mode, data will be stored and loaded from ZooKeeper."
rest,"The underlying storage I/O layer will be determined by the environment Solr is running in, e.g. in cloud mode, data will be stored and loaded from ZooKeeper."
rest,Enclosing class: ManagedResourceStorage public static class ManagedResourceStorage.JsonStorage extends ManagedResourceStorage Default storage implementation that uses JSON as the storage format for managed data.
rest,"public class RestManager extends Object Supports runtime mapping of REST API endpoints to ManagedResource implementations; endpoints can be registered at either the /schema or /config base paths, depending on which base path is more appropriate for the type of managed resource."
rest,"Enclosing class: RestManager public static class RestManager.ManagedEndpoint extends BaseSolrResource Request handling needs a lightweight object to delegate a request to. ManagedResource implementations are heavy-weight objects that live for the duration of a SolrCore, so this class acts as the proxy between the request handler and a ManagedResource when doing request processing."
rest,"Enclosing class: RestManager public static class RestManager.Registry extends Object Per-core registry of ManagedResources found during core initialization. Registering of managed resources can happen before the RestManager is fully initialized. To avoid timing issues, resources register themselves and then the RestManager initializes all ManagedResources before the core is activated."
rest,"Registering of managed resources can happen before the RestManager is fully initialized. To avoid timing issues, resources register themselves and then the RestManager initializes all ManagedResources before the core is activated."
rest,public class FieldTypeXmlAdapter extends Object Utility class for converting a JSON definition of a FieldType into the XML format expected by the FieldTypePluginLoader.
rest,"Fields inherited from classorg.apache.lucene.analysis.AbstractAnalysisFactory LUCENE_MATCH_VERSION_PARAM, luceneMatchVersion"
rest,"Methods inherited from classorg.apache.lucene.analysis.TokenFilterFactory availableTokenFilters, create, findSPIName, forName, lookupClass, normalize, reloadTokenFilters"
rest,"Methods inherited from classorg.apache.lucene.analysis.AbstractAnalysisFactory defaultCtorException, get, get, get, get, get, getBoolean, getChar, getClassArg, getFloat, getInt, getLines, getLuceneMatchVersion, getOriginalArgs, getPattern, getSet, getSnowballWordSet, getWordSet, isExplicitLuceneMatchVersion, require, require, require, requireBoolean, requireChar, requireFloat, requireInt, setExplicitLuceneMatchVersion, splitAt, splitFileNames"
rest,"Fields inherited from classorg.apache.lucene.analysis.AbstractAnalysisFactory LUCENE_MATCH_VERSION_PARAM, luceneMatchVersion"
rest,"Fields inherited from classorg.apache.lucene.analysis.AbstractAnalysisFactory LUCENE_MATCH_VERSION_PARAM, luceneMatchVersion"
rest,"Methods inherited from classorg.apache.lucene.analysis.TokenFilterFactory availableTokenFilters, findSPIName, forName, lookupClass, normalize, reloadTokenFilters"
rest,"Methods inherited from classorg.apache.lucene.analysis.AbstractAnalysisFactory defaultCtorException, get, get, get, get, get, getBoolean, getChar, getClassArg, getFloat, getInt, getLines, getLuceneMatchVersion, getOriginalArgs, getPattern, getSet, getSnowballWordSet, getWordSet, isExplicitLuceneMatchVersion, require, require, require, requireBoolean, requireChar, requireFloat, requireInt, setExplicitLuceneMatchVersion, splitAt, splitFileNames"
rest,"Fields inherited from classorg.apache.lucene.analysis.AbstractAnalysisFactory LUCENE_MATCH_VERSION_PARAM, luceneMatchVersion"
rest,"Fields inherited from classorg.apache.lucene.analysis.AbstractAnalysisFactory LUCENE_MATCH_VERSION_PARAM, luceneMatchVersion"
rest,"Methods inherited from classorg.apache.lucene.analysis.TokenFilterFactory availableTokenFilters, findSPIName, forName, lookupClass, normalize, reloadTokenFilters"
rest,"Methods inherited from classorg.apache.lucene.analysis.AbstractAnalysisFactory defaultCtorException, get, get, get, get, get, getBoolean, getChar, getClassArg, getFloat, getInt, getLines, getLuceneMatchVersion, getOriginalArgs, getPattern, getSet, getSnowballWordSet, getWordSet, isExplicitLuceneMatchVersion, require, require, require, requireBoolean, requireChar, requireFloat, requireInt, setExplicitLuceneMatchVersion, splitAt, splitFileNames"
rest,"Fields inherited from classorg.apache.lucene.analysis.AbstractAnalysisFactory LUCENE_MATCH_VERSION_PARAM, luceneMatchVersion"
rest,"Methods inherited from classorg.apache.lucene.analysis.TokenFilterFactory availableTokenFilters, findSPIName, forName, lookupClass, normalize, reloadTokenFilters"
rest,"Methods inherited from classorg.apache.lucene.analysis.AbstractAnalysisFactory defaultCtorException, get, get, get, get, get, getBoolean, getChar, getClassArg, getFloat, getInt, getLines, getLuceneMatchVersion, getOriginalArgs, getPattern, getSet, getSnowballWordSet, getWordSet, isExplicitLuceneMatchVersion, require, require, require, requireBoolean, requireChar, requireFloat, requireInt, setExplicitLuceneMatchVersion, splitAt, splitFileNames"
schema,Enclosing class: AbstractEnumField public static final class AbstractEnumField.EnumMapping extends Object Models all the info contained in an enums config XML file NOTE: This API is for internal purposes only and might change in incompatible ways in the next release.
schema,"Direct Known Subclasses: EnumField, EnumFieldType public abstract class AbstractEnumField extends PrimitiveFieldType Abstract Field type for support of string values with custom sort order."
schema,public class BinaryField extends FieldType
schema,public class BoolField extends PrimitiveFieldType
schema,"public class CollationField extends FieldType Field for collated sort keys. These can be used for locale-sensitive sort and range queries. This field can be created in two ways: Based upon a system collator associated with a Locale. Based upon a tailored ruleset. Using a System collator: language: ISO-639 language code (mandatory) country: ISO-3166 country code (optional) variant: vendor or browser-specific code (optional) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional) Using a Tailored ruleset: custom: UTF-8 text file containing rules supported by RuleBasedCollator (mandatory) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional) Since: solr 4.0 See Also: Collator, Locale, RuleBasedCollator"
schema,Based upon a system collator associated with a Locale. Based upon a tailored ruleset.
schema,Based upon a tailored ruleset.
schema,"language: ISO-639 language code (mandatory) country: ISO-3166 country code (optional) variant: vendor or browser-specific code (optional) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"country: ISO-3166 country code (optional) variant: vendor or browser-specific code (optional) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"variant: vendor or browser-specific code (optional) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"decomposition: 'no','canonical', or 'full' (optional)"
schema,"custom: UTF-8 text file containing rules supported by RuleBasedCollator (mandatory) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"decomposition: 'no','canonical', or 'full' (optional)"
schema,"This field can be created in two ways: Based upon a system collator associated with a Locale. Based upon a tailored ruleset. Using a System collator: language: ISO-639 language code (mandatory) country: ISO-3166 country code (optional) variant: vendor or browser-specific code (optional) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional) Using a Tailored ruleset: custom: UTF-8 text file containing rules supported by RuleBasedCollator (mandatory) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,Based upon a system collator associated with a Locale. Based upon a tailored ruleset.
schema,Based upon a tailored ruleset.
schema,"Using a System collator: language: ISO-639 language code (mandatory) country: ISO-3166 country code (optional) variant: vendor or browser-specific code (optional) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional) Using a Tailored ruleset: custom: UTF-8 text file containing rules supported by RuleBasedCollator (mandatory) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"language: ISO-639 language code (mandatory) country: ISO-3166 country code (optional) variant: vendor or browser-specific code (optional) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"country: ISO-3166 country code (optional) variant: vendor or browser-specific code (optional) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"variant: vendor or browser-specific code (optional) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"decomposition: 'no','canonical', or 'full' (optional)"
schema,"Using a Tailored ruleset: custom: UTF-8 text file containing rules supported by RuleBasedCollator (mandatory) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"custom: UTF-8 text file containing rules supported by RuleBasedCollator (mandatory) strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"strength: 'primary','secondary','tertiary', or 'identical' (optional) decomposition: 'no','canonical', or 'full' (optional)"
schema,"decomposition: 'no','canonical', or 'full' (optional)"
schema,public class CopyField extends Object CopyField contains all the information of a valid copy fields in an index. Since: solr 1.4
schema,See https://solr.apache.org/guide/solr/latest/indexing-guide/currencies-exchange-rates.html
schema,Methods inherited from interfaceorg.apache.lucene.util.ResourceLoaderAware inform
schema,See https://solr.apache.org/guide/solr/latest/indexing-guide/currencies-exchange-rates.html
schema,1995-12-31T23:59:59Z 1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,"Date Format for the XML, incoming and outgoing: A date field shall be of the form 1995-12-31T23:59:59Z The trailing ""Z"" designates UTC time and is mandatory (See below for an explanation of UTC). Optional fractional seconds are allowed, as long as they do not end in a trailing 0 (but any precision beyond milliseconds will be ignored). All other parts are mandatory. This format was derived to be standards compliant (ISO 8601) and is a more restricted form of the canonical representation of dateTime from XML schema part 2. Examples... 1995-12-31T23:59:59Z 1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z Note that DatePointField is lenient with regards to parsing fractional seconds that end in trailing zeros and will ensure that those values are indexed in the correct canonical format. This FieldType also supports incoming ""Date Math"" strings for computing values by adding/rounding internals of time relative either an explicit datetime (in the format specified above) or the literal string ""NOW"", ie: ""NOW+1YEAR"", ""NOW/DAY"", ""1995-12-31T23:59:59.999Z+5MINUTES"", etc... -- see DateMathParser for more examples. NOTE: Although it is possible to configure a DatePointField instance with a default value of ""NOW"" to compute a timestamp of when the document was indexed, this is not advisable when using SolrCloud since each replica of the document may compute a slightly different value. TimestampUpdateProcessorFactory is recommended instead. Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,"This format was derived to be standards compliant (ISO 8601) and is a more restricted form of the canonical representation of dateTime from XML schema part 2. Examples... 1995-12-31T23:59:59Z 1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z Note that DatePointField is lenient with regards to parsing fractional seconds that end in trailing zeros and will ensure that those values are indexed in the correct canonical format. This FieldType also supports incoming ""Date Math"" strings for computing values by adding/rounding internals of time relative either an explicit datetime (in the format specified above) or the literal string ""NOW"", ie: ""NOW+1YEAR"", ""NOW/DAY"", ""1995-12-31T23:59:59.999Z+5MINUTES"", etc... -- see DateMathParser for more examples. NOTE: Although it is possible to configure a DatePointField instance with a default value of ""NOW"" to compute a timestamp of when the document was indexed, this is not advisable when using SolrCloud since each replica of the document may compute a slightly different value. TimestampUpdateProcessorFactory is recommended instead. Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,1995-12-31T23:59:59Z 1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,"Note that DatePointField is lenient with regards to parsing fractional seconds that end in trailing zeros and will ensure that those values are indexed in the correct canonical format. This FieldType also supports incoming ""Date Math"" strings for computing values by adding/rounding internals of time relative either an explicit datetime (in the format specified above) or the literal string ""NOW"", ie: ""NOW+1YEAR"", ""NOW/DAY"", ""1995-12-31T23:59:59.999Z+5MINUTES"", etc... -- see DateMathParser for more examples. NOTE: Although it is possible to configure a DatePointField instance with a default value of ""NOW"" to compute a timestamp of when the document was indexed, this is not advisable when using SolrCloud since each replica of the document may compute a slightly different value. TimestampUpdateProcessorFactory is recommended instead. Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,"This FieldType also supports incoming ""Date Math"" strings for computing values by adding/rounding internals of time relative either an explicit datetime (in the format specified above) or the literal string ""NOW"", ie: ""NOW+1YEAR"", ""NOW/DAY"", ""1995-12-31T23:59:59.999Z+5MINUTES"", etc... -- see DateMathParser for more examples. NOTE: Although it is possible to configure a DatePointField instance with a default value of ""NOW"" to compute a timestamp of when the document was indexed, this is not advisable when using SolrCloud since each replica of the document may compute a slightly different value. TimestampUpdateProcessorFactory is recommended instead. Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,"NOTE: Although it is possible to configure a DatePointField instance with a default value of ""NOW"" to compute a timestamp of when the document was indexed, this is not advisable when using SolrCloud since each replica of the document may compute a slightly different value. TimestampUpdateProcessorFactory is recommended instead. Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,"Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,@Deprecated public class EnumField extends AbstractEnumField Deprecated. use EnumFieldType instead. Field type for support of string values with custom sort order.
schema,public class EnumFieldType extends AbstractEnumField Field type for support of string values with custom sort order.
schema,It's OK to have a keyField value that can't be found in the index It's OK to have some documents without a keyField in the file (defVal is used as the default) It's OK for a keyField value to point to multiple documents (no uniqueness requirement)
schema,It's OK to have some documents without a keyField in the file (defVal is used as the default) It's OK for a keyField value to point to multiple documents (no uniqueness requirement)
schema,It's OK for a keyField value to point to multiple documents (no uniqueness requirement)
schema,"keyField will normally be the unique key field, but it doesn't have to be. It's OK to have a keyField value that can't be found in the index It's OK to have some documents without a keyField in the file (defVal is used as the default) It's OK for a keyField value to point to multiple documents (no uniqueness requirement) The format of the external file is simply newline separated keyFieldValue=floatValue. Example: doc33=1.414 doc34=3.14159 doc40=42 Solr looks for the external file in the index directory under the name of external_<fieldname> or external_<fieldname>.* If any files of the latter pattern appear, the last (after being sorted by name) will be used and previous versions will be deleted. This is to help support systems where one may not be able to overwrite a file (like Windows, if the file is in use). If the external file has already been loaded, and it is changed, those changes will not be visible until a commit has been done. The external file may be sorted or unsorted by the key field, but it will be substantially slower (untested) if it isn't sorted. Fields of this type may currently only be used as a ValueSource in a FunctionQuery. You can return the value as a field in the document by wrapping it like so: fl=id,field(inventory_count)."
schema,It's OK to have a keyField value that can't be found in the index It's OK to have some documents without a keyField in the file (defVal is used as the default) It's OK for a keyField value to point to multiple documents (no uniqueness requirement)
schema,It's OK to have some documents without a keyField in the file (defVal is used as the default) It's OK for a keyField value to point to multiple documents (no uniqueness requirement)
schema,It's OK for a keyField value to point to multiple documents (no uniqueness requirement)
schema,"Solr looks for the external file in the index directory under the name of external_<fieldname> or external_<fieldname>.* If any files of the latter pattern appear, the last (after being sorted by name) will be used and previous versions will be deleted. This is to help support systems where one may not be able to overwrite a file (like Windows, if the file is in use). If the external file has already been loaded, and it is changed, those changes will not be visible until a commit has been done. The external file may be sorted or unsorted by the key field, but it will be substantially slower (untested) if it isn't sorted. Fields of this type may currently only be used as a ValueSource in a FunctionQuery. You can return the value as a field in the document by wrapping it like so: fl=id,field(inventory_count)."
schema,"If any files of the latter pattern appear, the last (after being sorted by name) will be used and previous versions will be deleted. This is to help support systems where one may not be able to overwrite a file (like Windows, if the file is in use). If the external file has already been loaded, and it is changed, those changes will not be visible until a commit has been done. The external file may be sorted or unsorted by the key field, but it will be substantially slower (untested) if it isn't sorted. Fields of this type may currently only be used as a ValueSource in a FunctionQuery. You can return the value as a field in the document by wrapping it like so: fl=id,field(inventory_count)."
schema,"If the external file has already been loaded, and it is changed, those changes will not be visible until a commit has been done. The external file may be sorted or unsorted by the key field, but it will be substantially slower (untested) if it isn't sorted. Fields of this type may currently only be used as a ValueSource in a FunctionQuery. You can return the value as a field in the document by wrapping it like so: fl=id,field(inventory_count)."
schema,"The external file may be sorted or unsorted by the key field, but it will be substantially slower (untested) if it isn't sorted. Fields of this type may currently only be used as a ValueSource in a FunctionQuery. You can return the value as a field in the document by wrapping it like so: fl=id,field(inventory_count)."
schema,"Fields of this type may currently only be used as a ValueSource in a FunctionQuery. You can return the value as a field in the document by wrapping it like so: fl=id,field(inventory_count)."
schema,"You can return the value as a field in the document by wrapping it like so: fl=id,field(inventory_count)."
schema,"Direct Known Subclasses: FieldType, SchemaField public abstract class FieldProperties extends Object NOTE: This API is for internal purposes only and might change in incompatible ways in the next release."
schema,"Nested classes/interfaces inherited from classorg.apache.lucene.analysis.Analyzer org.apache.lucene.analysis.Analyzer.ReuseStrategy, org.apache.lucene.analysis.Analyzer.TokenStreamComponents"
schema,"Fields inherited from classorg.apache.lucene.analysis.Analyzer GLOBAL_REUSE_STRATEGY, PER_FIELD_REUSE_STRATEGY"
schema,"Methods inherited from classorg.apache.lucene.analysis.Analyzer attributeFactory, close, getOffsetGap, getReuseStrategy, initReaderForNormalization, normalize, normalize, tokenStream, tokenStream"
schema,"Nested classes/interfaces inherited from classorg.apache.lucene.analysis.Analyzer org.apache.lucene.analysis.Analyzer.ReuseStrategy, org.apache.lucene.analysis.Analyzer.TokenStreamComponents"
schema,"Direct Known Subclasses: AbstractSpatialFieldType, AbstractSubTypeFieldType, BinaryField, CollationField, CurrencyFieldType, ExternalFileField, PrimitiveFieldType, RandomSortField, RankField, TextField public abstract class FieldType extends FieldProperties Base class for all field types used by an index schema. Since: 3.1"
schema,public final class FieldTypePluginLoader extends AbstractPluginLoader<FieldType>
schema,Enclosing class: IndexSchema.DynamicReplacement protected abstract static class IndexSchema.DynamicReplacement.DynamicPattern extends Object
schema,Direct Known Subclasses: ManagedIndexSchema public class IndexSchema extends Object IndexSchema contains information about the valid fields in an index and the types of those fields.
schema,Enclosing class: IndexSchemaFactory public static class IndexSchemaFactory.VersionedConfig extends Object
schema,Enclosing class: LatLonPointSpatialField public static class LatLonPointSpatialField.LatLonPointSpatialStrategy extends org.apache.lucene.spatial.SpatialStrategy
schema,Fields inherited from classorg.apache.lucene.spatial.SpatialStrategy ctx
schema,"Methods inherited from classorg.apache.lucene.spatial.SpatialStrategy getFieldName, getSpatialContext, makeDistanceValueSource, makeRecipDistanceValueSource, toString"
schema,Fields inherited from classorg.apache.lucene.spatial.SpatialStrategy ctx
schema,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
schema,"public final class ManagedIndexSchema extends IndexSchema Solr-managed schema - non-user-editable, but can be mutable via internal and external REST API requests."
schema,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
schema,"public class NestPathField extends SortableTextField To be used for field IndexSchema.NEST_PATH_FIELD_NAME for enhanced nested doc information. By defining a field type, we can encapsulate the configuration here so that the schema is free of it. Alternatively, some notion of ""implicit field types"" would be cool and a more general way of accomplishing this. Since: 8.0 See Also: NestedUpdateProcessorFactory"
schema,"Direct Known Subclasses: PointField, TrieField public abstract class NumericFieldType extends PrimitiveFieldType"
schema,ratesFileLocation - A file path or absolute URL specifying the JSON data to load (mandatory) refreshInterval - How frequently (in minutes) to reload the exchange rate data (default: 1440)
schema,refreshInterval - How frequently (in minutes) to reload the exchange rate data (default: 1440)
schema,"Configuration Options: ratesFileLocation - A file path or absolute URL specifying the JSON data to load (mandatory) refreshInterval - How frequently (in minutes) to reload the exchange rate data (default: 1440) Disclaimer: This data is collected from various providers and provided free of charge for informational purposes only, with no guarantee whatsoever of accuracy, validity, availability or fitness for any purpose; use at your own risk. Other than that - have fun, and please share/watch/fork if you think data like this should be free!"
schema,ratesFileLocation - A file path or absolute URL specifying the JSON data to load (mandatory) refreshInterval - How frequently (in minutes) to reload the exchange rate data (default: 1440)
schema,refreshInterval - How frequently (in minutes) to reload the exchange rate data (default: 1440)
schema,"Disclaimer: This data is collected from various providers and provided free of charge for informational purposes only, with no guarantee whatsoever of accuracy, validity, availability or fitness for any purpose; use at your own risk. Other than that - have fun, and please share/watch/fork if you think data like this should be free!"
schema,"Direct Known Subclasses: DatePointField, DoublePointField, FloatPointField, IntPointField, LongPointField public abstract class PointField extends NumericFieldType Provides field types to support for Lucene's IntPoint, LongPoint, FloatPoint and DoublePoint. See PointRangeQuery for more details. It supports integer, float, long and double types. See subclasses for details. DocValues are supported for single-value cases (NumericDocValues). FieldCache is not supported for PointFields, so sorting, faceting, etc on these fields require the use of docValues=""true"" in the schema."
schema,Enclosing class: PreAnalyzedField public static class PreAnalyzedField.ParseResult extends Object This is a simple holder of a stored part and the collected states (tokens with attributes).
schema,"Direct Known Subclasses: AbstractEnumField, BoolField, NumericFieldType, StrField public abstract class PrimitiveFieldType extends FieldType Abstract class defining shared behavior for primitive types Intended to be used as base class for non-analyzed fields like int, float, string, date etc, and set proper defaults for them"
schema,"public class RandomSortField extends FieldType Utility Field used for random sorting. It should not be passed a value. This random sorting implementation uses the dynamic field name to set the random 'seed'. To get random sorting order, you need to use a random dynamic field name. For example, you will need to configure schema.xml: <types> ... <fieldType name=""random"" class=""solr.RandomSortField"" /> ... </types> <fields> ... <dynamicField name=""random*"" type=""random"" indexed=""true"" stored=""false""/> ... </fields> Examples of queries: http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_1234%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_2345%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_ABDC%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_21%20desc Note that multiple calls to the same URL will return the same sorting order. Since: solr 1.3"
schema,http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_1234%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_2345%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_ABDC%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_21%20desc
schema,http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_2345%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_ABDC%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_21%20desc
schema,http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_ABDC%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_21%20desc
schema,"This random sorting implementation uses the dynamic field name to set the random 'seed'. To get random sorting order, you need to use a random dynamic field name. For example, you will need to configure schema.xml: <types> ... <fieldType name=""random"" class=""solr.RandomSortField"" /> ... </types> <fields> ... <dynamicField name=""random*"" type=""random"" indexed=""true"" stored=""false""/> ... </fields> Examples of queries: http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_1234%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_2345%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_ABDC%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_21%20desc Note that multiple calls to the same URL will return the same sorting order."
schema,http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_1234%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_2345%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_ABDC%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_21%20desc
schema,http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_2345%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_ABDC%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_21%20desc
schema,http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_ABDC%20desc http://localhost:8983/solr/select/?q=*:*&fl=name&sort=random_21%20desc
schema,Enclosing class: RandomSortField public static class RandomSortField.RandomValueSource extends org.apache.lucene.queries.function.ValueSource
schema,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
schema,"public class RankField extends FieldType RankFields can be used to store scoring factors to improve document ranking. They should be used in combination with RankQParserPlugin. To use: Define the RankField fieldType in your schema: <fieldType name=""rank"" class=""solr.RankField"" /> Add fields to the schema, i.e.: <field name=""pagerank"" type=""rank"" /> Query using the RankQParserPlugin, for example http://localhost:8983/solr/techproducts?q=memory _query_:{!rank f='pagerank', function='log' scalingFactor='1.2'} Since: 8.6 See Also: RankQParserPlugin WARNING: This API is experimental and might change in incompatible ways in the next release."
schema,"Define the RankField fieldType in your schema: <fieldType name=""rank"" class=""solr.RankField"" /> Add fields to the schema, i.e.: <field name=""pagerank"" type=""rank"" /> Query using the RankQParserPlugin, for example http://localhost:8983/solr/techproducts?q=memory _query_:{!rank f='pagerank', function='log' scalingFactor='1.2'}"
schema,"Add fields to the schema, i.e.: <field name=""pagerank"" type=""rank"" /> Query using the RankQParserPlugin, for example http://localhost:8983/solr/techproducts?q=memory _query_:{!rank f='pagerank', function='log' scalingFactor='1.2'}"
schema,public class SchemaManager extends Object A utility class to manipulate schema using the bulk mode. This class takes in all the commands and processes them completely. It is an all or nothing operation.
schema,"Direct Known Subclasses: BM25SimilarityFactory, BooleanSimilarityFactory, ClassicSimilarityFactory, DFISimilarityFactory, DFRSimilarityFactory, IBSimilarityFactory, LMDirichletSimilarityFactory, LMJelinekMercerSimilarityFactory, SchemaSimilarityFactory public abstract class SimilarityFactory extends Object A factory interface for configuring a Similarity in the Solr schema.xml. Subclasses of SimilarityFactory which are SchemaAware must take responsibility for either consulting the similarities configured on individual field types, or generating appropriate error/warning messages if field type specific similarities exist but are being ignored. The IndexSchema will provide such error checking if a non- SchemaAware instance of SimilarityFactory is used. See Also: FieldType.getSimilarity()"
schema,"Subclasses of SimilarityFactory which are SchemaAware must take responsibility for either consulting the similarities configured on individual field types, or generating appropriate error/warning messages if field type specific similarities exist but are being ignored. The IndexSchema will provide such error checking if a non- SchemaAware instance of SimilarityFactory is used."
schema,"Direct Known Subclasses: NestPathField public class SortableTextField extends TextField SortableTextField is a specialized form of TextField that supports Sorting and ValueSource functions, using docValues built from the first maxCharsForDocValues characters of the original (pre-analyzed) String values of this field. The implicit default value for maxCharsForDocValues is 1024. If a field type instance is configured with maxCharsForDocValues <= 0 this overrides the default with an effective value of ""no limit"" (Integer.MAX_VALUE). Instances of this FieldType implicitly default to docValues=""true"" unless explicitly configured with docValues=""false"". Just like StrField, instances of this field that are multiValued=""true"" support the field(name,min|max) function, and implicitly sort on min|max depending on the asc|desc direction selector. NOTE: Unlike most other FieldTypes, this class defaults to useDocValuesAsStored=""false"". If an instance of this type (or a field that uses this type) overrides this behavior to set useDocValuesAsStored=""true"" then instead of truncating the original string value based on the effective value of maxCharsForDocValues , this class will reject any documents w/a field value longer then that limit -- causing the document update to fail. This behavior exists to prevent situations that could result in a search client reieving only a truncated version of the original field value in place of a stored value."
schema,"The implicit default value for maxCharsForDocValues is 1024. If a field type instance is configured with maxCharsForDocValues <= 0 this overrides the default with an effective value of ""no limit"" (Integer.MAX_VALUE). Instances of this FieldType implicitly default to docValues=""true"" unless explicitly configured with docValues=""false"". Just like StrField, instances of this field that are multiValued=""true"" support the field(name,min|max) function, and implicitly sort on min|max depending on the asc|desc direction selector. NOTE: Unlike most other FieldTypes, this class defaults to useDocValuesAsStored=""false"". If an instance of this type (or a field that uses this type) overrides this behavior to set useDocValuesAsStored=""true"" then instead of truncating the original string value based on the effective value of maxCharsForDocValues , this class will reject any documents w/a field value longer then that limit -- causing the document update to fail. This behavior exists to prevent situations that could result in a search client reieving only a truncated version of the original field value in place of a stored value."
schema,"Instances of this FieldType implicitly default to docValues=""true"" unless explicitly configured with docValues=""false"". Just like StrField, instances of this field that are multiValued=""true"" support the field(name,min|max) function, and implicitly sort on min|max depending on the asc|desc direction selector. NOTE: Unlike most other FieldTypes, this class defaults to useDocValuesAsStored=""false"". If an instance of this type (or a field that uses this type) overrides this behavior to set useDocValuesAsStored=""true"" then instead of truncating the original string value based on the effective value of maxCharsForDocValues , this class will reject any documents w/a field value longer then that limit -- causing the document update to fail. This behavior exists to prevent situations that could result in a search client reieving only a truncated version of the original field value in place of a stored value."
schema,"Just like StrField, instances of this field that are multiValued=""true"" support the field(name,min|max) function, and implicitly sort on min|max depending on the asc|desc direction selector. NOTE: Unlike most other FieldTypes, this class defaults to useDocValuesAsStored=""false"". If an instance of this type (or a field that uses this type) overrides this behavior to set useDocValuesAsStored=""true"" then instead of truncating the original string value based on the effective value of maxCharsForDocValues , this class will reject any documents w/a field value longer then that limit -- causing the document update to fail. This behavior exists to prevent situations that could result in a search client reieving only a truncated version of the original field value in place of a stored value."
schema,"NOTE: Unlike most other FieldTypes, this class defaults to useDocValuesAsStored=""false"". If an instance of this type (or a field that uses this type) overrides this behavior to set useDocValuesAsStored=""true"" then instead of truncating the original string value based on the effective value of maxCharsForDocValues , this class will reject any documents w/a field value longer then that limit -- causing the document update to fail. This behavior exists to prevent situations that could result in a search client reieving only a truncated version of the original field value in place of a stored value."
schema,Direct Known Subclasses: UUIDField public class StrField extends PrimitiveFieldType
schema,public class StrFieldSource extends org.apache.lucene.queries.function.valuesource.FieldCacheSource
schema,Fields inherited from classorg.apache.lucene.queries.function.valuesource.FieldCacheSource field
schema,Methods inherited from classorg.apache.lucene.queries.function.valuesource.FieldCacheSource getField
schema,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
schema,Fields inherited from classorg.apache.lucene.queries.function.valuesource.FieldCacheSource field
schema,"Direct Known Subclasses: PreAnalyzedField, SortableTextField public class TextField extends FieldType TextField is the basic type for configurable text analysis. Analyzers for field types using this implementation should be defined in the schema."
schema,1995-12-31T23:59:59Z 1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,"Date Format for the XML, incoming and outgoing: A date field shall be of the form 1995-12-31T23:59:59Z The trailing ""Z"" designates UTC time and is mandatory (See below for an explanation of UTC). Optional fractional seconds are allowed, as long as they do not end in a trailing 0 (but any precision beyond milliseconds will be ignored). All other parts are mandatory. This format was derived to be standards compliant (ISO 8601) and is a more restricted form of the canonical representation of dateTime from XML schema part 2. Examples... 1995-12-31T23:59:59Z 1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z Note that TrieDateField is lenient with regards to parsing fractional seconds that end in trailing zeros and will ensure that those values are indexed in the correct canonical format. This FieldType also supports incoming ""Date Math"" strings for computing values by adding/rounding internals of time relative either an explicit datetime (in the format specified above) or the literal string ""NOW"", ie: ""NOW+1YEAR"", ""NOW/DAY"", ""1995-12-31T23:59:59.999Z+5MINUTES"", etc... -- see DateMathParser for more examples. NOTE: Although it is possible to configure a TrieDateField instance with a default value of ""NOW"" to compute a timestamp of when the document was indexed, this is not advisable when using SolrCloud since each replica of the document may compute a slightly different value. TimestampUpdateProcessorFactory is recommended instead. Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,"This format was derived to be standards compliant (ISO 8601) and is a more restricted form of the canonical representation of dateTime from XML schema part 2. Examples... 1995-12-31T23:59:59Z 1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z Note that TrieDateField is lenient with regards to parsing fractional seconds that end in trailing zeros and will ensure that those values are indexed in the correct canonical format. This FieldType also supports incoming ""Date Math"" strings for computing values by adding/rounding internals of time relative either an explicit datetime (in the format specified above) or the literal string ""NOW"", ie: ""NOW+1YEAR"", ""NOW/DAY"", ""1995-12-31T23:59:59.999Z+5MINUTES"", etc... -- see DateMathParser for more examples. NOTE: Although it is possible to configure a TrieDateField instance with a default value of ""NOW"" to compute a timestamp of when the document was indexed, this is not advisable when using SolrCloud since each replica of the document may compute a slightly different value. TimestampUpdateProcessorFactory is recommended instead. Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,1995-12-31T23:59:59Z 1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,1995-12-31T23:59:59.9Z 1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,1995-12-31T23:59:59.99Z 1995-12-31T23:59:59.999Z
schema,"Note that TrieDateField is lenient with regards to parsing fractional seconds that end in trailing zeros and will ensure that those values are indexed in the correct canonical format. This FieldType also supports incoming ""Date Math"" strings for computing values by adding/rounding internals of time relative either an explicit datetime (in the format specified above) or the literal string ""NOW"", ie: ""NOW+1YEAR"", ""NOW/DAY"", ""1995-12-31T23:59:59.999Z+5MINUTES"", etc... -- see DateMathParser for more examples. NOTE: Although it is possible to configure a TrieDateField instance with a default value of ""NOW"" to compute a timestamp of when the document was indexed, this is not advisable when using SolrCloud since each replica of the document may compute a slightly different value. TimestampUpdateProcessorFactory is recommended instead. Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,"This FieldType also supports incoming ""Date Math"" strings for computing values by adding/rounding internals of time relative either an explicit datetime (in the format specified above) or the literal string ""NOW"", ie: ""NOW+1YEAR"", ""NOW/DAY"", ""1995-12-31T23:59:59.999Z+5MINUTES"", etc... -- see DateMathParser for more examples. NOTE: Although it is possible to configure a TrieDateField instance with a default value of ""NOW"" to compute a timestamp of when the document was indexed, this is not advisable when using SolrCloud since each replica of the document may compute a slightly different value. TimestampUpdateProcessorFactory is recommended instead. Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,"NOTE: Although it is possible to configure a TrieDateField instance with a default value of ""NOW"" to compute a timestamp of when the document was indexed, this is not advisable when using SolrCloud since each replica of the document may compute a slightly different value. TimestampUpdateProcessorFactory is recommended instead. Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,"Explanation of ""UTC""... ""In 1970 the Coordinated Universal Time system was devised by an international advisory group of technical experts within the International Telecommunication Union (ITU). The ITU felt it was best to designate a single abbreviation for use in all languages in order to minimize confusion. Since unanimous agreement could not be achieved on using either the English word order, CUT, or the French word order, TUC, the acronym UTC was chosen as a compromise."""
schema,Min Value Allowed: 4.9E-324 Max Value Allowed: 1.7976931348623157E308
schema,Max Value Allowed: 1.7976931348623157E308
schema,Min Value Allowed: 4.9E-324 Max Value Allowed: 1.7976931348623157E308
schema,Max Value Allowed: 1.7976931348623157E308
schema,"Direct Known Subclasses: TrieDateField, TrieDoubleField, TrieFloatField, TrieIntField, TrieLongField @Deprecated public class TrieField extends NumericFieldType Deprecated. Trie fields are deprecated as of Solr 7.0 Provides field types to support for Lucene's LegacyIntField, LegacyLongField, LegacyFloatField and LegacyDoubleField. See LegacyNumericRangeQuery for more details. It supports integer, float, long, double and date types. For each number being added to this field, multiple terms are generated as per the algorithm described in the above link. The possible number of terms increases dramatically with lower precision steps. For the fast range search to work, trie fields must be indexed. Trie fields are sortable in numerical order and can be used in function queries. Note that if you use a precisionStep of 32 for int/float and 64 for long/double/date, then multiple terms will not be generated, range search will be no faster than any other number field, but sorting will still be possible. Since: solr 1.4 See Also: LegacyNumericRangeQuery, PointField"
schema,"For each number being added to this field, multiple terms are generated as per the algorithm described in the above link. The possible number of terms increases dramatically with lower precision steps. For the fast range search to work, trie fields must be indexed. Trie fields are sortable in numerical order and can be used in function queries. Note that if you use a precisionStep of 32 for int/float and 64 for long/double/date, then multiple terms will not be generated, range search will be no faster than any other number field, but sorting will still be possible."
schema,"Trie fields are sortable in numerical order and can be used in function queries. Note that if you use a precisionStep of 32 for int/float and 64 for long/double/date, then multiple terms will not be generated, range search will be no faster than any other number field, but sorting will still be possible."
schema,"Note that if you use a precisionStep of 32 for int/float and 64 for long/double/date, then multiple terms will not be generated, range search will be no faster than any other number field, but sorting will still be possible."
schema,Min Value Allowed: 1.401298464324817E-45 Max Value Allowed: 3.4028234663852886E38
schema,Max Value Allowed: 3.4028234663852886E38
schema,Min Value Allowed: 1.401298464324817E-45 Max Value Allowed: 3.4028234663852886E38
schema,Max Value Allowed: 3.4028234663852886E38
schema,Min Value Allowed: -2147483648 Max Value Allowed: 2147483647
schema,Max Value Allowed: 2147483647
schema,Min Value Allowed: -2147483648 Max Value Allowed: 2147483647
schema,Max Value Allowed: 2147483647
schema,Min Value Allowed: -9223372036854775808 Max Value Allowed: 9223372036854775807
schema,Max Value Allowed: 9223372036854775807
schema,Min Value Allowed: -9223372036854775808 Max Value Allowed: 9223372036854775807
schema,Max Value Allowed: 9223372036854775807
schema,"public class UUIDField extends StrField This FieldType accepts UUID string values, as well as the special value of ""NEW"" which triggers generation of a new random UUID. NOTE: Configuring a UUIDField instance with a default value of ""NEW "" is not advisable for most users when using SolrCloud (and not possible if the UUID value is configured as the unique key field) since the result will be that each replica of each document will get a unique UUID value. Using UUIDUpdateProcessorFactory to generate UUID values when documents are added is recommended instead. See Also: UUID.toString(), UUID.randomUUID()"
schema,"NOTE: Configuring a UUIDField instance with a default value of ""NEW "" is not advisable for most users when using SolrCloud (and not possible if the UUID value is configured as the unique key field) since the result will be that each replica of each document will get a unique UUID value. Using UUIDUpdateProcessorFactory to generate UUID values when documents are added is recommended instead."
schema,"Nested classes/interfaces inherited from interfaceorg.apache.zookeeper.Watcher org.apache.zookeeper.Watcher.Event, org.apache.zookeeper.Watcher.WatcherType"
schema,"Nested classes/interfaces inherited from interfaceorg.apache.zookeeper.Watcher org.apache.zookeeper.Watcher.Event, org.apache.zookeeper.Watcher.WatcherType"
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, equals, hashCode, rewrite, sameClassAs, toString"
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, createWeight, rewrite, rewrite, sameClassAs, toString"
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,Fields inherited from classorg.apache.lucene.index.FilterLeafReader.FilterPostingsEnum in
search,"Fields inherited from classorg.apache.lucene.index.PostingsEnum ALL, FREQS, NONE, OFFSETS, PAYLOADS, POSITIONS"
search,Fields inherited from classorg.apache.lucene.search.DocIdSetIterator NO_MORE_DOCS
search,"Methods inherited from classorg.apache.lucene.index.FilterLeafReader.FilterPostingsEnum cost, docID, endOffset, freq, getPayload, nextPosition, startOffset, unwrap"
search,Methods inherited from classorg.apache.lucene.index.PostingsEnum featureRequested
search,"Methods inherited from classorg.apache.lucene.search.DocIdSetIterator all, empty, range, slowAdvance"
search,Fields inherited from classorg.apache.lucene.index.FilterLeafReader.FilterPostingsEnum in
search,"Fields inherited from classorg.apache.lucene.index.PostingsEnum ALL, FREQS, NONE, OFFSETS, PAYLOADS, POSITIONS"
search,Fields inherited from classorg.apache.lucene.search.DocIdSetIterator NO_MORE_DOCS
search,Example: {!bool should=title:lucene should=title:solr must_not=id:1}
search,"Example: {!boost b=log(popularity)}foo creates a query ""foo"" which is boosted (scores are multiplied) by the function query log(popularity). The query to be boosted may be of any type. Example: {!boost b=recip(ms(NOW,mydatefield),3.16e-11,1,1)}foo creates a query ""foo"" which is boosted by the date boosting function referenced in ReciprocalFloatFunction"
search,"Example: {!boost b=recip(ms(NOW,mydatefield),3.16e-11,1,1)}foo creates a query ""foo"" which is boosted by the date boosting function referenced in ReciprocalFloatFunction"
search,"This cache supports either maximum size limit (the number of items) or maximum ram bytes limit, but not both. If both values are set then only maxRamMB limit is used and maximum size limit is ignored. W-TinyLFU [2] is a near optimal policy that uses recency and frequency to determine which entry to evict in O(1) time. The estimated frequency is retained in a Count-Min Sketch and entries reside on LRU priority queues [3]. By capturing the historic frequency of an entry, the cache is able to outperform classic policies like LRU and LFU, as well as modern policies like ARC and LIRS. This policy performed particularly well in search workloads. [1] https://github.com/ben-manes/caffeine [2] http://arxiv.org/pdf/1512.00727.pdf [3] http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html"
search,"W-TinyLFU [2] is a near optimal policy that uses recency and frequency to determine which entry to evict in O(1) time. The estimated frequency is retained in a Count-Min Sketch and entries reside on LRU priority queues [3]. By capturing the historic frequency of an entry, the cache is able to outperform classic policies like LRU and LFU, as well as modern policies like ARC and LIRS. This policy performed particularly well in search workloads. [1] https://github.com/ben-manes/caffeine [2] http://arxiv.org/pdf/1512.00727.pdf [3] http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html"
search,[1] https://github.com/ben-manes/caffeine [2] http://arxiv.org/pdf/1512.00727.pdf [3] http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
search,Methods inherited from interfaceorg.apache.lucene.search.Collector setWeight
search,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
search,Enclosing class: CollapsingQParserPlugin public static final class CollapsingQParserPlugin.CollapseScore extends Object
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, createWeight, rewrite, rewrite, sameClassAs, toString"
search,Enclosing class: CollapsingQParserPlugin public static final class CollapsingQParserPlugin.GroupHeadSelector extends Object Models all the information about how group head documents should be selected
search,"Sample syntax: Collapse based on the highest scoring document: fq=(!collapse field=field_name} Collapse based on the min value of a numeric field: fq={!collapse field=field_name min=field_name} Collapse based on the max value of a numeric field: fq={!collapse field=field_name max=field_name} Collapse with a null policy: fq={!collapse field=field_name nullPolicy=nullPolicy} There are three null policies: ignore : removes docs with a null value in the collapse field (default). expand : treats each doc with a null value in the collapse field as a separate group. collapse : collapses all docs with a null value into a single group using either highest score, or min/max. The CollapsingQParserPlugin fully supports the QueryElevationComponent"
search,"Collapse based on the highest scoring document: fq=(!collapse field=field_name} Collapse based on the min value of a numeric field: fq={!collapse field=field_name min=field_name} Collapse based on the max value of a numeric field: fq={!collapse field=field_name max=field_name} Collapse with a null policy: fq={!collapse field=field_name nullPolicy=nullPolicy} There are three null policies: ignore : removes docs with a null value in the collapse field (default). expand : treats each doc with a null value in the collapse field as a separate group. collapse : collapses all docs with a null value into a single group using either highest score, or min/max. The CollapsingQParserPlugin fully supports the QueryElevationComponent"
search,"fq=(!collapse field=field_name} Collapse based on the min value of a numeric field: fq={!collapse field=field_name min=field_name} Collapse based on the max value of a numeric field: fq={!collapse field=field_name max=field_name} Collapse with a null policy: fq={!collapse field=field_name nullPolicy=nullPolicy} There are three null policies: ignore : removes docs with a null value in the collapse field (default). expand : treats each doc with a null value in the collapse field as a separate group. collapse : collapses all docs with a null value into a single group using either highest score, or min/max. The CollapsingQParserPlugin fully supports the QueryElevationComponent"
search,"Collapse based on the min value of a numeric field: fq={!collapse field=field_name min=field_name} Collapse based on the max value of a numeric field: fq={!collapse field=field_name max=field_name} Collapse with a null policy: fq={!collapse field=field_name nullPolicy=nullPolicy} There are three null policies: ignore : removes docs with a null value in the collapse field (default). expand : treats each doc with a null value in the collapse field as a separate group. collapse : collapses all docs with a null value into a single group using either highest score, or min/max. The CollapsingQParserPlugin fully supports the QueryElevationComponent"
search,"fq={!collapse field=field_name min=field_name} Collapse based on the max value of a numeric field: fq={!collapse field=field_name max=field_name} Collapse with a null policy: fq={!collapse field=field_name nullPolicy=nullPolicy} There are three null policies: ignore : removes docs with a null value in the collapse field (default). expand : treats each doc with a null value in the collapse field as a separate group. collapse : collapses all docs with a null value into a single group using either highest score, or min/max. The CollapsingQParserPlugin fully supports the QueryElevationComponent"
search,"Collapse based on the max value of a numeric field: fq={!collapse field=field_name max=field_name} Collapse with a null policy: fq={!collapse field=field_name nullPolicy=nullPolicy} There are three null policies: ignore : removes docs with a null value in the collapse field (default). expand : treats each doc with a null value in the collapse field as a separate group. collapse : collapses all docs with a null value into a single group using either highest score, or min/max. The CollapsingQParserPlugin fully supports the QueryElevationComponent"
search,"fq={!collapse field=field_name max=field_name} Collapse with a null policy: fq={!collapse field=field_name nullPolicy=nullPolicy} There are three null policies: ignore : removes docs with a null value in the collapse field (default). expand : treats each doc with a null value in the collapse field as a separate group. collapse : collapses all docs with a null value into a single group using either highest score, or min/max. The CollapsingQParserPlugin fully supports the QueryElevationComponent"
search,"Collapse with a null policy: fq={!collapse field=field_name nullPolicy=nullPolicy} There are three null policies: ignore : removes docs with a null value in the collapse field (default). expand : treats each doc with a null value in the collapse field as a separate group. collapse : collapses all docs with a null value into a single group using either highest score, or min/max. The CollapsingQParserPlugin fully supports the QueryElevationComponent"
search,"fq={!collapse field=field_name nullPolicy=nullPolicy} There are three null policies: ignore : removes docs with a null value in the collapse field (default). expand : treats each doc with a null value in the collapse field as a separate group. collapse : collapses all docs with a null value into a single group using either highest score, or min/max. The CollapsingQParserPlugin fully supports the QueryElevationComponent"
search,"There are three null policies: ignore : removes docs with a null value in the collapse field (default). expand : treats each doc with a null value in the collapse field as a separate group. collapse : collapses all docs with a null value into a single group using either highest score, or min/max. The CollapsingQParserPlugin fully supports the QueryElevationComponent"
search,The CollapsingQParserPlugin fully supports the QueryElevationComponent
search,Modified from LuceneQParserPlugin and SurroundQParserPlugin
search,"Since this class uses ThreadCpuTimer it is irrevocably lock-hostile and can never be exposed to multiple threads, even if guarded by synchronization. Normally this is attached to objects ultimately held by a ThreadLocal in SolrRequestInfo to provide safe usage on the assumption that such objects are not shared to other threads."
search,"public final class CursorMark extends Object An object that encapsulates the basic information about the current Mark Point of a ""Cursor"" based request. CursorMark objects track the sort values of the last document returned to a user, so that SolrIndexSearcher can then be asked to find all documents ""after"" the values represented by this CursorMark."
search,Methods inherited from classorg.apache.lucene.search.SimpleCollector getLeafCollector
search,Methods inherited from interfaceorg.apache.lucene.search.Collector setWeight
search,"Methods inherited from interfaceorg.apache.lucene.search.LeafCollector collect, competitiveIterator"
search,public class DisMaxQParser extends QParser Query parser for dismax queries Note: This API is experimental and may change in non backward-compatible ways in the future
search,Note: This API is experimental and may change in non backward-compatible ways in the future
search,"q.alt - An alternate query to be used in cases where the main query (q) is not specified (or blank). This query should be expressed in the Standard SolrQueryParser syntax (you can use q.alt=*:* to denote that all documents should be returned when no query is specified) tie - (Tie breaker) float value to use as tiebreaker in DisjunctionMaxQueries (should be something much less than 1) qf - (Query Fields) fields and boosts to use when building DisjunctionMaxQueries from the users query. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. mm - (Minimum Match) this supports a wide variety of complex expressions. read SolrPluginUtils.setMinShouldMatch and mm expression format for details. pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"tie - (Tie breaker) float value to use as tiebreaker in DisjunctionMaxQueries (should be something much less than 1) qf - (Query Fields) fields and boosts to use when building DisjunctionMaxQueries from the users query. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. mm - (Minimum Match) this supports a wide variety of complex expressions. read SolrPluginUtils.setMinShouldMatch and mm expression format for details. pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"qf - (Query Fields) fields and boosts to use when building DisjunctionMaxQueries from the users query. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. mm - (Minimum Match) this supports a wide variety of complex expressions. read SolrPluginUtils.setMinShouldMatch and mm expression format for details. pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"mm - (Minimum Match) this supports a wide variety of complex expressions. read SolrPluginUtils.setMinShouldMatch and mm expression format for details. pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"q - (Query) the raw unparsed, unescaped, query from the user. sort - (Order By) list of fields and direction to sort on."
search,sort - (Order By) list of fields and direction to sort on.
search,"A Generic query plugin designed to be given a simple query expression from a user, which it will then query against a variety of pre-configured fields, in a variety of ways, using BooleanQueries, DisjunctionMaxQueries, and PhraseQueries. All of the following options may be configured for this plugin in the solrconfig as defaults, and may be overridden as request parameters q.alt - An alternate query to be used in cases where the main query (q) is not specified (or blank). This query should be expressed in the Standard SolrQueryParser syntax (you can use q.alt=*:* to denote that all documents should be returned when no query is specified) tie - (Tie breaker) float value to use as tiebreaker in DisjunctionMaxQueries (should be something much less than 1) qf - (Query Fields) fields and boosts to use when building DisjunctionMaxQueries from the users query. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. mm - (Minimum Match) this supports a wide variety of complex expressions. read SolrPluginUtils.setMinShouldMatch and mm expression format for details. pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive. The following options are only available as request params... q - (Query) the raw unparsed, unescaped, query from the user. sort - (Order By) list of fields and direction to sort on."
search,"All of the following options may be configured for this plugin in the solrconfig as defaults, and may be overridden as request parameters q.alt - An alternate query to be used in cases where the main query (q) is not specified (or blank). This query should be expressed in the Standard SolrQueryParser syntax (you can use q.alt=*:* to denote that all documents should be returned when no query is specified) tie - (Tie breaker) float value to use as tiebreaker in DisjunctionMaxQueries (should be something much less than 1) qf - (Query Fields) fields and boosts to use when building DisjunctionMaxQueries from the users query. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. mm - (Minimum Match) this supports a wide variety of complex expressions. read SolrPluginUtils.setMinShouldMatch and mm expression format for details. pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive. The following options are only available as request params... q - (Query) the raw unparsed, unescaped, query from the user. sort - (Order By) list of fields and direction to sort on."
search,"q.alt - An alternate query to be used in cases where the main query (q) is not specified (or blank). This query should be expressed in the Standard SolrQueryParser syntax (you can use q.alt=*:* to denote that all documents should be returned when no query is specified) tie - (Tie breaker) float value to use as tiebreaker in DisjunctionMaxQueries (should be something much less than 1) qf - (Query Fields) fields and boosts to use when building DisjunctionMaxQueries from the users query. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. mm - (Minimum Match) this supports a wide variety of complex expressions. read SolrPluginUtils.setMinShouldMatch and mm expression format for details. pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"tie - (Tie breaker) float value to use as tiebreaker in DisjunctionMaxQueries (should be something much less than 1) qf - (Query Fields) fields and boosts to use when building DisjunctionMaxQueries from the users query. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. mm - (Minimum Match) this supports a wide variety of complex expressions. read SolrPluginUtils.setMinShouldMatch and mm expression format for details. pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"qf - (Query Fields) fields and boosts to use when building DisjunctionMaxQueries from the users query. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. mm - (Minimum Match) this supports a wide variety of complex expressions. read SolrPluginUtils.setMinShouldMatch and mm expression format for details. pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"mm - (Minimum Match) this supports a wide variety of complex expressions. read SolrPluginUtils.setMinShouldMatch and mm expression format for details. pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"pf - (Phrase Fields) fields/boosts to make phrase queries out of, to boost the users query for exact matches on the specified fields. Format is: ""fieldA^1.0 fieldB^2.2"". This param can be specified multiple times, and the fields are additive. ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"ps - (Phrase Slop) amount of slop on phrase queries built for pf fields. qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"qs - (Query Slop) amount of slop on phrase queries explicitly specified in the ""q"" for qf fields. bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"bq - (Boost Query) a raw lucene query that will be included in the users query to influence the score. If this is a BooleanQuery with a default boost (1.0f), then the individual clauses will be added directly to the main query. Otherwise, the query will be included as is. This param can be specified multiple times, and the boosts are are additive. NOTE: the behaviour listed above is only in effect if a single bq paramter is specified. Hence you can disable it by specifying an additional, blank, bq parameter. bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"bf - (Boost Functions) functions (with optional boosts) that will be included in the users query to influence the score. Format is: ""funcA(arg1,arg2)^1.2 funcB(arg3,arg4)^2.2"". NOTE: Whitespace is not allowed in the function arguments. This param can be specified multiple times, and the functions are additive. fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"fq - (Filter Query) a raw lucene query that can be used to restrict the super set of products we are interested in - more efficient then using bq, but doesn't influence score. This param can be specified multiple times, and the filters are additive."
search,"The following options are only available as request params... q - (Query) the raw unparsed, unescaped, query from the user. sort - (Order By) list of fields and direction to sort on."
search,"q - (Query) the raw unparsed, unescaped, query from the user. sort - (Order By) list of fields and direction to sort on."
search,sort - (Order By) list of fields and direction to sort on.
search,"public final class DocListAndSet extends Object A struct whose only purpose is to hold both a DocList and a DocSet so that both may be returned from a single method. The DocList and DocSet returned should not be modified as they may have been retrieved or inserted into a cache and should be considered shared. Oh, if only java had ""out"" parameters or multiple return args... Since: solr 0.9"
search,"The DocList and DocSet returned should not be modified as they may have been retrieved or inserted into a cache and should be considered shared. Oh, if only java had ""out"" parameters or multiple return args..."
search,"Oh, if only java had ""out"" parameters or multiple return args..."
search,WARNING: Any DocSet returned from SolrIndexSearcher should not be modified as it may have been retrieved from a cache and could be shared.
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,"Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources, ramBytesUsed"
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,public final class DocSetBuilder extends Object Adapted from DocIdSetBuilder to build DocSets NOTE: This API is for internal purposes only and might change in incompatible ways in the next release.
search,Enclosing class: DocSetCollector protected static class DocSetCollector.ExpandingIntArray extends Object
search,Methods inherited from classorg.apache.lucene.search.SimpleCollector getLeafCollector
search,Methods inherited from interfaceorg.apache.lucene.search.Collector setWeight
search,"Methods inherited from interfaceorg.apache.lucene.search.LeafCollector collect, competitiveIterator, finish"
search,public class DocSetUtil extends Object WARNING: This API is experimental and might change in incompatible ways in the next release.
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,"Enclosing class: DocValuesIteratorCache public static class DocValuesIteratorCache.FieldDocValuesSupplier extends Object Supplies (and coordinates arbitrary-order value retrieval over) docValues iterators for a particular field, encapsulating the logic of iterator creation, reuse/caching, and advancing. Returned iterators are already positioned, and should not be advanced (though multi-valued iterators may consume/iterate over values/ords). Instances of this class are specifically designed to support arbitrary-order value retrieval, (e.g., useDocValuesAsStored, ExportWriter) and should generally not be used for ordered retrieval (although ordered retrieval would work perfectly fine, and would add only minimal overhead)."
search,"Instances of this class are specifically designed to support arbitrary-order value retrieval, (e.g., useDocValuesAsStored, ExportWriter) and should generally not be used for ordered retrieval (although ordered retrieval would work perfectly fine, and would add only minimal overhead)."
search,"public class DocValuesIteratorCache extends Object A helper class for random-order value access over docValues (such as in the case of useDocValuesAsStored). This class optimizes access by reusing DocValues iterators where possible, and by narrowing the scope of DocValues per-field/per-segment (shortcircuiting attempts to `advance()` to docs that are known to have no value for a given field)."
search,Fields inherited from classorg.apache.lucene.search.FilterCollector in
search,"Methods inherited from classorg.apache.lucene.search.FilterCollector scoreMode, setWeight, toString"
search,Fields inherited from classorg.apache.lucene.search.FilterCollector in
search,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
search,Enclosing class: ExportQParserPlugin public static class ExportQParserPlugin.ExportQParser extends QParser
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, sameClassAs, toString"
search,Enclosing class: ExtendedDismaxQParser protected static class ExtendedDismaxQParser.Clause extends Object
search,Enclosing class: ExtendedDismaxQParser public static class ExtendedDismaxQParser.ExtendedDismaxConfiguration extends Object Simple container for configuration information used when parsing queries
search,Enclosing class: ExtendedDismaxQParser.ExtendedSolrQueryParser protected static class ExtendedDismaxQParser.ExtendedSolrQueryParser.Alias extends Object A simple container for storing alias info
search,Nested classes/interfaces inherited from classorg.apache.lucene.util.QueryBuilder org.apache.lucene.util.QueryBuilder.TermAndBoost
search,"Fields inherited from classorg.apache.lucene.util.QueryBuilder analyzer, autoGenerateMultiTermSynonymsPhraseQuery, enableGraphQueries, enablePositionIncrements"
search,"Methods inherited from classorg.apache.lucene.util.QueryBuilder add, analyzeBoolean, analyzeGraphBoolean, analyzeGraphPhrase, analyzeMultiBoolean, analyzeMultiPhrase, analyzePhrase, analyzeTerm, createBooleanQuery, createBooleanQuery, createFieldQuery, createFieldQuery, createMinShouldMatchQuery, createPhraseQuery, createPhraseQuery, getAnalyzer, getAutoGenerateMultiTermSynonymsPhraseQuery, getEnableGraphQueries, getEnablePositionIncrements, newBooleanQuery, newMultiPhraseQueryBuilder, newTermQuery, setAnalyzer, setAutoGenerateMultiTermSynonymsPhraseQuery, setEnableGraphQueries, setEnablePositionIncrements"
search,Nested classes/interfaces inherited from classorg.apache.lucene.util.QueryBuilder org.apache.lucene.util.QueryBuilder.TermAndBoost
search,public class ExtendedDismaxQParser extends QParser Query parser that generates DisjunctionMaxQueries based on user configuration. See the Reference Guide page
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, createWeight, equals, hashCode, rewrite, rewrite, sameClassAs, toString, visit"
search,"public class FieldParams extends Object A class to hold ""phrase slop"" and ""boost"" parameters for pf, pf2, pf3 parameters"
search,"Technically, this same functionality could be achieved with ChainedFilter (under queries/), however the benefit of this class is it never materializes the full bitset for the filter. Instead, the match(int) method is invoked on-demand, per docID visited during searching. If you know few docIDs will be visited, and the logic behind match(int) is relatively costly, this may be a better way to filter than ChainedFilter."
search,Fields inherited from classorg.apache.lucene.search.DocIdSet EMPTY
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,Methods inherited from classorg.apache.lucene.search.DocIdSet all
search,Fields inherited from classorg.apache.lucene.search.DocIdSet EMPTY
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,public class FloatPayloadValueSource extends org.apache.lucene.queries.function.ValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class FunctionQParser extends QParser
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, rewrite, sameClassAs, toString"
search,"Syntax: {!graphTerms f=field maxDocFreq=10000}term1,term2,term3"
search,"Direct Known Subclasses: Grouping.CommandField, Grouping.CommandFunc, Grouping.CommandQuery Enclosing class: Grouping public abstract class Grouping.Command<T> extends Object General group command. A group command is responsible for creating the first and second pass collectors. A group command is also responsible for creating the response structure. Note: Maybe the creating the response structure should be done in something like a ReponseBuilder??? Warning NOT thread save!"
search,Note: Maybe the creating the response structure should be done in something like a ReponseBuilder??? Warning NOT thread save!
search,Enclosing class: Grouping public class Grouping.CommandField extends Grouping.Command<org.apache.lucene.util.BytesRef> A group command for grouping on a field.
search,Enclosing class: Grouping public class Grouping.CommandFunc extends Grouping.Command<org.apache.lucene.util.mutable.MutableValue> A command for grouping on a function.
search,Enclosing class: Grouping public class Grouping.CommandQuery extends Grouping.Command<Object> A group command for grouping on a query.
search,public class Grouping extends Object Basic Solr Grouping infrastructure. Warning NOT thread safe! WARNING: This API is experimental and might change in incompatible ways in the next release.
search,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
search,Attribute name
search,Specified by type
search,Specified by type
search,"int, long, float, double"
search,"true, false"
search,"true, false"
search,A ParserException will be thrown if an error occurs parsing the supplied lowerTerm or upperTerm into the numeric type specified by type.
search,Direct Known Subclasses: MaxScoreQParser public class LuceneQParser extends QParser See Also: LuceneQParserPlugin
search,"q.op - the default operator ""OR"" or ""AND"" df - the default field name sow - split on whitespace prior to analysis, boolean, default= false"
search,"df - the default field name sow - split on whitespace prior to analysis, boolean, default= false"
search,"sow - split on whitespace prior to analysis, boolean, default= false"
search,"q.op - the default operator ""OR"" or ""AND"" df - the default field name sow - split on whitespace prior to analysis, boolean, default= false"
search,"df - the default field name sow - split on whitespace prior to analysis, boolean, default= false"
search,"sow - split on whitespace prior to analysis, boolean, default= false"
search,public class MatchCostQuery extends org.apache.lucene.search.Query Wraps a Query to customize the TwoPhaseIterator.matchCost().
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, sameClassAs, toString"
search,"Methods inherited from classorg.apache.lucene.search.SimpleCollector doSetNextReader, getLeafCollector"
search,Methods inherited from interfaceorg.apache.lucene.search.Collector setWeight
search,"Methods inherited from interfaceorg.apache.lucene.search.LeafCollector collect, competitiveIterator, finish"
search,public class MaxScoreQParser extends LuceneQParser See Also: MaxScoreQParserPlugin
search,"This class tracks per-thread memory allocations during a request using its own ThreadLocal. It records the current thread allocation when the instance was created (typically at the start of SolrQueryRequest processing) as a starting point, and then on every call to shouldExit() it accumulates the amount of reported allocated memory since the previous call, and compares the accumulated amount to the configured threshold, expressed in mebi-bytes. NOTE: this class accesses com.sun.management.ThreadMXBean#getCurrentThreadAllocatedBytes using reflection. On JVM-s where this implementation is not available an exception will be thrown when attempting to use the memAllowed parameter."
search,NOTE: this class accesses com.sun.management.ThreadMXBean#getCurrentThreadAllocatedBytes using reflection. On JVM-s where this implementation is not available an exception will be thrown when attempting to use the memAllowed parameter.
search,public class MinHashQParser extends QParser The query parser can be used in two modes 1) where text is analysed and generates min hashes as part of normal lucene analysis 2) where text is pre-analysed and hashes are added as string to the index An analyzer can still be defined to support text based query against the text field Options: sim - required similary - default is 1 tp - required true positive rate - default is 1 field - when providing text the analyser for this field is used to generate the finger print sep - a separator for provided hashes analyzer_field - the field to use for for analysing suppplied text - if not supplied defaults to field
search,Options: sim - required similary - default is 1 tp - required true positive rate - default is 1 field - when providing text the analyser for this field is used to generate the finger print sep - a separator for provided hashes analyzer_field - the field to use for for analysing suppplied text - if not supplied defaults to field
search,public class MultiThreadedSearcher extends Object
search,This is useful for e.g. CachingWrapperFilters that are not invalidated by the creation of a new searcher.
search,calling ord or rord functions on a single-valued numeric field. doing grouped faceting (group.facet) on a single-valued numeric field.
search,doing grouped faceting (group.facet) on a single-valued numeric field.
search,calling ord or rord functions on a single-valued numeric field. doing grouped faceting (group.facet) on a single-valued numeric field.
search,doing grouped faceting (group.facet) on a single-valued numeric field.
search,"Nested classes/interfaces inherited from classorg.apache.lucene.index.FilterLeafReader org.apache.lucene.index.FilterLeafReader.FilterFields, org.apache.lucene.index.FilterLeafReader.FilterPostingsEnum, org.apache.lucene.index.FilterLeafReader.FilterTerms, org.apache.lucene.index.FilterLeafReader.FilterTermsEnum"
search,"Nested classes/interfaces inherited from classorg.apache.lucene.index.IndexReader org.apache.lucene.index.IndexReader.CacheHelper, org.apache.lucene.index.IndexReader.CacheKey, org.apache.lucene.index.IndexReader.ClosedListener"
search,Fields inherited from classorg.apache.lucene.index.FilterLeafReader in
search,"Methods inherited from classorg.apache.lucene.index.FilterLeafReader checkIntegrity, doClose, document, getByteVectorValues, getDelegate, getFloatVectorValues, getLiveDocs, getMetaData, getNormValues, getPointValues, getSortedNumericDocValues, getTermVectors, maxDoc, numDocs, searchNearestVectors, searchNearestVectors, storedFields, terms, termVectors, toString, unwrap"
search,"Methods inherited from classorg.apache.lucene.index.LeafReader docFreq, getContext, getDocCount, getSumDocFreq, getSumTotalTermFreq, postings, postings, searchNearestVectors, searchNearestVectors, totalTermFreq"
search,"Methods inherited from classorg.apache.lucene.index.IndexReader close, decRef, document, document, ensureOpen, equals, getRefCount, getTermVector, hasDeletions, hashCode, incRef, leaves, notifyReaderClosedListeners, numDeletedDocs, registerParentReader, tryIncRef"
search,"Nested classes/interfaces inherited from classorg.apache.lucene.index.FilterLeafReader org.apache.lucene.index.FilterLeafReader.FilterFields, org.apache.lucene.index.FilterLeafReader.FilterPostingsEnum, org.apache.lucene.index.FilterLeafReader.FilterTerms, org.apache.lucene.index.FilterLeafReader.FilterTermsEnum"
search,"Nested classes/interfaces inherited from classorg.apache.lucene.index.IndexReader org.apache.lucene.index.IndexReader.CacheHelper, org.apache.lucene.index.IndexReader.CacheKey, org.apache.lucene.index.IndexReader.ClosedListener"
search,public class PointMerger extends Object Merge multiple numeric point fields (segments) together. WARNING: This API is experimental and might change in incompatible ways in the next release. NOTE: This API is for internal purposes only and might change in incompatible ways in the next release.
search,Enclosing class: PointMerger public static class PointMerger.ValueIterator extends Object
search,"Direct Known Subclasses: AbstractVectorQParserBase, CrossCollectionJoinQParser, DisMaxQParser, ExportQParserPlugin.ExportQParser, ExtendedDismaxQParser, FiltersQParser, FunctionQParser, GraphQueryParser, HashRangeQParser, LuceneQParser, MinHashQParser, RankQParserPlugin.RankQParser, SimpleMLTQParser, SpatialFilterQParser public abstract class QParser extends Object Note: This API is experimental and may change in non backward-compatible ways in the future"
search,public class QueryCommand extends Object A query request command to avoid having to change the method signatures if we want to pass additional information to the searcher.
search,"Nested classes/interfaces inherited from classjava.util.AbstractMap AbstractMap.SimpleEntry<K extends Object,V extends Object>, AbstractMap.SimpleImmutableEntry<K extends Object,V extends Object>"
search,"Nested classes/interfaces inherited from interfacejava.util.Map Map.Entry<K extends Object,V extends Object>"
search,"Methods inherited from classjava.util.IdentityHashMap clear, clone, containsKey, containsValue, entrySet, equals, forEach, get, hashCode, isEmpty, keySet, put, putAll, remove, replaceAll, size, values"
search,Methods inherited from classjava.util.AbstractMap toString
search,"Methods inherited from interfacejava.util.Map compute, computeIfAbsent, computeIfPresent, getOrDefault, merge, putIfAbsent, remove, replace, replace"
search,"Nested classes/interfaces inherited from classjava.util.AbstractMap AbstractMap.SimpleEntry<K extends Object,V extends Object>, AbstractMap.SimpleImmutableEntry<K extends Object,V extends Object>"
search,"Nested classes/interfaces inherited from interfacejava.util.Map Map.Entry<K extends Object,V extends Object>"
search,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
search,public class QueryParsing extends Object Collection of static utilities useful for query parsing.
search,public class QueryResult extends Object The result of a search.
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,public class QueryUtils extends Object
search,Enclosing class: RankQParserPlugin public static class RankQParserPlugin.RankQParser extends QParser
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, createWeight, equals, hashCode, rewrite, rewrite, sameClassAs, toString, visit"
search,"Methods inherited from interfacejava.util.Comparator equals, reversed, thenComparing, thenComparing, thenComparing, thenComparingDouble, thenComparingInt, thenComparingLong"
search,"Fields inherited from classorg.apache.lucene.search.TopDocsCollector EMPTY_TOPDOCS, pq, totalHits, totalHitsRelation"
search,"Methods inherited from classorg.apache.lucene.search.TopDocsCollector newTopDocs, populateResults, topDocs, topDocs, topDocsSize"
search,Methods inherited from interfaceorg.apache.lucene.search.Collector setWeight
search,public class ReRankScaler extends Object
search,Enclosing class: ReRankScaler public static final class ReRankScaler.MinMaxExplain extends Object
search,Enclosing class: ReRankScaler public static final class ReRankScaler.ReRankScalerExplain extends Object
search,Nested classes/interfaces inherited from classorg.apache.lucene.search.Weight org.apache.lucene.search.Weight.DefaultBulkScorer
search,Fields inherited from classorg.apache.lucene.search.FilterWeight in
search,Fields inherited from classorg.apache.lucene.search.Weight parentQuery
search,"Methods inherited from classorg.apache.lucene.search.FilterWeight isCacheable, matches, scorer"
search,"Methods inherited from classorg.apache.lucene.search.Weight bulkScorer, count, getQuery, scorerSupplier"
search,Nested classes/interfaces inherited from classorg.apache.lucene.search.Weight org.apache.lucene.search.Weight.DefaultBulkScorer
search,Direct Known Subclasses: SolrReturnFields public abstract class ReturnFields extends Object A class representing the return fields Since: solr 4.0
search,"q.operators - Used to enable specific operations for parsing. The operations that can be enabled are and, not, or, prefix, phrase, precedence, escape, and whitespace. By default all operations are enabled. All operations can be disabled by passing in an empty string to this parameter. q.op - Used to specify the operator to be used if whitespace is a delimiter. Either 'AND' or 'OR' can be specified for this parameter. Any other string will cause an exception to be thrown. If this parameter is not specified 'OR' will be used by default. qf - The list of query fields and boosts to use when building the simple query. The format is the following: fieldA^1.0 fieldB^2.2. A field can also be specified without a boost by simply listing the field as fieldA fieldB. Any field without a boost will default to use a boost of 1.0. df - An override for the default field specified in the schema or a default field if one is not specified in the schema. If qf is not specified the default field will be used as the field to run the query against."
search,q.op - Used to specify the operator to be used if whitespace is a delimiter. Either 'AND' or 'OR' can be specified for this parameter. Any other string will cause an exception to be thrown. If this parameter is not specified 'OR' will be used by default. qf - The list of query fields and boosts to use when building the simple query. The format is the following: fieldA^1.0 fieldB^2.2. A field can also be specified without a boost by simply listing the field as fieldA fieldB. Any field without a boost will default to use a boost of 1.0. df - An override for the default field specified in the schema or a default field if one is not specified in the schema. If qf is not specified the default field will be used as the field to run the query against.
search,qf - The list of query fields and boosts to use when building the simple query. The format is the following: fieldA^1.0 fieldB^2.2. A field can also be specified without a boost by simply listing the field as fieldA fieldB. Any field without a boost will default to use a boost of 1.0. df - An override for the default field specified in the schema or a default field if one is not specified in the schema. If qf is not specified the default field will be used as the field to run the query against.
search,df - An override for the default field specified in the schema or a default field if one is not specified in the schema. If qf is not specified the default field will be used as the field to run the query against.
search,"q.operators - Used to enable specific operations for parsing. The operations that can be enabled are and, not, or, prefix, phrase, precedence, escape, and whitespace. By default all operations are enabled. All operations can be disabled by passing in an empty string to this parameter. q.op - Used to specify the operator to be used if whitespace is a delimiter. Either 'AND' or 'OR' can be specified for this parameter. Any other string will cause an exception to be thrown. If this parameter is not specified 'OR' will be used by default. qf - The list of query fields and boosts to use when building the simple query. The format is the following: fieldA^1.0 fieldB^2.2. A field can also be specified without a boost by simply listing the field as fieldA fieldB. Any field without a boost will default to use a boost of 1.0. df - An override for the default field specified in the schema or a default field if one is not specified in the schema. If qf is not specified the default field will be used as the field to run the query against."
search,q.op - Used to specify the operator to be used if whitespace is a delimiter. Either 'AND' or 'OR' can be specified for this parameter. Any other string will cause an exception to be thrown. If this parameter is not specified 'OR' will be used by default. qf - The list of query fields and boosts to use when building the simple query. The format is the following: fieldA^1.0 fieldB^2.2. A field can also be specified without a boost by simply listing the field as fieldA fieldB. Any field without a boost will default to use a boost of 1.0. df - An override for the default field specified in the schema or a default field if one is not specified in the schema. If qf is not specified the default field will be used as the field to run the query against.
search,qf - The list of query fields and boosts to use when building the simple query. The format is the following: fieldA^1.0 fieldB^2.2. A field can also be specified without a boost by simply listing the field as fieldA fieldB. Any field without a boost will default to use a boost of 1.0. df - An override for the default field specified in the schema or a default field if one is not specified in the schema. If qf is not specified the default field will be used as the field to run the query against.
search,df - An override for the default field specified in the schema or a default field if one is not specified in the schema. If qf is not specified the default field will be used as the field to run the query against.
search,Enclosing class: SolrCacheBase public static class SolrCacheBase.AutoWarmCountRef extends Object Decides how many things to autowarm based on the size of another cache
search,Direct Known Subclasses: CaffeineCache public abstract class SolrCacheBase extends Object Common base class of reusable functionality for SolrCaches
search,"Fields inherited from classorg.apache.lucene.queryparser.xml.CoreParser analyzer, defaultField, DISALLOW_EXTERNAL_ENTITY_RESOLVER, parser, queryFactory, spanFactory"
search,"Methods inherited from classorg.apache.lucene.queryparser.xml.CoreParser addQueryBuilder, addSpanBuilder, addSpanQueryBuilder, getEntityResolver, getQuery, getSpanQuery, parse"
search,"Fields inherited from classorg.apache.lucene.queryparser.xml.CoreParser analyzer, defaultField, DISALLOW_EXTERNAL_ENTITY_RESOLVER, parser, queryFactory, spanFactory"
search,public class SolrDocumentFetcher extends Object A helper class of SolrIndexSearcher for stored Document related matters including DocValue substitutions.
search,Enclosing class: SolrIndexSearcher public static class SolrIndexSearcher.DocsEnumState extends Object
search,"Nested classes/interfaces inherited from classorg.apache.lucene.search.IndexSearcher org.apache.lucene.search.IndexSearcher.LeafSlice, org.apache.lucene.search.IndexSearcher.TooManyClauses, org.apache.lucene.search.IndexSearcher.TooManyNestedClauses"
search,"Fields inherited from classorg.apache.lucene.search.IndexSearcher leafContexts, readerContext"
search,"Methods inherited from classorg.apache.lucene.search.IndexSearcher count, createWeight, explain, getDefaultQueryCache, getDefaultQueryCachingPolicy, getDefaultSimilarity, getExecutor, getLeafContexts, getMaxClauseCount, getQueryCache, getQueryCachingPolicy, getSimilarity, getSlices, getTaskExecutor, getTimeout, getTopReaderContext, rewrite, search, search, search, search, search, searchAfter, searchAfter, searchAfter, setDefaultQueryCache, setDefaultQueryCachingPolicy, setMaxClauseCount, setQueryCache, setQueryCachingPolicy, setSimilarity, setTimeout, slices, slices, storedFields, timedOut"
search,"Nested classes/interfaces inherited from classorg.apache.lucene.search.IndexSearcher org.apache.lucene.search.IndexSearcher.LeafSlice, org.apache.lucene.search.IndexSearcher.TooManyClauses, org.apache.lucene.search.IndexSearcher.TooManyNestedClauses"
search,"Enclosing class: SolrIndexSearcher public static class SolrIndexSearcher.ProcessedFilter extends Object INTERNAL: The response object from SolrIndexSearcher.getProcessedFilter(DocSet, List). Holds a filter and postFilter pair that together match a set of documents. Either of them may be null, in which case the semantics are to match everything. See Also: SolrIndexSearcher.getProcessedFilter(DocSet, List)"
search,Methods inherited from interfaceorg.apache.lucene.queryparser.xml.QueryBuilder getQuery
search,Nested classes/interfaces inherited from classorg.apache.lucene.util.QueryBuilder org.apache.lucene.util.QueryBuilder.TermAndBoost
search,"Fields inherited from classorg.apache.lucene.util.QueryBuilder analyzer, autoGenerateMultiTermSynonymsPhraseQuery, enableGraphQueries, enablePositionIncrements"
search,"Methods inherited from classorg.apache.lucene.util.QueryBuilder add, analyzeBoolean, analyzeGraphBoolean, analyzeGraphPhrase, analyzeMultiBoolean, analyzeMultiPhrase, analyzePhrase, analyzeTerm, createBooleanQuery, createBooleanQuery, createFieldQuery, createFieldQuery, createMinShouldMatchQuery, createPhraseQuery, createPhraseQuery, getAnalyzer, getAutoGenerateMultiTermSynonymsPhraseQuery, getEnableGraphQueries, getEnablePositionIncrements, newBooleanQuery, newMultiPhraseQueryBuilder, newTermQuery, setAnalyzer, setAutoGenerateMultiTermSynonymsPhraseQuery, setEnableGraphQueries, setEnablePositionIncrements"
search,Nested classes/interfaces inherited from classorg.apache.lucene.util.QueryBuilder org.apache.lucene.util.QueryBuilder.TermAndBoost
search,public class SolrReturnFields extends ReturnFields The default implementation of return fields parsing for Solr.
search,"public class SolrSearcherRequirementDetector extends org.apache.lucene.search.QueryVisitor Detects whether a query can be run using a standard Lucene IndexSearcher Some Solr Query implementations are written to assume access to a SolrIndexSearcher. But these objects aren't always available: some code-paths (e.g. when executing a ""delete-by-query"") execute the query using the standard IndexSearcher available in Lucene. This QueryVisitor allows code to detect whether a given Query requires SolrIndexSearcher or not. Instances should not be reused for multiple query-tree inspections. See Also: SolrSearcherRequirer WARNING: This API is experimental and might change in incompatible ways in the next release."
search,"Some Solr Query implementations are written to assume access to a SolrIndexSearcher. But these objects aren't always available: some code-paths (e.g. when executing a ""delete-by-query"") execute the query using the standard IndexSearcher available in Lucene. This QueryVisitor allows code to detect whether a given Query requires SolrIndexSearcher or not. Instances should not be reused for multiple query-tree inspections."
search,Instances should not be reused for multiple query-tree inspections.
search,Fields inherited from classorg.apache.lucene.search.QueryVisitor EMPTY_VISITOR
search,"Methods inherited from classorg.apache.lucene.search.QueryVisitor acceptField, consumeTerms, consumeTermsMatching, termCollector"
search,Fields inherited from classorg.apache.lucene.search.QueryVisitor EMPTY_VISITOR
search,Methods inherited from interfaceorg.apache.lucene.queryparser.xml.QueryBuilder getQuery
search,Methods inherited from interfaceorg.apache.lucene.queryparser.xml.builders.SpanQueryBuilder getSpanQuery
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,public class SortSpec extends Object SortSpec encapsulates a Lucene Sort and a count of the number of documents to return.
search,public class SortSpecParsing extends Object
search,public class SpatialFilterQParser extends QParser See Also: SpatialFilterQParserPlugin
search,sfield - The field to filter on. Required. pt - The point to use as a reference. Must match the dimension of the field. Required. d - The distance in km. Required.
search,pt - The point to use as a reference. Must match the dimension of the field. Required. d - The distance in km. Required.
search,d - The distance in km. Required.
search,"The field must implement SpatialQueryable All units are in Kilometers Syntax: {!geofilt sfield=<location_field> pt=<lat,lon> d=<distance>} Parameters: sfield - The field to filter on. Required. pt - The point to use as a reference. Must match the dimension of the field. Required. d - The distance in km. Required. The distance measure used currently depends on the FieldType. LatLonPointSpatialField defaults to using haversine, PointType defaults to Euclidean (2-norm). Examples: fq={!geofilt sfield=store pt=10.312,-20.556 d=3.5} fq={!geofilt sfield=store}&pt=10.312,-20&d=3.5 fq={!geofilt}&sfield=store&pt=10.312,-20&d=3.5 Note: The geofilt for LatLonPointSpatialField is capable of also producing scores equal to the computed distance from the point to the field, making it useful as a component of the main query or a boosting query."
search,"All units are in Kilometers Syntax: {!geofilt sfield=<location_field> pt=<lat,lon> d=<distance>} Parameters: sfield - The field to filter on. Required. pt - The point to use as a reference. Must match the dimension of the field. Required. d - The distance in km. Required. The distance measure used currently depends on the FieldType. LatLonPointSpatialField defaults to using haversine, PointType defaults to Euclidean (2-norm). Examples: fq={!geofilt sfield=store pt=10.312,-20.556 d=3.5} fq={!geofilt sfield=store}&pt=10.312,-20&d=3.5 fq={!geofilt}&sfield=store&pt=10.312,-20&d=3.5 Note: The geofilt for LatLonPointSpatialField is capable of also producing scores equal to the computed distance from the point to the field, making it useful as a component of the main query or a boosting query."
search,"Syntax: {!geofilt sfield=<location_field> pt=<lat,lon> d=<distance>} Parameters: sfield - The field to filter on. Required. pt - The point to use as a reference. Must match the dimension of the field. Required. d - The distance in km. Required. The distance measure used currently depends on the FieldType. LatLonPointSpatialField defaults to using haversine, PointType defaults to Euclidean (2-norm). Examples: fq={!geofilt sfield=store pt=10.312,-20.556 d=3.5} fq={!geofilt sfield=store}&pt=10.312,-20&d=3.5 fq={!geofilt}&sfield=store&pt=10.312,-20&d=3.5 Note: The geofilt for LatLonPointSpatialField is capable of also producing scores equal to the computed distance from the point to the field, making it useful as a component of the main query or a boosting query."
search,"Parameters: sfield - The field to filter on. Required. pt - The point to use as a reference. Must match the dimension of the field. Required. d - The distance in km. Required. The distance measure used currently depends on the FieldType. LatLonPointSpatialField defaults to using haversine, PointType defaults to Euclidean (2-norm). Examples: fq={!geofilt sfield=store pt=10.312,-20.556 d=3.5} fq={!geofilt sfield=store}&pt=10.312,-20&d=3.5 fq={!geofilt}&sfield=store&pt=10.312,-20&d=3.5 Note: The geofilt for LatLonPointSpatialField is capable of also producing scores equal to the computed distance from the point to the field, making it useful as a component of the main query or a boosting query."
search,sfield - The field to filter on. Required. pt - The point to use as a reference. Must match the dimension of the field. Required. d - The distance in km. Required.
search,pt - The point to use as a reference. Must match the dimension of the field. Required. d - The distance in km. Required.
search,d - The distance in km. Required.
search,"Examples: fq={!geofilt sfield=store pt=10.312,-20.556 d=3.5} fq={!geofilt sfield=store}&pt=10.312,-20&d=3.5 fq={!geofilt}&sfield=store&pt=10.312,-20&d=3.5 Note: The geofilt for LatLonPointSpatialField is capable of also producing scores equal to the computed distance from the point to the field, making it useful as a component of the main query or a boosting query."
search,"Note: The geofilt for LatLonPointSpatialField is capable of also producing scores equal to the computed distance from the point to the field, making it useful as a component of the main query or a boosting query."
search,public class SpatialOptions extends Object
search,public class StrParser extends Object Simple class to help with parsing a string. Note: This API is experimental and may change in non backward-compatible ways in the future
search,Note that the query string is not analyzed in any way
search,"QParser's produced by this plugin will take their primary input string, trimmed and prefixed with ""case."", to use as a key to lookup a ""switch case"" in the parser's local params. If a matching local param is found the resulting param value will then be parsed as a subquery, and returned as the parse result. The ""case"" local param can be optionally be specified as a switch case to match missing (or blank) input strings. The ""default"" local param can optionally be specified as a default case to use if the input string does not match any other switch case local params. If default is not specified, then any input which does not match a switch case local param will result in a syntax error. In the examples below, the result of each query would be XXX.... q={!switch case.foo=XXX case.bar=zzz case.yak=qqq}foo q={!switch case.foo=qqq case.bar=XXX case.yak=zzz} bar // extra whitespace q={!switch case.foo=qqq case.bar=zzz default=XXX}asdf // fallback on default q={!switch case=XXX case.bar=zzz case.yak=qqq} // blank input A practical usage of this QParsePlugin, is in specifying ""appends"" fq params in the configuration of a SearchHandler, to provide a fixed set of filter options for clients using custom parameter names. Using the example configuration below, clients can optionally specify the custom parameters in_stock and shipping to override the default filtering behavior, but are limited to the specific set of legal values ( shipping=any|free, in_stock=yes|no|all). <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""defaults""> <str name=""in_stock"">yes</str> <str name=""shipping"">any</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case.all='*:*' case.yes='inStock:true' case.no='inStock:false' v=$in_stock}</str> <str name=""fq"">{!switch case.any='*:*' case.free='shipping_cost:0.0' v=$shipping}</str> </lst> </requestHandler> A slightly more interesting variant of the shipping example above, would be to combine the switch parser with the frange parser, to allow the client to specify an arbitrary ""max shipping"" amount that will be used to build a filter if and only if a value is specified. Example: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""shipping_fq"">{!frange u=$shipping}shipping_cost</str> </lst> <lst name=""defaults""> <str name=""shipping"">any</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' case.any='*:*' default=$shipping_fq v=$shipping}</str> </lst> </requestHandler> With the above configuration a client that specifies shipping=any, or does not specify a shipping param at all, will not have the results filtered. But if a client specifies a numeric value (ie: shipping=10, shipping=5, etc..) then the results will be limited to documents whose shipping_cost field has a value less then that number. A similar use case would be to combine the switch parser with the bbox parser to support an optional geographic filter that is applied if and only if the client specifies a location param containing a lat,lon pair to be used as the center of the bounding box: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""bbox_fq"">{!bbox pt=$location sfield=geo d=$dist}</str> </lst> <lst name=""defaults""> <str name=""dist"">100</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' default=$bbox_fq v=$location}</str> </lst> </requestHandler>"
search,"The ""case"" local param can be optionally be specified as a switch case to match missing (or blank) input strings. The ""default"" local param can optionally be specified as a default case to use if the input string does not match any other switch case local params. If default is not specified, then any input which does not match a switch case local param will result in a syntax error. In the examples below, the result of each query would be XXX.... q={!switch case.foo=XXX case.bar=zzz case.yak=qqq}foo q={!switch case.foo=qqq case.bar=XXX case.yak=zzz} bar // extra whitespace q={!switch case.foo=qqq case.bar=zzz default=XXX}asdf // fallback on default q={!switch case=XXX case.bar=zzz case.yak=qqq} // blank input A practical usage of this QParsePlugin, is in specifying ""appends"" fq params in the configuration of a SearchHandler, to provide a fixed set of filter options for clients using custom parameter names. Using the example configuration below, clients can optionally specify the custom parameters in_stock and shipping to override the default filtering behavior, but are limited to the specific set of legal values ( shipping=any|free, in_stock=yes|no|all). <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""defaults""> <str name=""in_stock"">yes</str> <str name=""shipping"">any</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case.all='*:*' case.yes='inStock:true' case.no='inStock:false' v=$in_stock}</str> <str name=""fq"">{!switch case.any='*:*' case.free='shipping_cost:0.0' v=$shipping}</str> </lst> </requestHandler> A slightly more interesting variant of the shipping example above, would be to combine the switch parser with the frange parser, to allow the client to specify an arbitrary ""max shipping"" amount that will be used to build a filter if and only if a value is specified. Example: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""shipping_fq"">{!frange u=$shipping}shipping_cost</str> </lst> <lst name=""defaults""> <str name=""shipping"">any</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' case.any='*:*' default=$shipping_fq v=$shipping}</str> </lst> </requestHandler> With the above configuration a client that specifies shipping=any, or does not specify a shipping param at all, will not have the results filtered. But if a client specifies a numeric value (ie: shipping=10, shipping=5, etc..) then the results will be limited to documents whose shipping_cost field has a value less then that number. A similar use case would be to combine the switch parser with the bbox parser to support an optional geographic filter that is applied if and only if the client specifies a location param containing a lat,lon pair to be used as the center of the bounding box: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""bbox_fq"">{!bbox pt=$location sfield=geo d=$dist}</str> </lst> <lst name=""defaults""> <str name=""dist"">100</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' default=$bbox_fq v=$location}</str> </lst> </requestHandler>"
search,"In the examples below, the result of each query would be XXX.... q={!switch case.foo=XXX case.bar=zzz case.yak=qqq}foo q={!switch case.foo=qqq case.bar=XXX case.yak=zzz} bar // extra whitespace q={!switch case.foo=qqq case.bar=zzz default=XXX}asdf // fallback on default q={!switch case=XXX case.bar=zzz case.yak=qqq} // blank input A practical usage of this QParsePlugin, is in specifying ""appends"" fq params in the configuration of a SearchHandler, to provide a fixed set of filter options for clients using custom parameter names. Using the example configuration below, clients can optionally specify the custom parameters in_stock and shipping to override the default filtering behavior, but are limited to the specific set of legal values ( shipping=any|free, in_stock=yes|no|all). <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""defaults""> <str name=""in_stock"">yes</str> <str name=""shipping"">any</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case.all='*:*' case.yes='inStock:true' case.no='inStock:false' v=$in_stock}</str> <str name=""fq"">{!switch case.any='*:*' case.free='shipping_cost:0.0' v=$shipping}</str> </lst> </requestHandler> A slightly more interesting variant of the shipping example above, would be to combine the switch parser with the frange parser, to allow the client to specify an arbitrary ""max shipping"" amount that will be used to build a filter if and only if a value is specified. Example: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""shipping_fq"">{!frange u=$shipping}shipping_cost</str> </lst> <lst name=""defaults""> <str name=""shipping"">any</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' case.any='*:*' default=$shipping_fq v=$shipping}</str> </lst> </requestHandler> With the above configuration a client that specifies shipping=any, or does not specify a shipping param at all, will not have the results filtered. But if a client specifies a numeric value (ie: shipping=10, shipping=5, etc..) then the results will be limited to documents whose shipping_cost field has a value less then that number. A similar use case would be to combine the switch parser with the bbox parser to support an optional geographic filter that is applied if and only if the client specifies a location param containing a lat,lon pair to be used as the center of the bounding box: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""bbox_fq"">{!bbox pt=$location sfield=geo d=$dist}</str> </lst> <lst name=""defaults""> <str name=""dist"">100</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' default=$bbox_fq v=$location}</str> </lst> </requestHandler>"
search,"A practical usage of this QParsePlugin, is in specifying ""appends"" fq params in the configuration of a SearchHandler, to provide a fixed set of filter options for clients using custom parameter names. Using the example configuration below, clients can optionally specify the custom parameters in_stock and shipping to override the default filtering behavior, but are limited to the specific set of legal values ( shipping=any|free, in_stock=yes|no|all). <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""defaults""> <str name=""in_stock"">yes</str> <str name=""shipping"">any</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case.all='*:*' case.yes='inStock:true' case.no='inStock:false' v=$in_stock}</str> <str name=""fq"">{!switch case.any='*:*' case.free='shipping_cost:0.0' v=$shipping}</str> </lst> </requestHandler> A slightly more interesting variant of the shipping example above, would be to combine the switch parser with the frange parser, to allow the client to specify an arbitrary ""max shipping"" amount that will be used to build a filter if and only if a value is specified. Example: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""shipping_fq"">{!frange u=$shipping}shipping_cost</str> </lst> <lst name=""defaults""> <str name=""shipping"">any</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' case.any='*:*' default=$shipping_fq v=$shipping}</str> </lst> </requestHandler> With the above configuration a client that specifies shipping=any, or does not specify a shipping param at all, will not have the results filtered. But if a client specifies a numeric value (ie: shipping=10, shipping=5, etc..) then the results will be limited to documents whose shipping_cost field has a value less then that number. A similar use case would be to combine the switch parser with the bbox parser to support an optional geographic filter that is applied if and only if the client specifies a location param containing a lat,lon pair to be used as the center of the bounding box: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""bbox_fq"">{!bbox pt=$location sfield=geo d=$dist}</str> </lst> <lst name=""defaults""> <str name=""dist"">100</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' default=$bbox_fq v=$location}</str> </lst> </requestHandler>"
search,"A slightly more interesting variant of the shipping example above, would be to combine the switch parser with the frange parser, to allow the client to specify an arbitrary ""max shipping"" amount that will be used to build a filter if and only if a value is specified. Example: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""shipping_fq"">{!frange u=$shipping}shipping_cost</str> </lst> <lst name=""defaults""> <str name=""shipping"">any</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' case.any='*:*' default=$shipping_fq v=$shipping}</str> </lst> </requestHandler> With the above configuration a client that specifies shipping=any, or does not specify a shipping param at all, will not have the results filtered. But if a client specifies a numeric value (ie: shipping=10, shipping=5, etc..) then the results will be limited to documents whose shipping_cost field has a value less then that number. A similar use case would be to combine the switch parser with the bbox parser to support an optional geographic filter that is applied if and only if the client specifies a location param containing a lat,lon pair to be used as the center of the bounding box: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""bbox_fq"">{!bbox pt=$location sfield=geo d=$dist}</str> </lst> <lst name=""defaults""> <str name=""dist"">100</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' default=$bbox_fq v=$location}</str> </lst> </requestHandler>"
search,"With the above configuration a client that specifies shipping=any, or does not specify a shipping param at all, will not have the results filtered. But if a client specifies a numeric value (ie: shipping=10, shipping=5, etc..) then the results will be limited to documents whose shipping_cost field has a value less then that number. A similar use case would be to combine the switch parser with the bbox parser to support an optional geographic filter that is applied if and only if the client specifies a location param containing a lat,lon pair to be used as the center of the bounding box: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""bbox_fq"">{!bbox pt=$location sfield=geo d=$dist}</str> </lst> <lst name=""defaults""> <str name=""dist"">100</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' default=$bbox_fq v=$location}</str> </lst> </requestHandler>"
search,"A similar use case would be to combine the switch parser with the bbox parser to support an optional geographic filter that is applied if and only if the client specifies a location param containing a lat,lon pair to be used as the center of the bounding box: <requestHandler name=""/select"" class=""solr.SearchHandler""> <lst name=""invariants""> <str name=""bbox_fq"">{!bbox pt=$location sfield=geo d=$dist}</str> </lst> <lst name=""defaults""> <str name=""dist"">100</str> </lst> <lst name=""appends""> <str name=""fq"">{!switch case='*:*' default=$bbox_fq v=$location}</str> </lst> </requestHandler>"
search,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
search,"For text fields, no analysis is done since raw terms are already returned from the faceting and terms components, and not all text analysis is idempotent. To apply analysis to text fields as well, see the FieldQParserPlugin. If no analysis or transformation is desired for any type of field, see the RawQParserPlugin. Other parameters: f, the field Example: {!term f=weight}1.5"
search,"Other parameters: f, the field Example: {!term f=weight}1.5"
search,Note that if no values are specified then the query matches no documents.
search,The TextLogitStream provides the parallel iterative framework for this class.
search,Enclosing class: TopLevelJoinQuery protected static class TopLevelJoinQuery.BitsetBounds extends Object
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, sameClassAs, toString"
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, sameClassAs, toString"
search,"Example: <BooleanQuery fieldName=""description""> <Clause occurs=""must""> <TermQuery>shirt</TermQuery> </Clause> <Clause occurs=""mustnot""> <TermQuery>plain</TermQuery> </Clause> <Clause occurs=""should""> <TermQuery>cotton</TermQuery> </Clause> <Clause occurs=""must""> <BooleanQuery fieldName=""size""> <Clause occurs=""should""> <TermsQuery>S M L</TermsQuery> </Clause> </BooleanQuery> </Clause> </BooleanQuery> You can configure your own custom query builders for additional XML elements. The custom builders need to extend the SolrQueryBuilder or the SolrSpanQueryBuilder class. Example solrconfig.xml snippet: <queryParser name=""xmlparser"" class=""XmlQParserPlugin""> <str name=""MyCustomQuery"">com.mycompany.solr.search.MyCustomQueryBuilder</str> </queryParser>"
search,"Example solrconfig.xml snippet: <queryParser name=""xmlparser"" class=""XmlQParserPlugin""> <str name=""MyCustomQuery"">com.mycompany.solr.search.MyCustomQueryBuilder</str> </queryParser>"
search,public class AggUtil extends Object
search,"Direct Known Subclasses: RelatednessAgg, SimpleAggValueSource, StrAggValueSource public abstract class AggValueSource extends org.apache.lucene.queries.function.ValueSource"
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, description, fromDoubleValuesSource, getSortField, hashCode, newContext, toString"
search,public class AvgAgg extends SimpleAggValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class BlockJoin extends Object WARNING: This API is experimental and might change in incompatible ways in the next release.
search,public class Constants extends Object constants used in facets package
search,public class CountAgg extends SimpleAggValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class CountValsAgg extends SimpleAggValueSource AggValueSource to count values for given ValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class FacetBucket extends Object
search,public class FacetContext extends Object
search,public class FacetDebugInfo extends Object
search,public class FacetField extends FacetRequest
search,public class FacetFieldMerger extends FacetMerger
search,public class FacetHeatmap extends FacetRequest JSON Facet API request for a 2D spatial summary of a rectangular region. Version: 7.5.0 See Also: HeatmapFacetCounter
search,Enclosing class: FacetMerger public static class FacetMerger.Context extends Object
search,"Direct Known Subclasses: FacetFieldMerger, FacetRangeMerger, SumAgg.Merger public abstract class FacetMerger extends Object"
search,public abstract class FacetProcessor<T extends FacetRequest> extends Object Base abstraction for a class that computes facets. This is fairly internal to the module.
search,public class FacetQuery extends FacetRequest
search,public class FacetRange extends FacetRequest
search,public class FacetRangeMerger extends FacetMerger
search,Enclosing class: FacetRequest.Domain public static class FacetRequest.Domain.GraphField extends Object Are we doing a query time graph across other documents
search,Enclosing class: FacetRequest.Domain public static class FacetRequest.Domain.JoinField extends Object Are we doing a query time join across other documents
search,Enclosing class: FacetRequest public static class FacetRequest.Domain extends Object
search,Enclosing class: FacetRequest public static final class FacetRequest.FacetSort extends Object Simple structure for encapsulating a sort variable and a direction
search,"Direct Known Subclasses: FacetField, FacetHeatmap, FacetQuery, FacetRange public abstract class FacetRequest extends Object A request to do facets/stats that might itself be composed of sub-FacetRequests. This is a cornerstone of the facet module. See Also: parse(SolrQueryRequest, Map)"
search,Enclosing class: FieldUtil public static class FieldUtil.DVOrdValues extends FieldUtil.OrdValues
search,Fields inherited from classorg.apache.lucene.search.DocIdSetIterator NO_MORE_DOCS
search,"Methods inherited from classorg.apache.lucene.index.SortedDocValues intersect, lookupTerm, termsEnum"
search,"Methods inherited from classorg.apache.lucene.search.DocIdSetIterator all, empty, range, slowAdvance"
search,Fields inherited from classorg.apache.lucene.search.DocIdSetIterator NO_MORE_DOCS
search,Enclosing class: FieldUtil public static class FieldUtil.FCOrdValues extends FieldUtil.OrdValues
search,Fields inherited from classorg.apache.lucene.search.DocIdSetIterator NO_MORE_DOCS
search,"Methods inherited from classorg.apache.lucene.index.SortedDocValues intersect, lookupTerm, termsEnum"
search,"Methods inherited from classorg.apache.lucene.search.DocIdSetIterator all, empty, range, slowAdvance"
search,Fields inherited from classorg.apache.lucene.search.DocIdSetIterator NO_MORE_DOCS
search,public class FieldUtil extends Object NOTE: This API is for internal purposes only and might change in incompatible ways in the next release. Porting helper... may be removed if it offers no value in the future.
search,"Direct Known Subclasses: FieldUtil.DVOrdValues, FieldUtil.FCOrdValues Enclosing class: FieldUtil public abstract static class FieldUtil.OrdValues extends org.apache.lucene.index.SortedDocValues"
search,Fields inherited from classorg.apache.lucene.search.DocIdSetIterator NO_MORE_DOCS
search,"Methods inherited from classorg.apache.lucene.index.SortedDocValues intersect, lookupOrd, lookupTerm, ordValue, termsEnum"
search,"Methods inherited from classorg.apache.lucene.search.DocIdSetIterator all, empty, range, slowAdvance"
search,Fields inherited from classorg.apache.lucene.search.DocIdSetIterator NO_MORE_DOCS
search,Enclosing class: HLLAgg public static class HLLAgg.HLLFactory extends Object
search,public class HLLAgg extends StrAggValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class LegacyFacet extends Object
search,Enclosing class: LegacyFacet protected static class LegacyFacet.Subfacet extends Object
search,public class MinMaxAgg extends SimpleAggValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class MissingAgg extends SimpleAggValueSource AggValueSource to compute missing counts for given ValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class PercentileAgg extends SimpleAggValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,"public class RelatednessAgg extends AggValueSource An aggregation function designed to be nested under other (possibly deeply nested) facets for the purposes of computing the ""relatedness"" of facet buckets relative to ""foreground"" and ""background"" sets -- primarily for the purpose of building ""Semantic Knowledge Graphs"" See Also: The Semantic Knowledge Graph: A compact, auto-generated model for real-time traversal and ranking of any relationship within a domain"
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,"Direct Known Subclasses: AvgAgg, CountAgg, CountValsAgg, MinMaxAgg, MissingAgg, PercentileAgg, StddevAgg, SumAgg, SumsqAgg, VarianceAgg public abstract class SimpleAggValueSource extends AggValueSource"
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,Enclosing class: SlotAcc public abstract static class SlotAcc.Resizer extends Object
search,"Enclosing class: SlotAcc public static class SlotAcc.SlotContext extends Object Incapsulates information about the current slot, for Accumulators that may want additional info during collection."
search,public class StddevAgg extends SimpleAggValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,"Direct Known Subclasses: HLLAgg, UniqueAgg public abstract class StrAggValueSource extends AggValueSource"
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class SumAgg extends SimpleAggValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,Enclosing class: SumAgg public static class SumAgg.Merger extends FacetMerger
search,public class SumsqAgg extends SimpleAggValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,"Term numbers are in sorted order, and are encoded as variable-length deltas from the previous term number. Real term numbers start at 2 since 0 and 1 are reserved. A term number of 0 signals the end of the termNumber list. There is a single int[maxDoc()] which either contains a pointer into a byte[] for the termNumber lists, or directly contains the termNumber list if it fits in the 4 bytes of an integer. If the first byte in the integer is 1, the next 3 bytes are a pointer into a byte[] where the termNumber list starts. There are actually 256 byte arrays, to compensate for the fact that the pointers into the byte arrays are only 3 bytes long. The correct byte array for a document is a function of its id. To save space and speed up faceting, any term that matches enough documents will not be un-inverted... it will be skipped while building the un-inverted field structure, and will use a set intersection method during faceting. To further save memory, the terms (the actual string values) are not all stored in memory, but a TermIndex is used to convert term numbers to term values only for the terms needed after faceting has completed. Only every 128th term value is stored, along with its corresponding term number, and this is used as an index to find the closest term and iterate until the desired number is hit (very much like Lucene's own internal term index)."
search,"There is a single int[maxDoc()] which either contains a pointer into a byte[] for the termNumber lists, or directly contains the termNumber list if it fits in the 4 bytes of an integer. If the first byte in the integer is 1, the next 3 bytes are a pointer into a byte[] where the termNumber list starts. There are actually 256 byte arrays, to compensate for the fact that the pointers into the byte arrays are only 3 bytes long. The correct byte array for a document is a function of its id. To save space and speed up faceting, any term that matches enough documents will not be un-inverted... it will be skipped while building the un-inverted field structure, and will use a set intersection method during faceting. To further save memory, the terms (the actual string values) are not all stored in memory, but a TermIndex is used to convert term numbers to term values only for the terms needed after faceting has completed. Only every 128th term value is stored, along with its corresponding term number, and this is used as an index to find the closest term and iterate until the desired number is hit (very much like Lucene's own internal term index)."
search,"There are actually 256 byte arrays, to compensate for the fact that the pointers into the byte arrays are only 3 bytes long. The correct byte array for a document is a function of its id. To save space and speed up faceting, any term that matches enough documents will not be un-inverted... it will be skipped while building the un-inverted field structure, and will use a set intersection method during faceting. To further save memory, the terms (the actual string values) are not all stored in memory, but a TermIndex is used to convert term numbers to term values only for the terms needed after faceting has completed. Only every 128th term value is stored, along with its corresponding term number, and this is used as an index to find the closest term and iterate until the desired number is hit (very much like Lucene's own internal term index)."
search,"To save space and speed up faceting, any term that matches enough documents will not be un-inverted... it will be skipped while building the un-inverted field structure, and will use a set intersection method during faceting. To further save memory, the terms (the actual string values) are not all stored in memory, but a TermIndex is used to convert term numbers to term values only for the terms needed after faceting has completed. Only every 128th term value is stored, along with its corresponding term number, and this is used as an index to find the closest term and iterate until the desired number is hit (very much like Lucene's own internal term index)."
search,"To further save memory, the terms (the actual string values) are not all stored in memory, but a TermIndex is used to convert term numbers to term values only for the terms needed after faceting has completed. Only every 128th term value is stored, along with its corresponding term number, and this is used as an index to find the closest term and iterate until the desired number is hit (very much like Lucene's own internal term index)."
search,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
search,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
search,Direct Known Subclasses: UniqueBlockAgg public class UniqueAgg extends StrAggValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,"Direct Known Subclasses: UniqueBlockFieldAgg, UniqueBlockQueryAgg public abstract class UniqueBlockAgg extends UniqueAgg"
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class UniqueBlockFieldAgg extends UniqueBlockAgg
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class UniqueBlockQueryAgg extends UniqueBlockAgg
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class VarianceAgg extends SimpleAggValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,Enclosing class: CollapseScoreFunction public static class CollapseScoreFunction.CollapseScoreFunctionValues extends org.apache.lucene.queries.function.FunctionValues
search,Nested classes/interfaces inherited from classorg.apache.lucene.queries.function.FunctionValues org.apache.lucene.queries.function.FunctionValues.ValueFiller
search,"Methods inherited from classorg.apache.lucene.queries.function.FunctionValues boolVal, bytesVal, byteVal, byteVal, byteVectorVal, cost, doubleVal, exists, explain, floatVal, floatVectorVal, getRangeScorer, getScorer, getValueFiller, intVal, longVal, longVal, numOrd, objectVal, ordVal, shortVal, shortVal, strVal, strVal"
search,Nested classes/interfaces inherited from classorg.apache.lucene.queries.function.FunctionValues org.apache.lucene.queries.function.FunctionValues.ValueFiller
search,public class CollapseScoreFunction extends org.apache.lucene.queries.function.ValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class ConcatStringFunction extends MultiStringFunction ConcatStringFunction concatenates the string values of its components in the order given.
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public abstract class DualDoubleFunction extends org.apache.lucene.queries.function.ValueSource Abstract ValueSource implementation which wraps two ValueSources and applies an extendible double function to their values.
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class EqualFunction extends org.apache.lucene.queries.function.valuesource.ComparisonBoolFunction Compares two values for equality. It should work on not only numbers but strings and custom things. Since: 7.4
search,"Methods inherited from classorg.apache.lucene.queries.function.valuesource.ComparisonBoolFunction createWeight, description, equals, getValues, hashCode, name"
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class FieldNameValueSource extends org.apache.lucene.queries.function.ValueSource Placeholder value source. NOTE: This API is for internal purposes only and might change in incompatible ways in the next release.
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,"public class FileFloatSource extends org.apache.lucene.queries.function.ValueSource Obtains float field values from an external file. See Also: ExternalFileField, ExternalFileFieldReloader"
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext"
search,Direct Known Subclasses: ConcatStringFunction public abstract class MultiStringFunction extends org.apache.lucene.queries.function.ValueSource Abstract ValueSource implementation which wraps multiple ValueSources and applies an extendible string function to their values.
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,"public class OrdFieldSource extends org.apache.lucene.queries.function.ValueSource Obtains the ordinal of the field value from LeafReader.getSortedDocValues(java.lang.String). The native lucene index order is used to assign an ordinal value for each field value. Field values (terms) are lexicographically ordered by unicode value, and numbered starting at 1. Example: If there were only three field values: ""apple"",""banana"",""pear"" then ord(""apple"")=1, ord(""banana"")=2, ord(""pear"")=3 WARNING: ord() depends on the position in an index and can thus change when other documents are inserted or deleted, or if a MultiSearcher is used. WARNING: as of Solr 1.4, ord() and rord() can cause excess memory use since they must use a FieldCache entry at the top level reader, while sorting and function queries now use entries at the segment level. Hence sorting or using a different function query, in addition to ord()/rord() will double memory use."
search,"WARNING: ord() depends on the position in an index and can thus change when other documents are inserted or deleted, or if a MultiSearcher is used. WARNING: as of Solr 1.4, ord() and rord() can cause excess memory use since they must use a FieldCache entry at the top level reader, while sorting and function queries now use entries at the segment level. Hence sorting or using a different function query, in addition to ord()/rord() will double memory use."
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,"public class ReverseOrdFieldSource extends org.apache.lucene.queries.function.ValueSource Obtains the ordinal of the field value from LeafReader.getSortedDocValues(java.lang.String) and reverses the order. The native lucene index order is used to assign an ordinal value for each field value. Field values (terms) are lexicographically ordered by unicode value, and numbered starting at 1. Example of reverse ordinal (rord): If there were only three field values: ""apple"",""banana"",""pear"" then rord(""apple"")=3, rord(""banana"")=2, ord(""pear"")=1 WARNING: ord() depends on the position in an index and can thus change when other documents are inserted or deleted, or if a MultiSearcher is used. WARNING: as of Solr 1.4, ord() and rord() can cause excess memory use since they must use a FieldCache entry at the top level reader, while sorting and function queries now use entries at the segment level. Hence sorting or using a different function query, in addition to ord()/rord() will double memory use."
search,"WARNING: ord() depends on the position in an index and can thus change when other documents are inserted or deleted, or if a MultiSearcher is used. WARNING: as of Solr 1.4, ord() and rord() can cause excess memory use since they must use a FieldCache entry at the top level reader, while sorting and function queries now use entries at the segment level. Hence sorting or using a different function query, in addition to ord()/rord() will double memory use."
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class SolrComparisonBoolFunction extends org.apache.lucene.queries.function.valuesource.ComparisonBoolFunction Refines ComparisonBoolFunction to compare based on a 'long' or 'double' depending on if the any of the FunctionValues are LongDocValues.
search,"Methods inherited from classorg.apache.lucene.queries.function.valuesource.ComparisonBoolFunction createWeight, description, equals, getValues, hashCode, name"
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class ValueSourceRangeFilter extends org.apache.lucene.search.Query RangeFilter over a ValueSource.
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, rewrite, sameClassAs, toString"
search,"@Deprecated public class GeohashFunction extends org.apache.lucene.queries.function.ValueSource Deprecated. Takes in a latitude and longitude ValueSource and produces a GeoHash. Ex: geohash(lat, lon) Note, there is no reciprocal function for this."
search,"Ex: geohash(lat, lon) Note, there is no reciprocal function for this."
search,"Note, there is no reciprocal function for this."
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,"@Deprecated public class GeohashHaversineFunction extends org.apache.lucene.queries.function.ValueSource Deprecated. Calculate the Haversine distance between two geo hash codes. Ex: ghhsin(ValueSource, ValueSource, radius) See Also: for more details on the implementation"
search,"Ex: ghhsin(ValueSource, ValueSource, radius)"
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class HaversineConstFunction extends org.apache.lucene.queries.function.ValueSource Haversine function with one point constant
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, fromDoubleValuesSource, getSortField, newContext, toString"
search,"public class HaversineFunction extends org.apache.lucene.queries.function.ValueSource Calculate the Haversine formula (distance) between any two points on a sphere Takes in four value sources: (latA, lonA); (latB, lonB). Assumes the value sources are in radians unless See http://en.wikipedia.org/wiki/Great-circle_distance and http://en.wikipedia.org/wiki/Haversine_formula for the actual formula and also http://www.movable-type.co.uk/scripts/latlong.html"
search,Assumes the value sources are in radians unless See http://en.wikipedia.org/wiki/Great-circle_distance and http://en.wikipedia.org/wiki/Haversine_formula for the actual formula and also http://www.movable-type.co.uk/scripts/latlong.html
search,See http://en.wikipedia.org/wiki/Great-circle_distance and http://en.wikipedia.org/wiki/Haversine_formula for the actual formula and also http://www.movable-type.co.uk/scripts/latlong.html
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, fromDoubleValuesSource, getSortField, newContext, toString"
search,"public class SquaredEuclideanFunction extends VectorDistanceFunction While not strictly a distance, the Sq. Euclidean Distance is often all that is needed in many applications that require a distance, thus saving a sq. rt. calculation"
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, fromDoubleValuesSource, getSortField, newContext, toString"
search,public class StringDistanceFunction extends org.apache.lucene.queries.function.ValueSource
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, createWeight, fromDoubleValuesSource, getSortField, newContext, toString"
search,Direct Known Subclasses: SquaredEuclideanFunction public class VectorDistanceFunction extends org.apache.lucene.queries.function.ValueSource Calculate the p-norm for a Vector. See http://en.wikipedia.org/wiki/Lp_space Common cases: 0 = Sparseness calculation 1 = Manhattan distance 2 = Euclidean distance Integer.MAX_VALUE = infinite norm See Also: for the special case
search,0 = Sparseness calculation 1 = Manhattan distance 2 = Euclidean distance Integer.MAX_VALUE = infinite norm
search,1 = Manhattan distance 2 = Euclidean distance Integer.MAX_VALUE = infinite norm
search,2 = Euclidean distance Integer.MAX_VALUE = infinite norm
search,Integer.MAX_VALUE = infinite norm
search,Common cases: 0 = Sparseness calculation 1 = Manhattan distance 2 = Euclidean distance Integer.MAX_VALUE = infinite norm
search,0 = Sparseness calculation 1 = Manhattan distance 2 = Euclidean distance Integer.MAX_VALUE = infinite norm
search,1 = Manhattan distance 2 = Euclidean distance Integer.MAX_VALUE = infinite norm
search,2 = Euclidean distance Integer.MAX_VALUE = infinite norm
search,Integer.MAX_VALUE = infinite norm
search,"Methods inherited from classorg.apache.lucene.queries.function.ValueSource asDoubleValuesSource, asLongValuesSource, fromDoubleValuesSource, getSortField, newContext, toString"
search,Enclosing class: CommandHandler public static class CommandHandler.Builder extends Object
search,public class CommandHandler extends Object Responsible for executing a search with a number of Command instances. A typical search can have more then one Command instances. WARNING: This API is experimental and might change in incompatible ways in the next release.
search,public class GroupingSpecification extends Object Encapsulates the grouping options like fields group sort and more specified by clients. WARNING: This API is experimental and might change in incompatible ways in the next release.
search,Fields inherited from classorg.apache.lucene.search.FilterCollector in
search,"Methods inherited from classorg.apache.lucene.search.FilterCollector scoreMode, setWeight, toString"
search,Fields inherited from classorg.apache.lucene.search.FilterCollector in
search,Enclosing class: QueryCommand public static class QueryCommand.Builder extends Object
search,public class QueryCommandResult extends Object Encapsulates TopDocs and the number of matches.
search,Enclosing class: SearchGroupsFieldCommand public static class SearchGroupsFieldCommand.Builder extends Object
search,public class SearchGroupsFieldCommandResult extends Object Encapsulates the result of a SearchGroupsFieldCommand command
search,Enclosing class: TopGroupsFieldCommand public static class TopGroupsFieldCommand.Builder extends Object
search,public class BlockJoinChildQParser extends BlockJoinParentQParser
search,Direct Known Subclasses: BlockJoinChildQParser public class BlockJoinParentQParser extends FiltersQParser
search,public class CrossCollectionJoinQParser extends QParser
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, rewrite, sameClassAs, toString"
search,Direct Known Subclasses: BlockJoinParentQParser public class FiltersQParser extends QParser
search,"Methods inherited from classorg.apache.lucene.search.SimpleCollector getLeafCollector, setScorer"
search,"Methods inherited from interfaceorg.apache.lucene.search.Collector getLeafCollector, setWeight"
search,"Methods inherited from interfaceorg.apache.lucene.search.LeafCollector collect, competitiveIterator, finish"
search,Nested classes/interfaces inherited from classorg.apache.lucene.search.Weight org.apache.lucene.search.Weight.DefaultBulkScorer
search,Fields inherited from classorg.apache.lucene.search.Weight parentQuery
search,"Methods inherited from classorg.apache.lucene.search.Weight bulkScorer, count, getQuery, matches, scorerSupplier"
search,Nested classes/interfaces inherited from classorg.apache.lucene.search.Weight org.apache.lucene.search.Weight.DefaultBulkScorer
search,"Params: fromField = the field that contains the node id toField = the field that contains the edge ids traversalFilter = a query that can be applied for each hop in the graph. maxDepth = the max depth to traverse. (start nodes is depth=1) onlyLeafNodes = only return documents that have no edge id values. returnRoot = if false, the documents matching the initial query will not be returned."
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, rewrite, sameClassAs, toString"
search,public class GraphQueryParser extends QParser Solr query parser that will handle parsing graph query requests.
search,public class HashRangeQParser extends QParser
search,"Methods inherited from classorg.apache.lucene.search.Query classHash, rewrite, rewrite, sameClassAs, toString"
search,"Methods inherited from classorg.apache.lucene.search.SimpleCollector getLeafCollector, setScorer"
search,Methods inherited from interfaceorg.apache.lucene.search.Collector setWeight
search,"Methods inherited from interfaceorg.apache.lucene.search.LeafCollector collect, competitiveIterator, finish"
search,"from - ""foreign key"" field name to collect values while enumerating subordinate query (denoted as foo in example above). it's better to have this field declared as type=""string"" docValues=""true"". note: if docValues are not enabled for this field, it will work anyway, but it costs some memory for UninvertingReader. Also, numeric doc values are not supported until LUCENE-5868. Thus, it only supports DocValuesType.SORTED, DocValuesType.SORTED_SET, DocValuesType.BINARY. fromIndex - optional parameter, a core name where subordinate query should run (and from values are collected) rather than current core. Example:q={!join from=manu_id_s to=id score=total fromIndex=products}foo to - ""primary key"" field name which is searched for values collected from subordinate query. it should be declared as indexed=""true"". Now it's treated as a single value field. score - one of ScoreMode: none,avg,total,max,min. Capital case is also accepted."
search,"fromIndex - optional parameter, a core name where subordinate query should run (and from values are collected) rather than current core. Example:q={!join from=manu_id_s to=id score=total fromIndex=products}foo to - ""primary key"" field name which is searched for values collected from subordinate query. it should be declared as indexed=""true"". Now it's treated as a single value field. score - one of ScoreMode: none,avg,total,max,min. Capital case is also accepted."
search,"to - ""primary key"" field name which is searched for values collected from subordinate query. it should be declared as indexed=""true"". Now it's treated as a single value field. score - one of ScoreMode: none,avg,total,max,min. Capital case is also accepted."
search,"score - one of ScoreMode: none,avg,total,max,min. Capital case is also accepted."
search,"from - ""foreign key"" field name to collect values while enumerating subordinate query (denoted as foo in example above). it's better to have this field declared as type=""string"" docValues=""true"". note: if docValues are not enabled for this field, it will work anyway, but it costs some memory for UninvertingReader. Also, numeric doc values are not supported until LUCENE-5868. Thus, it only supports DocValuesType.SORTED, DocValuesType.SORTED_SET, DocValuesType.BINARY. fromIndex - optional parameter, a core name where subordinate query should run (and from values are collected) rather than current core. Example:q={!join from=manu_id_s to=id score=total fromIndex=products}foo to - ""primary key"" field name which is searched for values collected from subordinate query. it should be declared as indexed=""true"". Now it's treated as a single value field. score - one of ScoreMode: none,avg,total,max,min. Capital case is also accepted."
search,"fromIndex - optional parameter, a core name where subordinate query should run (and from values are collected) rather than current core. Example:q={!join from=manu_id_s to=id score=total fromIndex=products}foo to - ""primary key"" field name which is searched for values collected from subordinate query. it should be declared as indexed=""true"". Now it's treated as a single value field. score - one of ScoreMode: none,avg,total,max,min. Capital case is also accepted."
search,"to - ""primary key"" field name which is searched for values collected from subordinate query. it should be declared as indexed=""true"". Now it's treated as a single value field. score - one of ScoreMode: none,avg,total,max,min. Capital case is also accepted."
search,"score - one of ScoreMode: none,avg,total,max,min. Capital case is also accepted."
search,public class CloudMLTQParser extends SimpleMLTQParser
search,Direct Known Subclasses: CloudMLTQParser public class SimpleMLTQParser extends QParser
search,"Direct Known Subclasses: KnnQParser, VectorSimilarityQParser public abstract class AbstractVectorQParserBase extends QParser"
search,public class KnnQParser extends AbstractVectorQParserBase
search,public class VectorSimilarityQParser extends AbstractVectorQParserBase
search,public class BM25SimilarityFactory extends SimilarityFactory Factory for BM25Similarity. This is the default similarity since 8.x. Parameters: k1 (float): Controls non-linear term frequency normalization (saturation). The default is 1.2 b (float): Controls to what degree document length normalizes tf values. The default is 0.75 discountOverlaps (bool): True if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. The default is true Since: 8.0.0 WARNING: This API is experimental and might change in incompatible ways in the next release.
search,k1 (float): Controls non-linear term frequency normalization (saturation). The default is 1.2 b (float): Controls to what degree document length normalizes tf values. The default is 0.75 discountOverlaps (bool): True if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. The default is true
search,b (float): Controls to what degree document length normalizes tf values. The default is 0.75 discountOverlaps (bool): True if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. The default is true
search,discountOverlaps (bool): True if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. The default is true
search,Parameters: k1 (float): Controls non-linear term frequency normalization (saturation). The default is 1.2 b (float): Controls to what degree document length normalizes tf values. The default is 0.75 discountOverlaps (bool): True if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. The default is true
search,k1 (float): Controls non-linear term frequency normalization (saturation). The default is 1.2 b (float): Controls to what degree document length normalizes tf values. The default is 0.75 discountOverlaps (bool): True if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. The default is true
search,b (float): Controls to what degree document length normalizes tf values. The default is 0.75 discountOverlaps (bool): True if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. The default is true
search,discountOverlaps (bool): True if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length. The default is true
search,public class BooleanSimilarityFactory extends SimilarityFactory Factory for BooleanSimilarity Simple similarity that gives terms a score that is equal to their query boost. This similarity is typically used with disabled norms since neither document statistics nor index statistics are used for scoring.
search,Simple similarity that gives terms a score that is equal to their query boost. This similarity is typically used with disabled norms since neither document statistics nor index statistics are used for scoring.
search,"Direct Known Subclasses: SweetSpotSimilarityFactory public class ClassicSimilarityFactory extends SimilarityFactory Factory for ClassicSimilarity ClassicSimilarity is Lucene's original scoring implementation, based upon the Vector Space Model. Optional settings: discountOverlaps (bool): Sets TFIDFSimilarity.setDiscountOverlaps(boolean) See Also: TFIDFSimilarity WARNING: This API is experimental and might change in incompatible ways in the next release."
search,discountOverlaps (bool): Sets TFIDFSimilarity.setDiscountOverlaps(boolean)
search,"ClassicSimilarity is Lucene's original scoring implementation, based upon the Vector Space Model. Optional settings: discountOverlaps (bool): Sets TFIDFSimilarity.setDiscountOverlaps(boolean)"
search,Optional settings: discountOverlaps (bool): Sets TFIDFSimilarity.setDiscountOverlaps(boolean)
search,discountOverlaps (bool): Sets TFIDFSimilarity.setDiscountOverlaps(boolean)
search,"public class DFISimilarityFactory extends SimilarityFactory Factory for DFISimilarity You must specify the measure of divergence from independence (""independenceMeasure"") ""Standardized"": IndependenceStandardized ""Saturated"": IndependenceSaturated ""ChiSquared"": IndependenceChiSquared Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean) WARNING: This API is experimental and might change in incompatible ways in the next release."
search,"""Standardized"": IndependenceStandardized ""Saturated"": IndependenceSaturated ""ChiSquared"": IndependenceChiSquared"
search,"""Saturated"": IndependenceSaturated ""ChiSquared"": IndependenceChiSquared"
search,"""ChiSquared"": IndependenceChiSquared"
search,discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,"You must specify the measure of divergence from independence (""independenceMeasure"") ""Standardized"": IndependenceStandardized ""Saturated"": IndependenceSaturated ""ChiSquared"": IndependenceChiSquared Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)"
search,"""Standardized"": IndependenceStandardized ""Saturated"": IndependenceSaturated ""ChiSquared"": IndependenceChiSquared"
search,"""Saturated"": IndependenceSaturated ""ChiSquared"": IndependenceChiSquared"
search,"""ChiSquared"": IndependenceChiSquared"
search,discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,"public class DFRSimilarityFactory extends SimilarityFactory Factory for DFRSimilarity You must specify the implementations for all three components of DFR (strings). In general the models are parameter-free, but two of the normalizations take floating point parameters (see below): basicModel: Basic model of information content: G: Geometric approximation of Bose-Einstein I(n): Inverse document frequency I(ne): Inverse expected document frequency [mixture of Poisson and IDF] I(F): Inverse term frequency [approximation of I(ne)] afterEffect: First normalization of information gain: L: Laplace's law of succession B: Ratio of two Bernoulli processes normalization: Second (length) normalization: H1: Uniform distribution of term frequency parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H2: term frequency density inversely related to length parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean) WARNING: This API is experimental and might change in incompatible ways in the next release."
search,basicModel: Basic model of information content: G: Geometric approximation of Bose-Einstein I(n): Inverse document frequency I(ne): Inverse expected document frequency [mixture of Poisson and IDF] I(F): Inverse term frequency [approximation of I(ne)] afterEffect: First normalization of information gain: L: Laplace's law of succession B: Ratio of two Bernoulli processes normalization: Second (length) normalization: H1: Uniform distribution of term frequency parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H2: term frequency density inversely related to length parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,G: Geometric approximation of Bose-Einstein I(n): Inverse document frequency I(ne): Inverse expected document frequency [mixture of Poisson and IDF] I(F): Inverse term frequency [approximation of I(ne)]
search,I(n): Inverse document frequency I(ne): Inverse expected document frequency [mixture of Poisson and IDF] I(F): Inverse term frequency [approximation of I(ne)]
search,I(ne): Inverse expected document frequency [mixture of Poisson and IDF] I(F): Inverse term frequency [approximation of I(ne)]
search,I(F): Inverse term frequency [approximation of I(ne)]
search,afterEffect: First normalization of information gain: L: Laplace's law of succession B: Ratio of two Bernoulli processes normalization: Second (length) normalization: H1: Uniform distribution of term frequency parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H2: term frequency density inversely related to length parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,L: Laplace's law of succession B: Ratio of two Bernoulli processes
search,B: Ratio of two Bernoulli processes
search,normalization: Second (length) normalization: H1: Uniform distribution of term frequency parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H2: term frequency density inversely related to length parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,H1: Uniform distribution of term frequency parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H2: term frequency density inversely related to length parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1
search,H2: term frequency density inversely related to length parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1
search,H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,parameter mu (float): smoothing parameter . The default is 800
search,Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3
search,none: no second normalization
search,discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,"You must specify the implementations for all three components of DFR (strings). In general the models are parameter-free, but two of the normalizations take floating point parameters (see below): basicModel: Basic model of information content: G: Geometric approximation of Bose-Einstein I(n): Inverse document frequency I(ne): Inverse expected document frequency [mixture of Poisson and IDF] I(F): Inverse term frequency [approximation of I(ne)] afterEffect: First normalization of information gain: L: Laplace's law of succession B: Ratio of two Bernoulli processes normalization: Second (length) normalization: H1: Uniform distribution of term frequency parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H2: term frequency density inversely related to length parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)"
search,G: Geometric approximation of Bose-Einstein I(n): Inverse document frequency I(ne): Inverse expected document frequency [mixture of Poisson and IDF] I(F): Inverse term frequency [approximation of I(ne)]
search,I(n): Inverse document frequency I(ne): Inverse expected document frequency [mixture of Poisson and IDF] I(F): Inverse term frequency [approximation of I(ne)]
search,I(ne): Inverse expected document frequency [mixture of Poisson and IDF] I(F): Inverse term frequency [approximation of I(ne)]
search,I(F): Inverse term frequency [approximation of I(ne)]
search,L: Laplace's law of succession B: Ratio of two Bernoulli processes
search,B: Ratio of two Bernoulli processes
search,H1: Uniform distribution of term frequency parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H2: term frequency density inversely related to length parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1
search,H2: term frequency density inversely related to length parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1 H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1
search,H3: term frequency normalization provided by Dirichlet prior parameter mu (float): smoothing parameter . The default is 800 Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,parameter mu (float): smoothing parameter . The default is 800
search,Z: term frequency normalization provided by a Zipfian relation parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3 none: no second normalization
search,parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3
search,none: no second normalization
search,parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1
search,parameter c (float): hyper-parameter that controls the term frequency normalization with respect to the document length. The default is 1
search,parameter mu (float): smoothing parameter . The default is 800
search,parameter z (float): represents A/(A+1) where A measures the specificity of the language. The default is 0.3
search,Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,public class IBSimilarityFactory extends SimilarityFactory Factory for IBSimilarity You must specify the implementations for all three components of the Information-Based model (strings). distribution: Probabilistic distribution used to model term occurrence LL: Log-logistic SPL: Smoothed power-law lambda: w parameter of the probability distribution DF: Nw/N or average number of documents where w occurs TTF: Fw/N or average number of occurrences of w in the collection normalization: Term frequency normalization Any supported DFR normalization listed in DFRSimilarityFactory Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean) WARNING: This API is experimental and might change in incompatible ways in the next release.
search,distribution: Probabilistic distribution used to model term occurrence LL: Log-logistic SPL: Smoothed power-law lambda: w parameter of the probability distribution DF: Nw/N or average number of documents where w occurs TTF: Fw/N or average number of occurrences of w in the collection normalization: Term frequency normalization Any supported DFR normalization listed in DFRSimilarityFactory
search,LL: Log-logistic SPL: Smoothed power-law
search,SPL: Smoothed power-law
search,lambda: w parameter of the probability distribution DF: Nw/N or average number of documents where w occurs TTF: Fw/N or average number of occurrences of w in the collection normalization: Term frequency normalization Any supported DFR normalization listed in DFRSimilarityFactory
search,DF: Nw/N or average number of documents where w occurs TTF: Fw/N or average number of occurrences of w in the collection
search,TTF: Fw/N or average number of occurrences of w in the collection
search,normalization: Term frequency normalization Any supported DFR normalization listed in DFRSimilarityFactory
search,discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,You must specify the implementations for all three components of the Information-Based model (strings). distribution: Probabilistic distribution used to model term occurrence LL: Log-logistic SPL: Smoothed power-law lambda: w parameter of the probability distribution DF: Nw/N or average number of documents where w occurs TTF: Fw/N or average number of occurrences of w in the collection normalization: Term frequency normalization Any supported DFR normalization listed in DFRSimilarityFactory Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,LL: Log-logistic SPL: Smoothed power-law
search,SPL: Smoothed power-law
search,DF: Nw/N or average number of documents where w occurs TTF: Fw/N or average number of occurrences of w in the collection
search,TTF: Fw/N or average number of occurrences of w in the collection
search,Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,public class LMDirichletSimilarityFactory extends SimilarityFactory Factory for LMDirichletSimilarity Parameters: parameter mu (float): smoothing parameter . The default is 2000 Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean) WARNING: This API is experimental and might change in incompatible ways in the next release.
search,parameter mu (float): smoothing parameter . The default is 2000
search,discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,Parameters: parameter mu (float): smoothing parameter . The default is 2000 Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,parameter mu (float): smoothing parameter . The default is 2000
search,Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,public class LMJelinekMercerSimilarityFactory extends SimilarityFactory Factory for LMJelinekMercerSimilarity Parameters: parameter lambda (float): smoothing parameter . The default is 0.7 Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean) WARNING: This API is experimental and might change in incompatible ways in the next release.
search,parameter lambda (float): smoothing parameter . The default is 0.7
search,discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,Parameters: parameter lambda (float): smoothing parameter . The default is 0.7 Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,parameter lambda (float): smoothing parameter . The default is 0.7
search,Optional settings: discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,discountOverlaps (bool): Sets SimilarityBase.setDiscountOverlaps(boolean)
search,luceneMatchVersion >= 8.0 = BM25Similarity
search,luceneMatchVersion >= 8.0 = BM25Similarity
search,"The defaultSimFromFieldType option accepts the name of any fieldtype, and uses whatever Similarity is explicitly configured for that fieldType as the default for all other field types. For example: <similarity class=""solr.SchemaSimilarityFactory"" > <str name=""defaultSimFromFieldType"">type-using-custom-dfr</str> </similarity> ... <fieldType name=""type-using-custom-dfr"" class=""solr.TextField""> ... <similarity class=""solr.DFRSimilarityFactory""> <str name=""basicModel"">I(F)</str> <str name=""afterEffect"">B</str> <str name=""normalization"">H3</str> <float name=""mu"">900</float> </similarity> </fieldType> In the example above, any fieldtypes that do not define their own </similarity/> will use the Similarity configured for the type-using-custom-dfr . NOTE: Users should be aware that even when this factory uses a single default Similarity for some or all fields in a Query, the behavior can be inconsistent with the behavior of explicitly configuring that same Similarity globally, because of differences in how some multi-field / multi-clause behavior is defined in PerFieldSimilarityWrapper."
search,"In the example above, any fieldtypes that do not define their own </similarity/> will use the Similarity configured for the type-using-custom-dfr . NOTE: Users should be aware that even when this factory uses a single default Similarity for some or all fields in a Query, the behavior can be inconsistent with the behavior of explicitly configuring that same Similarity globally, because of differences in how some multi-field / multi-clause behavior is defined in PerFieldSimilarityWrapper."
search,"NOTE: Users should be aware that even when this factory uses a single default Similarity for some or all fields in a Query, the behavior can be inconsistent with the behavior of explicitly configuring that same Similarity globally, because of differences in how some multi-field / multi-clause behavior is defined in PerFieldSimilarityWrapper."
search,"public class SweetSpotSimilarityFactory extends ClassicSimilarityFactory Factory for SweetSpotSimilarity. SweetSpotSimilarity is an extension of ClassicSimilarity that provides additional tuning options for specifying the ""sweetspot"" of optimal tf and lengthNorm values in the source data. In addition to the discountOverlaps init param supported by ClassicSimilarityFactory The following sets of init params are supported by this factory: Length Norm Settings: lengthNormMin (int) lengthNormMax (int) lengthNormSteepness (float) Baseline TF Settings: baselineTfBase (float) baselineTfMin (float) Hyperbolic TF Settings: hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float) Note: If any individual settings from one of the above mentioned sets are specified, then all settings from that set must be specified. If Baseline TF settings are specified, then Hyperbolic TF settings are not permitted, and vice versa. (The settings specified will determine whether SweetSpotSimilarity.baselineTf(float) or SweetSpotSimilarity.hyperbolicTf(float) will be used. Example usage... <!-- using baseline TF --> <fieldType name=""text_baseline"" class=""solr.TextField"" indexed=""true"" stored=""false""> <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> <similarity class=""solr.SweetSpotSimilarityFactory""> <!-- TF --> <float name=""baselineTfMin"">6.0</float> <float name=""baselineTfBase"">1.5</float> <!-- plateau norm --> <int name=""lengthNormMin"">3</int> <int name=""lengthNormMax"">5</int> <float name=""lengthNormSteepness"">0.5</float> </similarity> </fieldType> <!-- using hyperbolic TF --> <fieldType name=""text_hyperbolic"" class=""solr.TextField"" indexed=""true"" stored=""false"" > <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> <similarity class=""solr.SweetSpotSimilarityFactory""> <float name=""hyperbolicTfMin"">3.3</float> <float name=""hyperbolicTfMax"">7.7</float> <double name=""hyperbolicTfBase"">2.718281828459045</double> <!-- e --> <float name=""hyperbolicTfOffset"">5.0</float> <!-- plateau norm, shallower slope --> <int name=""lengthNormMin"">1</int> <int name=""lengthNormMax"">5</int> <float name=""lengthNormSteepness"">0.2</float> </similarity> </fieldType> See Also: The javadocs for the individual methods in SweetSpotSimilarity for SVG diagrams showing how the each function behaves with various settings/inputs."
search,Length Norm Settings: lengthNormMin (int) lengthNormMax (int) lengthNormSteepness (float) Baseline TF Settings: baselineTfBase (float) baselineTfMin (float) Hyperbolic TF Settings: hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,lengthNormMin (int) lengthNormMax (int) lengthNormSteepness (float)
search,lengthNormMax (int) lengthNormSteepness (float)
search,lengthNormSteepness (float)
search,Baseline TF Settings: baselineTfBase (float) baselineTfMin (float) Hyperbolic TF Settings: hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,baselineTfBase (float) baselineTfMin (float)
search,baselineTfMin (float)
search,Hyperbolic TF Settings: hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfOffset (float)
search,"If any individual settings from one of the above mentioned sets are specified, then all settings from that set must be specified. If Baseline TF settings are specified, then Hyperbolic TF settings are not permitted, and vice versa. (The settings specified will determine whether SweetSpotSimilarity.baselineTf(float) or SweetSpotSimilarity.hyperbolicTf(float) will be used."
search,"If Baseline TF settings are specified, then Hyperbolic TF settings are not permitted, and vice versa. (The settings specified will determine whether SweetSpotSimilarity.baselineTf(float) or SweetSpotSimilarity.hyperbolicTf(float) will be used."
search,"SweetSpotSimilarity is an extension of ClassicSimilarity that provides additional tuning options for specifying the ""sweetspot"" of optimal tf and lengthNorm values in the source data. In addition to the discountOverlaps init param supported by ClassicSimilarityFactory The following sets of init params are supported by this factory: Length Norm Settings: lengthNormMin (int) lengthNormMax (int) lengthNormSteepness (float) Baseline TF Settings: baselineTfBase (float) baselineTfMin (float) Hyperbolic TF Settings: hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float) Note: If any individual settings from one of the above mentioned sets are specified, then all settings from that set must be specified. If Baseline TF settings are specified, then Hyperbolic TF settings are not permitted, and vice versa. (The settings specified will determine whether SweetSpotSimilarity.baselineTf(float) or SweetSpotSimilarity.hyperbolicTf(float) will be used. Example usage... <!-- using baseline TF --> <fieldType name=""text_baseline"" class=""solr.TextField"" indexed=""true"" stored=""false""> <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> <similarity class=""solr.SweetSpotSimilarityFactory""> <!-- TF --> <float name=""baselineTfMin"">6.0</float> <float name=""baselineTfBase"">1.5</float> <!-- plateau norm --> <int name=""lengthNormMin"">3</int> <int name=""lengthNormMax"">5</int> <float name=""lengthNormSteepness"">0.5</float> </similarity> </fieldType> <!-- using hyperbolic TF --> <fieldType name=""text_hyperbolic"" class=""solr.TextField"" indexed=""true"" stored=""false"" > <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> <similarity class=""solr.SweetSpotSimilarityFactory""> <float name=""hyperbolicTfMin"">3.3</float> <float name=""hyperbolicTfMax"">7.7</float> <double name=""hyperbolicTfBase"">2.718281828459045</double> <!-- e --> <float name=""hyperbolicTfOffset"">5.0</float> <!-- plateau norm, shallower slope --> <int name=""lengthNormMin"">1</int> <int name=""lengthNormMax"">5</int> <float name=""lengthNormSteepness"">0.2</float> </similarity> </fieldType>"
search,"In addition to the discountOverlaps init param supported by ClassicSimilarityFactory The following sets of init params are supported by this factory: Length Norm Settings: lengthNormMin (int) lengthNormMax (int) lengthNormSteepness (float) Baseline TF Settings: baselineTfBase (float) baselineTfMin (float) Hyperbolic TF Settings: hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float) Note: If any individual settings from one of the above mentioned sets are specified, then all settings from that set must be specified. If Baseline TF settings are specified, then Hyperbolic TF settings are not permitted, and vice versa. (The settings specified will determine whether SweetSpotSimilarity.baselineTf(float) or SweetSpotSimilarity.hyperbolicTf(float) will be used. Example usage... <!-- using baseline TF --> <fieldType name=""text_baseline"" class=""solr.TextField"" indexed=""true"" stored=""false""> <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> <similarity class=""solr.SweetSpotSimilarityFactory""> <!-- TF --> <float name=""baselineTfMin"">6.0</float> <float name=""baselineTfBase"">1.5</float> <!-- plateau norm --> <int name=""lengthNormMin"">3</int> <int name=""lengthNormMax"">5</int> <float name=""lengthNormSteepness"">0.5</float> </similarity> </fieldType> <!-- using hyperbolic TF --> <fieldType name=""text_hyperbolic"" class=""solr.TextField"" indexed=""true"" stored=""false"" > <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> <similarity class=""solr.SweetSpotSimilarityFactory""> <float name=""hyperbolicTfMin"">3.3</float> <float name=""hyperbolicTfMax"">7.7</float> <double name=""hyperbolicTfBase"">2.718281828459045</double> <!-- e --> <float name=""hyperbolicTfOffset"">5.0</float> <!-- plateau norm, shallower slope --> <int name=""lengthNormMin"">1</int> <int name=""lengthNormMax"">5</int> <float name=""lengthNormSteepness"">0.2</float> </similarity> </fieldType>"
search,Length Norm Settings: lengthNormMin (int) lengthNormMax (int) lengthNormSteepness (float) Baseline TF Settings: baselineTfBase (float) baselineTfMin (float) Hyperbolic TF Settings: hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,lengthNormMin (int) lengthNormMax (int) lengthNormSteepness (float)
search,lengthNormMax (int) lengthNormSteepness (float)
search,lengthNormSteepness (float)
search,Baseline TF Settings: baselineTfBase (float) baselineTfMin (float) Hyperbolic TF Settings: hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,baselineTfBase (float) baselineTfMin (float)
search,baselineTfMin (float)
search,Hyperbolic TF Settings: hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfOffset (float)
search,lengthNormMin (int) lengthNormMax (int) lengthNormSteepness (float)
search,lengthNormMax (int) lengthNormSteepness (float)
search,lengthNormSteepness (float)
search,baselineTfBase (float) baselineTfMin (float)
search,baselineTfMin (float)
search,hyperbolicTfMin (float) hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfMax (float) hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfBase (double) hyperbolicTfOffset (float)
search,hyperbolicTfOffset (float)
search,"Note: If any individual settings from one of the above mentioned sets are specified, then all settings from that set must be specified. If Baseline TF settings are specified, then Hyperbolic TF settings are not permitted, and vice versa. (The settings specified will determine whether SweetSpotSimilarity.baselineTf(float) or SweetSpotSimilarity.hyperbolicTf(float) will be used. Example usage... <!-- using baseline TF --> <fieldType name=""text_baseline"" class=""solr.TextField"" indexed=""true"" stored=""false""> <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> <similarity class=""solr.SweetSpotSimilarityFactory""> <!-- TF --> <float name=""baselineTfMin"">6.0</float> <float name=""baselineTfBase"">1.5</float> <!-- plateau norm --> <int name=""lengthNormMin"">3</int> <int name=""lengthNormMax"">5</int> <float name=""lengthNormSteepness"">0.5</float> </similarity> </fieldType> <!-- using hyperbolic TF --> <fieldType name=""text_hyperbolic"" class=""solr.TextField"" indexed=""true"" stored=""false"" > <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> <similarity class=""solr.SweetSpotSimilarityFactory""> <float name=""hyperbolicTfMin"">3.3</float> <float name=""hyperbolicTfMax"">7.7</float> <double name=""hyperbolicTfBase"">2.718281828459045</double> <!-- e --> <float name=""hyperbolicTfOffset"">5.0</float> <!-- plateau norm, shallower slope --> <int name=""lengthNormMin"">1</int> <int name=""lengthNormMax"">5</int> <float name=""lengthNormSteepness"">0.2</float> </similarity> </fieldType>"
search,"If any individual settings from one of the above mentioned sets are specified, then all settings from that set must be specified. If Baseline TF settings are specified, then Hyperbolic TF settings are not permitted, and vice versa. (The settings specified will determine whether SweetSpotSimilarity.baselineTf(float) or SweetSpotSimilarity.hyperbolicTf(float) will be used."
search,"If Baseline TF settings are specified, then Hyperbolic TF settings are not permitted, and vice versa. (The settings specified will determine whether SweetSpotSimilarity.baselineTf(float) or SweetSpotSimilarity.hyperbolicTf(float) will be used."
search,"Example usage... <!-- using baseline TF --> <fieldType name=""text_baseline"" class=""solr.TextField"" indexed=""true"" stored=""false""> <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> <similarity class=""solr.SweetSpotSimilarityFactory""> <!-- TF --> <float name=""baselineTfMin"">6.0</float> <float name=""baselineTfBase"">1.5</float> <!-- plateau norm --> <int name=""lengthNormMin"">3</int> <int name=""lengthNormMax"">5</int> <float name=""lengthNormSteepness"">0.5</float> </similarity> </fieldType> <!-- using hyperbolic TF --> <fieldType name=""text_hyperbolic"" class=""solr.TextField"" indexed=""true"" stored=""false"" > <analyzer class=""org.apache.lucene.analysis.standard.StandardAnalyzer""/> <similarity class=""solr.SweetSpotSimilarityFactory""> <float name=""hyperbolicTfMin"">3.3</float> <float name=""hyperbolicTfMax"">7.7</float> <double name=""hyperbolicTfBase"">2.718281828459045</double> <!-- e --> <float name=""hyperbolicTfOffset"">5.0</float> <!-- plateau norm, shallower slope --> <int name=""lengthNormMin"">1</int> <int name=""lengthNormMax"">5</int> <float name=""lengthNormSteepness"">0.2</float> </similarity> </fieldType>"
search,public class CollectionStats extends Object Modifiable version of CollectionStatistics useful for aggregation of per-shard stats.
search,"Global statistics are accumulated in the instance of this component (with the same life-cycle as SolrSearcher), in unbounded maps. NOTE: This may lead to excessive memory usage, in which case a LRUStatsCache should be considered."
search,Enclosing class: ExactStatsCache protected static class ExactStatsCache.ExactStatsSource extends StatsSource
search,"Global statistics are cached in the current request's context and discarded once the processing of the current request is complete. There's no support for longer-term caching, and each request needs to build the global statistics from scratch, even for repeating queries."
search,public final class LocalStatsSource extends StatsSource Convenience class that wraps a local SolrIndexSearcher to provide local statistics.
search,"Query terms, their stats and field stats are maintained in LRU caches, with the size by default DEFAULT_MAX_SIZE, one cache per shard. These caches are updated as needed (when term or field statistics are missing). Each instance of the component keeps also a global stats cache, which is aggregated from per-shard caches. Cache entries expire after a max idle time, by default DEFAULT_MAX_IDLE_TIME."
search,"Cache entries expire after a max idle time, by default DEFAULT_MAX_IDLE_TIME."
search,"There are instances of this class at the aggregator node (where the partial data from shards is aggregated), and on each core involved in a shard request (where this data is maintained and updated from the aggregator's cache)."
search,Enclosing class: StatsCache public static final class StatsCache.StatsCacheMetrics extends Object
search,"Direct Known Subclasses: ExactStatsCache.ExactStatsSource, LocalStatsSource public abstract class StatsSource extends Object The purpose of this class is only to provide two pieces of information necessary to create Weight from a Query, that is TermStatistics for a term and CollectionStatistics for the whole collection."
search,public class StatsUtil extends Object Various utilities for de/serialization of term stats and collection stats. TODO: serialization format is very simple and does nothing to compress the data.
search,TODO: serialization format is very simple and does nothing to compress the data.
search,public class TermStats extends Object Modifiable version of TermStatistics useful for aggregation of per-shard stats.
security,public class AllowListUrlChecker extends Object Validates URLs based on an allow list or a ClusterState in SolrCloud.
security,public class AuditEvent extends Object Audit event that takes request and auth context as input to be able to audit log custom things. This interface may change in next release and is marked experimental Since: 8.1.0 WARNING: This API is experimental and might change in incompatible ways in the next release.
security,Enclosing class: AuthorizationContext public static class AuthorizationContext.CollectionRequest extends Object
security,Direct Known Subclasses: HttpServletAuthorizationContext public abstract class AuthorizationContext extends Object Request context for Solr to be used by Authorization plugin.
security,public class AuthorizationResponse extends Object
security,Enclosing class: AuthorizationUtils public static class AuthorizationUtils.AuthorizationFailure extends Object
security,public class AuthorizationUtils extends Object
security,This API is analogous to the v1 /admin/info/key endpoint.
security,public abstract class HttpServletAuthorizationContext extends AuthorizationContext An AuthorizationContext implementation that delegates many methods to an underlying HttpServletRequest
security,Enclosing class: PKIAuthenticationPlugin public static class PKIAuthenticationPlugin.PKIHeaderData extends Object
security,Fields inherited from classjava.io.PrintWriter out
security,Fields inherited from classjava.io.Writer lock
security,Methods inherited from classjava.io.Writer nullWriter
security,Fields inherited from classjava.io.PrintWriter out
security,Fields inherited from classjava.io.Writer lock
security,public class SecurityPluginHolder<T> extends Object
security,public class SolrNodeKeyPair extends Object Creates and mediates access to the CryptoKeys.RSAKeyPair used by this Solr node. Expected to be created once on each Solr node for the life of that process.
security,Expected to be created once on each Solr node for the life of that process.
servlet,public class CoordinatorHttpSolrCall extends HttpSolrCall A coordinator node can serve requests as if it hosts all collections in the cluster. it does so by hosting a synthetic replica for each configset used in the cluster. This class is responsible for forwarding the requests to the right core when the node is acting as a Coordinator The responsibilities also involve creating a synthetic collection or replica if they do not exist. It also sets the right threadlocal variables which reflects the current collection being served.
servlet,This class is responsible for forwarding the requests to the right core when the node is acting as a Coordinator The responsibilities also involve creating a synthetic collection or replica if they do not exist. It also sets the right threadlocal variables which reflects the current collection being served.
servlet,"public class DirectSolrConnection extends Object DirectSolrConnection provides an interface to Solr that is similar to the HTTP interface, but does not require an HTTP connection. This class is designed to be as simple as possible and allow for more flexibility in how you interface to Solr. Since: solr 1.2"
servlet,This class is designed to be as simple as possible and allow for more flexibility in how you interface to Solr.
servlet,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
servlet,"Direct Known Subclasses: CoordinatorHttpSolrCall, V2HttpCall @ThreadSafe public class HttpSolrCall extends Object This class represents a call made to Solr"
servlet,"Methods inherited from classjavax.servlet.http.HttpServlet doDelete, doHead, doOptions, doPost, doPut, doTrace, getLastModified, service, service"
servlet,"Methods inherited from classjavax.servlet.GenericServlet destroy, getInitParameter, getInitParameterNames, getServletConfig, getServletContext, getServletInfo, getServletName, init, init, log, log"
servlet,public class QueryRateLimiter extends RequestRateLimiter Implementation of RequestRateLimiter specific to query request types. Most of the actual work is delegated to the parent class but specific configurations and parsing are handled by this class.
servlet,Enclosing class: RateLimitManager public static class RateLimitManager.Builder extends Object
servlet,The actual rate limiting and the limits should be implemented in the corresponding RequestRateLimiter implementation. RateLimitManager is responsible for the orchestration but not the specifics of how the rate limiting is being done for a specific request type.
servlet,"Methods inherited from classjavax.servlet.http.HttpServlet doDelete, doHead, doOptions, doPost, doPut, doTrace, getLastModified, service, service"
servlet,"Methods inherited from classjavax.servlet.GenericServlet destroy, getInitParameter, getInitParameterNames, getServletConfig, getServletContext, getServletInfo, getServletName, init, log, log"
servlet,"Direct Known Subclasses: QueryRateLimiter @ThreadSafe public class RequestRateLimiter extends Object Handles rate limiting for a specific request type. The control flow is as follows: Handle request -- Check if slot is available -- If available, acquire slot and proceed -- else reject the same."
servlet,"The control flow is as follows: Handle request -- Check if slot is available -- If available, acquire slot and proceed -- else reject the same."
servlet,public class ResponseUtils extends Object Response helper methods.
servlet,This class implements the Wrapper or Decorator pattern. Methods default to calling through to the wrapped stream.
servlet,"Methods inherited from classjava.io.InputStream nullInputStream, readAllBytes, readNBytes, readNBytes, transferTo"
servlet,This class implements the Wrapper or Decorator pattern. Methods default to calling through to the wrapped stream.
servlet,Methods inherited from classjava.io.OutputStream nullOutputStream
servlet,Methods inherited from classjavax.servlet.ServletInputStream readLine
servlet,"Methods inherited from classjava.io.InputStream available, close, mark, markSupported, nullInputStream, read, read, readAllBytes, readNBytes, readNBytes, reset, skip, transferTo"
servlet,"Methods inherited from classjavax.servlet.ServletOutputStream print, print, print, print, print, print, print, println, println, println, println, println, println, println, println"
servlet,"Methods inherited from classjava.io.OutputStream close, nullOutputStream, write, write"
servlet,"public abstract class ServletUtils extends Object Various Util methods for interaction on servlet level, i.e. HttpServletRequest"
servlet,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
servlet,Methods inherited from classjavax.servlet.http.HttpFilter doFilter
servlet,"Methods inherited from classjavax.servlet.GenericFilter getFilterConfig, getFilterName, getInitParameter, getInitParameterNames, getServletContext, init"
servlet,Methods inherited from interfacejavax.servlet.Filter destroy
servlet,public class SolrRequestParsers extends Object
servlet,public final class HttpCacheHeaderUtil extends Object
spelling,"Direct Known Subclasses: FileBasedSpellChecker, IndexBasedSpellChecker public abstract class AbstractLuceneSpellChecker extends SolrSpellChecker Abstract base class for all Lucene-based spell checking implementations. Refer to https://solr.apache.org/guide/solr/latest/query-guide/spell-checking.html for more details. Since: solr 1.3"
spelling,Refer to https://solr.apache.org/guide/solr/latest/query-guide/spell-checking.html for more details.
spelling,"public class ConjunctionSolrSpellChecker extends SolrSpellChecker This class lets a query be run through multiple spell checkers. The initial use-case is to use WordBreakSolrSpellChecker in conjunction with a ""standard"" spell checker (such as DirectSolrSpellChecker"
spelling,"public class DirectSolrSpellChecker extends SolrSpellChecker Spellchecker implementation that uses DirectSpellChecker Requires no auxiliary index or data structure. Supported options: field: Used as the source of terms. distanceMeasure: Sets DirectSpellChecker.setDistance(StringDistance). Note: to set the default DirectSpellChecker.INTERNAL_LEVENSHTEIN, use ""internal"". accuracy: Sets DirectSpellChecker.setAccuracy(float). maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float). See Also: DirectSpellChecker"
spelling,"field: Used as the source of terms. distanceMeasure: Sets DirectSpellChecker.setDistance(StringDistance). Note: to set the default DirectSpellChecker.INTERNAL_LEVENSHTEIN, use ""internal"". accuracy: Sets DirectSpellChecker.setAccuracy(float). maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"distanceMeasure: Sets DirectSpellChecker.setDistance(StringDistance). Note: to set the default DirectSpellChecker.INTERNAL_LEVENSHTEIN, use ""internal"". accuracy: Sets DirectSpellChecker.setAccuracy(float). maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"accuracy: Sets DirectSpellChecker.setAccuracy(float). maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float).
spelling,minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float).
spelling,maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float).
spelling,maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float).
spelling,"Requires no auxiliary index or data structure. Supported options: field: Used as the source of terms. distanceMeasure: Sets DirectSpellChecker.setDistance(StringDistance). Note: to set the default DirectSpellChecker.INTERNAL_LEVENSHTEIN, use ""internal"". accuracy: Sets DirectSpellChecker.setAccuracy(float). maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"Supported options: field: Used as the source of terms. distanceMeasure: Sets DirectSpellChecker.setDistance(StringDistance). Note: to set the default DirectSpellChecker.INTERNAL_LEVENSHTEIN, use ""internal"". accuracy: Sets DirectSpellChecker.setAccuracy(float). maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"field: Used as the source of terms. distanceMeasure: Sets DirectSpellChecker.setDistance(StringDistance). Note: to set the default DirectSpellChecker.INTERNAL_LEVENSHTEIN, use ""internal"". accuracy: Sets DirectSpellChecker.setAccuracy(float). maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"distanceMeasure: Sets DirectSpellChecker.setDistance(StringDistance). Note: to set the default DirectSpellChecker.INTERNAL_LEVENSHTEIN, use ""internal"". accuracy: Sets DirectSpellChecker.setAccuracy(float). maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"accuracy: Sets DirectSpellChecker.setAccuracy(float). maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"maxEdits: Sets DirectSpellChecker.setMaxEdits(int). minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"minPrefix: Sets DirectSpellChecker.setMinPrefix(int). maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"maxInspections: Sets DirectSpellChecker.setMaxInspections(int). comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,"comparatorClass: Sets DirectSpellChecker.setComparator(Comparator). Note: score-then-frequency can be specified as ""score"" and frequency-then-score can be specified as ""freq"". thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float)."
spelling,thresholdTokenFrequency: sets DirectSpellChecker.setThresholdFrequency(float). minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float).
spelling,minQueryLength: sets DirectSpellChecker.setMinQueryLength(int). maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float).
spelling,maxQueryLength: sets DirectSpellChecker.setMaxQueryLength(int). maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float).
spelling,maxQueryFrequency: sets DirectSpellChecker.setMaxQueryFrequency(float).
spelling,public class FileBasedSpellChecker extends AbstractLuceneSpellChecker A spell checker implementation that loads words from a text file (one word per line). Since: solr 1.3
spelling,public class IndexBasedSpellChecker extends AbstractLuceneSpellChecker A spell checker implementation that loads words from Solr as well as arbitrary Lucene indices. Refer to https://solr.apache.org/guide/solr/latest/query-guide/spell-checking.html for more details. Since: solr 1.3
spelling,Refer to https://solr.apache.org/guide/solr/latest/query-guide/spell-checking.html for more details.
spelling,Methods inherited from interfacejava.util.Iterator forEachRemaining
spelling,Enclosing class: PossibilityIterator public static class PossibilityIterator.RankedSpellPossibility extends Object
spelling,"It is only invoked for the CommonParams.Q parameter, and not the ""spellcheck.q"" parameter. Systems that use their own query parser or those that find issue with the basic implementation should implement their own QueryConverter instead of using the provided implementation (SpellingQueryConverter) by overriding the appropriate methods on the SpellingQueryConverter and registering it in the solrconfig.xml Refer to https://solr.apache.org/guide/solr/latest/query-guide/spell-checking.html for more details"
spelling,Refer to https://solr.apache.org/guide/solr/latest/query-guide/spell-checking.html for more details
spelling,public class ResultEntry extends Object
spelling,"Direct Known Subclasses: AbstractLuceneSpellChecker, ConjunctionSolrSpellChecker, DirectSolrSpellChecker, Suggester, WordBreakSolrSpellChecker public abstract class SolrSpellChecker extends Object Refer to https://solr.apache.org/guide/solr/latest/query-guide/spell-checking.html for more details. Since: solr 1.3"
spelling,public class SpellCheckCollator extends Object
spelling,public class SpellCheckCorrection extends Object
spelling,public class SpellingOptions extends Object
spelling,"Each term is checked to determine if it is optional, required or prohibited. Required terms output a Token with the QueryConverter.REQUIRED_TERM_FLAG set. Prohibited terms output a Token with the QueryConverter.PROHIBITED_TERM_FLAG set. If the query uses the plus (+) and minus (-) to denote required and prohibited, this determination will be accurate. In the case boolean AND/OR/NOTs are used, this converter makes an uninformed guess as to whether the term would likely behave as if it is Required or Prohibited and sets the flags accordingly. These flags are used downstream to generate collations for WordBreakSolrSpellChecker, in cases where an original term is split up into multiple Tokens."
spelling,public class SpellingResult extends Object Implementations of SolrSpellChecker must return suggestions as SpellResult instance. This is converted into the required NamedList format in SpellCheckComponent. Since: solr 1.3
spelling,"clear() initializes all of the fields to default values. This was changed in contrast to Lucene 2.4, but should affect no one. Because TokenStreams can be chained, one cannot assume that the Token's current type is correct. The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them. When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"Because TokenStreams can be chained, one cannot assume that the Token's current type is correct. The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them. When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them. When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"The start and end offsets permit applications to re-associate a token with its source text, e.g., to display highlighted query terms in a document browser, or to show matching text fragments in a KWIC display, etc. The type is a string, assigned by a lexical analyzer (a.k.a. tokenizer), naming the lexical or syntactic class that the token belongs to. For example an end of sentence marker token might be implemented with type ""eos"". The default token type is ""word"". A Token can optionally have metadata (a.k.a. payload) in the form of a variable length byte array. Use PostingsEnum.getPayload() to retrieve the payloads from the index. A few things to note: clear() initializes all of the fields to default values. This was changed in contrast to Lucene 2.4, but should affect no one. Because TokenStreams can be chained, one cannot assume that the Token's current type is correct. The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them. When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"The type is a string, assigned by a lexical analyzer (a.k.a. tokenizer), naming the lexical or syntactic class that the token belongs to. For example an end of sentence marker token might be implemented with type ""eos"". The default token type is ""word"". A Token can optionally have metadata (a.k.a. payload) in the form of a variable length byte array. Use PostingsEnum.getPayload() to retrieve the payloads from the index. A few things to note: clear() initializes all of the fields to default values. This was changed in contrast to Lucene 2.4, but should affect no one. Because TokenStreams can be chained, one cannot assume that the Token's current type is correct. The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them. When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"A Token can optionally have metadata (a.k.a. payload) in the form of a variable length byte array. Use PostingsEnum.getPayload() to retrieve the payloads from the index. A few things to note: clear() initializes all of the fields to default values. This was changed in contrast to Lucene 2.4, but should affect no one. Because TokenStreams can be chained, one cannot assume that the Token's current type is correct. The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them. When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"A few things to note: clear() initializes all of the fields to default values. This was changed in contrast to Lucene 2.4, but should affect no one. Because TokenStreams can be chained, one cannot assume that the Token's current type is correct. The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them. When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"clear() initializes all of the fields to default values. This was changed in contrast to Lucene 2.4, but should affect no one. Because TokenStreams can be chained, one cannot assume that the Token's current type is correct. The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them. When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"Because TokenStreams can be chained, one cannot assume that the Token's current type is correct. The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them. When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them. When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,"When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again."
spelling,Fields inherited from classorg.apache.lucene.analysis.tokenattributes.CharTermAttributeImpl builder
spelling,Fields inherited from interfaceorg.apache.lucene.analysis.tokenattributes.TypeAttribute DEFAULT_TYPE
spelling,"Methods inherited from classorg.apache.lucene.analysis.tokenattributes.PackedTokenAttributeImpl end, endOffset, getPositionIncrement, getPositionLength, getTermFrequency, setOffset, setPositionIncrement, setPositionLength, setTermFrequency, setType, startOffset, type"
spelling,"Methods inherited from classorg.apache.lucene.analysis.tokenattributes.CharTermAttributeImpl append, append, append, append, append, append, buffer, charAt, copyBuffer, getBytesRef, length, resizeBuffer, setEmpty, setLength, subSequence, toString"
spelling,Methods inherited from classorg.apache.lucene.util.AttributeImpl reflectAsString
spelling,"Methods inherited from interfacejava.lang.CharSequence chars, codePoints"
spelling,Fields inherited from classorg.apache.lucene.analysis.tokenattributes.CharTermAttributeImpl builder
spelling,Fields inherited from interfaceorg.apache.lucene.analysis.tokenattributes.TypeAttribute DEFAULT_TYPE
spelling,"public class WordBreakSolrSpellChecker extends SolrSpellChecker A spellchecker that breaks and combines words. This will not combine adjacent tokens that do not have the same required status (prohibited, required, optional). However, this feature depends on incoming term flags being properly set. (QueryConverter.PROHIBITED_TERM_FLAG, QueryConverter.REQUIRED_TERM_FLAG, QueryConverter.TERM_IN_BOOLEAN_QUERY_FLAG, and QueryConverter.TERM_PRECEDES_NEW_BOOLEAN_OPERATOR_FLAG ) This feature breaks completely if the upstream analyzer or query converter sets flags with the same values but different meanings. The default query converter (if not using ""spellcheck.q"") is SpellingQueryConverter, which properly sets these flags."
spelling,"This will not combine adjacent tokens that do not have the same required status (prohibited, required, optional). However, this feature depends on incoming term flags being properly set. (QueryConverter.PROHIBITED_TERM_FLAG, QueryConverter.REQUIRED_TERM_FLAG, QueryConverter.TERM_IN_BOOLEAN_QUERY_FLAG, and QueryConverter.TERM_PRECEDES_NEW_BOOLEAN_OPERATOR_FLAG ) This feature breaks completely if the upstream analyzer or query converter sets flags with the same values but different meanings. The default query converter (if not using ""spellcheck.q"") is SpellingQueryConverter, which properly sets these flags."
spelling,"Direct Known Subclasses: DocumentDictionaryFactory, DocumentExpressionDictionaryFactory, FileDictionaryFactory, HighFrequencyDictionaryFactory public abstract class DictionaryFactory extends Object Encapsulates shared fields for all types of dictionaryFactory classes"
spelling,public class DocumentDictionaryFactory extends DictionaryFactory Factory for DocumentDictionary
spelling,public class DocumentExpressionDictionaryFactory extends DictionaryFactory Factory for DocumentValueSourceDictionary
spelling,public class FileDictionaryFactory extends DictionaryFactory Factory for FileDictionary
spelling,public class HighFrequencyDictionaryFactory extends DictionaryFactory Factory for HighFrequencyDictionary
spelling,"Direct Known Subclasses: AnalyzingInfixLookupFactory, AnalyzingLookupFactory, FreeTextLookupFactory, FSTLookupFactory, FuzzyLookupFactory, JaspellLookupFactory, TSTLookupFactory, WFSTLookupFactory public abstract class LookupFactory extends Object Suggester factory for creating Lookup instances."
spelling,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
spelling,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
spelling,public class Suggester extends SolrSpellChecker
spelling,public class SuggesterOptions extends Object Encapsulates the inputs required to be passed on to the underlying suggester in SolrSuggester
spelling,public class SuggesterResult extends Object Encapsulates the results returned by the suggester in SolrSuggester
spelling,Direct Known Subclasses: BlendedInfixLookupFactory public class AnalyzingInfixLookupFactory extends LookupFactory Factory for AnalyzingInfixSuggester WARNING: This API is experimental and might change in incompatible ways in the next release.
spelling,public class AnalyzingLookupFactory extends LookupFactory Factory for AnalyzingSuggester WARNING: This API is experimental and might change in incompatible ways in the next release.
spelling,public class BlendedInfixLookupFactory extends AnalyzingInfixLookupFactory Factory for BlendedInfixLookupFactory WARNING: This API is experimental and might change in incompatible ways in the next release.
spelling,public class FreeTextLookupFactory extends LookupFactory LookupFactory implementation for FreeTextSuggester
spelling,public class FSTLookupFactory extends LookupFactory Factory for FSTCompletionLookup
spelling,public class FuzzyLookupFactory extends LookupFactory Factory for FuzzySuggester WARNING: This API is experimental and might change in incompatible ways in the next release.
spelling,public class WFSTLookupFactory extends LookupFactory Factory for WFSTCompletionLookup WARNING: This API is experimental and might change in incompatible ways in the next release.
spelling,"Nested classes/interfaces inherited from classorg.apache.lucene.search.suggest.Lookup org.apache.lucene.search.suggest.Lookup.LookupPriorityQueue, org.apache.lucene.search.suggest.Lookup.LookupResult"
spelling,Fields inherited from classorg.apache.lucene.search.suggest.Lookup CHARSEQUENCE_COMPARATOR
spelling,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
spelling,"Methods inherited from classorg.apache.lucene.search.suggest.Lookup build, load, lookup, lookup, store"
spelling,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
spelling,"Nested classes/interfaces inherited from classorg.apache.lucene.search.suggest.Lookup org.apache.lucene.search.suggest.Lookup.LookupPriorityQueue, org.apache.lucene.search.suggest.Lookup.LookupResult"
spelling,public class JaspellLookupFactory extends LookupFactory Factory for JaspellLookup Note: This Suggester is not very RAM efficient.
spelling,"This data structure is faster than hashing for many typical search problems, and supports a broader range of useful problems and operations. Ternary searches are faster than hashing and more powerful, too. The theory of ternary search trees was described at a symposium in 1997 (see ""Fast Algorithms for Sorting and Searching Strings,"" by J.L. Bentley and R. Sedgewick, Proceedings of the 8th Annual ACM-SIAM Symposium on Discrete Algorithms, January 1997). Algorithms in C, Third Edition, by Robert Sedgewick (Addison-Wesley, 1998) provides yet another view of ternary search trees."
spelling,"The theory of ternary search trees was described at a symposium in 1997 (see ""Fast Algorithms for Sorting and Searching Strings,"" by J.L. Bentley and R. Sedgewick, Proceedings of the 8th Annual ACM-SIAM Symposium on Discrete Algorithms, January 1997). Algorithms in C, Third Edition, by Robert Sedgewick (Addison-Wesley, 1998) provides yet another view of ternary search trees."
spelling,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
spelling,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
spelling,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
spelling,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
spelling,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
spelling,public class TSTLookupFactory extends LookupFactory Factory for TSTLookup
uninverting,"Like FieldCache, it uninverts the index and holds a packed data structure in RAM to enable fast access. Unlike FieldCache, it can handle multi-valued fields, and, it does not hold the term bytes in RAM. Rather, you must obtain a TermsEnum from the getOrdTermsEnum(org.apache.lucene.index.LeafReader) method, and then seek-by-ord to get the term's bytes. While normally term ords are type long, in this API they are int as the internal representation here cannot address more than MAX_INT unique terms. Also, typically this class is used on fields with relatively few unique terms vs the number of documents. A previous internal limit (16 MB) on how many bytes each chunk of documents may consume has been increased to 2 GB. Deleted documents are skipped during uninversion, and if you look them up you'll get 0 ords. The returned per-document ords do not retain their original order in the document. Instead they are returned in sorted (by ord, ie term's BytesRef comparator) order. They are also de-dup'd (ie if doc has same term more than once in this field, you'll only get that ord back once). This class will create its own term index internally, allowing to create a wrapped TermsEnum that can handle ord. The getOrdTermsEnum(org.apache.lucene.index.LeafReader) method then provides this wrapped enum. The RAM consumption of this class can be high!"
uninverting,"While normally term ords are type long, in this API they are int as the internal representation here cannot address more than MAX_INT unique terms. Also, typically this class is used on fields with relatively few unique terms vs the number of documents. A previous internal limit (16 MB) on how many bytes each chunk of documents may consume has been increased to 2 GB. Deleted documents are skipped during uninversion, and if you look them up you'll get 0 ords. The returned per-document ords do not retain their original order in the document. Instead they are returned in sorted (by ord, ie term's BytesRef comparator) order. They are also de-dup'd (ie if doc has same term more than once in this field, you'll only get that ord back once). This class will create its own term index internally, allowing to create a wrapped TermsEnum that can handle ord. The getOrdTermsEnum(org.apache.lucene.index.LeafReader) method then provides this wrapped enum. The RAM consumption of this class can be high!"
uninverting,"Deleted documents are skipped during uninversion, and if you look them up you'll get 0 ords. The returned per-document ords do not retain their original order in the document. Instead they are returned in sorted (by ord, ie term's BytesRef comparator) order. They are also de-dup'd (ie if doc has same term more than once in this field, you'll only get that ord back once). This class will create its own term index internally, allowing to create a wrapped TermsEnum that can handle ord. The getOrdTermsEnum(org.apache.lucene.index.LeafReader) method then provides this wrapped enum. The RAM consumption of this class can be high!"
uninverting,"The returned per-document ords do not retain their original order in the document. Instead they are returned in sorted (by ord, ie term's BytesRef comparator) order. They are also de-dup'd (ie if doc has same term more than once in this field, you'll only get that ord back once). This class will create its own term index internally, allowing to create a wrapped TermsEnum that can handle ord. The getOrdTermsEnum(org.apache.lucene.index.LeafReader) method then provides this wrapped enum. The RAM consumption of this class can be high!"
uninverting,"This class will create its own term index internally, allowing to create a wrapped TermsEnum that can handle ord. The getOrdTermsEnum(org.apache.lucene.index.LeafReader) method then provides this wrapped enum. The RAM consumption of this class can be high!"
uninverting,The RAM consumption of this class can be high!
uninverting,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
uninverting,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
uninverting,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
uninverting,Enclosing interface: FieldCache public static final class FieldCache.CacheEntry extends Object EXPERT: A unique Identifier/Description for each item in the FieldCache. Can be useful for logging/debugging. WARNING: This API is experimental and might change in incompatible ways in the next release.
uninverting,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
uninverting,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
uninverting,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
uninverting,This abstraction can be cleaned up when Parser.termsEnum is removed.
uninverting,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
uninverting,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
uninverting,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
uninverting,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
uninverting,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
uninverting,Enclosing class: FieldCacheImpl.SortedDocValuesImpl public class FieldCacheImpl.SortedDocValuesImpl.Iter extends org.apache.lucene.index.SortedDocValues
uninverting,Fields inherited from classorg.apache.lucene.search.DocIdSetIterator NO_MORE_DOCS
uninverting,"Methods inherited from classorg.apache.lucene.index.SortedDocValues intersect, lookupTerm, termsEnum"
uninverting,"Methods inherited from classorg.apache.lucene.search.DocIdSetIterator all, empty, range, slowAdvance"
uninverting,Fields inherited from classorg.apache.lucene.search.DocIdSetIterator NO_MORE_DOCS
uninverting,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
uninverting,Enclosing class: UninvertingReader public static class UninvertingReader.FieldCacheStats extends Object Return information about the backing cache NOTE: This API is for internal purposes only and might change in incompatible ways in the next release.
uninverting,"This is accomplished by ""inverting the inverted index"" or ""uninversion"". The uninversion process happens lazily: upon the first request for the field's docvalues (e.g. via LeafReader.getNumericDocValues(String) or similar), it will create the docvalues on-the-fly if needed and cache it, based on the core cache key of the wrapped LeafReader."
uninverting,"The uninversion process happens lazily: upon the first request for the field's docvalues (e.g. via LeafReader.getNumericDocValues(String) or similar), it will create the docvalues on-the-fly if needed and cache it, based on the core cache key of the wrapped LeafReader."
uninverting,"Nested classes/interfaces inherited from classorg.apache.lucene.index.FilterLeafReader org.apache.lucene.index.FilterLeafReader.FilterFields, org.apache.lucene.index.FilterLeafReader.FilterPostingsEnum, org.apache.lucene.index.FilterLeafReader.FilterTerms, org.apache.lucene.index.FilterLeafReader.FilterTermsEnum"
uninverting,"Nested classes/interfaces inherited from classorg.apache.lucene.index.IndexReader org.apache.lucene.index.IndexReader.CacheHelper, org.apache.lucene.index.IndexReader.CacheKey, org.apache.lucene.index.IndexReader.ClosedListener"
uninverting,Fields inherited from classorg.apache.lucene.index.FilterLeafReader in
uninverting,"Methods inherited from classorg.apache.lucene.index.FilterLeafReader checkIntegrity, doClose, document, getByteVectorValues, getDelegate, getFloatVectorValues, getLiveDocs, getMetaData, getNormValues, getPointValues, getSortedNumericDocValues, getTermVectors, maxDoc, numDocs, searchNearestVectors, searchNearestVectors, storedFields, terms, termVectors, unwrap"
uninverting,"Methods inherited from classorg.apache.lucene.index.LeafReader docFreq, getContext, getDocCount, getSumDocFreq, getSumTotalTermFreq, postings, postings, searchNearestVectors, searchNearestVectors, totalTermFreq"
uninverting,"Methods inherited from classorg.apache.lucene.index.IndexReader close, decRef, document, document, ensureOpen, equals, getRefCount, getTermVector, hasDeletions, hashCode, incRef, leaves, notifyReaderClosedListeners, numDeletedDocs, registerParentReader, tryIncRef"
uninverting,"Nested classes/interfaces inherited from classorg.apache.lucene.index.FilterLeafReader org.apache.lucene.index.FilterLeafReader.FilterFields, org.apache.lucene.index.FilterLeafReader.FilterPostingsEnum, org.apache.lucene.index.FilterLeafReader.FilterTerms, org.apache.lucene.index.FilterLeafReader.FilterTermsEnum"
uninverting,"Nested classes/interfaces inherited from classorg.apache.lucene.index.IndexReader org.apache.lucene.index.IndexReader.CacheHelper, org.apache.lucene.index.IndexReader.CacheKey, org.apache.lucene.index.IndexReader.ClosedListener"
update,"Note: This is purely an implementation detail of autoCommit and will definitely change in the future, so the interface should not be relied-upon Note: all access must be synchronized. Public for tests."
update,Note: all access must be synchronized. Public for tests.
update,Public for tests.
update,public class DocumentBuilder extends Object Builds a Lucene Document from a SolrInputDocument.
update,Fields inherited from classorg.apache.lucene.util.InfoStream NO_OUTPUT
update,"Methods inherited from classorg.apache.lucene.util.InfoStream getDefault, setDefault"
update,Fields inherited from classorg.apache.lucene.util.InfoStream NO_OUTPUT
update,Methods inherited from classjava.io.OutputStream nullOutputStream
update,Enclosing class: PeerSync public static class PeerSync.MissedUpdatesFinder extends Object Helper class for doing comparison ourUpdates and other replicas's updates to find the updates that we missed
update,Enclosing class: PeerSync public static class PeerSync.MissedUpdatesRequest extends Object Result of PeerSync.MissedUpdatesFinder
update,Enclosing class: PeerSync public static class PeerSync.PeerSyncResult extends Object
update,Enclosing class: PeerSyncWithLeader public static class PeerSyncWithLeader.MissedUpdatesFinder extends Object Helper class for doing comparison ourUpdates and other replicas's updates to find the updates that we missed
update,Enclosing class: SolrCmdDistributor public static class SolrCmdDistributor.ForwardNode extends SolrCmdDistributor.StdNode
update,Direct Known Subclasses: SolrCmdDistributor.StdNode Enclosing class: SolrCmdDistributor public abstract static class SolrCmdDistributor.Node extends Object
update,Enclosing class: SolrCmdDistributor public static class SolrCmdDistributor.Req extends Object
update,Enclosing class: SolrCmdDistributor public static class SolrCmdDistributor.Response extends Object
update,Enclosing class: SolrCmdDistributor public static class SolrCmdDistributor.SolrError extends Object
update,Direct Known Subclasses: SolrCmdDistributor.ForwardNode Enclosing class: SolrCmdDistributor public static class SolrCmdDistributor.StdNode extends SolrCmdDistributor.Node
update,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
update,Direct Known Subclasses: DefaultSolrCoreState public abstract class SolrCoreState extends Object The state in this class can be easily shared between SolrCores across SolrCore reloads.
update,public class SolrIndexSplitter extends Object
update,"Nested classes/interfaces inherited from classorg.apache.lucene.index.IndexWriter org.apache.lucene.index.IndexWriter.DocStats, org.apache.lucene.index.IndexWriter.IndexReaderWarmer"
update,"Fields inherited from classorg.apache.lucene.index.IndexWriter MAX_DOCS, MAX_POSITION, MAX_STORED_STRING_LENGTH, MAX_TERM_LENGTH, SOURCE, SOURCE_ADDINDEXES_READERS, SOURCE_FLUSH, SOURCE_MERGE, WRITE_LOCK_NAME"
update,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
update,"Methods inherited from classorg.apache.lucene.index.IndexWriter addDocument, addDocuments, addIndexes, addIndexes, addIndexesReaderMerge, advanceSegmentInfosVersion, commit, decRefDeleter, deleteAll, deleteDocuments, deleteDocuments, deleteUnusedFiles, doBeforeFlush, ensureOpen, ensureOpen, flush, flushNextBuffer, forceMerge, forceMerge, forceMergeDeletes, forceMergeDeletes, getAnalyzer, getConfig, getDirectory, getDocStats, getFieldNames, getFlushingBytes, getInfoStream, getLiveCommitData, getMaxCompletedSequenceNumber, getMergingSegments, getPendingNumDocs, getTragicException, hasDeletions, hasPendingMerges, hasUncommittedChanges, incRefDeleter, isEnableTestPoints, isOpen, maybeMerge, mergeSuccess, numDeletedDocs, numDeletesToMerge, numRamDocs, onTragicEvent, prepareCommit, ramBytesUsed, setLiveCommitData, setLiveCommitData, softUpdateDocument, softUpdateDocuments, tryDeleteDocument, tryUpdateDocValue, updateBinaryDocValue, updateDocument, updateDocuments, updateDocuments, updateDocValues, updateNumericDocValue"
update,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
update,"Nested classes/interfaces inherited from classorg.apache.lucene.index.IndexWriter org.apache.lucene.index.IndexWriter.DocStats, org.apache.lucene.index.IndexWriter.IndexReaderWarmer"
update,public class StreamingSolrClients extends Object
update,"Methods inherited from classjava.io.InputStream mark, markSupported, nullInputStream, read, readAllBytes, readNBytes, readNBytes, reset, skip, transferTo"
update,Enclosing class: TransactionLog public class TransactionLog.FSReverseReader extends TransactionLog.ReverseReader
update,"TODO: keep two files, one for [operation, version, id] and the other for the actual document data. That way we could throw away document log files more readily while retaining the smaller operation log files longer (and we can retrieve the stored fields from the latest documents from the index). This would require keeping all source fields stored of course. This would also allow to not log document data for requests with commit=true in them (since we know that if the request succeeds, all docs will be committed)"
update,"This would require keeping all source fields stored of course. This would also allow to not log document data for requests with commit=true in them (since we know that if the request succeeds, all docs will be committed)"
update,"This would also allow to not log document data for requests with commit=true in them (since we know that if the request succeeds, all docs will be committed)"
update,Direct Known Subclasses: TransactionLog.SortedLogReader Enclosing class: TransactionLog public class TransactionLog.LogReader extends Object
update,Direct Known Subclasses: TransactionLog.FSReverseReader Enclosing class: TransactionLog public abstract static class TransactionLog.ReverseReader extends Object
update,Enclosing class: TransactionLog public class TransactionLog.SortedLogReader extends TransactionLog.LogReader
update,public class UpdateLocks extends Object Locks associated with updates in connection with the UpdateLog. NOTE: This API is for internal purposes only and might change in incompatible ways in the next release.
update,Enclosing class: UpdateLog public static class UpdateLog.DBQ extends Object Holds the query and the version for a DeleteByQuery command
update,Enclosing class: UpdateLog protected static class UpdateLog.DeleteUpdate extends Object
update,Enclosing class: UpdateLog public static class UpdateLog.LogPtr extends Object
update,Enclosing class: UpdateLog public static class UpdateLog.RecoveryInfo extends Object
update,Enclosing class: UpdateLog protected static class UpdateLog.Update extends Object
update,public class UpdateShardHandlerConfig extends Object
update,"public class VersionInfo extends Object Related to the _version_ field, in connection with the UpdateLog. NOTE: This API is for internal purposes only and might change in incompatible ways in the next release."
update,"This base class handles initialization of the fieldName init param, and provides an AbstractDefaultValueUpdateProcessorFactory.DefaultValueUpdateProcessor that Factory subclasses may choose to return from their getInstance implementation."
update,"By default, this processor selects all fields that don't match a schema field or dynamic field. The ""fieldName"" and ""fieldRegex"" selectors may be specified to further restrict the selected fields, but the other selectors (""typeName"", ""typeClass"", and ""fieldNameMatchesSchemaField"") may not be specified. This processor is configured to map from each field's values' class(es) to the schema field type that will be used when adding the new field to the schema. All new fields are then added to the schema in a single batch. If schema addition fails for any field, addition is re-attempted only for those that dont match any schema field. This process is repeated, either until all new fields are successfully added, or until there are no new fields (presumably because the fields that were new when this processor started its work were subsequently added by a different update request, possibly on a different node). This processor takes as configuration a sequence of zero or more ""typeMapping""-s from one or more ""valueClass""-s, specified as either an <arr> of <str>, or multiple <str> with the same name, to an existing schema ""fieldType"". If more than one ""valueClass"" is specified in a ""typeMapping"", field values with any of the specified ""valueClass""-s will be mapped to the specified target ""fieldType"". The ""typeMapping""-s are attempted in the specified order; if a field value's class is not specified in a ""valueClass"", the next ""typeMapping"" is attempted. If no ""typeMapping"" succeeds, then either the ""typeMapping"" configured with <bool name=""default"">true</bool> is used, or if none is so configured, the lt;str name=""defaultFieldType"">...</str> is used. Zero or more ""copyField"" directives may be included with each ""typeMapping"", using a <lst>. The copy field source is automatically set to the new field name; ""dest"" must specify the destination field or dynamic field in a <str>; and ""maxChars"" may optionally be specified in an <int>. Example configuration: <updateProcessor class=""solr.AddSchemaFieldsUpdateProcessorFactory"" name=""add-schema-fields""> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.String</str> <str name=""fieldType"">text_general</str> <lst name=""copyField""> <str name=""dest"">*_str</str> <int name=""maxChars"">256</int> </lst> <!-- Use as default mapping instead of defaultFieldType --> <bool name=""default"">true</bool> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Boolean</str> <str name=""fieldType"">booleans</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.util.Date</str> <str name=""fieldType"">pdates</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Long</str> <str name=""valueClass"">java.lang.Integer</str> <str name=""fieldType"">plongs</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Number</str> <str name=""fieldType"">pdoubles</str> </lst> </updateProcessor>"
update,"This processor is configured to map from each field's values' class(es) to the schema field type that will be used when adding the new field to the schema. All new fields are then added to the schema in a single batch. If schema addition fails for any field, addition is re-attempted only for those that dont match any schema field. This process is repeated, either until all new fields are successfully added, or until there are no new fields (presumably because the fields that were new when this processor started its work were subsequently added by a different update request, possibly on a different node). This processor takes as configuration a sequence of zero or more ""typeMapping""-s from one or more ""valueClass""-s, specified as either an <arr> of <str>, or multiple <str> with the same name, to an existing schema ""fieldType"". If more than one ""valueClass"" is specified in a ""typeMapping"", field values with any of the specified ""valueClass""-s will be mapped to the specified target ""fieldType"". The ""typeMapping""-s are attempted in the specified order; if a field value's class is not specified in a ""valueClass"", the next ""typeMapping"" is attempted. If no ""typeMapping"" succeeds, then either the ""typeMapping"" configured with <bool name=""default"">true</bool> is used, or if none is so configured, the lt;str name=""defaultFieldType"">...</str> is used. Zero or more ""copyField"" directives may be included with each ""typeMapping"", using a <lst>. The copy field source is automatically set to the new field name; ""dest"" must specify the destination field or dynamic field in a <str>; and ""maxChars"" may optionally be specified in an <int>. Example configuration: <updateProcessor class=""solr.AddSchemaFieldsUpdateProcessorFactory"" name=""add-schema-fields""> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.String</str> <str name=""fieldType"">text_general</str> <lst name=""copyField""> <str name=""dest"">*_str</str> <int name=""maxChars"">256</int> </lst> <!-- Use as default mapping instead of defaultFieldType --> <bool name=""default"">true</bool> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Boolean</str> <str name=""fieldType"">booleans</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.util.Date</str> <str name=""fieldType"">pdates</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Long</str> <str name=""valueClass"">java.lang.Integer</str> <str name=""fieldType"">plongs</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Number</str> <str name=""fieldType"">pdoubles</str> </lst> </updateProcessor>"
update,"This processor takes as configuration a sequence of zero or more ""typeMapping""-s from one or more ""valueClass""-s, specified as either an <arr> of <str>, or multiple <str> with the same name, to an existing schema ""fieldType"". If more than one ""valueClass"" is specified in a ""typeMapping"", field values with any of the specified ""valueClass""-s will be mapped to the specified target ""fieldType"". The ""typeMapping""-s are attempted in the specified order; if a field value's class is not specified in a ""valueClass"", the next ""typeMapping"" is attempted. If no ""typeMapping"" succeeds, then either the ""typeMapping"" configured with <bool name=""default"">true</bool> is used, or if none is so configured, the lt;str name=""defaultFieldType"">...</str> is used. Zero or more ""copyField"" directives may be included with each ""typeMapping"", using a <lst>. The copy field source is automatically set to the new field name; ""dest"" must specify the destination field or dynamic field in a <str>; and ""maxChars"" may optionally be specified in an <int>. Example configuration: <updateProcessor class=""solr.AddSchemaFieldsUpdateProcessorFactory"" name=""add-schema-fields""> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.String</str> <str name=""fieldType"">text_general</str> <lst name=""copyField""> <str name=""dest"">*_str</str> <int name=""maxChars"">256</int> </lst> <!-- Use as default mapping instead of defaultFieldType --> <bool name=""default"">true</bool> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Boolean</str> <str name=""fieldType"">booleans</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.util.Date</str> <str name=""fieldType"">pdates</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Long</str> <str name=""valueClass"">java.lang.Integer</str> <str name=""fieldType"">plongs</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Number</str> <str name=""fieldType"">pdoubles</str> </lst> </updateProcessor>"
update,"If more than one ""valueClass"" is specified in a ""typeMapping"", field values with any of the specified ""valueClass""-s will be mapped to the specified target ""fieldType"". The ""typeMapping""-s are attempted in the specified order; if a field value's class is not specified in a ""valueClass"", the next ""typeMapping"" is attempted. If no ""typeMapping"" succeeds, then either the ""typeMapping"" configured with <bool name=""default"">true</bool> is used, or if none is so configured, the lt;str name=""defaultFieldType"">...</str> is used. Zero or more ""copyField"" directives may be included with each ""typeMapping"", using a <lst>. The copy field source is automatically set to the new field name; ""dest"" must specify the destination field or dynamic field in a <str>; and ""maxChars"" may optionally be specified in an <int>. Example configuration: <updateProcessor class=""solr.AddSchemaFieldsUpdateProcessorFactory"" name=""add-schema-fields""> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.String</str> <str name=""fieldType"">text_general</str> <lst name=""copyField""> <str name=""dest"">*_str</str> <int name=""maxChars"">256</int> </lst> <!-- Use as default mapping instead of defaultFieldType --> <bool name=""default"">true</bool> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Boolean</str> <str name=""fieldType"">booleans</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.util.Date</str> <str name=""fieldType"">pdates</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Long</str> <str name=""valueClass"">java.lang.Integer</str> <str name=""fieldType"">plongs</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Number</str> <str name=""fieldType"">pdoubles</str> </lst> </updateProcessor>"
update,"Zero or more ""copyField"" directives may be included with each ""typeMapping"", using a <lst>. The copy field source is automatically set to the new field name; ""dest"" must specify the destination field or dynamic field in a <str>; and ""maxChars"" may optionally be specified in an <int>. Example configuration: <updateProcessor class=""solr.AddSchemaFieldsUpdateProcessorFactory"" name=""add-schema-fields""> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.String</str> <str name=""fieldType"">text_general</str> <lst name=""copyField""> <str name=""dest"">*_str</str> <int name=""maxChars"">256</int> </lst> <!-- Use as default mapping instead of defaultFieldType --> <bool name=""default"">true</bool> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Boolean</str> <str name=""fieldType"">booleans</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.util.Date</str> <str name=""fieldType"">pdates</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Long</str> <str name=""valueClass"">java.lang.Integer</str> <str name=""fieldType"">plongs</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Number</str> <str name=""fieldType"">pdoubles</str> </lst> </updateProcessor>"
update,"Example configuration: <updateProcessor class=""solr.AddSchemaFieldsUpdateProcessorFactory"" name=""add-schema-fields""> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.String</str> <str name=""fieldType"">text_general</str> <lst name=""copyField""> <str name=""dest"">*_str</str> <int name=""maxChars"">256</int> </lst> <!-- Use as default mapping instead of defaultFieldType --> <bool name=""default"">true</bool> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Boolean</str> <str name=""fieldType"">booleans</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.util.Date</str> <str name=""fieldType"">pdates</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Long</str> <str name=""valueClass"">java.lang.Integer</str> <str name=""fieldType"">plongs</str> </lst> <lst name=""typeMapping""> <str name=""valueClass"">java.lang.Number</str> <str name=""fieldType"">pdoubles</str> </lst> </updateProcessor>"
update,public class AtomicUpdateDocumentMerger extends Object WARNING: This API is experimental and might change in incompatible ways in the next release.
update,"sample request: curl -X POST -H Content-Type: application/json http://localhost:8983/solr/test/update/json/docs?processor=atomic;ampersand;atomic.my_newfield=add;ampersand;atomic.subject=set;ampersand;atomic.count_i=inc;ampersand;commit=true --data-binary {""id"": 1,""title"": ""titleA""} currently supports all types of atomic updates"
update,public class ClassificationUpdateProcessorParams extends Object
update,One or more <str> An <arr> of <str> A <lst> containing FieldMutatingUpdateProcessorFactory style selector arguments
update,An <arr> of <str> A <lst> containing FieldMutatingUpdateProcessorFactory style selector arguments
update,A <lst> containing FieldMutatingUpdateProcessorFactory style selector arguments
update,The category field will be cloned into the category_s field Both the authors and editors fields will be cloned into the contributors field Any field with a name ending in _price -- except for list_price -- will be cloned into the all_prices Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature)
update,Both the authors and editors fields will be cloned into the contributors field Any field with a name ending in _price -- except for list_price -- will be cloned into the all_prices Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature)
update,Any field with a name ending in _price -- except for list_price -- will be cloned into the all_prices Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature)
update,Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature)
update,"The source field(s) can be configured as either: One or more <str> An <arr> of <str> A <lst> containing FieldMutatingUpdateProcessorFactory style selector arguments The dest field can be a single <str> containing the literal name of a destination field, or it may be a <lst> specifying a regex pattern and a replacement string. If the pattern + replacement option is used the pattern will be matched against all fields matched by the source selector, and the replacement string (including any capture groups specified from the pattern) will be evaluated a using Matcher.replaceAll(String) to generate the literal name of the destination field. If the resolved dest field already exists in the document, then the values from the source fields will be added to it. The ""boost"" value associated with the dest will not be changed, and any boost specified on the source fields will be ignored. (If the dest field did not exist prior to this processor, the newly created dest field will have the default boost of 1.0) In the example below: The category field will be cloned into the category_s field Both the authors and editors fields will be cloned into the contributors field Any field with a name ending in _price -- except for list_price -- will be cloned into the all_prices Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature) <updateRequestProcessorChain name=""multiple-clones""> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <str name=""source"">category</str> <str name=""dest"">category_s</str> </processor> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <arr name=""source""> <str>authors</str> <str>editors</str> </arr> <str name=""dest"">contributors</str> </processor> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex"">.*_price$</str> <lst name=""exclude""> <str name=""fieldName"">list_price</str> </lst> </lst> <str name=""dest"">all_prices</str> </processor> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex"">^feat(.*)s$</str> </lst> <lst name=""dest""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </str> </processor> </updateRequestProcessorChain> In common case situations where you wish to use a single regular expression as both a fieldRegex selector and a destination pattern, a ""short hand"" syntax is support for convinience: The pattern and replacement may be specified at the top level, omitting source and dest declarations completely, and the pattern will be used to construct an equivalent source selector internally. For example, both of the following configurations are equivalent: <!-- full syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex""^gt;$feat(.*)s$</str> </lst> <lst name=""dest""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </str> </processor> <!-- syntactic sugar syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </processor> When cloning multiple fields (or a single multivalued field) into a single valued field, one of the FieldValueSubsetUpdateProcessorFactory implementations configured after the CloneFieldUpdateProcessorFactory can be useful to reduce the list of values down to a single value."
update,One or more <str> An <arr> of <str> A <lst> containing FieldMutatingUpdateProcessorFactory style selector arguments
update,An <arr> of <str> A <lst> containing FieldMutatingUpdateProcessorFactory style selector arguments
update,A <lst> containing FieldMutatingUpdateProcessorFactory style selector arguments
update,"The dest field can be a single <str> containing the literal name of a destination field, or it may be a <lst> specifying a regex pattern and a replacement string. If the pattern + replacement option is used the pattern will be matched against all fields matched by the source selector, and the replacement string (including any capture groups specified from the pattern) will be evaluated a using Matcher.replaceAll(String) to generate the literal name of the destination field. If the resolved dest field already exists in the document, then the values from the source fields will be added to it. The ""boost"" value associated with the dest will not be changed, and any boost specified on the source fields will be ignored. (If the dest field did not exist prior to this processor, the newly created dest field will have the default boost of 1.0) In the example below: The category field will be cloned into the category_s field Both the authors and editors fields will be cloned into the contributors field Any field with a name ending in _price -- except for list_price -- will be cloned into the all_prices Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature) <updateRequestProcessorChain name=""multiple-clones""> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <str name=""source"">category</str> <str name=""dest"">category_s</str> </processor> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <arr name=""source""> <str>authors</str> <str>editors</str> </arr> <str name=""dest"">contributors</str> </processor> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex"">.*_price$</str> <lst name=""exclude""> <str name=""fieldName"">list_price</str> </lst> </lst> <str name=""dest"">all_prices</str> </processor> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex"">^feat(.*)s$</str> </lst> <lst name=""dest""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </str> </processor> </updateRequestProcessorChain> In common case situations where you wish to use a single regular expression as both a fieldRegex selector and a destination pattern, a ""short hand"" syntax is support for convinience: The pattern and replacement may be specified at the top level, omitting source and dest declarations completely, and the pattern will be used to construct an equivalent source selector internally. For example, both of the following configurations are equivalent: <!-- full syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex""^gt;$feat(.*)s$</str> </lst> <lst name=""dest""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </str> </processor> <!-- syntactic sugar syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </processor> When cloning multiple fields (or a single multivalued field) into a single valued field, one of the FieldValueSubsetUpdateProcessorFactory implementations configured after the CloneFieldUpdateProcessorFactory can be useful to reduce the list of values down to a single value."
update,"If the resolved dest field already exists in the document, then the values from the source fields will be added to it. The ""boost"" value associated with the dest will not be changed, and any boost specified on the source fields will be ignored. (If the dest field did not exist prior to this processor, the newly created dest field will have the default boost of 1.0) In the example below: The category field will be cloned into the category_s field Both the authors and editors fields will be cloned into the contributors field Any field with a name ending in _price -- except for list_price -- will be cloned into the all_prices Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature) <updateRequestProcessorChain name=""multiple-clones""> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <str name=""source"">category</str> <str name=""dest"">category_s</str> </processor> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <arr name=""source""> <str>authors</str> <str>editors</str> </arr> <str name=""dest"">contributors</str> </processor> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex"">.*_price$</str> <lst name=""exclude""> <str name=""fieldName"">list_price</str> </lst> </lst> <str name=""dest"">all_prices</str> </processor> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex"">^feat(.*)s$</str> </lst> <lst name=""dest""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </str> </processor> </updateRequestProcessorChain> In common case situations where you wish to use a single regular expression as both a fieldRegex selector and a destination pattern, a ""short hand"" syntax is support for convinience: The pattern and replacement may be specified at the top level, omitting source and dest declarations completely, and the pattern will be used to construct an equivalent source selector internally. For example, both of the following configurations are equivalent: <!-- full syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex""^gt;$feat(.*)s$</str> </lst> <lst name=""dest""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </str> </processor> <!-- syntactic sugar syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </processor> When cloning multiple fields (or a single multivalued field) into a single valued field, one of the FieldValueSubsetUpdateProcessorFactory implementations configured after the CloneFieldUpdateProcessorFactory can be useful to reduce the list of values down to a single value."
update,"In the example below: The category field will be cloned into the category_s field Both the authors and editors fields will be cloned into the contributors field Any field with a name ending in _price -- except for list_price -- will be cloned into the all_prices Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature) <updateRequestProcessorChain name=""multiple-clones""> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <str name=""source"">category</str> <str name=""dest"">category_s</str> </processor> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <arr name=""source""> <str>authors</str> <str>editors</str> </arr> <str name=""dest"">contributors</str> </processor> <processor class=""solr.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex"">.*_price$</str> <lst name=""exclude""> <str name=""fieldName"">list_price</str> </lst> </lst> <str name=""dest"">all_prices</str> </processor> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex"">^feat(.*)s$</str> </lst> <lst name=""dest""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </str> </processor> </updateRequestProcessorChain> In common case situations where you wish to use a single regular expression as both a fieldRegex selector and a destination pattern, a ""short hand"" syntax is support for convinience: The pattern and replacement may be specified at the top level, omitting source and dest declarations completely, and the pattern will be used to construct an equivalent source selector internally. For example, both of the following configurations are equivalent: <!-- full syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex""^gt;$feat(.*)s$</str> </lst> <lst name=""dest""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </str> </processor> <!-- syntactic sugar syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </processor> When cloning multiple fields (or a single multivalued field) into a single valued field, one of the FieldValueSubsetUpdateProcessorFactory implementations configured after the CloneFieldUpdateProcessorFactory can be useful to reduce the list of values down to a single value."
update,The category field will be cloned into the category_s field Both the authors and editors fields will be cloned into the contributors field Any field with a name ending in _price -- except for list_price -- will be cloned into the all_prices Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature)
update,Both the authors and editors fields will be cloned into the contributors field Any field with a name ending in _price -- except for list_price -- will be cloned into the all_prices Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature)
update,Any field with a name ending in _price -- except for list_price -- will be cloned into the all_prices Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature)
update,Any field name beginning with feat and ending in s (i.e. feats or features) will be cloned into a field prefixed with key_ and not ending in s. (i.e. key_feat or key_feature)
update,"In common case situations where you wish to use a single regular expression as both a fieldRegex selector and a destination pattern, a ""short hand"" syntax is support for convinience: The pattern and replacement may be specified at the top level, omitting source and dest declarations completely, and the pattern will be used to construct an equivalent source selector internally. For example, both of the following configurations are equivalent: <!-- full syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex""^gt;$feat(.*)s$</str> </lst> <lst name=""dest""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </str> </processor> <!-- syntactic sugar syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </processor> When cloning multiple fields (or a single multivalued field) into a single valued field, one of the FieldValueSubsetUpdateProcessorFactory implementations configured after the CloneFieldUpdateProcessorFactory can be useful to reduce the list of values down to a single value."
update,"For example, both of the following configurations are equivalent: <!-- full syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <lst name=""source""> <str name=""fieldRegex""^gt;$feat(.*)s$</str> </lst> <lst name=""dest""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </str> </processor> <!-- syntactic sugar syntax --> <processor class=""solr.processor.CloneFieldUpdateProcessorFactory""> <str name=""pattern"">^feat(.*)s$</str> <str name=""replacement"">key_feat$1</str> </processor> When cloning multiple fields (or a single multivalued field) into a single valued field, one of the FieldValueSubsetUpdateProcessorFactory implementations configured after the CloneFieldUpdateProcessorFactory can be useful to reduce the list of values down to a single value."
update,"When cloning multiple fields (or a single multivalued field) into a single valued field, one of the FieldValueSubsetUpdateProcessorFactory implementations configured after the CloneFieldUpdateProcessorFactory can be useful to reduce the list of values down to a single value."
update,"By default, this processor concatenates the values for any field name which according to the schema is multiValued=""false"" and uses TextField or StrField For example, in the configuration below, any ""single valued"" string and text field which is found to contain multiple values except for the primary_author field will be concatenated using the string ""; "" as a delimiter. For the primary_author field, the multiple values will be left alone for FirstFieldValueUpdateProcessorFactory to deal with. <processor class=""solr.ConcatFieldUpdateProcessorFactory""> <str name=""delimiter"">; </str> <lst name=""exclude""> <str name=""fieldName"">primary_author</str> </lst> </processor> <processor class=""solr.FirstFieldValueUpdateProcessorFactory""> <str name=""fieldName"">primary_author</str> </processor>"
update,"For example, in the configuration below, any ""single valued"" string and text field which is found to contain multiple values except for the primary_author field will be concatenated using the string ""; "" as a delimiter. For the primary_author field, the multiple values will be left alone for FirstFieldValueUpdateProcessorFactory to deal with. <processor class=""solr.ConcatFieldUpdateProcessorFactory""> <str name=""delimiter"">; </str> <lst name=""exclude""> <str name=""fieldName"">primary_author</str> </lst> </processor> <processor class=""solr.FirstFieldValueUpdateProcessorFactory""> <str name=""fieldName"">primary_author</str> </processor>"
update,"By default, this processor matches no fields. The typical use case for this processor would be in combination with the CloneFieldUpdateProcessorFactory so that it's possible to query by the quantity of values in the source field. For example, in the configuration below, the end result will be that the category_count field can be used to search for documents based on how many values they contain in the category field. <processor class=""solr.CloneFieldUpdateProcessorFactory""> <str name=""source"">category</str> <str name=""dest"">category_count</str> </processor> <processor class=""solr.CountFieldValuesUpdateProcessorFactory""> <str name=""fieldName"">category_count</str> </processor> <processor class=""solr.DefaultValueUpdateProcessorFactory""> <str name=""fieldName"">category_count</str> <int name=""value"">0</int> </processor> NOTE: The use of DefaultValueUpdateProcessorFactory is important in this example to ensure that all documents have a value for the category_count field, because CountFieldValuesUpdateProcessorFactory only replaces the list of values with the size of that list. If DefaultValueUpdateProcessorFactory was not used, then any document that had no values for the category field, would also have no value in the category_count field."
update,"The typical use case for this processor would be in combination with the CloneFieldUpdateProcessorFactory so that it's possible to query by the quantity of values in the source field. For example, in the configuration below, the end result will be that the category_count field can be used to search for documents based on how many values they contain in the category field. <processor class=""solr.CloneFieldUpdateProcessorFactory""> <str name=""source"">category</str> <str name=""dest"">category_count</str> </processor> <processor class=""solr.CountFieldValuesUpdateProcessorFactory""> <str name=""fieldName"">category_count</str> </processor> <processor class=""solr.DefaultValueUpdateProcessorFactory""> <str name=""fieldName"">category_count</str> <int name=""value"">0</int> </processor> NOTE: The use of DefaultValueUpdateProcessorFactory is important in this example to ensure that all documents have a value for the category_count field, because CountFieldValuesUpdateProcessorFactory only replaces the list of values with the size of that list. If DefaultValueUpdateProcessorFactory was not used, then any document that had no values for the category field, would also have no value in the category_count field."
update,"For example, in the configuration below, the end result will be that the category_count field can be used to search for documents based on how many values they contain in the category field. <processor class=""solr.CloneFieldUpdateProcessorFactory""> <str name=""source"">category</str> <str name=""dest"">category_count</str> </processor> <processor class=""solr.CountFieldValuesUpdateProcessorFactory""> <str name=""fieldName"">category_count</str> </processor> <processor class=""solr.DefaultValueUpdateProcessorFactory""> <str name=""fieldName"">category_count</str> <int name=""value"">0</int> </processor> NOTE: The use of DefaultValueUpdateProcessorFactory is important in this example to ensure that all documents have a value for the category_count field, because CountFieldValuesUpdateProcessorFactory only replaces the list of values with the size of that list. If DefaultValueUpdateProcessorFactory was not used, then any document that had no values for the category field, would also have no value in the category_count field."
update,"NOTE: The use of DefaultValueUpdateProcessorFactory is important in this example to ensure that all documents have a value for the category_count field, because CountFieldValuesUpdateProcessorFactory only replaces the list of values with the size of that list. If DefaultValueUpdateProcessorFactory was not used, then any document that had no values for the category field, would also have no value in the category_count field."
update,"In the example configuration below, if a document does not contain a value in the price and/or type fields, it will be given default values of 0.0 and/or unknown (respectively). <processor class=""solr.DefaultValueUpdateProcessorFactory""> <str name=""fieldName"">price</str> <float name=""value"">0.0</float> </processor> <processor class=""solr.DefaultValueUpdateProcessorFactory""> <str name=""fieldName"">type</str> <str name=""value"">unknown</str> </processor>"
update,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
update,Enclosing class: DistributedUpdateProcessor public static class DistributedUpdateProcessor.LeaderRequestReplicationTracker extends Object
update,Enclosing class: DistributedUpdateProcessor public static class DistributedUpdateProcessor.RollupRequestReplicationTracker extends Object
update,"deleteVersionParam - This string parameter controls whether this processor will intercept and inspect Delete By Id commands in addition to adding documents. If specified, then the value will specify the name(s) of the request parameter(s) which becomes mandatory for all Delete By Id commands. Like versionField, deleteVersionParam is comma-delimited. For each of the params given, it specifies the document version associated with the delete, where the index matches versionField . For example, if versionField was set to 'a,b' and deleteVersionParam was set to 'p1,p2', p1 should give the version for field 'a' and p2 should give the version for field 'b'. If the versions specified using these params are not greater then the value in the versionField for any existing document, then the delete will fail with a 409 Version Conflict error. When using this param, Any Delete By Id command with a high enough document version number to succeed will be internally converted into an Add Document command that replaces the existing document with a new one which is empty except for the Unique Key and fields corresponding to the fields listed in versionField to keeping a record of the deleted version so future Add Document commands will fail if their ""new"" version is not high enough. ignoreOldUpdates - This boolean parameter defaults to false, but if set to true causes any update with a document version that is not great enough to be silently ignored (and return a status 200 to the client) instead of generating a 409 Version Conflict error. supportMissingVersionOnOldDocs - This boolean parameter defaults to false, but if set to true allows any documents written *before* this feature is enabled and which are missing the versionField to be overwritten. tombstoneConfig - a list of field names to values to add to the created tombstone document. In general is not a good idea to populate tombsone documents with anything other than the minimum required fields so that it doean't match queries"
update,"ignoreOldUpdates - This boolean parameter defaults to false, but if set to true causes any update with a document version that is not great enough to be silently ignored (and return a status 200 to the client) instead of generating a 409 Version Conflict error. supportMissingVersionOnOldDocs - This boolean parameter defaults to false, but if set to true allows any documents written *before* this feature is enabled and which are missing the versionField to be overwritten. tombstoneConfig - a list of field names to values to add to the created tombstone document. In general is not a good idea to populate tombsone documents with anything other than the minimum required fields so that it doean't match queries"
update,"supportMissingVersionOnOldDocs - This boolean parameter defaults to false, but if set to true allows any documents written *before* this feature is enabled and which are missing the versionField to be overwritten. tombstoneConfig - a list of field names to values to add to the created tombstone document. In general is not a good idea to populate tombsone documents with anything other than the minimum required fields so that it doean't match queries"
update,tombstoneConfig - a list of field names to values to add to the created tombstone document. In general is not a good idea to populate tombsone documents with anything other than the minimum required fields so that it doean't match queries
update,"When documents are added through this processor, if a document with the same unique key already exists in the collection, then the values within the fields as specified by the comma-delimited versionField property are checked, and if in the existing document the values for all fields are not less than the field values in the new document, then the new document is rejected with a 409 Version Conflict error. In addition to the mandatory versionField init param, two additional optional init params affect the behavior of this factory: deleteVersionParam - This string parameter controls whether this processor will intercept and inspect Delete By Id commands in addition to adding documents. If specified, then the value will specify the name(s) of the request parameter(s) which becomes mandatory for all Delete By Id commands. Like versionField, deleteVersionParam is comma-delimited. For each of the params given, it specifies the document version associated with the delete, where the index matches versionField . For example, if versionField was set to 'a,b' and deleteVersionParam was set to 'p1,p2', p1 should give the version for field 'a' and p2 should give the version for field 'b'. If the versions specified using these params are not greater then the value in the versionField for any existing document, then the delete will fail with a 409 Version Conflict error. When using this param, Any Delete By Id command with a high enough document version number to succeed will be internally converted into an Add Document command that replaces the existing document with a new one which is empty except for the Unique Key and fields corresponding to the fields listed in versionField to keeping a record of the deleted version so future Add Document commands will fail if their ""new"" version is not high enough. ignoreOldUpdates - This boolean parameter defaults to false, but if set to true causes any update with a document version that is not great enough to be silently ignored (and return a status 200 to the client) instead of generating a 409 Version Conflict error. supportMissingVersionOnOldDocs - This boolean parameter defaults to false, but if set to true allows any documents written *before* this feature is enabled and which are missing the versionField to be overwritten. tombstoneConfig - a list of field names to values to add to the created tombstone document. In general is not a good idea to populate tombsone documents with anything other than the minimum required fields so that it doean't match queries"
update,"In addition to the mandatory versionField init param, two additional optional init params affect the behavior of this factory: deleteVersionParam - This string parameter controls whether this processor will intercept and inspect Delete By Id commands in addition to adding documents. If specified, then the value will specify the name(s) of the request parameter(s) which becomes mandatory for all Delete By Id commands. Like versionField, deleteVersionParam is comma-delimited. For each of the params given, it specifies the document version associated with the delete, where the index matches versionField . For example, if versionField was set to 'a,b' and deleteVersionParam was set to 'p1,p2', p1 should give the version for field 'a' and p2 should give the version for field 'b'. If the versions specified using these params are not greater then the value in the versionField for any existing document, then the delete will fail with a 409 Version Conflict error. When using this param, Any Delete By Id command with a high enough document version number to succeed will be internally converted into an Add Document command that replaces the existing document with a new one which is empty except for the Unique Key and fields corresponding to the fields listed in versionField to keeping a record of the deleted version so future Add Document commands will fail if their ""new"" version is not high enough. ignoreOldUpdates - This boolean parameter defaults to false, but if set to true causes any update with a document version that is not great enough to be silently ignored (and return a status 200 to the client) instead of generating a 409 Version Conflict error. supportMissingVersionOnOldDocs - This boolean parameter defaults to false, but if set to true allows any documents written *before* this feature is enabled and which are missing the versionField to be overwritten. tombstoneConfig - a list of field names to values to add to the created tombstone document. In general is not a good idea to populate tombsone documents with anything other than the minimum required fields so that it doean't match queries"
update,"deleteVersionParam - This string parameter controls whether this processor will intercept and inspect Delete By Id commands in addition to adding documents. If specified, then the value will specify the name(s) of the request parameter(s) which becomes mandatory for all Delete By Id commands. Like versionField, deleteVersionParam is comma-delimited. For each of the params given, it specifies the document version associated with the delete, where the index matches versionField . For example, if versionField was set to 'a,b' and deleteVersionParam was set to 'p1,p2', p1 should give the version for field 'a' and p2 should give the version for field 'b'. If the versions specified using these params are not greater then the value in the versionField for any existing document, then the delete will fail with a 409 Version Conflict error. When using this param, Any Delete By Id command with a high enough document version number to succeed will be internally converted into an Add Document command that replaces the existing document with a new one which is empty except for the Unique Key and fields corresponding to the fields listed in versionField to keeping a record of the deleted version so future Add Document commands will fail if their ""new"" version is not high enough. ignoreOldUpdates - This boolean parameter defaults to false, but if set to true causes any update with a document version that is not great enough to be silently ignored (and return a status 200 to the client) instead of generating a 409 Version Conflict error. supportMissingVersionOnOldDocs - This boolean parameter defaults to false, but if set to true allows any documents written *before* this feature is enabled and which are missing the versionField to be overwritten. tombstoneConfig - a list of field names to values to add to the created tombstone document. In general is not a good idea to populate tombsone documents with anything other than the minimum required fields so that it doean't match queries"
update,"ignoreOldUpdates - This boolean parameter defaults to false, but if set to true causes any update with a document version that is not great enough to be silently ignored (and return a status 200 to the client) instead of generating a 409 Version Conflict error. supportMissingVersionOnOldDocs - This boolean parameter defaults to false, but if set to true allows any documents written *before* this feature is enabled and which are missing the versionField to be overwritten. tombstoneConfig - a list of field names to values to add to the created tombstone document. In general is not a good idea to populate tombsone documents with anything other than the minimum required fields so that it doean't match queries"
update,"supportMissingVersionOnOldDocs - This boolean parameter defaults to false, but if set to true allows any documents written *before* this feature is enabled and which are missing the versionField to be overwritten. tombstoneConfig - a list of field names to values to add to the created tombstone document. In general is not a good idea to populate tombsone documents with anything other than the minimum required fields so that it doean't match queries"
update,tombstoneConfig - a list of field names to values to add to the created tombstone document. In general is not a good idea to populate tombsone documents with anything other than the minimum required fields so that it doean't match queries
update,"Computing expiration field values for documents from a ""time to live"" (TTL) Periodically delete documents from the index based on an expiration field"
update,Periodically delete documents from the index based on an expiration field
update,"expirationFieldName - The name of the expiration field to use in any operations (mandatory). ttlFieldName - Name of a field this process should look for in each document processed, defaulting to _ttl_. If the specified field name exists in a document, the document field value will be parsed as a Date Math Expression relative to NOW and the result will be added to the document using the expirationFieldName. Use <null name=""ttlFieldName""/> to disable this feature. ttlParamName - Name of an update request param this process should look for in each request when processing document additions, defaulting to _ttl_. If the specified param name exists in an update request, the param value will be parsed as a Date Math Expression relative to NOW and the result will be used as a default for any document included in that request that does not already have a value in the field specified by ttlFieldName. Use <null name=""ttlParamName""/> to disable this feature. autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative."
update,"ttlFieldName - Name of a field this process should look for in each document processed, defaulting to _ttl_. If the specified field name exists in a document, the document field value will be parsed as a Date Math Expression relative to NOW and the result will be added to the document using the expirationFieldName. Use <null name=""ttlFieldName""/> to disable this feature. ttlParamName - Name of an update request param this process should look for in each request when processing document additions, defaulting to _ttl_. If the specified param name exists in an update request, the param value will be parsed as a Date Math Expression relative to NOW and the result will be used as a default for any document included in that request that does not already have a value in the field specified by ttlFieldName. Use <null name=""ttlParamName""/> to disable this feature. autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative."
update,"ttlParamName - Name of an update request param this process should look for in each request when processing document additions, defaulting to _ttl_. If the specified param name exists in an update request, the param value will be parsed as a Date Math Expression relative to NOW and the result will be used as a default for any document included in that request that does not already have a value in the field specified by ttlFieldName. Use <null name=""ttlParamName""/> to disable this feature. autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative."
update,"autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative."
update,"autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative."
update,"The DocExpirationUpdateProcessorFactory provides two features related to the ""expiration"" of documents which can be used individually, or in combination: Computing expiration field values for documents from a ""time to live"" (TTL) Periodically delete documents from the index based on an expiration field Documents with expiration field values computed from a TTL can be be excluded from searchers using simple date based filters relative to NOW, or completely removed from the index using the periodic delete function of this factory. Alternatively, the periodic delete function of this factory can be used to remove any document with an expiration value - even if that expiration was explicitly set with-out leveraging the TTL feature of this factory. The following configuration options are supported: expirationFieldName - The name of the expiration field to use in any operations (mandatory). ttlFieldName - Name of a field this process should look for in each document processed, defaulting to _ttl_. If the specified field name exists in a document, the document field value will be parsed as a Date Math Expression relative to NOW and the result will be added to the document using the expirationFieldName. Use <null name=""ttlFieldName""/> to disable this feature. ttlParamName - Name of an update request param this process should look for in each request when processing document additions, defaulting to _ttl_. If the specified param name exists in an update request, the param value will be parsed as a Date Math Expression relative to NOW and the result will be used as a default for any document included in that request that does not already have a value in the field specified by ttlFieldName. Use <null name=""ttlParamName""/> to disable this feature. autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative. For example: The configuration below will cause any document with a field named _ttl_ to have a Date field named _expire_at_ computed for it when added -- but no automatic deletion will happen. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <str name=""expirationFieldName"">_expire_at_</str> </processor> Alternatively, in this configuration deletes will occur automatically against the _expire_at_ field every 5 minutes - but this processor will not automatically populate the _expire_at_ using any sort of TTL expression. Only documents that were added with an explicit _expire_at_ field value will ever be deleted. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <null name=""ttlFieldName""/> <null name=""ttlParamName""/> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""expirationFieldName"">_expire_at_</str> </processor> This last example shows the combination of both features using a custom ttlFieldName : Documents with a my_ttl field will have an _expire_at_ field computed, and deletes will be triggered every 5 minutes to remove documents whose _expire_at_ field value is in the past. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""ttlFieldName"">my_ttl</str> <null name=""ttlParamName""/> <str name=""expirationFieldName"">_expire_at_</str> </processor>"
update,"Documents with expiration field values computed from a TTL can be be excluded from searchers using simple date based filters relative to NOW, or completely removed from the index using the periodic delete function of this factory. Alternatively, the periodic delete function of this factory can be used to remove any document with an expiration value - even if that expiration was explicitly set with-out leveraging the TTL feature of this factory. The following configuration options are supported: expirationFieldName - The name of the expiration field to use in any operations (mandatory). ttlFieldName - Name of a field this process should look for in each document processed, defaulting to _ttl_. If the specified field name exists in a document, the document field value will be parsed as a Date Math Expression relative to NOW and the result will be added to the document using the expirationFieldName. Use <null name=""ttlFieldName""/> to disable this feature. ttlParamName - Name of an update request param this process should look for in each request when processing document additions, defaulting to _ttl_. If the specified param name exists in an update request, the param value will be parsed as a Date Math Expression relative to NOW and the result will be used as a default for any document included in that request that does not already have a value in the field specified by ttlFieldName. Use <null name=""ttlParamName""/> to disable this feature. autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative. For example: The configuration below will cause any document with a field named _ttl_ to have a Date field named _expire_at_ computed for it when added -- but no automatic deletion will happen. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <str name=""expirationFieldName"">_expire_at_</str> </processor> Alternatively, in this configuration deletes will occur automatically against the _expire_at_ field every 5 minutes - but this processor will not automatically populate the _expire_at_ using any sort of TTL expression. Only documents that were added with an explicit _expire_at_ field value will ever be deleted. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <null name=""ttlFieldName""/> <null name=""ttlParamName""/> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""expirationFieldName"">_expire_at_</str> </processor> This last example shows the combination of both features using a custom ttlFieldName : Documents with a my_ttl field will have an _expire_at_ field computed, and deletes will be triggered every 5 minutes to remove documents whose _expire_at_ field value is in the past. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""ttlFieldName"">my_ttl</str> <null name=""ttlParamName""/> <str name=""expirationFieldName"">_expire_at_</str> </processor>"
update,"The following configuration options are supported: expirationFieldName - The name of the expiration field to use in any operations (mandatory). ttlFieldName - Name of a field this process should look for in each document processed, defaulting to _ttl_. If the specified field name exists in a document, the document field value will be parsed as a Date Math Expression relative to NOW and the result will be added to the document using the expirationFieldName. Use <null name=""ttlFieldName""/> to disable this feature. ttlParamName - Name of an update request param this process should look for in each request when processing document additions, defaulting to _ttl_. If the specified param name exists in an update request, the param value will be parsed as a Date Math Expression relative to NOW and the result will be used as a default for any document included in that request that does not already have a value in the field specified by ttlFieldName. Use <null name=""ttlParamName""/> to disable this feature. autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative. For example: The configuration below will cause any document with a field named _ttl_ to have a Date field named _expire_at_ computed for it when added -- but no automatic deletion will happen. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <str name=""expirationFieldName"">_expire_at_</str> </processor> Alternatively, in this configuration deletes will occur automatically against the _expire_at_ field every 5 minutes - but this processor will not automatically populate the _expire_at_ using any sort of TTL expression. Only documents that were added with an explicit _expire_at_ field value will ever be deleted. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <null name=""ttlFieldName""/> <null name=""ttlParamName""/> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""expirationFieldName"">_expire_at_</str> </processor> This last example shows the combination of both features using a custom ttlFieldName : Documents with a my_ttl field will have an _expire_at_ field computed, and deletes will be triggered every 5 minutes to remove documents whose _expire_at_ field value is in the past. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""ttlFieldName"">my_ttl</str> <null name=""ttlParamName""/> <str name=""expirationFieldName"">_expire_at_</str> </processor>"
update,"expirationFieldName - The name of the expiration field to use in any operations (mandatory). ttlFieldName - Name of a field this process should look for in each document processed, defaulting to _ttl_. If the specified field name exists in a document, the document field value will be parsed as a Date Math Expression relative to NOW and the result will be added to the document using the expirationFieldName. Use <null name=""ttlFieldName""/> to disable this feature. ttlParamName - Name of an update request param this process should look for in each request when processing document additions, defaulting to _ttl_. If the specified param name exists in an update request, the param value will be parsed as a Date Math Expression relative to NOW and the result will be used as a default for any document included in that request that does not already have a value in the field specified by ttlFieldName. Use <null name=""ttlParamName""/> to disable this feature. autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative."
update,"ttlFieldName - Name of a field this process should look for in each document processed, defaulting to _ttl_. If the specified field name exists in a document, the document field value will be parsed as a Date Math Expression relative to NOW and the result will be added to the document using the expirationFieldName. Use <null name=""ttlFieldName""/> to disable this feature. ttlParamName - Name of an update request param this process should look for in each request when processing document additions, defaulting to _ttl_. If the specified param name exists in an update request, the param value will be parsed as a Date Math Expression relative to NOW and the result will be used as a default for any document included in that request that does not already have a value in the field specified by ttlFieldName. Use <null name=""ttlParamName""/> to disable this feature. autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative."
update,"ttlParamName - Name of an update request param this process should look for in each request when processing document additions, defaulting to _ttl_. If the specified param name exists in an update request, the param value will be parsed as a Date Math Expression relative to NOW and the result will be used as a default for any document included in that request that does not already have a value in the field specified by ttlFieldName. Use <null name=""ttlParamName""/> to disable this feature. autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative."
update,"autoDeletePeriodSeconds - Optional numeric value indicating how often this factory should trigger a delete to remove documents. If this option is used, and specifies a non-negative numeric value, a background thread will be created that will execute recurring deleteByQuery commands using the specified period. The delete query will remove all documents with an expirationFieldName up to NOW. autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative."
update,"autoDeleteChainName - Optional name of an updateRequestProcessorChain to use when executing automatic deletes. If not specified, or <null/> , the default updateRequestProcessorChain for this collection is used. This option is ignored unless autoDeletePeriodSeconds is configured and is non-negative."
update,"For example: The configuration below will cause any document with a field named _ttl_ to have a Date field named _expire_at_ computed for it when added -- but no automatic deletion will happen. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <str name=""expirationFieldName"">_expire_at_</str> </processor> Alternatively, in this configuration deletes will occur automatically against the _expire_at_ field every 5 minutes - but this processor will not automatically populate the _expire_at_ using any sort of TTL expression. Only documents that were added with an explicit _expire_at_ field value will ever be deleted. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <null name=""ttlFieldName""/> <null name=""ttlParamName""/> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""expirationFieldName"">_expire_at_</str> </processor> This last example shows the combination of both features using a custom ttlFieldName : Documents with a my_ttl field will have an _expire_at_ field computed, and deletes will be triggered every 5 minutes to remove documents whose _expire_at_ field value is in the past. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""ttlFieldName"">my_ttl</str> <null name=""ttlParamName""/> <str name=""expirationFieldName"">_expire_at_</str> </processor>"
update,"Alternatively, in this configuration deletes will occur automatically against the _expire_at_ field every 5 minutes - but this processor will not automatically populate the _expire_at_ using any sort of TTL expression. Only documents that were added with an explicit _expire_at_ field value will ever be deleted. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <null name=""ttlFieldName""/> <null name=""ttlParamName""/> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""expirationFieldName"">_expire_at_</str> </processor> This last example shows the combination of both features using a custom ttlFieldName : Documents with a my_ttl field will have an _expire_at_ field computed, and deletes will be triggered every 5 minutes to remove documents whose _expire_at_ field value is in the past. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""ttlFieldName"">my_ttl</str> <null name=""ttlParamName""/> <str name=""expirationFieldName"">_expire_at_</str> </processor>"
update,"This last example shows the combination of both features using a custom ttlFieldName : Documents with a my_ttl field will have an _expire_at_ field computed, and deletes will be triggered every 5 minutes to remove documents whose _expire_at_ field value is in the past. <processor class=""solr.processor.DocExpirationUpdateProcessorFactory""> <int name=""autoDeletePeriodSeconds"">300</int> <str name=""ttlFieldName"">my_ttl</str> <null name=""ttlParamName""/> <str name=""expirationFieldName"">_expire_at_</str> </processor>"
update,"By default, this processor matches no fields. For example, with the configuration listed below any documents containing String values (such as ""abcdef"" or ""xyz"") in a field declared in the schema using IntPointField or LongPointField would have those Strings replaced with the length of those fields as an Integer (ie: 6 and 3 respectively) <processor class=""solr.FieldLengthUpdateProcessorFactory""> <arr name=""typeClass""> <str>solr.IntPointField</str> <str>solr.LongPointField</str> </arr> </processor>"
update,"For example, with the configuration listed below any documents containing String values (such as ""abcdef"" or ""xyz"") in a field declared in the schema using IntPointField or LongPointField would have those Strings replaced with the length of those fields as an Integer (ie: 6 and 3 respectively) <processor class=""solr.FieldLengthUpdateProcessorFactory""> <arr name=""typeClass""> <str>solr.IntPointField</str> <str>solr.LongPointField</str> </arr> </processor>"
update,Subclasses should override the mutate method to specify how individual SolrInputFields identified by the selector associated with this instance will be mutated.
update,"fieldName - selecting specific fields by field name lookup fieldRegex - selecting specific fields by field name regex match (regexes are checked in the order specified) typeName - selecting specific fields by fieldType name lookup typeClass - selecting specific fields by fieldType class lookup, including inheritence and interfaces"
update,"fieldRegex - selecting specific fields by field name regex match (regexes are checked in the order specified) typeName - selecting specific fields by fieldType name lookup typeClass - selecting specific fields by fieldType class lookup, including inheritence and interfaces"
update,"typeName - selecting specific fields by fieldType name lookup typeClass - selecting specific fields by fieldType class lookup, including inheritence and interfaces"
update,"typeClass - selecting specific fields by fieldType class lookup, including inheritence and interfaces"
update,fieldNameMatchesSchemaField - selecting specific fields based on whether or not they match a schema field
update,"This class provides all of the plumbing for configuring the FieldNameSelector using the following init params to specify selection criteria... fieldName - selecting specific fields by field name lookup fieldRegex - selecting specific fields by field name regex match (regexes are checked in the order specified) typeName - selecting specific fields by fieldType name lookup typeClass - selecting specific fields by fieldType class lookup, including inheritence and interfaces Each criteria can specified as either an <arr> of <str>, or multiple <str> with the same name. When multiple criteria of a single type exist, fields must match at least one to be selected. If more then one type of criteria exist, fields must match at least one of each to be selected. The following additional selector may be specified as a <bool> - when specified as false, only fields that do not match a schema field/dynamic field are selected; when specified as true, only fields that do match a schema field/dynamic field are selected: fieldNameMatchesSchemaField - selecting specific fields based on whether or not they match a schema field One or more excludes <lst> params may also be specified, containing any of the above criteria, identifying fields to be excluded from seelction even if they match the selection criteria. As with the main selection critiera a field must match all of criteria in a single exclusion in order to be excluded, but multiple exclusions may be specified to get an OR behavior In the ExampleFieldMutatingUpdateProcessorFactory configured below, fields will be mutated if the name starts with ""foo"" or ""bar""; unless the field name contains the substring ""SKIP"" or the fieldType is (or subclasses) DatePointField. Meaning a field named ""foo_SKIP"" is guaranteed not to be selected, but a field named ""bar_smith"" that uses StrField will be selected. <processor class=""solr.ExampleFieldMutatingUpdateProcessorFactory""> <str name=""fieldRegex"">foo.*</str> <str name=""fieldRegex"">bar.*</str> <!-- each set of exclusions is checked independently --> <lst name=""exclude""> <str name=""fieldRegex"">.*SKIP.*</str> </lst> <lst name=""exclude""> <str name=""typeClass"">solr.DatePointField</str> </lst> </processor> Subclasses define the default selection behavior to be applied if no criteria is configured by the user. User configured ""exclude"" criteria will be applied to the subclass defined default selector."
update,"fieldName - selecting specific fields by field name lookup fieldRegex - selecting specific fields by field name regex match (regexes are checked in the order specified) typeName - selecting specific fields by fieldType name lookup typeClass - selecting specific fields by fieldType class lookup, including inheritence and interfaces"
update,"fieldRegex - selecting specific fields by field name regex match (regexes are checked in the order specified) typeName - selecting specific fields by fieldType name lookup typeClass - selecting specific fields by fieldType class lookup, including inheritence and interfaces"
update,"typeName - selecting specific fields by fieldType name lookup typeClass - selecting specific fields by fieldType class lookup, including inheritence and interfaces"
update,"typeClass - selecting specific fields by fieldType class lookup, including inheritence and interfaces"
update,"Each criteria can specified as either an <arr> of <str>, or multiple <str> with the same name. When multiple criteria of a single type exist, fields must match at least one to be selected. If more then one type of criteria exist, fields must match at least one of each to be selected. The following additional selector may be specified as a <bool> - when specified as false, only fields that do not match a schema field/dynamic field are selected; when specified as true, only fields that do match a schema field/dynamic field are selected: fieldNameMatchesSchemaField - selecting specific fields based on whether or not they match a schema field One or more excludes <lst> params may also be specified, containing any of the above criteria, identifying fields to be excluded from seelction even if they match the selection criteria. As with the main selection critiera a field must match all of criteria in a single exclusion in order to be excluded, but multiple exclusions may be specified to get an OR behavior In the ExampleFieldMutatingUpdateProcessorFactory configured below, fields will be mutated if the name starts with ""foo"" or ""bar""; unless the field name contains the substring ""SKIP"" or the fieldType is (or subclasses) DatePointField. Meaning a field named ""foo_SKIP"" is guaranteed not to be selected, but a field named ""bar_smith"" that uses StrField will be selected. <processor class=""solr.ExampleFieldMutatingUpdateProcessorFactory""> <str name=""fieldRegex"">foo.*</str> <str name=""fieldRegex"">bar.*</str> <!-- each set of exclusions is checked independently --> <lst name=""exclude""> <str name=""fieldRegex"">.*SKIP.*</str> </lst> <lst name=""exclude""> <str name=""typeClass"">solr.DatePointField</str> </lst> </processor> Subclasses define the default selection behavior to be applied if no criteria is configured by the user. User configured ""exclude"" criteria will be applied to the subclass defined default selector."
update,"The following additional selector may be specified as a <bool> - when specified as false, only fields that do not match a schema field/dynamic field are selected; when specified as true, only fields that do match a schema field/dynamic field are selected: fieldNameMatchesSchemaField - selecting specific fields based on whether or not they match a schema field One or more excludes <lst> params may also be specified, containing any of the above criteria, identifying fields to be excluded from seelction even if they match the selection criteria. As with the main selection critiera a field must match all of criteria in a single exclusion in order to be excluded, but multiple exclusions may be specified to get an OR behavior In the ExampleFieldMutatingUpdateProcessorFactory configured below, fields will be mutated if the name starts with ""foo"" or ""bar""; unless the field name contains the substring ""SKIP"" or the fieldType is (or subclasses) DatePointField. Meaning a field named ""foo_SKIP"" is guaranteed not to be selected, but a field named ""bar_smith"" that uses StrField will be selected. <processor class=""solr.ExampleFieldMutatingUpdateProcessorFactory""> <str name=""fieldRegex"">foo.*</str> <str name=""fieldRegex"">bar.*</str> <!-- each set of exclusions is checked independently --> <lst name=""exclude""> <str name=""fieldRegex"">.*SKIP.*</str> </lst> <lst name=""exclude""> <str name=""typeClass"">solr.DatePointField</str> </lst> </processor> Subclasses define the default selection behavior to be applied if no criteria is configured by the user. User configured ""exclude"" criteria will be applied to the subclass defined default selector."
update,fieldNameMatchesSchemaField - selecting specific fields based on whether or not they match a schema field
update,"One or more excludes <lst> params may also be specified, containing any of the above criteria, identifying fields to be excluded from seelction even if they match the selection criteria. As with the main selection critiera a field must match all of criteria in a single exclusion in order to be excluded, but multiple exclusions may be specified to get an OR behavior In the ExampleFieldMutatingUpdateProcessorFactory configured below, fields will be mutated if the name starts with ""foo"" or ""bar""; unless the field name contains the substring ""SKIP"" or the fieldType is (or subclasses) DatePointField. Meaning a field named ""foo_SKIP"" is guaranteed not to be selected, but a field named ""bar_smith"" that uses StrField will be selected. <processor class=""solr.ExampleFieldMutatingUpdateProcessorFactory""> <str name=""fieldRegex"">foo.*</str> <str name=""fieldRegex"">bar.*</str> <!-- each set of exclusions is checked independently --> <lst name=""exclude""> <str name=""fieldRegex"">.*SKIP.*</str> </lst> <lst name=""exclude""> <str name=""typeClass"">solr.DatePointField</str> </lst> </processor> Subclasses define the default selection behavior to be applied if no criteria is configured by the user. User configured ""exclude"" criteria will be applied to the subclass defined default selector."
update,"In the ExampleFieldMutatingUpdateProcessorFactory configured below, fields will be mutated if the name starts with ""foo"" or ""bar""; unless the field name contains the substring ""SKIP"" or the fieldType is (or subclasses) DatePointField. Meaning a field named ""foo_SKIP"" is guaranteed not to be selected, but a field named ""bar_smith"" that uses StrField will be selected. <processor class=""solr.ExampleFieldMutatingUpdateProcessorFactory""> <str name=""fieldRegex"">foo.*</str> <str name=""fieldRegex"">bar.*</str> <!-- each set of exclusions is checked independently --> <lst name=""exclude""> <str name=""fieldRegex"">.*SKIP.*</str> </lst> <lst name=""exclude""> <str name=""typeClass"">solr.DatePointField</str> </lst> </processor> Subclasses define the default selection behavior to be applied if no criteria is configured by the user. User configured ""exclude"" criteria will be applied to the subclass defined default selector."
update,"Subclasses define the default selection behavior to be applied if no criteria is configured by the user. User configured ""exclude"" criteria will be applied to the subclass defined default selector."
update,Enclosing class: FieldMutatingUpdateProcessorFactory public static final class FieldMutatingUpdateProcessorFactory.SelectorParams extends Object
update,"By default, this processor matches no fields. For example, in the configuration below, if a field named primary_author contained multiple values (ie: ""Adam Doe"", ""Bob Smith"", ""Carla Jones"") then only the first value (ie: ""Adam Doe"") will be kept <processor class=""solr.FirstFieldValueUpdateProcessorFactory""> <str name=""fieldName"">primary_author</str> </processor>"
update,"For example, in the configuration below, if a field named primary_author contained multiple values (ie: ""Adam Doe"", ""Bob Smith"", ""Carla Jones"") then only the first value (ie: ""Adam Doe"") will be kept <processor class=""solr.FirstFieldValueUpdateProcessorFactory""> <str name=""fieldName"">primary_author</str> </processor>"
update,"By default this processor matches no fields For example, with the configuration listed below any documents containing HTML markup in any field declared in the schema using StrField will have that HTML striped away. <processor class=""solr.HTMLStripFieldUpdateProcessorFactory""> <str name=""typeClass"">solr.StrField</str> </processor>"
update,"For example, with the configuration listed below any documents containing HTML markup in any field declared in the schema using StrField will have that HTML striped away. <processor class=""solr.HTMLStripFieldUpdateProcessorFactory""> <str name=""typeClass"">solr.StrField</str> </processor>"
update,"By default, this processor ignores any field name which does not exist according to the schema For example, in the configuration below, any field name which would cause an error because it does not exist, or match a dynamicField, in the schema.xml would be silently removed from any added documents... <processor class=""solr.IgnoreFieldUpdateProcessorFactory"" /> In this second example, any field name ending in ""_raw"" found in a document being added would be removed... <processor class=""solr.IgnoreFieldUpdateProcessorFactory""> <str name=""fieldRegex"">.*_raw</str> </processor>"
update,"For example, in the configuration below, any field name which would cause an error because it does not exist, or match a dynamicField, in the schema.xml would be silently removed from any added documents... <processor class=""solr.IgnoreFieldUpdateProcessorFactory"" /> In this second example, any field name ending in ""_raw"" found in a document being added would be removed... <processor class=""solr.IgnoreFieldUpdateProcessorFactory""> <str name=""fieldRegex"">.*_raw</str> </processor>"
update,"In this second example, any field name ending in ""_raw"" found in a document being added would be removed... <processor class=""solr.IgnoreFieldUpdateProcessorFactory""> <str name=""fieldRegex"">.*_raw</str> </processor>"
update,"By default, this processor matches no fields. For example, in the configuration below, if a field named primary_author contained multiple values (ie: ""Adam Doe"", ""Bob Smith"", ""Carla Jones"") then only the last value (ie: ""Carla Jones"") will be kept <processor class=""solr.LastFieldValueUpdateProcessorFactory""> <str name=""fieldName"">primary_author</str> </processor>"
update,"For example, in the configuration below, if a field named primary_author contained multiple values (ie: ""Adam Doe"", ""Bob Smith"", ""Carla Jones"") then only the last value (ie: ""Carla Jones"") will be kept <processor class=""solr.LastFieldValueUpdateProcessorFactory""> <str name=""fieldName"">primary_author</str> </processor>"
update,If the Log level is not >= INFO the processor will not be created or added to the chain.
update,public class Lookup3Signature extends Signature
update,"By default, this processor matches no fields. In the example configuration below, if a document contains multiple integer values (ie: 64, 128, 1024) in the field largestFileSize then only the biggest value (ie: 1024) will be kept in that field. <processor class=""solr.MaxFieldValueUpdateProcessorFactory""> <str name=""fieldName"">largestFileSize</str> </processor>"
update,"In the example configuration below, if a document contains multiple integer values (ie: 64, 128, 1024) in the field largestFileSize then only the biggest value (ie: 1024) will be kept in that field. <processor class=""solr.MaxFieldValueUpdateProcessorFactory""> <str name=""fieldName"">largestFileSize</str> </processor>"
update,Direct Known Subclasses: TextProfileSignature public class MD5Signature extends Signature
update,"By default, this processor matches no fields. In the example configuration below, if a document contains multiple integer values (ie: 64, 128, 1024) in the field smallestFileSize then only the smallest value (ie: 64) will be kept in that field. <processor class=""solr.MinFieldValueUpdateProcessorFactory""> <str name=""fieldName"">smallestFileSize</str> </processor>"
update,"In the example configuration below, if a document contains multiple integer values (ie: 64, 128, 1024) in the field smallestFileSize then only the smallest value (ie: 64) will be kept in that field. <processor class=""solr.MinFieldValueUpdateProcessorFactory""> <str name=""fieldName"">smallestFileSize</str> </processor>"
update,This implementation may be useful for Solr installations in which neither the DistributedUpdateProcessorFactory nor any custom implementation of DistributingUpdateProcessorFactory is desired (ie: shards are managed externally from Solr)
update,"maxFields - (required) The maximum number of fields before update requests should be aborted. Once this limit has been exceeded, additional update requests will fail until fields have been removed or the ""maxFields"" is increased. warnOnly - (optional) If true then the URP logs verbose warnings about the limit being exceeded but doesn't abort update requests. Defaults to false if not specified"
update,warnOnly - (optional) If true then the URP logs verbose warnings about the limit being exceeded but doesn't abort update requests. Defaults to false if not specified
update,"The URP uses the core's SolrIndexSearcher to judge the current number of fields. Accordingly, it undercounts the number of fields in the core - missing all fields added since the previous searcher was opened. As such, the URP's request-blocking is ""best effort"" - it cannot be relied on as a precise limit on the number of fields. Additionally, the field-counting includes all documents present in the index, including any deleted docs that haven't yet been purged via segment merging. Note that this can differ significantly from the number of fields defined in managed-schema.xml - especially when dynamic fields are enabled. The only way to reduce this field count is to delete documents and wait until the deleted documents have been removed by segment merges. Users may of course speed up this process by tweaking Solr's segment-merging, triggering an ""optimize"" operation, etc. NumFieldLimitingUpdateRequestProcessorFactory accepts two configuration parameters: maxFields - (required) The maximum number of fields before update requests should be aborted. Once this limit has been exceeded, additional update requests will fail until fields have been removed or the ""maxFields"" is increased. warnOnly - (optional) If true then the URP logs verbose warnings about the limit being exceeded but doesn't abort update requests. Defaults to false if not specified"
update,"Additionally, the field-counting includes all documents present in the index, including any deleted docs that haven't yet been purged via segment merging. Note that this can differ significantly from the number of fields defined in managed-schema.xml - especially when dynamic fields are enabled. The only way to reduce this field count is to delete documents and wait until the deleted documents have been removed by segment merges. Users may of course speed up this process by tweaking Solr's segment-merging, triggering an ""optimize"" operation, etc. NumFieldLimitingUpdateRequestProcessorFactory accepts two configuration parameters: maxFields - (required) The maximum number of fields before update requests should be aborted. Once this limit has been exceeded, additional update requests will fail until fields have been removed or the ""maxFields"" is increased. warnOnly - (optional) If true then the URP logs verbose warnings about the limit being exceeded but doesn't abort update requests. Defaults to false if not specified"
update,"NumFieldLimitingUpdateRequestProcessorFactory accepts two configuration parameters: maxFields - (required) The maximum number of fields before update requests should be aborted. Once this limit has been exceeded, additional update requests will fail until fields have been removed or the ""maxFields"" is increased. warnOnly - (optional) If true then the URP logs verbose warnings about the limit being exceeded but doesn't abort update requests. Defaults to false if not specified"
update,"maxFields - (required) The maximum number of fields before update requests should be aborted. Once this limit has been exceeded, additional update requests will fail until fields have been removed or the ""maxFields"" is increased. warnOnly - (optional) If true then the URP logs verbose warnings about the limit being exceeded but doesn't abort update requests. Defaults to false if not specified"
update,warnOnly - (optional) If true then the URP logs verbose warnings about the limit being exceeded but doesn't abort update requests. Defaults to false if not specified
update,"The default selection behavior is to mutate both those fields that don't match a schema field, as well as those fields that do match a schema field and have a field type that uses class solr.BooleanField. If all values are parseable as boolean (or are already Boolean), then the field will be mutated, replacing each value with its parsed Boolean equivalent; otherwise, no mutation will occur. The default true and false values are ""true"" and ""false"", respectively, and match case-insensitively. The following configuration changes the acceptable values, and requires a case-sensitive match - note that either individual <str> elements or <arr>-s of <str> elements may be used to specify the trueValue-s and falseValue-s: <processor class=""solr.ParseBooleanFieldUpdateProcessorFactory""> <str name=""caseSensitive"">true</str> <str name=""trueValue"">True</str> <str name=""trueValue"">Yes</str> <arr name=""falseValue""> <str>False</str> <str>No</str> </arr> </processor>"
update,"If all values are parseable as boolean (or are already Boolean), then the field will be mutated, replacing each value with its parsed Boolean equivalent; otherwise, no mutation will occur. The default true and false values are ""true"" and ""false"", respectively, and match case-insensitively. The following configuration changes the acceptable values, and requires a case-sensitive match - note that either individual <str> elements or <arr>-s of <str> elements may be used to specify the trueValue-s and falseValue-s: <processor class=""solr.ParseBooleanFieldUpdateProcessorFactory""> <str name=""caseSensitive"">true</str> <str name=""trueValue"">True</str> <str name=""trueValue"">Yes</str> <arr name=""falseValue""> <str>False</str> <str>No</str> </arr> </processor>"
update,"The default true and false values are ""true"" and ""false"", respectively, and match case-insensitively. The following configuration changes the acceptable values, and requires a case-sensitive match - note that either individual <str> elements or <arr>-s of <str> elements may be used to specify the trueValue-s and falseValue-s: <processor class=""solr.ParseBooleanFieldUpdateProcessorFactory""> <str name=""caseSensitive"">true</str> <str name=""trueValue"">True</str> <str name=""trueValue"">Yes</str> <arr name=""falseValue""> <str>False</str> <str>No</str> </arr> </processor>"
update,"The default selection behavior is to mutate both those fields that don't match a schema field, as well as those fields that match a schema field with a date field type. If all values are parseable as dates (or are already Date), then the field will be mutated, replacing each value with its parsed Date equivalent; otherwise, no mutation will occur. One or more date ""format"" specifiers must be specified. See Java 8's DateTimeFormatter javadocs for a description of format strings. Note that ""lenient"" and case insensitivity is enabled. Furthermore, inputs surrounded in single quotes will be removed if found. A default time zone name or offset may optionally be specified for those dates that don't include an explicit zone/offset. NOTE: three-letter zone designations like ""EST"" are not parseable (with the single exception of ""UTC""), because they are ambiguous. If no default time zone is specified, UTC will be used. See Wikipedia's list of TZ database time zone names. The locale to use when parsing field values using the specified formats may optionally be specified. If no locale is configured, then en_US will be used since it's implied by some well-known formats. Recent versions of Java have become sensitive to this. The following configuration specifies the French/France locale and two date formats that will parse the strings ""le mardi 8 janvier 2013"" and ""le 28 dc. 2010 15 h 30"", respectively. Note that either individual <str> elements or <arr>-s of <str> elements may be used to specify the date format(s): <processor class=""solr.ParseDateFieldUpdateProcessorFactory""> <str name=""defaultTimeZone"">Europe/Paris</str> <str name=""locale"">fr_FR</str> <arr name=""format""> <str>'le' EEEE dd MMMM yyyy</str> <str>'le' dd MMM. yyyy '' HH 'h' mm</str> </arr> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s). Tip: you can use multiple instances of this URP in your chain with different locales or default time zones if you wish to vary those settings for different format patterns."
update,"If all values are parseable as dates (or are already Date), then the field will be mutated, replacing each value with its parsed Date equivalent; otherwise, no mutation will occur. One or more date ""format"" specifiers must be specified. See Java 8's DateTimeFormatter javadocs for a description of format strings. Note that ""lenient"" and case insensitivity is enabled. Furthermore, inputs surrounded in single quotes will be removed if found. A default time zone name or offset may optionally be specified for those dates that don't include an explicit zone/offset. NOTE: three-letter zone designations like ""EST"" are not parseable (with the single exception of ""UTC""), because they are ambiguous. If no default time zone is specified, UTC will be used. See Wikipedia's list of TZ database time zone names. The locale to use when parsing field values using the specified formats may optionally be specified. If no locale is configured, then en_US will be used since it's implied by some well-known formats. Recent versions of Java have become sensitive to this. The following configuration specifies the French/France locale and two date formats that will parse the strings ""le mardi 8 janvier 2013"" and ""le 28 dc. 2010 15 h 30"", respectively. Note that either individual <str> elements or <arr>-s of <str> elements may be used to specify the date format(s): <processor class=""solr.ParseDateFieldUpdateProcessorFactory""> <str name=""defaultTimeZone"">Europe/Paris</str> <str name=""locale"">fr_FR</str> <arr name=""format""> <str>'le' EEEE dd MMMM yyyy</str> <str>'le' dd MMM. yyyy '' HH 'h' mm</str> </arr> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s). Tip: you can use multiple instances of this URP in your chain with different locales or default time zones if you wish to vary those settings for different format patterns."
update,"One or more date ""format"" specifiers must be specified. See Java 8's DateTimeFormatter javadocs for a description of format strings. Note that ""lenient"" and case insensitivity is enabled. Furthermore, inputs surrounded in single quotes will be removed if found. A default time zone name or offset may optionally be specified for those dates that don't include an explicit zone/offset. NOTE: three-letter zone designations like ""EST"" are not parseable (with the single exception of ""UTC""), because they are ambiguous. If no default time zone is specified, UTC will be used. See Wikipedia's list of TZ database time zone names. The locale to use when parsing field values using the specified formats may optionally be specified. If no locale is configured, then en_US will be used since it's implied by some well-known formats. Recent versions of Java have become sensitive to this. The following configuration specifies the French/France locale and two date formats that will parse the strings ""le mardi 8 janvier 2013"" and ""le 28 dc. 2010 15 h 30"", respectively. Note that either individual <str> elements or <arr>-s of <str> elements may be used to specify the date format(s): <processor class=""solr.ParseDateFieldUpdateProcessorFactory""> <str name=""defaultTimeZone"">Europe/Paris</str> <str name=""locale"">fr_FR</str> <arr name=""format""> <str>'le' EEEE dd MMMM yyyy</str> <str>'le' dd MMM. yyyy '' HH 'h' mm</str> </arr> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s). Tip: you can use multiple instances of this URP in your chain with different locales or default time zones if you wish to vary those settings for different format patterns."
update,"A default time zone name or offset may optionally be specified for those dates that don't include an explicit zone/offset. NOTE: three-letter zone designations like ""EST"" are not parseable (with the single exception of ""UTC""), because they are ambiguous. If no default time zone is specified, UTC will be used. See Wikipedia's list of TZ database time zone names. The locale to use when parsing field values using the specified formats may optionally be specified. If no locale is configured, then en_US will be used since it's implied by some well-known formats. Recent versions of Java have become sensitive to this. The following configuration specifies the French/France locale and two date formats that will parse the strings ""le mardi 8 janvier 2013"" and ""le 28 dc. 2010 15 h 30"", respectively. Note that either individual <str> elements or <arr>-s of <str> elements may be used to specify the date format(s): <processor class=""solr.ParseDateFieldUpdateProcessorFactory""> <str name=""defaultTimeZone"">Europe/Paris</str> <str name=""locale"">fr_FR</str> <arr name=""format""> <str>'le' EEEE dd MMMM yyyy</str> <str>'le' dd MMM. yyyy '' HH 'h' mm</str> </arr> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s). Tip: you can use multiple instances of this URP in your chain with different locales or default time zones if you wish to vary those settings for different format patterns."
update,"The locale to use when parsing field values using the specified formats may optionally be specified. If no locale is configured, then en_US will be used since it's implied by some well-known formats. Recent versions of Java have become sensitive to this. The following configuration specifies the French/France locale and two date formats that will parse the strings ""le mardi 8 janvier 2013"" and ""le 28 dc. 2010 15 h 30"", respectively. Note that either individual <str> elements or <arr>-s of <str> elements may be used to specify the date format(s): <processor class=""solr.ParseDateFieldUpdateProcessorFactory""> <str name=""defaultTimeZone"">Europe/Paris</str> <str name=""locale"">fr_FR</str> <arr name=""format""> <str>'le' EEEE dd MMMM yyyy</str> <str>'le' dd MMM. yyyy '' HH 'h' mm</str> </arr> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s). Tip: you can use multiple instances of this URP in your chain with different locales or default time zones if you wish to vary those settings for different format patterns."
update,"See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s). Tip: you can use multiple instances of this URP in your chain with different locales or default time zones if you wish to vary those settings for different format patterns."
update,Tip: you can use multiple instances of this URP in your chain with different locales or default time zones if you wish to vary those settings for different format patterns.
update,"The default selection behavior is to mutate both those fields that don't match a schema field, as well as those fields that match a schema field with a double field type. If all values are parseable as double (or are already Double), then the field will be mutated, replacing each value with its parsed Double equivalent; otherwise, no mutation will occur. The locale to use when parsing field values, which will affect the recognized grouping separator and decimal characters, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string string ""12345,899"" as double value 12345.899 (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseDoubleFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"If all values are parseable as double (or are already Double), then the field will be mutated, replacing each value with its parsed Double equivalent; otherwise, no mutation will occur. The locale to use when parsing field values, which will affect the recognized grouping separator and decimal characters, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string string ""12345,899"" as double value 12345.899 (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseDoubleFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"The locale to use when parsing field values, which will affect the recognized grouping separator and decimal characters, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string string ""12345,899"" as double value 12345.899 (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseDoubleFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"The default selection behavior is to mutate both those fields that don't match a schema field, as well as those fields that match a schema field with a float field type. If all values are parseable as float (or are already Float), then the field will be mutated, replacing each value with its parsed Float equivalent; otherwise, no mutation will occur. The locale to use when parsing field values, which will affect the recognized grouping separator and decimal characters, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string ""12345,899"" as 12345.899f (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseFloatFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"If all values are parseable as float (or are already Float), then the field will be mutated, replacing each value with its parsed Float equivalent; otherwise, no mutation will occur. The locale to use when parsing field values, which will affect the recognized grouping separator and decimal characters, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string ""12345,899"" as 12345.899f (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseFloatFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"The locale to use when parsing field values, which will affect the recognized grouping separator and decimal characters, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string ""12345,899"" as 12345.899f (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseFloatFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"The default selection behavior is to mutate both those fields that don't match a schema field, as well as those fields that match a schema field with an int field type. If all values are parseable as int (or are already Integer), then the field will be mutated, replacing each value with its parsed Integer equivalent; otherwise, no mutation will occur. The locale to use when parsing field values, which will affect the recognized grouping separator character, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string ""12345899"" as 12345899L (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseIntFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"If all values are parseable as int (or are already Integer), then the field will be mutated, replacing each value with its parsed Integer equivalent; otherwise, no mutation will occur. The locale to use when parsing field values, which will affect the recognized grouping separator character, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string ""12345899"" as 12345899L (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseIntFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"The locale to use when parsing field values, which will affect the recognized grouping separator character, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string ""12345899"" as 12345899L (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseIntFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"The default selection behavior is to mutate both those fields that don't match a schema field, as well as those fields that match a schema field with a long field type. If all values are parseable as long (or are already Long), then the field will be mutated, replacing each value with its parsed Long equivalent; otherwise, no mutation will occur. The locale to use when parsing field values, which will affect the recognized grouping separator character, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string ""12345899"" as 12345899L (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseLongFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"If all values are parseable as long (or are already Long), then the field will be mutated, replacing each value with its parsed Long equivalent; otherwise, no mutation will occur. The locale to use when parsing field values, which will affect the recognized grouping separator character, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string ""12345899"" as 12345899L (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseLongFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"The locale to use when parsing field values, which will affect the recognized grouping separator character, may optionally be specified. If no locale is configured, then Locale.ROOT will be used. The following configuration specifies the Russian/Russia locale, which will parse the string ""12345899"" as 12345899L (the grouping separator character is U+00AO NO-BREAK SPACE). <processor class=""solr.ParseLongFieldUpdateProcessorFactory""> <str name=""locale"">ru_RU</str> </processor> See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"See Locale for a description of acceptable language, country (optional) and variant (optional) values, joined with underscore(s)."
update,"Fields are specified using the same patterns as in FieldMutatingUpdateProcessorFactory. They are then checked whether they follow a pre-analyzed format defined by parser. Valid fields are then parsed. The original SchemaField is used for the initial creation of IndexableField, which is then modified to add the results from parsing (token stream value and/or string value) and then it will be directly added to the final Lucene Document to be indexed. Fields that are declared in the patterns list but are not present in the current schema will be removed from the input document. Implementation details This update processor uses PreAnalyzedField.PreAnalyzedParser to parse the original field content (interpreted as a string value), and thus obtain the stored part and the token stream part. Then it creates the ""template"" Field-s using the original SchemaField.createFields(Object) as declared in the current schema. Finally it sets the pre-analyzed parts if available (string value and the token stream value) on the first field of these ""template"" fields. If the declared field type does not support stored or indexed parts then such parts are silently discarded. Finally the updated ""template"" Field-s are added to the resulting SolrInputField, and the original value of that field is removed. Example configuration In the example configuration below there are two update chains, one that uses the ""simple"" parser (SimplePreAnalyzedParser) and one that uses the ""json"" parser (JsonPreAnalyzedParser). Field ""nonexistent"" will be removed from input documents if not present in the schema. Other fields will be analyzed and if valid they will be converted to IndexableField-s or if they are not in a valid format that can be parsed with the selected parser they will be passed as-is. Assuming that ssto field is stored but not indexed, and sind field is indexed but not stored: if ssto input value contains the indexed part then this part will be discarded and only the stored value part will be retained. Similarly, if sind input value contains the stored part then it will be discarded and only the token stream part will be retained. <updateRequestProcessorChain name=""pre-analyzed-simple""> <processor class=""solr.PreAnalyzedUpdateProcessorFactory""> <str name=""fieldName"">title</str> <str name=""fieldName"">nonexistent</str> <str name=""fieldName"">ssto</str> <str name=""fieldName"">sind</str> <str name=""parser"">simple</str> </processor> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain> <updateRequestProcessorChain name=""pre-analyzed-json""> <processor class=""solr.PreAnalyzedUpdateProcessorFactory""> <str name=""fieldName"">title</str> <str name=""fieldName"">nonexistent</str> <str name=""fieldName"">ssto</str> <str name=""fieldName"">sind</str> <str name=""parser"">json</str> </processor> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain>"
update,"Fields that are declared in the patterns list but are not present in the current schema will be removed from the input document. Implementation details This update processor uses PreAnalyzedField.PreAnalyzedParser to parse the original field content (interpreted as a string value), and thus obtain the stored part and the token stream part. Then it creates the ""template"" Field-s using the original SchemaField.createFields(Object) as declared in the current schema. Finally it sets the pre-analyzed parts if available (string value and the token stream value) on the first field of these ""template"" fields. If the declared field type does not support stored or indexed parts then such parts are silently discarded. Finally the updated ""template"" Field-s are added to the resulting SolrInputField, and the original value of that field is removed. Example configuration In the example configuration below there are two update chains, one that uses the ""simple"" parser (SimplePreAnalyzedParser) and one that uses the ""json"" parser (JsonPreAnalyzedParser). Field ""nonexistent"" will be removed from input documents if not present in the schema. Other fields will be analyzed and if valid they will be converted to IndexableField-s or if they are not in a valid format that can be parsed with the selected parser they will be passed as-is. Assuming that ssto field is stored but not indexed, and sind field is indexed but not stored: if ssto input value contains the indexed part then this part will be discarded and only the stored value part will be retained. Similarly, if sind input value contains the stored part then it will be discarded and only the token stream part will be retained. <updateRequestProcessorChain name=""pre-analyzed-simple""> <processor class=""solr.PreAnalyzedUpdateProcessorFactory""> <str name=""fieldName"">title</str> <str name=""fieldName"">nonexistent</str> <str name=""fieldName"">ssto</str> <str name=""fieldName"">sind</str> <str name=""parser"">simple</str> </processor> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain> <updateRequestProcessorChain name=""pre-analyzed-json""> <processor class=""solr.PreAnalyzedUpdateProcessorFactory""> <str name=""fieldName"">title</str> <str name=""fieldName"">nonexistent</str> <str name=""fieldName"">ssto</str> <str name=""fieldName"">sind</str> <str name=""parser"">json</str> </processor> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain>"
update,"A typical use case may be to match a URL against patterns to boost or deboost web documents based on the URL itself: # Format of each line: <pattern><TAB><boost> # Example: https?://my.domain.com/temp.* 0.2 Both inputField, boostField and boostFilename are mandatory parameters."
update,"Both inputField, boostField and boostFilename are mandatory parameters."
update,The factory initializes a shared object cache which is passed to the processor and this way reduces rules file parsing to the first time the UpdateChain is initialized.
update,"By default this processor applies itself to no fields. By default, literalReplacement is set to true, in which case, the replacement string will be treated literally by quoting via Matcher.quoteReplacement(String). And hence, '\' and '$' signs will not be processed. When literalReplacement is set to false, one can perform backreference operations and capture group substitutions. For example, with the configuration listed below, any sequence of multiple whitespace characters found in values for field named title or content will be replaced by a single space character. <processor class=""solr.RegexReplaceProcessorFactory""> <str name=""fieldName"">content</str> <str name=""fieldName"">title</str> <str name=""pattern"">\s+</str> <str name=""replacement""> </str> <bool name=""literalReplacement"">true</bool> </processor>"
update,"By default, literalReplacement is set to true, in which case, the replacement string will be treated literally by quoting via Matcher.quoteReplacement(String). And hence, '\' and '$' signs will not be processed. When literalReplacement is set to false, one can perform backreference operations and capture group substitutions. For example, with the configuration listed below, any sequence of multiple whitespace characters found in values for field named title or content will be replaced by a single space character. <processor class=""solr.RegexReplaceProcessorFactory""> <str name=""fieldName"">content</str> <str name=""fieldName"">title</str> <str name=""pattern"">\s+</str> <str name=""replacement""> </str> <bool name=""literalReplacement"">true</bool> </processor>"
update,"For example, with the configuration listed below, any sequence of multiple whitespace characters found in values for field named title or content will be replaced by a single space character. <processor class=""solr.RegexReplaceProcessorFactory""> <str name=""fieldName"">content</str> <str name=""fieldName"">title</str> <str name=""pattern"">\s+</str> <str name=""replacement""> </str> <bool name=""literalReplacement"">true</bool> </processor>"
update,"By default this processor applies itself to all fields. For example, with the configuration listed below, blank strings will be removed from all fields except those whose name ends with ""_literal"". <processor class=""solr.RemoveBlankFieldUpdateProcessorFactory""> <lst name=""exclude""> <str name=""fieldRegex"">.*_literal</str> </lst> </processor>"
update,"For example, with the configuration listed below, blank strings will be removed from all fields except those whose name ends with ""_literal"". <processor class=""solr.RemoveBlankFieldUpdateProcessorFactory""> <lst name=""exclude""> <str name=""fieldRegex"">.*_literal</str> </lst> </processor>"
update,"Depends on this core having a special core property that points to the alias name that this collection is a part of. And further requires certain properties on the Alias. Collections pointed to by the alias must be named for the alias plus underscored ('_') and a routing specifier specific to the type of routed alias. These collections should not be created by the user, but are created automatically by the routed alias."
update,"Direct Known Subclasses: Lookup3Signature, MD5Signature public abstract class Signature extends Object"
update,"skipInsertIfExists - This boolean parameter defaults to true, but if set to false then inserts (i.e. not Atomic Updates) will be passed through unchanged even if the document already exists. skipUpdateIfMissing - This boolean parameter defaults to true, but if set to false then Atomic Updates will be passed through unchanged regardless of whether the document exists."
update,"skipUpdateIfMissing - This boolean parameter defaults to true, but if set to false then Atomic Updates will be passed through unchanged regardless of whether the document exists."
update,"skipInsertIfExists - This boolean parameter defaults to true, but if set to false then inserts (i.e. not Atomic Updates) will be passed through unchanged even if the document already exists. skipUpdateIfMissing - This boolean parameter defaults to true, but if set to false then Atomic Updates will be passed through unchanged regardless of whether the document exists."
update,"skipUpdateIfMissing - This boolean parameter defaults to true, but if set to false then Atomic Updates will be passed through unchanged regardless of whether the document exists."
update,"These params can also be specified per-request, to override the configured behaviour for specific updates e.g. /update?skipUpdateIfMissing=true This implementation is a simpler alternative to DocBasedVersionConstraintsProcessorFactory when you are not concerned with versioning, and just want to quietly ignore duplicate documents and/or silently skip updates to non-existent documents (in the same way a database UPDATE would). If your documents do have an explicit version field, and you want to ensure older versions are skipped instead of replacing the indexed document, you should consider DocBasedVersionConstraintsProcessorFactory instead. An example chain configuration to use this for skipping duplicate inserts, but not skipping updates to missing documents by default, is: <updateRequestProcessorChain name=""skipexisting""> <processor class=""solr.LogUpdateProcessorFactory"" /> <processor class=""solr.SkipExistingDocumentsProcessorFactory""> <bool name=""skipInsertIfExists"">true</bool> <bool name=""skipUpdateIfMissing"">false</bool> <!-- Can override this per-request --> </processor> <processor class=""solr.DistributedUpdateProcessorFactory"" /> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain>"
update,"This implementation is a simpler alternative to DocBasedVersionConstraintsProcessorFactory when you are not concerned with versioning, and just want to quietly ignore duplicate documents and/or silently skip updates to non-existent documents (in the same way a database UPDATE would). If your documents do have an explicit version field, and you want to ensure older versions are skipped instead of replacing the indexed document, you should consider DocBasedVersionConstraintsProcessorFactory instead. An example chain configuration to use this for skipping duplicate inserts, but not skipping updates to missing documents by default, is: <updateRequestProcessorChain name=""skipexisting""> <processor class=""solr.LogUpdateProcessorFactory"" /> <processor class=""solr.SkipExistingDocumentsProcessorFactory""> <bool name=""skipInsertIfExists"">true</bool> <bool name=""skipUpdateIfMissing"">false</bool> <!-- Can override this per-request --> </processor> <processor class=""solr.DistributedUpdateProcessorFactory"" /> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain>"
update,"If your documents do have an explicit version field, and you want to ensure older versions are skipped instead of replacing the indexed document, you should consider DocBasedVersionConstraintsProcessorFactory instead. An example chain configuration to use this for skipping duplicate inserts, but not skipping updates to missing documents by default, is: <updateRequestProcessorChain name=""skipexisting""> <processor class=""solr.LogUpdateProcessorFactory"" /> <processor class=""solr.SkipExistingDocumentsProcessorFactory""> <bool name=""skipInsertIfExists"">true</bool> <bool name=""skipUpdateIfMissing"">false</bool> <!-- Can override this per-request --> </processor> <processor class=""solr.DistributedUpdateProcessorFactory"" /> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain>"
update,"An example chain configuration to use this for skipping duplicate inserts, but not skipping updates to missing documents by default, is: <updateRequestProcessorChain name=""skipexisting""> <processor class=""solr.LogUpdateProcessorFactory"" /> <processor class=""solr.SkipExistingDocumentsProcessorFactory""> <bool name=""skipInsertIfExists"">true</bool> <bool name=""skipUpdateIfMissing"">false</bool> <!-- Can override this per-request --> </processor> <processor class=""solr.DistributedUpdateProcessorFactory"" /> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain>"
update,"The format of the parameter is <field-name>:<the-template-string>, for example: Template.field=fname:${somefield}some_string${someotherfield}"
update,Enclosing class: TemplateUpdateProcessorFactory public static class TemplateUpdateProcessorFactory.Resolved extends Object
update,"public class TextProfileSignature extends MD5Signature This implementation is copied from Apache Nutch. An implementation of a page signature. It calculates an MD5 hash of a plain text ""profile"" of a page. The algorithm to calculate a page ""profile"" takes the plain text version of a page and performs the following steps: remove all characters except letters and digits, and bring all characters to lower case, split the text into tokens (all consecutive non-whitespace characters), discard tokens equal or shorter than MIN_TOKEN_LEN (default 2 characters), sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency. This list is then submitted to an MD5 hash calculation."
update,"remove all characters except letters and digits, and bring all characters to lower case, split the text into tokens (all consecutive non-whitespace characters), discard tokens equal or shorter than MIN_TOKEN_LEN (default 2 characters), sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"split the text into tokens (all consecutive non-whitespace characters), discard tokens equal or shorter than MIN_TOKEN_LEN (default 2 characters), sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"discard tokens equal or shorter than MIN_TOKEN_LEN (default 2 characters), sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"An implementation of a page signature. It calculates an MD5 hash of a plain text ""profile"" of a page. The algorithm to calculate a page ""profile"" takes the plain text version of a page and performs the following steps: remove all characters except letters and digits, and bring all characters to lower case, split the text into tokens (all consecutive non-whitespace characters), discard tokens equal or shorter than MIN_TOKEN_LEN (default 2 characters), sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency. This list is then submitted to an MD5 hash calculation."
update,"The algorithm to calculate a page ""profile"" takes the plain text version of a page and performs the following steps: remove all characters except letters and digits, and bring all characters to lower case, split the text into tokens (all consecutive non-whitespace characters), discard tokens equal or shorter than MIN_TOKEN_LEN (default 2 characters), sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency. This list is then submitted to an MD5 hash calculation."
update,"remove all characters except letters and digits, and bring all characters to lower case, split the text into tokens (all consecutive non-whitespace characters), discard tokens equal or shorter than MIN_TOKEN_LEN (default 2 characters), sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"split the text into tokens (all consecutive non-whitespace characters), discard tokens equal or shorter than MIN_TOKEN_LEN (default 2 characters), sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"discard tokens equal or shorter than MIN_TOKEN_LEN (default 2 characters), sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"sort the list of tokens by decreasing frequency, round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"round down the counts of tokens to the nearest multiple of QUANT ( QUANT = QUANT_RATE * maxFreq, where QUANT_RATE is 0.01f by default, and maxFreq is the maximum token frequency). If maxFreq is higher than 1, then QUANT is always higher than 2 (which means that tokens with frequency 1 are always discarded). tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"tokens, which frequency after quantization falls below QUANT, are discarded. create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"create a list of tokens and their quantized frequency, separated by spaces, in the order of decreasing frequency."
update,"In the example configuration below, if a document does not contain a value in the timestamp field, a new Date will be generated and added as the value of that field. <processor class=""solr.TimestampUpdateProcessorFactory""> <str name=""fieldName"">timestamp</str> </processor>"
update,"If more then maxErrors occur, the first exception recorded will be re-thrown, Solr will respond with status==5xx or status==4xx (depending on the underlying exceptions) and it won't finish processing any more updates in the request. (ie: subsequent update commands in the request will not be processed even if they are valid). NOTE: In cloud based collections, this processor expects to NOT be used on DistributedUpdateProcessor.DistribPhase.FROMLEADER requests (because any successes that occur locally on the leader are considered successes even if there is some subsequent error on a replica). TolerantUpdateProcessorFactory will short circut it away in those requests."
update,"NOTE: In cloud based collections, this processor expects to NOT be used on DistributedUpdateProcessor.DistribPhase.FROMLEADER requests (because any successes that occur locally on the leader are considered successes even if there is some subsequent error on a replica). TolerantUpdateProcessorFactory will short circut it away in those requests."
update,"If more then maxErrors occur, the first exception recorded will be re-thrown, Solr will respond with status==5xx or status==4xx (depending on the underlying exceptions) and it won't finish processing any more updates in the request. (ie: subsequent update commands in the request will not be processed even if they are valid). maxErrors is an int value that can be specified in the configuration and/or overridden per request. If unset, it will default to Integer.MAX_VALUE. Specifying an explicit value of -1 is supported as shorthand for Integer.MAX_VALUE, all other negative integer values are not supported. An example configuration would be: <updateRequestProcessorChain name=""tolerant-chain""> <processor class=""solr.TolerantUpdateProcessorFactory""> <int name=""maxErrors"">10</int> </processor> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain> The maxErrors parameter in the above chain could be overwritten per request, for example: curl http://localhost:8983/update?update.chain=tolerant-chain&maxErrors=100 -H ""Content-Type: text/xml"" -d @myfile.xml NOTE: The behavior of this UpdateProcessofFactory in conjunction with indexing operations while a Shard Split is actively in progress is not well defined (or sufficiently tested). Users of this update processor are encouraged to either disable it, or pause updates, while any shard splitting is in progress (see SOLR-8881 for more details.)"
update,"maxErrors is an int value that can be specified in the configuration and/or overridden per request. If unset, it will default to Integer.MAX_VALUE. Specifying an explicit value of -1 is supported as shorthand for Integer.MAX_VALUE, all other negative integer values are not supported. An example configuration would be: <updateRequestProcessorChain name=""tolerant-chain""> <processor class=""solr.TolerantUpdateProcessorFactory""> <int name=""maxErrors"">10</int> </processor> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain> The maxErrors parameter in the above chain could be overwritten per request, for example: curl http://localhost:8983/update?update.chain=tolerant-chain&maxErrors=100 -H ""Content-Type: text/xml"" -d @myfile.xml NOTE: The behavior of this UpdateProcessofFactory in conjunction with indexing operations while a Shard Split is actively in progress is not well defined (or sufficiently tested). Users of this update processor are encouraged to either disable it, or pause updates, while any shard splitting is in progress (see SOLR-8881 for more details.)"
update,"An example configuration would be: <updateRequestProcessorChain name=""tolerant-chain""> <processor class=""solr.TolerantUpdateProcessorFactory""> <int name=""maxErrors"">10</int> </processor> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain> The maxErrors parameter in the above chain could be overwritten per request, for example: curl http://localhost:8983/update?update.chain=tolerant-chain&maxErrors=100 -H ""Content-Type: text/xml"" -d @myfile.xml NOTE: The behavior of this UpdateProcessofFactory in conjunction with indexing operations while a Shard Split is actively in progress is not well defined (or sufficiently tested). Users of this update processor are encouraged to either disable it, or pause updates, while any shard splitting is in progress (see SOLR-8881 for more details.)"
update,"The maxErrors parameter in the above chain could be overwritten per request, for example: curl http://localhost:8983/update?update.chain=tolerant-chain&maxErrors=100 -H ""Content-Type: text/xml"" -d @myfile.xml NOTE: The behavior of this UpdateProcessofFactory in conjunction with indexing operations while a Shard Split is actively in progress is not well defined (or sufficiently tested). Users of this update processor are encouraged to either disable it, or pause updates, while any shard splitting is in progress (see SOLR-8881 for more details.)"
update,"NOTE: The behavior of this UpdateProcessofFactory in conjunction with indexing operations while a Shard Split is actively in progress is not well defined (or sufficiently tested). Users of this update processor are encouraged to either disable it, or pause updates, while any shard splitting is in progress (see SOLR-8881 for more details.)"
update,"By default this processor matches all fields For example, with the configuration listed all String field values will have leading and trailing spaces removed except for fields whose named ends with ""_literal"". <processor class=""solr.TrimFieldUpdateProcessorFactory""> <lst name=""exclude""> <str name=""fieldRegex"">.*_literal</str> </lst> </processor>"
update,"For example, with the configuration listed all String field values will have leading and trailing spaces removed except for fields whose named ends with ""_literal"". <processor class=""solr.TrimFieldUpdateProcessorFactory""> <lst name=""exclude""> <str name=""fieldRegex"">.*_literal</str> </lst> </processor>"
update,"By default this processor matches no fields For example, with the configuration listed below any documents containing a String in any field declared in the schema using StrField will be truncated to no more then 100 characters <processor class=""solr.TruncateFieldUpdateProcessorFactory""> <str name=""typeClass"">solr.StrField</str> <int name=""maxLength"">100</int> </processor>"
update,"For example, with the configuration listed below any documents containing a String in any field declared in the schema using StrField will be truncated to no more then 100 characters <processor class=""solr.TruncateFieldUpdateProcessorFactory""> <str name=""typeClass"">solr.StrField</str> <int name=""maxLength"">100</int> </processor>"
update,"By default this processor matches no fields. In the example configuration below, if a document initially contains the values ""Steve"",""Lucy"",""Jim"",Steve"",""Alice"",""Bob"",""Alice"" in a field named foo_uniq then using this processor will result in the final list of field values being ""Steve"",""Lucy"",""Jim"",""Alice"",""Bob"" <processor class=""solr.UniqFieldsUpdateProcessorFactory""> <str name=""fieldRegex"">.*_uniq</str> </processor>"
update,"In the example configuration below, if a document initially contains the values ""Steve"",""Lucy"",""Jim"",Steve"",""Alice"",""Bob"",""Alice"" in a field named foo_uniq then using this processor will result in the final list of field values being ""Steve"",""Lucy"",""Jim"",""Alice"",""Bob"" <processor class=""solr.UniqFieldsUpdateProcessorFactory""> <str name=""fieldRegex"">.*_uniq</str> </processor>"
update,"Perhaps you continue adding an error message (without indexing the document)... perhaps you throw an error and halt indexing (remove anything already indexed??) By default, this just passes the request to the next processor in the chain."
update,"By default, this just passes the request to the next processor in the chain."
update,"A single configured chain may explicitly be declared with default=""true"" (see example above) If no chain is explicitly declared as the default, Solr will look for any chain that does not have a name, and treat it as the default As a last resort, Solr will create an implicit default chain consisting of: LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory"
update,"If no chain is explicitly declared as the default, Solr will look for any chain that does not have a name, and treat it as the default As a last resort, Solr will create an implicit default chain consisting of: LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory"
update,"As a last resort, Solr will create an implicit default chain consisting of: LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory"
update,LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory
update,DistributedUpdateProcessorFactory RunUpdateProcessorFactory
update,"Chains can be configured via solrconfig.xml using the following syntax... <updateRequestProcessorChain name=""key"" default=""true""> <processor class=""package.Class1"" /> <processor class=""package.Class2"" > <str name=""someInitParam1"">value</str> <int name=""someInitParam2"">42</int> </processor> <processor class=""solr.LogUpdateProcessorFactory"" > <int name=""maxNumToLog"">100</int> </processor> <processor class=""solr.RunUpdateProcessorFactory"" /> </updateRequestProcessorChain> Multiple Chains can be defined, each with a distinct name. The name of a chain used to handle an update request may be specified using the request param update.chain. If no chain is explicitly selected by name, then Solr will attempt to determine a default chain: A single configured chain may explicitly be declared with default=""true"" (see example above) If no chain is explicitly declared as the default, Solr will look for any chain that does not have a name, and treat it as the default As a last resort, Solr will create an implicit default chain consisting of: LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory Almost all processor chains should end with an instance of RunUpdateProcessorFactory unless the user is explicitly executing the update commands in an alternative custom UpdateRequestProcessorFactory. If a chain includes RunUpdateProcessorFactory but does not include a DistributingUpdateProcessorFactory, it will be added automatically by init()."
update,"Multiple Chains can be defined, each with a distinct name. The name of a chain used to handle an update request may be specified using the request param update.chain. If no chain is explicitly selected by name, then Solr will attempt to determine a default chain: A single configured chain may explicitly be declared with default=""true"" (see example above) If no chain is explicitly declared as the default, Solr will look for any chain that does not have a name, and treat it as the default As a last resort, Solr will create an implicit default chain consisting of: LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory Almost all processor chains should end with an instance of RunUpdateProcessorFactory unless the user is explicitly executing the update commands in an alternative custom UpdateRequestProcessorFactory. If a chain includes RunUpdateProcessorFactory but does not include a DistributingUpdateProcessorFactory, it will be added automatically by init()."
update,"A single configured chain may explicitly be declared with default=""true"" (see example above) If no chain is explicitly declared as the default, Solr will look for any chain that does not have a name, and treat it as the default As a last resort, Solr will create an implicit default chain consisting of: LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory"
update,"If no chain is explicitly declared as the default, Solr will look for any chain that does not have a name, and treat it as the default As a last resort, Solr will create an implicit default chain consisting of: LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory"
update,"As a last resort, Solr will create an implicit default chain consisting of: LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory"
update,LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory
update,DistributedUpdateProcessorFactory RunUpdateProcessorFactory
update,LogUpdateProcessorFactory DistributedUpdateProcessorFactory RunUpdateProcessorFactory
update,DistributedUpdateProcessorFactory RunUpdateProcessorFactory
update,"Almost all processor chains should end with an instance of RunUpdateProcessorFactory unless the user is explicitly executing the update commands in an alternative custom UpdateRequestProcessorFactory. If a chain includes RunUpdateProcessorFactory but does not include a DistributingUpdateProcessorFactory, it will be added automatically by init()."
update,Enclosing class: UpdateRequestProcessorChain public static class UpdateRequestProcessorChain.ProcessorInfo extends Object
update,"If the factory needs access to SolrCore in initialization, it could implement SolrCoreAware"
update,url_length url_levels url_toplevel url_landingpage
update,url_levels url_toplevel url_landingpage
update,url_toplevel url_landingpage
update,"Then, at index time, Solr will look at the id field value and extract it's domain portion into a new hostname field. By default, the following fields will also be added: url_length url_levels url_toplevel url_landingpage For example, adding the following document { ""id"":""http://wwww.mydomain.com/subpath/document.html"" } will result in this document in Solr: { ""id"":""http://wwww.mydomain.com/subpath/document.html"", ""url_length"":46, ""url_levels"":2, ""url_toplevel"":0, ""url_landingpage"":0, ""hostname"":""wwww.mydomain.com"", ""_version_"":1603193062117343232}] }"
update,url_length url_levels url_toplevel url_landingpage
update,url_levels url_toplevel url_landingpage
update,url_toplevel url_landingpage
update,"For example, adding the following document { ""id"":""http://wwww.mydomain.com/subpath/document.html"" } will result in this document in Solr: { ""id"":""http://wwww.mydomain.com/subpath/document.html"", ""url_length"":46, ""url_levels"":2, ""url_toplevel"":0, ""url_landingpage"":0, ""hostname"":""wwww.mydomain.com"", ""_version_"":1603193062117343232}] }"
update,"will result in this document in Solr: { ""id"":""http://wwww.mydomain.com/subpath/document.html"", ""url_length"":46, ""url_levels"":2, ""url_toplevel"":0, ""url_landingpage"":0, ""hostname"":""wwww.mydomain.com"", ""_version_"":1603193062117343232}] }"
update,"In the example configuration below, if a document does not contain a value in the id field, a new UUID will be generated and added as the value of that field. <processor class=""solr.UUIDUpdateProcessorFactory""> <str name=""fieldName"">id</str> </processor> You can also invoke the processor with request handler param(s) as uuid.fieldname with processor=uuid curl -X POST -H Content-Type: application/json http://localhost:8983/solr/test/update/json/docs?processor=uuid;ampersand;uuid.fieldName=id;ampersand;commit=true --data-binary {""id"":""1"",""title"": ""titleA""} NOTE: The param(s) provided in request handler will override / supersede processor's config. If field name is omitted in processor configuration and not provided in request handler param(s), then IndexSchema.getUniqueKeyField() is used as field and a new UUID will be generated and added as the value of that field. The field type of the uniqueKeyField must be anything which accepts a string or UUID value."
update,"You can also invoke the processor with request handler param(s) as uuid.fieldname with processor=uuid curl -X POST -H Content-Type: application/json http://localhost:8983/solr/test/update/json/docs?processor=uuid;ampersand;uuid.fieldName=id;ampersand;commit=true --data-binary {""id"":""1"",""title"": ""titleA""} NOTE: The param(s) provided in request handler will override / supersede processor's config. If field name is omitted in processor configuration and not provided in request handler param(s), then IndexSchema.getUniqueKeyField() is used as field and a new UUID will be generated and added as the value of that field. The field type of the uniqueKeyField must be anything which accepts a string or UUID value."
update,"NOTE: The param(s) provided in request handler will override / supersede processor's config. If field name is omitted in processor configuration and not provided in request handler param(s), then IndexSchema.getUniqueKeyField() is used as field and a new UUID will be generated and added as the value of that field. The field type of the uniqueKeyField must be anything which accepts a string or UUID value."
update,"If field name is omitted in processor configuration and not provided in request handler param(s), then IndexSchema.getUniqueKeyField() is used as field and a new UUID will be generated and added as the value of that field. The field type of the uniqueKeyField must be anything which accepts a string or UUID value."
util,public final class AddressUtils extends Object Simple utilities for working Hostname/IP Addresses
util,public final class AdjustableSemaphore extends Object
util,"Methods inherited from classjava.util.TreeSet ceiling, clear, clone, comparator, contains, descendingIterator, descendingSet, first, floor, headSet, headSet, higher, isEmpty, iterator, last, lower, pollFirst, pollLast, remove, size, spliterator, subSet, subSet, tailSet, tailSet"
util,"Methods inherited from classjava.util.AbstractSet equals, hashCode, removeAll"
util,"Methods inherited from classjava.util.AbstractCollection containsAll, retainAll, toArray, toArray, toString"
util,"Methods inherited from interfacejava.util.Collection parallelStream, removeIf, stream, toArray"
util,Methods inherited from interfacejava.lang.Iterable forEach
util,"Methods inherited from interfacejava.util.Set containsAll, equals, hashCode, removeAll, retainAll, toArray, toArray"
util,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
util,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
util,"Note that the implementation does not follow a true LRU (least-recently-used) eviction strategy. Instead it strives to remove least recently used items but when the initial cleanup does not remove enough items to reach the 'acceptableWaterMark' limit, it can remove more items forcefully regardless of access order."
util,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
util,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
util,Methods inherited from interfaceorg.apache.lucene.util.Accountable getChildResources
util,Fields inherited from interfaceorg.apache.lucene.util.Accountable NULL_ACCOUNTABLE
util,public final class CryptoKeys extends Object A utility class with helpers for various signature and certificate tasks
util,Enclosing class: CryptoKeys public static class CryptoKeys.RSAKeyPair extends Object
util,"public class DateMathParser extends Object A Simple Utility class for parsing ""math"" like strings relating to Dates. The basic syntax support addition, subtraction and rounding at various levels of granularity (or ""units""). Commands can be chained together and are parsed from left to right. '+' and '-' denote addition and subtraction, while '/' denotes ""round"". Round requires only a unit, while addition/subtraction require an integer value and a unit. Command strings must not include white space, but the ""No-Op"" command (empty string) is allowed.... /HOUR ... Round to the start of the current hour /DAY ... Round to the start of the current day +2YEARS ... Exactly two years in the future from now -1DAY ... Exactly 1 day prior to now /DAY+6MONTHS+3DAYS ... 6 months and 3 days in the future from the start of the current day +6MONTHS+3DAYS/DAY ... 6 months and 3 days in the future from now, rounded down to nearest day (Multiple aliases exist for the various units of time (ie: MINUTE and MINUTES; MILLI, MILLIS, MILLISECOND, and MILLISECONDS.) The complete list can be found by inspecting the keySet of CALENDAR_UNITS) All commands are relative to a ""now"" which is fixed in an instance of DateMathParser such that p.parseMath(""+0MILLISECOND"").equals(p.parseMath(""+0MILLISECOND"")) no matter how many wall clock milliseconds elapse between the two distinct calls to parse (Assuming no other thread calls ""setNow"" in the interim). The default value of 'now' is the time at the moment the DateMathParser instance is constructed, unless overridden by the NOW request param. All commands are also affected to the rules of a specified TimeZone (including the start/end of DST if any) which determine when each arbitrary day starts. This not only impacts rounding/adding of DAYs, but also cascades to rounding of HOUR, MIN, MONTH, YEAR as well. The default TimeZone used is UTC unless overridden by the TZ request param. Historical dates: The calendar computation is completely done with the Gregorian system/algorithm. It does not switch to Julian or anything else, unlike the default GregorianCalendar. See Also: SolrRequestInfo.getClientTimeZone(), SolrRequestInfo.getNOW()"
util,"The basic syntax support addition, subtraction and rounding at various levels of granularity (or ""units""). Commands can be chained together and are parsed from left to right. '+' and '-' denote addition and subtraction, while '/' denotes ""round"". Round requires only a unit, while addition/subtraction require an integer value and a unit. Command strings must not include white space, but the ""No-Op"" command (empty string) is allowed.... /HOUR ... Round to the start of the current hour /DAY ... Round to the start of the current day +2YEARS ... Exactly two years in the future from now -1DAY ... Exactly 1 day prior to now /DAY+6MONTHS+3DAYS ... 6 months and 3 days in the future from the start of the current day +6MONTHS+3DAYS/DAY ... 6 months and 3 days in the future from now, rounded down to nearest day (Multiple aliases exist for the various units of time (ie: MINUTE and MINUTES; MILLI, MILLIS, MILLISECOND, and MILLISECONDS.) The complete list can be found by inspecting the keySet of CALENDAR_UNITS) All commands are relative to a ""now"" which is fixed in an instance of DateMathParser such that p.parseMath(""+0MILLISECOND"").equals(p.parseMath(""+0MILLISECOND"")) no matter how many wall clock milliseconds elapse between the two distinct calls to parse (Assuming no other thread calls ""setNow"" in the interim). The default value of 'now' is the time at the moment the DateMathParser instance is constructed, unless overridden by the NOW request param. All commands are also affected to the rules of a specified TimeZone (including the start/end of DST if any) which determine when each arbitrary day starts. This not only impacts rounding/adding of DAYs, but also cascades to rounding of HOUR, MIN, MONTH, YEAR as well. The default TimeZone used is UTC unless overridden by the TZ request param. Historical dates: The calendar computation is completely done with the Gregorian system/algorithm. It does not switch to Julian or anything else, unlike the default GregorianCalendar."
util,"(Multiple aliases exist for the various units of time (ie: MINUTE and MINUTES; MILLI, MILLIS, MILLISECOND, and MILLISECONDS.) The complete list can be found by inspecting the keySet of CALENDAR_UNITS) All commands are relative to a ""now"" which is fixed in an instance of DateMathParser such that p.parseMath(""+0MILLISECOND"").equals(p.parseMath(""+0MILLISECOND"")) no matter how many wall clock milliseconds elapse between the two distinct calls to parse (Assuming no other thread calls ""setNow"" in the interim). The default value of 'now' is the time at the moment the DateMathParser instance is constructed, unless overridden by the NOW request param. All commands are also affected to the rules of a specified TimeZone (including the start/end of DST if any) which determine when each arbitrary day starts. This not only impacts rounding/adding of DAYs, but also cascades to rounding of HOUR, MIN, MONTH, YEAR as well. The default TimeZone used is UTC unless overridden by the TZ request param. Historical dates: The calendar computation is completely done with the Gregorian system/algorithm. It does not switch to Julian or anything else, unlike the default GregorianCalendar."
util,"All commands are relative to a ""now"" which is fixed in an instance of DateMathParser such that p.parseMath(""+0MILLISECOND"").equals(p.parseMath(""+0MILLISECOND"")) no matter how many wall clock milliseconds elapse between the two distinct calls to parse (Assuming no other thread calls ""setNow"" in the interim). The default value of 'now' is the time at the moment the DateMathParser instance is constructed, unless overridden by the NOW request param. All commands are also affected to the rules of a specified TimeZone (including the start/end of DST if any) which determine when each arbitrary day starts. This not only impacts rounding/adding of DAYs, but also cascades to rounding of HOUR, MIN, MONTH, YEAR as well. The default TimeZone used is UTC unless overridden by the TZ request param. Historical dates: The calendar computation is completely done with the Gregorian system/algorithm. It does not switch to Julian or anything else, unlike the default GregorianCalendar."
util,"All commands are also affected to the rules of a specified TimeZone (including the start/end of DST if any) which determine when each arbitrary day starts. This not only impacts rounding/adding of DAYs, but also cascades to rounding of HOUR, MIN, MONTH, YEAR as well. The default TimeZone used is UTC unless overridden by the TZ request param. Historical dates: The calendar computation is completely done with the Gregorian system/algorithm. It does not switch to Julian or anything else, unlike the default GregorianCalendar."
util,"Historical dates: The calendar computation is completely done with the Gregorian system/algorithm. It does not switch to Julian or anything else, unlike the default GregorianCalendar."
util,public class DistanceUnits extends Object Used with a spatial field type for all distance measurements. See Also: AbstractSpatialFieldType
util,public class FileUtils extends Object
util,public class IdUtils extends Object Helper class for generating unique ID-s.
util,"Methods inherited from classjava.io.OutputStream flush, nullOutputStream, write"
util,public final class JmxUtil extends Object Utility methods to find a MBeanServer.
util,public class LocaleUtils extends Object
util,public class LongPriorityQueue extends Object A native long priority queue. NOTE: This API is for internal purposes only and might change in incompatible ways in the next release.
util,"Nested classes/interfaces inherited from interfacejava.util.Map Map.Entry<K extends Object,V extends Object>"
util,"Methods inherited from interfacejava.util.Map compute, computeIfAbsent, computeIfPresent, equals, forEach, getOrDefault, hashCode, merge, putIfAbsent, remove, replace, replace, replaceAll"
util,"Nested classes/interfaces inherited from interfacejava.util.Map Map.Entry<K extends Object,V extends Object>"
util,"public class ModuleUtils extends Object Parses the list of modules the user has requested in solr.xml, property solr.modules or environment SOLR_MODULES. Then resolves the lib folder for each, so they can be added to class path."
util,public class NumberUtils extends Object
util,public class PayloadUtils extends Object
util,Enclosing class: PrimUtils public abstract static class PrimUtils.IntComparator extends Object
util,public class PrimUtils extends Object Utilities for primitive Java data types.
util,"Methods inherited from classjava.io.InputStream available, mark, markSupported, nullInputStream, read, read, readAllBytes, readNBytes, readNBytes, reset, skip, transferTo"
util,public class RecordingJSONParser extends org.noggit.JSONParser
util,Nested classes/interfaces inherited from classorg.noggit.JSONParser org.noggit.JSONParser.ParseException
util,"Fields inherited from classorg.noggit.JSONParser ALLOW_BACKSLASH_ESCAPING_ANY_CHARACTER, ALLOW_COMMENTS, ALLOW_EXTRA_COMMAS, ALLOW_MISSING_COLON_COMMA_BEFORE_OBJECT, ALLOW_SINGLE_QUOTES, ALLOW_UNQUOTED_KEYS, ALLOW_UNQUOTED_STRING_VALUES, ARRAY_END, ARRAY_START, BIGNUMBER, BOOLEAN, end, eof, EOF, event, flags, FLAGS_DEFAULT, FLAGS_STRICT, gpos, in, LONG, missingOpeningBrace, NULL, NUMBER, OBJECT_END, OBJECT_START, OPTIONAL_OUTER_BRACES, start, STRING, stringTerm"
util,"Methods inherited from classorg.noggit.JSONParser err, getBoolean, getChar, getCharExpected, getCharNWS, getCharNWS, getDouble, getEventString, getFlags, getLevel, getLong, getNewlineComment, getNull, getNumberChars, getNumberChars, getPosition, getSlashComment, getString, getString, getStringChars, handleNonDoubleQuoteString, isWhitespace, lastEvent, matchBareWord, setFlags, toString, wasKey"
util,Nested classes/interfaces inherited from classorg.noggit.JSONParser org.noggit.JSONParser.ParseException
util,"public abstract class RefCounted<Type> extends Object Keep track of a reference count on a resource and close it when the count hits zero. By itself, this class could have some race conditions since there is no synchronization between the refcount check and the close. Solr's use in reference counting searchers is safe since the count can only hit zero if it's unregistered (and hence incref() will not be called again on it)."
util,"By itself, this class could have some race conditions since there is no synchronization between the refcount check and the close. Solr's use in reference counting searchers is safe since the count can only hit zero if it's unregistered (and hence incref() will not be called again on it)."
util,Direct Known Subclasses: RTimerTree public class RTimer extends Object A simple timer. RTimers are started automatically when instantiated. Since: solr 1.3
util,RTimers are started automatically when instantiated.
util,public class RTimerTree extends RTimer A recursive timer. RTimerTree's are started automatically when instantiated; sub-timers are also started automatically when created.
util,RTimerTree's are started automatically when instantiated; sub-timers are also started automatically when created.
util,"public final class SafeXMLParsing extends Object Some utility methods for parsing XML in a safe way. This class can be used to parse XML coming from network (completely untrusted) or it can load a config file from a ResourceLoader. In this case it allows external entities and xincludes, but only referring to files reachable by the loader."
util,"Nested classes/interfaces inherited from classcom.fasterxml.jackson.databind.AnnotationIntrospector com.fasterxml.jackson.databind.AnnotationIntrospector.ReferenceProperty, com.fasterxml.jackson.databind.AnnotationIntrospector.XmlExtensions"
util,"Methods inherited from classcom.fasterxml.jackson.databind.AnnotationIntrospector _findAnnotation, _hasAnnotation, _hasOneOf, allIntrospectors, allIntrospectors, findAndAddVirtualProperties, findAutoDetectVisibility, findClassDescription, findContentDeserializer, findContentSerializer, findCreatorAnnotation, findCreatorBinding, findDefaultCreator, findDefaultEnumValue, findDefaultEnumValue, findDeserializationContentConverter, findDeserializationConverter, findDeserializer, findEnumAliases, findEnumAliases, findEnumNamingStrategy, findEnumValue, findEnumValues, findEnumValues, findFilterId, findFormat, findImplicitPropertyName, findInjectableValue, findInjectableValueId, findKeyDeserializer, findKeySerializer, findMergeInfo, findNamingStrategy, findNullSerializer, findObjectIdInfo, findObjectReferenceInfo, findPOJOBuilder, findPOJOBuilderConfig, findPolymorphicTypeInfo, findPropertyAccess, findPropertyAliases, findPropertyContentTypeResolver, findPropertyDescription, findPropertyIgnoralByName, findPropertyIgnorals, findPropertyInclusion, findPropertyInclusionByName, findPropertyIndex, findPropertyTypeResolver, findReferenceType, findRenameByField, findRootName, findSerializationContentConverter, findSerializationConverter, findSerializationPropertyOrder, findSerializationSortAlphabetically, findSerializationTyping, findSerializer, findSetterInfo, findSubtypes, findTypeName, findTypeResolver, findUnwrappingNameTransformer, findValueInstantiator, findViews, findWrapperName, hasAnyGetter, hasAnyGetterAnnotation, hasAnySetter, hasAnySetterAnnotation, hasAsKey, hasAsValue, hasAsValueAnnotation, hasCreatorAnnotation, hasIgnoreMarker, isAnnotationBundle, isIgnorableType, isTypeId, nopInstance, pair, refineDeserializationType, refineSerializationType, resolveSetterConflict"
util,"Nested classes/interfaces inherited from classcom.fasterxml.jackson.databind.AnnotationIntrospector com.fasterxml.jackson.databind.AnnotationIntrospector.ReferenceProperty, com.fasterxml.jackson.databind.AnnotationIntrospector.XmlExtensions"
util,Enclosing class: SolrPluginUtils.DisjunctionMaxQueryParser protected static class SolrPluginUtils.DisjunctionMaxQueryParser.Alias extends Object A simple container for storing alias info See Also: SolrPluginUtils.DisjunctionMaxQueryParser.aliases
util,Nested classes/interfaces inherited from classorg.apache.lucene.util.QueryBuilder org.apache.lucene.util.QueryBuilder.TermAndBoost
util,"Fields inherited from classorg.apache.lucene.util.QueryBuilder analyzer, autoGenerateMultiTermSynonymsPhraseQuery, enableGraphQueries, enablePositionIncrements"
util,"Methods inherited from classorg.apache.lucene.util.QueryBuilder add, analyzeBoolean, analyzeGraphBoolean, analyzeGraphPhrase, analyzeMultiBoolean, analyzeMultiPhrase, analyzePhrase, analyzeTerm, createBooleanQuery, createBooleanQuery, createFieldQuery, createFieldQuery, createMinShouldMatchQuery, createPhraseQuery, createPhraseQuery, getAnalyzer, getAutoGenerateMultiTermSynonymsPhraseQuery, getEnableGraphQueries, getEnablePositionIncrements, newBooleanQuery, newMultiPhraseQueryBuilder, newTermQuery, setAnalyzer, setAutoGenerateMultiTermSynonymsPhraseQuery, setEnableGraphQueries, setEnablePositionIncrements"
util,Nested classes/interfaces inherited from classorg.apache.lucene.util.QueryBuilder org.apache.lucene.util.QueryBuilder.TermAndBoost
util,public class SolrPluginUtils extends Object Utilities that may be of use to RequestHandlers.
util,public class SolrResponseUtil extends Object
util,public class SpatialUtils extends Object Utility methods pertaining to spatial.
util,"public final class StartupLoggingUtils extends Object Handles programmatic modification of logging during startup WARNING: This class should only be used during startup. For modifying log levels etc during runtime, SLF4J and LogWatcher must be used."
util,"WARNING: This class should only be used during startup. For modifying log levels etc during runtime, SLF4J and LogWatcher must be used."
util,In general create the InputSource to be passed to the parser like: InputSource is = new InputSource(loader.openSchema(name)); is.setSystemId(SystemIdResolver.createSystemIdFromResourceName(name)); final DocumentBuilder db = DocumentBuilderFactory.newInstance().newDocumentBuilder(); db.setEntityResolver(new SystemIdResolver(loader)); Document doc = db.parse(is);
util,"public class TestInjection extends Object Allows random faults to be injected in running code during test runs. Set static strings to ""true"" or ""false"" or ""true:60"" for true 60% of the time. All methods are No-Ops unless LuceneTestCase is loadable via the ClassLoader used to load this class. LuceneTestCase.random() is used as the source of all entropy. NOTE: This API is for internal purposes only and might change in incompatible ways in the next release."
util,"Set static strings to ""true"" or ""false"" or ""true:60"" for true 60% of the time. All methods are No-Ops unless LuceneTestCase is loadable via the ClassLoader used to load this class. LuceneTestCase.random() is used as the source of all entropy."
util,All methods are No-Ops unless LuceneTestCase is loadable via the ClassLoader used to load this class. LuceneTestCase.random() is used as the source of all entropy.
util,"Methods inherited from classjava.lang.Throwable addSuppressed, fillInStackTrace, getCause, getLocalizedMessage, getMessage, getStackTrace, getSuppressed, initCause, printStackTrace, printStackTrace, printStackTrace, setStackTrace, toString"
util,"public class ThreadCpuTimer extends Object Allows tracking information about the current thread using the JVM's built-in management bean ThreadMXBean. Methods on this class are safe for use on any thread, but will return different values for different threads by design."
util,public class TimeOut extends Object
util,public final class TimeZoneUtils extends Object Simple utilities for working with TimeZones See Also: TimeZone
util,public class VersionedFile extends Object Since: solr 1.3
util,Have them checked at admission control by default (use CircuitBreakerRegistry for the same). Use the circuit breaker in a specific code path(s).
util,Use the circuit breaker in a specific code path(s).
util,There are two (typical) ways to use circuit breakers: Have them checked at admission control by default (use CircuitBreakerRegistry for the same). Use the circuit breaker in a specific code path(s).
util,public class CircuitBreakerUtils extends Object
util,This circuit breaker gets the recent average CPU usage and uses that data to take a decision. We depend on OperatingSystemMXBean which does not allow a configurable interval of collection of data.
util,"This circuit breaker gets the load average (length of the run queue) over the last minute and uses that data to take a decision. We depend on OperatingSystemMXBean which does not allow a configurable interval of collection of data. This Circuit breaker is dependent on the operating system, and may typically not work on Microsoft Windows."
util,"This Circuit breaker is dependent on the operating system, and may typically not work on Microsoft Windows."
util,The memory threshold is defined as a percentage of the maximum memory allocated -- see memThreshold in solrconfig.xml.
util,public class SSLConfigurations extends Object Dedicated object to handle Solr configurations
util,Enclosing class: SSLConfigurations public static class SSLConfigurations.SysProps extends Object
util,public class SSLConfigurationsFactory extends Object
util,public class SSLCredentialProviderFactory extends Object Class responsible to build SSL credential providers
util,Enclosing class: EnvSSLCredentialProvider public static class EnvSSLCredentialProvider.EnvVars extends Object
util,"postgresql-hll, and js-hll"
util,"A modified version of the 'HyperLogLog' data structure and algorithm is used, which combines both probabilistic and non-probabilistic techniques to improve the accuracy and storage requirements of the original algorithm. More specifically, initializing and storing a new HLL will allocate a sentinel value symbolizing the empty set (HLLType.EMPTY). After adding the first few values, a sorted list of unique integers is stored in a HLLType.EXPLICIT hash set. When configured, accuracy can be sacrificed for memory footprint: the values in the sorted list are ""promoted"" to a ""HLLType.SPARSE"" map-based HyperLogLog structure. Finally, when enough registers are set, the map-based HLL will be converted to a bit-packed ""HLLType.FULL"" HyperLogLog structure. This data structure is interoperable with the implementations found at: postgresql-hll, and js-hll when properly serialized."
util,"More specifically, initializing and storing a new HLL will allocate a sentinel value symbolizing the empty set (HLLType.EMPTY). After adding the first few values, a sorted list of unique integers is stored in a HLLType.EXPLICIT hash set. When configured, accuracy can be sacrificed for memory footprint: the values in the sorted list are ""promoted"" to a ""HLLType.SPARSE"" map-based HyperLogLog structure. Finally, when enough registers are set, the map-based HLL will be converted to a bit-packed ""HLLType.FULL"" HyperLogLog structure. This data structure is interoperable with the implementations found at: postgresql-hll, and js-hll when properly serialized."
util,"This data structure is interoperable with the implementations found at: postgresql-hll, and js-hll when properly serialized."
util,"postgresql-hll, and js-hll"
util,"Direct Known Subclasses: FieldTypePluginLoader, MapPluginLoader, NamedListPluginLoader public abstract class AbstractPluginLoader<T> extends Object An abstract super class that manages standard solr-style plugin configuration. Since: solr 1.3"
util,public class MapPluginLoader<T extends MapInitializedPlugin> extends AbstractPluginLoader<T> Since: solr 1.3
util,public class NamedListPluginLoader<T extends NamedListInitializedPlugin> extends AbstractPluginLoader<T> Since: solr 1.3
util,Fields inherited from classorg.apache.http.protocol.HttpRequestExecutor DEFAULT_WAIT_FOR_CONTINUE
util,"Methods inherited from classorg.apache.http.protocol.HttpRequestExecutor canResponseHaveBody, doReceiveResponse, doSendRequest, postProcess, preProcess"
util,Fields inherited from classorg.apache.http.protocol.HttpRequestExecutor DEFAULT_WAIT_FOR_CONTINUE
util,"Methods inherited from classorg.apache.http.impl.conn.PoolingHttpClientConnectionManager closeExpiredConnections, closeIdleConnections, connect, enumAvailable, enumLeased, finalize, getConnectionConfig, getDefaultConnectionConfig, getDefaultMaxPerRoute, getDefaultSocketConfig, getMaxPerRoute, getMaxTotal, getRoutes, getSocketConfig, getStats, getTotalStats, getValidateAfterInactivity, leaseConnection, releaseConnection, requestConnection, routeComplete, setConnectionConfig, setDefaultConnectionConfig, setDefaultMaxPerRoute, setDefaultSocketConfig, setMaxPerRoute, setMaxTotal, setSocketConfig, setValidateAfterInactivity, shutdown, upgrade"
util,public class MetricUtils extends Object Metrics specific utility functions.
util,"Methods inherited from interfacejava.lang.Iterable forEach, spliterator"
util,public class SimplePropagator extends Object
util,"Methods inherited from interfacejava.lang.Iterable forEach, spliterator"
util,"Methods inherited from interfacejava.lang.Iterable forEach, spliterator"
util,public class TraceUtils extends Object Utilities for distributed tracing.
util,public class ByteDenseVectorParser extends DenseVectorParser
util,"Direct Known Subclasses: ByteDenseVectorParser, FloatDenseVectorParser public abstract class DenseVectorParser extends Object"
util,public class FloatDenseVectorParser extends DenseVectorParser
