Module,Text
calib3d,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
calib3d,Abstract base class for 2D image feature detectors and descriptor extractors.
calib3d,"Template ""trait"" class for OpenCV primitive data types."
calib3d,"A primitive OpenCV data type is one of unsigned char, bool, signed char, unsigned short, signed short, int, float, double, or a tuple of values of one of these types, where all the values in the tuple have the same type. Any primitive type from the list can be defined by an identifier in the form CV_<bit-depth>{U|S|F}C(<number_of_channels>), for example: uchar CV_8UC1, 3-element floating-point tuple CV_32FC3, and so on. A universal OpenCV structure that is able to store a single instance of such a primitive data type is Vec. Multiple instances of such a type can be stored in a std::vector, Mat, Mat_, SparseMat, SparseMat_, or any other container that is able to store Vec instances."
calib3d,"The DataType class is basically used to provide a description of such primitive data types without adding any fields or methods to the corresponding classes (and it is actually impossible to add anything to primitive C/C++ data types). This technique is known in C++ as class traits. It is not DataType itself that is used but its specialized versions, such as:"
calib3d,"The main purpose of this class is to convert compilation-time type information to an OpenCV-compatible data type identifier, for example:"
calib3d,"So, such traits are used to tell OpenCV which data type you are working with, even if such a type is not native to OpenCV. For example, the matrix B initialization above is compiled because OpenCV defines the proper specialized template class DataType<complex<_Tp> > . This mechanism is also useful (and used in OpenCV this way) for generic algorithms implementations."
calib3d,opencv2/core/traits.hpp
calib3d,src source array dst destination array len length of arrays
calib3d,"Functions int hal_ni_invSqrt32f (const float *src, float *dst, int len)  int hal_ni_invSqrt64f (const double *src, double *dst, int len) "
calib3d,Base storage class for GPU memory with reference counting.
calib3d,Its interface matches the Mat interface with the following limitations:
calib3d,no arbitrary dimensions support (only 2D) no functions that return references to their data (because references on GPU are not valid for CPU) no expression templates technique support
calib3d,Beware that the latter limitation may lead to overloaded matrix operators that cause memory allocations. The GpuMat class is convertible to cuda::PtrStepSz and cuda::PtrStep so it can be passed directly to the kernel.
calib3d,"Some member functions are described as a ""Blocking Call"" while some are described as a ""Non-Blocking Call"". Blocking functions are synchronous to host. It is guaranteed that the GPU operation is finished when the function returns. However, non-blocking functions are asynchronous to host. Those functions may return even if the GPU operation is not finished."
calib3d,"Compared to their blocking counterpart, non-blocking functions accept Stream as an additional argument. If a non-default stream is passed, the GPU operation may overlap with operations in other streams."
calib3d,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
calib3d,Class for extracting blobs from an image. :
calib3d,The class implements a simple algorithm for extracting blobs from an image:
calib3d,This class performs several filtrations of returned blobs. You should set filterBy* to true/false to turn on/off corresponding filtration. Available filtrations:
calib3d,"By color. This filter compares the intensity of a binary image at the center of a blob to blobColor. If they differ, the blob is filtered out. Use blobColor = 0 to extract dark blobs and blobColor = 255 to extract light blobs. By area. Extracted blobs have an area between minArea (inclusive) and maxArea (exclusive). By circularity. Extracted blobs have circularity ( \(\frac{4*\pi*Area}{perimeter * perimeter}\)) between minCircularity (inclusive) and maxCircularity (exclusive). By ratio of the minimum inertia to maximum inertia. Extracted blobs have this ratio between minInertiaRatio (inclusive) and maxInertiaRatio (exclusive). By convexity. Extracted blobs have convexity (area / area of blob convex hull) between minConvexity (inclusive) and maxConvexity (exclusive)."
calib3d,Default values of parameters are tuned to extract dark circular blobs.
calib3d,These functions are provided for OpenCV-Eigen interoperability. They convert Mat objects to corresponding Eigen::Matrix objects and vice-versa. Consult the Eigen documentation for information about the Matrix template type.
calib3d,Namespaces namespace cv::traits 
calib3d,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
calib3d,This module includes photo processing algorithms
calib3d,"Matrix expression representation This is a list of implemented matrix operations that can be combined in arbitrary complex expressions (here A, B stand for matrices ( Mat ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double )):"
calib3d,"Addition, subtraction, negation: A+B, A-B, A+s, A-s, s+A, s-A, -A Scaling: A*alpha Per-element multiplication and division: A.mul(B), A/B, alpha/A Matrix multiplication: A*B Transposition: A.t() (means AT) Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems: A.inv([method]) (~ A<sup>-1</sup>), A.inv([method])*B (~ X: AX=B) Comparison: A cmpop B, A cmpop alpha, alpha cmpop A, where cmpop is one of >, >=, ==, !=, <=, <. The result of comparison is an 8-bit single channel mask whose elements are set to 255 (if the particular element or pair of elements satisfy the condition) or 0. Bitwise logical operations: A logicop B, A logicop s, s logicop A, ~A, where logicop is one of &, |, ^. Element-wise minimum and maximum: min(A, B), min(A, alpha), max(A, B), max(A, alpha) Element-wise absolute value: abs(A) Cross-product, dot-product: A.cross(B), A.dot(B) Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm, mean, sum, countNonZero, trace, determinant, repeat, and others. Matrix initializers ( Mat::eye(), Mat::zeros(), Mat::ones() ), matrix comma-separated initializers, matrix constructors and operators that extract sub-matrices (see Mat description). Mat_<destination_type>() constructors to cast the result to the proper type. NoteComma-separated initializers and probably some other operations may require additional explicit Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity. Here are examples of matrix expressions: // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD) SVD svd(A); Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t(); // compute the new vector of parameters in the Levenberg-Marquardt algorithm x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err); // sharpen image using ""unsharp mask"" algorithm Mat blurred; double sigma = 1, threshold = 5, amount = 1; GaussianBlur(img, blurred, Size(), sigma, sigma); Mat lowContrastMask = abs(img - blurred) < threshold; Mat sharpened = img*(1+amount) + blurred*(-amount); img.copyTo(sharpened, lowContrastMask); cv::MatExpr::absMatExpr abs(const Mat &m)Calculates an absolute value of each matrix element. cv::MatExpr::invMatExpr inv(int method=DECOMP_LU) const cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::Mat::diagMat diag(int d=0) constExtracts a diagonal from a matrix. cv::Mat::uUMatData * uinteraction with UMatDefinition mat.hpp:2174 cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Mat::tMatExpr t() constTransposes a matrix. cv::SVDSingular Value Decomposition.Definition core.hpp:2739 cv::DECOMP_CHOLESKY@ DECOMP_CHOLESKYDefinition base.hpp:143 cv::SizeSize2i SizeDefinition types.hpp:370 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. cv::thresholddouble threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)Applies a fixed-level threshold to each array element."
calib3d,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
calib3d,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
calib3d,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
calib3d,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
calib3d,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
calib3d,The implemented stitching pipeline is very similar to the one proposed in [41] .
calib3d,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
calib3d,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
calib3d,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
calib3d,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
calib3d,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
calib3d,Template class for a 4-element vector derived from Vec.
calib3d,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
calib3d,Random Number Generator.
calib3d,"Random number generator. It encapsulates the state (currently, a 64-bit integer) and has methods to return scalar random values and to fill arrays with random values. Currently it supports uniform and Gaussian (normal) distributions. The generator uses Multiply-With-Carry algorithm, introduced by G. Marsaglia ( http://en.wikipedia.org/wiki/Multiply-with-carry ). Gaussian-distribution random numbers are generated using the Ziggurat algorithm ( http://en.wikipedia.org/wiki/Ziggurat_algorithm ), introduced by G. Marsaglia and W. W. Tsang."
calib3d,The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows:
calib3d,"By default, the algorithm uses only 4 directions which are horizontal and vertical path instead of 8. Set mode=StereoSGM::MODE_HH in createStereoSGM to run the full variant of the algorithm. Mutual Information cost function is not implemented. Instead, Center-Symmetric Census Transform with \(9 \times 7\) window size from [250] is used for robustness."
calib3d,Matrix read-only iterator.
calib3d,The methods in this namespace use a so-called fisheye camera model.
calib3d,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
calib3d,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
calib3d,Bitwise AND: dst[i] = src1[i] & src2[i] Bitwise OR: dst[i] = src1[i] | src2[i] Bitwise XOR: dst[i] = src1[i] ^ src2[i] Bitwise NOT: dst[i] = !src[i]
calib3d,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
calib3d,"Functions int hal_ni_and8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_not8u (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_or8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_xor8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height) "
calib3d,This module includes signal processing algorithms.
calib3d,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
calib3d,Data structure for salient point detectors.
calib3d,"The class instance stores a keypoint, i.e. a point feature found by one of many available keypoint detectors, such as Harris corner detector, FAST, StarDetector, SURF, SIFT etc."
calib3d,"The keypoint is characterized by the 2D position, scale (proportional to the diameter of the neighborhood that needs to be taken into account), orientation and some other parameters. The keypoint neighborhood is then analyzed by another algorithm that builds a descriptor (usually represented as a feature vector). The keypoints representing the same object in different images can then be matched using KDTree or another method."
calib3d,ICP point-to-plane odometry algorithm
calib3d,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
calib3d,Template class for 2D rectangles.
calib3d,described by the following parameters:
calib3d,"Coordinates of the top-left corner. This is a default interpretation of Rect_::x and Rect_::y in OpenCV. Though, in your algorithms you may count x and y from the bottom-left corner. Rectangle width and height."
calib3d,"OpenCV typically assumes that the top and left boundary of the rectangle are inclusive, while the right and bottom boundaries are not. For example, the method Rect_::contains returns true if"
calib3d,"\[x \leq pt.x < x+width, y \leq pt.y < y+height\]"
calib3d,Virtually every loop over an image ROI in OpenCV (where ROI is specified by Rect_<int> ) is implemented as:
calib3d,"In addition to the class members, the following operations on rectangles are implemented:"
calib3d,"\(\texttt{rect} = \texttt{rect} \pm \texttt{point}\) (shifting a rectangle by a certain offset) \(\texttt{rect} = \texttt{rect} \pm \texttt{size}\) (expanding or shrinking a rectangle by a certain amount) rect += point, rect -= point, rect += size, rect -= size (augmenting operations) rect = rect1 & rect2 (rectangle intersection) rect = rect1 | rect2 (minimum area rectangle containing rect1 and rect2 ) rect &= rect1, rect |= rect1 (and the corresponding augmenting operations) rect == rect1, rect != rect1 (rectangle comparison)"
calib3d,This is an example how the partial ordering on rectangles can be established (rect1 \(\subseteq\) rect2):
calib3d,"For your convenience, the Rect_<> alias is available: cv::Rect"
calib3d,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
calib3d,The base class for stereo correspondence algorithms.
calib3d,YOUR ATTENTION PLEASE!
calib3d,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
calib3d,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
calib3d,Note for developers: please don't put videoio dependency in G-API because of this file.
calib3d,Dense optical flow algorithms compute motion for each point:
calib3d,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
calib3d,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
calib3d,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
calib3d,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
calib3d,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
calib3d,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
calib3d,The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows:
calib3d,"By default, the algorithm is single-pass, which means that you consider only 5 directions instead of 8. Set mode=StereoSGBM::MODE_HH in createStereoSGBM to run the full variant of the algorithm but beware that it may consume a lot of memory. The algorithm matches blocks, not individual pixels. Though, setting blockSize=1 reduces the blocks to single pixels. Mutual information cost function is not implemented. Instead, a simpler Birchfield-Tomasi sub-pixel metric from [28] is used. Though, the color images are supported as well. Some pre- and post- processing steps from K. Konolige algorithm StereoBM are included, for example: pre-filtering (StereoBM::PREFILTER_XSOBEL type) and post-filtering (uniqueness check, quadratic interpolation and speckle filtering)."
calib3d,(Python) An example illustrating the use of the StereoSGBM matching algorithm can be found at opencv_source_code/samples/python/stereo_match.py
calib3d,This type is very similar to InputArray except that it is used for input/output and output function parameters.
calib3d,"Just like with InputArray, OpenCV users should not care about OutputArray, they just pass Mat, vector<T> etc. to the functions. The same limitation as for InputArray: Do not explicitly create OutputArray instances applies here too."
calib3d,"If you want to make your function polymorphic (i.e. accept different arrays as output parameters), it is also not very difficult. Take the sample above as the reference. Note that _OutputArray::create() needs to be called before _OutputArray::getMat(). This way you guarantee that the output array is properly allocated."
calib3d,"Optional output parameters. If you do not need certain output array to be computed and returned to you, pass cv::noArray(), just like you would in the case of optional input array. At the implementation level, use _OutputArray::needed() to check if certain output array needs to be computed or not."
calib3d,There are several synonyms for OutputArray that are used to assist automatic Python/Java/... wrapper generators:
calib3d,The function performs generalized matrix multiplication similar to the gemm functions in BLAS level 3: \(D = \alpha*AB+\beta*C\)
calib3d,"src1 pointer to input \(M\times N\) matrix \(A\) or \(A^T\) stored in row major order. src1_step number of bytes between two consequent rows of matrix \(A\) or \(A^T\). src2 pointer to input \(N\times K\) matrix \(B\) or \(B^T\) stored in row major order. src2_step number of bytes between two consequent rows of matrix \(B\) or \(B^T\). alpha \(\alpha\) multiplier before \(AB\) src3 pointer to input \(M\times K\) matrix \(C\) or \(C^T\) stored in row major order. src3_step number of bytes between two consequent rows of matrix \(C\) or \(C^T\). beta \(\beta\) multiplier before \(C\) dst pointer to input \(M\times K\) matrix \(D\) stored in row major order. dst_step number of bytes between two consequent rows of matrix \(D\). m number of rows in matrix \(A\) or \(A^T\), equals to number of rows in matrix \(D\) n number of columns in matrix \(A\) or \(A^T\) k number of columns in matrix \(B\) or \(B^T\), equals to number of columns in matrix \(D\) flags algorithm options (combination of CV_HAL_GEMM_1_T, ...)."
calib3d,"Functions int hal_ni_gemm32f (const float *src1, size_t src1_step, const float *src2, size_t src2_step, float alpha, const float *src3, size_t src3_step, float beta, float *dst, size_t dst_step, int m, int n, int k, int flags)  int hal_ni_gemm32fc (const float *src1, size_t src1_step, const float *src2, size_t src2_step, float alpha, const float *src3, size_t src3_step, float beta, float *dst, size_t dst_step, int m, int n, int k, int flags)  int hal_ni_gemm64f (const double *src1, size_t src1_step, const double *src2, size_t src2_step, double alpha, const double *src3, size_t src3_step, double beta, double *dst, size_t dst_step, int m, int n, int k, int flags)  int hal_ni_gemm64fc (const double *src1, size_t src1_step, const double *src2, size_t src2_step, double alpha, const double *src3, size_t src3_step, double beta, double *dst, size_t dst_step, int m, int n, int k, int flags) "
calib3d,src source array dst destination array len length of arrays
calib3d,"Functions int hal_ni_log32f (const float *src, float *dst, int len)  int hal_ni_log64f (const double *src, double *dst, int len) "
calib3d,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
calib3d,Template Read-Write Sparse Matrix Iterator Class.
calib3d,This is the derived from cv::SparseMatConstIterator_ class that introduces more convenient operator *() for accessing the current element.
calib3d,Maximally stable extremal region extractor.
calib3d,The class encapsulates all the parameters of the MSER extraction algorithm (see wiki article).
calib3d,"there are two different implementation of MSER: one for grey image, one for color image the grey image algorithm is taken from: [208] ; the paper claims to be faster than union-find method; it actually get 1.5~2m/s on my centrino L7200 1.2GHz laptop. the color image algorithm is taken from: [94] ; it should be much slower than grey image method ( 3~4 times ) (Python) A complete example showing the use of the MSER detector can be found at samples/python/mser.py"
calib3d,This is a base class for all more or less complex algorithms in OpenCV.
calib3d,"especially for classes of algorithms, for which there can be multiple implementations. The examples are stereo correspondence (for which there are algorithms like block matching, semi-global block matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck etc.)."
calib3d,Here is example of SimpleBlobDetector use in your application via Algorithm interface:
calib3d,"Class implementing the KAZE keypoint detector and descriptor extractor, described in [9] ."
calib3d,n-dimensional dense array class
calib3d,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
calib3d,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
calib3d,"In case of a 2-dimensional array, the above formula is reduced to:"
calib3d,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
calib3d,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
calib3d,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
calib3d,There are many different ways to create a Mat object. The most popular options are listed below:
calib3d,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
calib3d,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
calib3d,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
calib3d,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
calib3d,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
calib3d,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
calib3d,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
calib3d,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
calib3d,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
calib3d,"Levenberg-Marquardt solver. Starting with the specified vector of parameters it optimizes the target vector criteria ""err"" (finds local minima of each target vector component absolute value)."
calib3d,"When needed, it calls user-provided callback."
calib3d,Principal Component Analysis.
calib3d,"The class is used to calculate a special basis for a set of vectors. The basis will consist of eigenvectors of the covariance matrix calculated from the input set of vectors. The class PCA can also transform vectors to/from the new coordinate space defined by the basis. Usually, in this new coordinate system, each vector from the original set (and any linear combination of such vectors) can be quite accurately approximated by taking its first few components, corresponding to the eigenvectors of the largest eigenvalues of the covariance matrix. Geometrically it means that you calculate a projection of the vector to a subspace formed by a few eigenvectors corresponding to the dominant eigenvalues of the covariance matrix. And usually such a projection is very close to the original vector. So, you can represent the original vector from a high-dimensional space with a much shorter vector consisting of the projected vector's coordinates in the subspace. Such a transformation is also known as Karhunen-Loeve Transform, or KLT. See http://en.wikipedia.org/wiki/Principal_component_analysis"
calib3d,"The sample below is the function that takes two matrices. The first function stores a set of vectors (a row per vector) that is used to calculate PCA. The second function stores another ""test"" set of vectors (a row per vector). First, these vectors are compressed with PCA, then reconstructed back, and then the reconstruction error norm is computed and printed for each vector. :"
calib3d,Namespaces namespace cv::traits 
calib3d,Namespaces namespace cv::omnidir::internal 
calib3d,This is the proxy class for passing read-only input arrays into OpenCV functions.
calib3d,It is defined as:
calib3d,"where _InputArray is a class that can be constructed from Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat_<T> >, UMat, std::vector<UMat> or double. It can also be constructed from a matrix expression."
calib3d,"Since this is mostly implementation-level class, and its interface may change in future versions, we do not describe it in details. There are a few key things, though, that should be kept in mind:"
calib3d,"When you see in the reference manual or in OpenCV source code a function that takes InputArray, it means that you can actually pass Mat, Matx, vector<T> etc. (see above the complete list). Optional input arguments: If some of the input arrays may be empty, pass cv::noArray() (or simply cv::Mat() as you probably did before). The class is designed solely for passing parameters. That is, normally you should not declare class members, local and global variables of this type. If you want to design your own function or a class method that can operate of arrays of multiple types, you can use InputArray (or OutputArray) for the respective parameters. Inside a function you should use _InputArray::getMat() method to construct a matrix header for the array (without copying data). _InputArray::kind() can be used to distinguish Mat from vector<> etc., but normally it is not needed."
calib3d,Here is how you can use a function that takes InputArray :
calib3d,"That is, we form an STL vector containing points, and apply in-place affine transformation to the vector using the 2x3 matrix created inline as Matx<float, 2, 3> instance."
calib3d,"Here is how such a function can be implemented (for simplicity, we implement a very specific case of it, according to the assertion statement inside) :"
calib3d,"There is another related type, InputArrayOfArrays, which is currently defined as a synonym for InputArray:"
calib3d,"It denotes function arguments that are either vectors of vectors or vectors of matrices. A separate synonym is needed to generate Python/Java etc. wrappers properly. At the function implementation level their use is similar, but _InputArray::getMat(idx) should be used to get header for the idx-th component of the outer vector and _InputArray::size().area() should be used to find the number of components (vectors/matrices) of the outer vector."
calib3d,"In general, type support is limited to cv::Mat types. Other types are forbidden. But in some cases we need to support passing of custom non-general Mat types, like arrays of cv::KeyPoint, cv::DMatch, etc. This data is not intended to be interpreted as an image data, or processed somehow like regular cv::Mat. To pass such custom type use rawIn() / rawOut() / rawInOut() wrappers. Custom type is wrapped as Mat-compatible CV_8UC<N> values (N = sizeof(T), N <= CV_CN_MAX)."
calib3d,Information Flow algorithm implementaton for alphamatting
calib3d,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
calib3d,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
calib3d,The implementation is based on [7].
calib3d,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
calib3d,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
calib3d,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
calib3d,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
calib3d,Lookup table replacement Table consists of 256 elements of a size from 1 to 8 bytes having 1 channel or src_channels For 8s input type 128 is added to LUT index Destination should have the same element type and number of channels as lookup table elements
calib3d,src_data Source image data src_step Source image step src_type Sorce image type lut_data Pointer to lookup table lut_channel_size Size of each channel in bytes lut_channels Number of channels in lookup table dst_data Destination data dst_step Destination step width Width of images height Height of images
calib3d,"Functions int hal_ni_lut (const uchar *src_data, size_t src_step, size_t src_type, const uchar *lut_data, size_t lut_channel_size, size_t lut_channels, uchar *dst_data, size_t dst_step, int width, int height) "
calib3d,"Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as Mat's). It means that for each pixel location \((x,y)\) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location \((x,y)\). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one."
calib3d,"Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian \(3 \times 3\) filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (""replicated border"" extrapolation method), or assume that all the non-existing pixels are zeros (""constant border"" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see BorderTypes"
calib3d,src source array dst destination array len length of arrays
calib3d,"Functions int hal_ni_exp32f (const float *src, float *dst, int len)  int hal_ni_exp64f (const double *src, double *dst, int len) "
calib3d,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
calib3d,Namespace for all functions is cv::img_hash.
calib3d,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
calib3d,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
calib3d,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
calib3d,Class for matching keypoint descriptors.
calib3d,"query descriptor index, train descriptor index, train image index, and distance between descriptors."
calib3d,Computes reciprocial: dst[i] = scale / src[i]
calib3d,src_data source image data src_step source image step dst_data destination image data dst_step destination image step width width of the images height height of the images scale additional multiplier
calib3d,"Functions int hal_ni_recip16s (const short *src_data, size_t src_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip16u (const ushort *src_data, size_t src_step, ushort *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip32f (const float *src_data, size_t src_step, float *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip32s (const int *src_data, size_t src_step, int *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip64f (const double *src_data, size_t src_step, double *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip8s (const schar *src_data, size_t src_step, schar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip8u (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, double scale) "
calib3d,This modules is to draw UTF-8 strings with freetype/harfbuzz.
calib3d,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
calib3d,Classes class cv::freetype::FreeType2 
calib3d,Matrix read-write iterator.
calib3d,Performs \(LU\) decomposition of square matrix \(A=P*L*U\) (where \(P\) is permutation matrix) and solves matrix equation \(A*X=B\). Function returns the \(sign\) of permutation \(P\) via parameter info.
calib3d,"src1 pointer to input matrix \(A\) stored in row major order. After finish of work src1 contains at least \(U\) part of \(LU\) decomposition which is appropriate for determainant calculation: \(det(A)=sign*\prod_{j=1}^{M}a_{jj}\). src1_step number of bytes between two consequent rows of matrix \(A\). m size of square matrix \(A\). src2 pointer to \(M\times N\) matrix \(B\) which is the right-hand side of system \(A*X=B\). \(B\) stored in row major order. If src2 is null pointer only \(LU\) decomposition will be performed. After finish of work src2 contains solution \(X\) of system \(A*X=B\). src2_step number of bytes between two consequent rows of matrix \(B\). n number of right-hand vectors in \(M\times N\) matrix \(B\). info indicates success of decomposition. If *info is equals to zero decomposition failed, othervise *info is equals to \(sign\)."
calib3d,"Functions int hal_ni_LU32f (float *src1, size_t src1_step, int m, float *src2, size_t src2_step, int n, int *info)  int hal_ni_LU64f (double *src1, size_t src1_step, int m, double *src2, size_t src2_step, int n, int *info) "
calib3d,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
calib3d,"Classes struct cv::Accumulator< T >  struct cv::Accumulator< char >  struct cv::Accumulator< short >  struct cv::Accumulator< unsigned char >  struct cv::Accumulator< unsigned short >  class cv::AffineFeature  Class for implementing the wrapper which makes detectors and extractors to be affine invariant, described as ASIFT in [312] . More...  class cv::AgastFeatureDetector  Wrapping class for feature detection using the AGAST method. : More...  class cv::AKAZE  Class implementing the AKAZE keypoint detector and descriptor extractor, described in [10]. More...  class cv::BRISK  Class implementing the BRISK keypoint detector and descriptor extractor, described in [159] . More...  class cv::FastFeatureDetector  Wrapping class for feature detection using the FAST method. : More...  class cv::Feature2D  Abstract base class for 2D image feature detectors and descriptor extractors. More...  class cv::GFTTDetector  Wrapping class for feature detection using the goodFeaturesToTrack function. : More...  class cv::KAZE  Class implementing the KAZE keypoint detector and descriptor extractor, described in [9] . More...  class cv::KeyPointsFilter  A class filters a vector of keypoints. More...  struct cv::L1< T >  struct cv::L2< T >  class cv::MSER  Maximally stable extremal region extractor. More...  class cv::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More...  class cv::SIFT  Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe [174] . More...  class cv::SimpleBlobDetector  Class for extracting blobs from an image. : More...  struct cv::SL2< T > "
calib3d,Read-write Sparse Matrix Iterator.
calib3d,"The class is similar to cv::SparseMatConstIterator, but can be used for in-place modification of the matrix elements."
calib3d,src source array dst destination array len length of arrays
calib3d,"Functions int hal_ni_sqrt32f (const float *src, float *dst, int len)  int hal_ni_sqrt64f (const double *src, double *dst, int len) "
calib3d,Namespaces namespace NcvCTprep 
calib3d,n-ary multi-dimensional array iterator.
calib3d,"Use the class to implement unary, binary, and, generally, n-ary element-wise operations on multi-dimensional arrays. Some of the arguments of an n-ary function may be continuous arrays, some may be not. It is possible to use conventional MatIterator 's for each array but incrementing all of the iterators after each small operations may be a big overhead. In this case consider using NAryMatIterator to iterate through several matrices simultaneously as long as they have the same geometry (dimensionality and all the dimension sizes are the same). On each iteration it.planes[0], it.planes[1],... will be the slices of the corresponding matrices."
calib3d,The example below illustrates how you can compute a normalized and threshold 3D color histogram:
calib3d,Absolute difference: dst[i] = | src1[i] - src2[i] |
calib3d,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
calib3d,"Functions int hal_ni_absdiff16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height) "
calib3d,"x source X arrays y source Y arrays mag destination magnitude array angle destination angle array len length of arrays angleInDegrees if set to true return angles in degrees, otherwise in radians y source Y arrays x source X arrays dst destination array len length of arrays angleInDegrees if set to true return angles in degrees, otherwise in radians mag source magnitude arrays mag source angle arrays x destination X array y destination Y array len length of arrays angleInDegrees if set to true interpret angles from degrees, otherwise from radians"
calib3d,"Functions int hal_ni_cartToPolar32f (const float *x, const float *y, float *mag, float *angle, int len, bool angleInDegrees)  int hal_ni_cartToPolar64f (const double *x, const double *y, double *mag, double *angle, int len, bool angleInDegrees)  int hal_ni_fastAtan32f (const float *y, const float *x, float *dst, int len, bool angleInDegrees)  int hal_ni_fastAtan64f (const double *y, const double *x, double *dst, int len, bool angleInDegrees)  int hal_ni_polarToCart32f (const float *mag, const float *angle, float *x, float *y, int len, bool angleInDegrees)  int hal_ni_polarToCart64f (const double *mag, const double *angle, double *x, double *y, int len, bool angleInDegrees) "
calib3d,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
calib3d,This module contains:
calib3d,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
calib3d,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
calib3d,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
calib3d,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
calib3d,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
calib3d,Template class for specifying the size of an image or rectangle.
calib3d,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
calib3d,OpenCV defines the following Size_<> aliases:
calib3d,"Drawing functions work with matrices/images of arbitrary depth. The boundaries of the shapes can be rendered with antialiasing (implemented only for 8-bit images for now). All the functions include the parameter color that uses an RGB value (that may be constructed with the Scalar constructor ) for color images and brightness for grayscale images. For color images, the channel ordering is normally Blue, Green, Red. This is what imshow, imread, and imwrite expect. So, if you form a color using the Scalar constructor, it should look like:"
calib3d,"\[\texttt{Scalar} (blue \_ component, green \_ component, red \_ component[, alpha \_ component])\]"
calib3d,"If you are using your own image rendering and I/O functions, you can use any channel ordering. The drawing functions process each channel independently and do not depend on the channel order or even on the used color space. The whole image can be converted from BGR to RGB or to a different color space using cvtColor ."
calib3d,"If a drawn figure is partially or completely outside the image, the drawing functions clip it. Also, many drawing functions can handle pixel coordinates specified with sub-pixel accuracy. This means that the coordinates can be passed as fixed-point numbers encoded as integers. The number of fractional bits is specified by the shift parameter and the real point coordinates are calculated as \(\texttt{Point}(x,y)\rightarrow\texttt{Point2f}(x*2^{-shift},y*2^{-shift})\) . This feature is especially effective when rendering antialiased shapes."
calib3d,Classes class cv::LineIterator  Class for iterating over all pixels on a raster line segment. More... 
calib3d,Comma-separated Matrix Initializer.
calib3d,"The class instances are usually not created explicitly. Instead, they are created on ""matrix << firstValue"" operator."
calib3d,The sample below initializes 2x2 rotation matrix:
calib3d,Compare: dst[i] = src1[i] op src2[i]
calib3d,"src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images operation one of (CV_HAL_CMP_EQ, CV_HAL_CMP_GT, ...)"
calib3d,"Functions int hal_ni_cmp16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation) "
calib3d,Base class for Contrast Limited Adaptive Histogram Equalization.
calib3d,Classes class cv::CLAHE  Base class for Contrast Limited Adaptive Histogram Equalization. More... 
calib3d,"Template class for short numerical vectors, a partial case of Matx."
calib3d,"This template class represents short numerical vectors (of 1, 2, 3, 4 ... elements) on which you can perform basic arithmetical operations, access individual elements using [] operator etc. The vectors are allocated on stack, as opposite to std::valarray, std::vector, cv::Mat etc., which elements are dynamically allocated in the heap."
calib3d,The template takes 2 parameters:
calib3d,_Tp element type cn the number of elements
calib3d,"In addition to the universal notation like Vec<float, 3>, you can use shorter aliases for the most popular specialized variants of Vec, e.g. Vec3f ~ Vec<float, 3>."
calib3d,"It is possible to convert Vec<T,2> to/from Point_, Vec<T,3> to/from Point3_ , and Vec<T,4> to CvScalar or Scalar_. Use operator[] to access the elements of Vec."
calib3d,All the expected vector operations are also implemented:
calib3d,"v1 = v2 + v3 v1 = v2 - v3 v1 = v2 * scale v1 = scale * v2 v1 = -v2 v1 += v2 and other augmenting operations v1 == v2, v1 != v2 norm(v1) (euclidean norm) The Vec class is commonly used to describe pixel types of multi-channel arrays. See Mat for details."
calib3d,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
calib3d,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
calib3d,A helper class for cv::DataType.
calib3d,The class is specialized for each fundamental numerical data type supported by OpenCV. It provides DataDepth<T>::value constant.
calib3d,Wrapping class for feature detection using the AGAST method. :
calib3d,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
calib3d,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
calib3d,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
calib3d,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
calib3d,"Functions void cv::julia::initJulia (int argc, char **argv) "
calib3d,Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe [174] .
calib3d,Template sparse n-dimensional array class derived from SparseMat.
calib3d,SparseMat_ is a thin wrapper on top of SparseMat created in the same way as Mat_ . It simplifies notation of some operations:
calib3d,x source X array y source Y array dst destination array len length of arrays
calib3d,"Functions int hal_ni_magnitude32f (const float *x, const float *y, float *dst, int len)  int hal_ni_magnitude64f (const double *x, const double *y, double *dst, int len) "
calib3d,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
calib3d,"SoftFloat is a software implementation of floating-point calculations according to IEEE 754 standard. All calculations are done in integers, that's why they are machine-independent and bit-exact. This library can be useful in accuracy-critical parts like look-up tables generation, tests, etc. OpenCV contains a subset of SoftFloat partially rewritten to C++."
calib3d,"Functions void cv::accumulate (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds an image to the accumulator image.  void cv::accumulateProduct (InputArray src1, InputArray src2, InputOutputArray dst, InputArray mask=noArray())  Adds the per-element product of two input images to the accumulator image.  void cv::accumulateSquare (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds the square of a source image to the accumulator image.  void cv::accumulateWeighted (InputArray src, InputOutputArray dst, double alpha, InputArray mask=noArray())  Updates a running average.  void cv::createHanningWindow (OutputArray dst, Size winSize, int type)  This function computes a Hanning window coefficients in two dimensions.  void cv::divSpectrums (InputArray a, InputArray b, OutputArray c, int flags, bool conjB=false)  Performs the per-element division of the first Fourier spectrum by the second Fourier spectrum.  Point2d cv::phaseCorrelate (InputArray src1, InputArray src2, InputArray window=noArray(), double *response=0)  The function is used to detect translational shifts that occur between two images. "
calib3d,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
calib3d,It provides easy interface to:
calib3d,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
calib3d,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
calib3d,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
calib3d,It is planned to have:
calib3d,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
calib3d,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
calib3d,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
calib3d,Classes class cv::DualQuat< _Tp >  class cv::Quat< _Tp >  class cv::QuatEnum 
calib3d,Performs Cholesky decomposition of matrix \(A = L*L^T\) and solves matrix equation \(A*X=B\).
calib3d,src1 pointer to input matrix \(A\) stored in row major order. After finish of work src1 contains lower triangular matrix \(L\). src1_step number of bytes between two consequent rows of matrix \(A\). m size of square matrix \(A\). src2 pointer to \(M\times N\) matrix \(B\) which is the right-hand side of system \(A*X=B\). B stored in row major order. If src2 is null pointer only Cholesky decomposition will be performed. After finish of work src2 contains solution \(X\) of system \(A*X=B\). src2_step number of bytes between two consequent rows of matrix \(B\). n number of right-hand vectors in \(M\times N\) matrix \(B\). info indicates success of decomposition. If *info is false decomposition failed.
calib3d,"Functions int hal_ni_Cholesky32f (float *src1, size_t src1_step, int m, float *src2, size_t src2_step, int n, bool *info)  int hal_ni_Cholesky64f (double *src1, size_t src1_step, int m, double *src2, size_t src2_step, int n, bool *info) "
calib3d,"Class implementing the AKAZE keypoint detector and descriptor extractor, described in [10]."
calib3d,AKAZE descriptors can only be used with KAZE or AKAZE keypoints. This class is thread-safe.
calib3d,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
calib3d,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
calib3d,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
calib3d,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
calib3d,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
calib3d,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
calib3d,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
calib3d,"Enumerations enum cv::ImreadModes { cv::IMREAD_UNCHANGED = -1 , cv::IMREAD_GRAYSCALE = 0 , cv::IMREAD_COLOR_BGR = 1 , cv::IMREAD_COLOR = 1 , cv::IMREAD_ANYDEPTH = 2 , cv::IMREAD_ANYCOLOR = 4 , cv::IMREAD_LOAD_GDAL = 8 , cv::IMREAD_REDUCED_GRAYSCALE_2 = 16 , cv::IMREAD_REDUCED_COLOR_2 = 17 , cv::IMREAD_REDUCED_GRAYSCALE_4 = 32 , cv::IMREAD_REDUCED_COLOR_4 = 33 , cv::IMREAD_REDUCED_GRAYSCALE_8 = 64 , cv::IMREAD_REDUCED_COLOR_8 = 65 , cv::IMREAD_IGNORE_ORIENTATION = 128 , cv::IMREAD_COLOR_RGB = 256 }  Imread flags. More...  enum cv::ImwriteEXRCompressionFlags { cv::IMWRITE_EXR_COMPRESSION_NO = 0 , cv::IMWRITE_EXR_COMPRESSION_RLE = 1 , cv::IMWRITE_EXR_COMPRESSION_ZIPS = 2 , cv::IMWRITE_EXR_COMPRESSION_ZIP = 3 , cv::IMWRITE_EXR_COMPRESSION_PIZ = 4 , cv::IMWRITE_EXR_COMPRESSION_PXR24 = 5 , cv::IMWRITE_EXR_COMPRESSION_B44 = 6 , cv::IMWRITE_EXR_COMPRESSION_B44A = 7 , cv::IMWRITE_EXR_COMPRESSION_DWAA = 8 , cv::IMWRITE_EXR_COMPRESSION_DWAB = 9 }  enum cv::ImwriteEXRTypeFlags { cv::IMWRITE_EXR_TYPE_HALF = 1 , cv::IMWRITE_EXR_TYPE_FLOAT = 2 }  enum cv::ImwriteFlags { cv::IMWRITE_JPEG_QUALITY = 1 , cv::IMWRITE_JPEG_PROGRESSIVE = 2 , cv::IMWRITE_JPEG_OPTIMIZE = 3 , cv::IMWRITE_JPEG_RST_INTERVAL = 4 , cv::IMWRITE_JPEG_LUMA_QUALITY = 5 , cv::IMWRITE_JPEG_CHROMA_QUALITY = 6 , cv::IMWRITE_JPEG_SAMPLING_FACTOR = 7 , cv::IMWRITE_PNG_COMPRESSION = 16 , cv::IMWRITE_PNG_STRATEGY = 17 , cv::IMWRITE_PNG_BILEVEL = 18 , cv::IMWRITE_PXM_BINARY = 32 , cv::IMWRITE_EXR_TYPE = (3 << 4) + 0 , cv::IMWRITE_EXR_COMPRESSION = (3 << 4) + 1 , cv::IMWRITE_EXR_DWA_COMPRESSION_LEVEL = (3 << 4) + 2 , cv::IMWRITE_WEBP_QUALITY = 64 , cv::IMWRITE_HDR_COMPRESSION = (5 << 4) + 0 , cv::IMWRITE_PAM_TUPLETYPE = 128 , cv::IMWRITE_TIFF_RESUNIT = 256 , cv::IMWRITE_TIFF_XDPI = 257 , cv::IMWRITE_TIFF_YDPI = 258 , cv::IMWRITE_TIFF_COMPRESSION = 259 , cv::IMWRITE_TIFF_ROWSPERSTRIP = 278 , cv::IMWRITE_TIFF_PREDICTOR = 317 , cv::IMWRITE_JPEG2000_COMPRESSION_X1000 = 272 , cv::IMWRITE_AVIF_QUALITY = 512 , cv::IMWRITE_AVIF_DEPTH = 513 , cv::IMWRITE_AVIF_SPEED = 514 }  Imwrite flags. More...  enum cv::ImwriteHDRCompressionFlags { cv::IMWRITE_HDR_COMPRESSION_NONE = 0 , cv::IMWRITE_HDR_COMPRESSION_RLE = 1 }  Imwrite HDR specific values for IMWRITE_HDR_COMPRESSION parameter key. More...  enum cv::ImwriteJPEGSamplingFactorParams { cv::IMWRITE_JPEG_SAMPLING_FACTOR_411 = 0x411111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_420 = 0x221111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_422 = 0x211111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_440 = 0x121111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_444 = 0x111111 }  enum cv::ImwritePAMFlags { cv::IMWRITE_PAM_FORMAT_NULL = 0 , cv::IMWRITE_PAM_FORMAT_BLACKANDWHITE = 1 , cv::IMWRITE_PAM_FORMAT_GRAYSCALE = 2 , cv::IMWRITE_PAM_FORMAT_GRAYSCALE_ALPHA = 3 , cv::IMWRITE_PAM_FORMAT_RGB = 4 , cv::IMWRITE_PAM_FORMAT_RGB_ALPHA = 5 }  Imwrite PAM specific tupletype flags used to define the 'TUPLETYPE' field of a PAM file. More...  enum cv::ImwritePNGFlags { cv::IMWRITE_PNG_STRATEGY_DEFAULT = 0 , cv::IMWRITE_PNG_STRATEGY_FILTERED = 1 , cv::IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY = 2 , cv::IMWRITE_PNG_STRATEGY_RLE = 3 , cv::IMWRITE_PNG_STRATEGY_FIXED = 4 }  Imwrite PNG specific flags used to tune the compression algorithm. More...  enum cv::ImwriteTiffCompressionFlags { cv::IMWRITE_TIFF_COMPRESSION_NONE = 1 , cv::IMWRITE_TIFF_COMPRESSION_CCITTRLE = 2 , cv::IMWRITE_TIFF_COMPRESSION_CCITTFAX3 = 3 , cv::IMWRITE_TIFF_COMPRESSION_CCITT_T4 = 3 , cv::IMWRITE_TIFF_COMPRESSION_CCITTFAX4 = 4 , cv::IMWRITE_TIFF_COMPRESSION_CCITT_T6 = 4 , cv::IMWRITE_TIFF_COMPRESSION_LZW = 5 , cv::IMWRITE_TIFF_COMPRESSION_OJPEG = 6 , cv::IMWRITE_TIFF_COMPRESSION_JPEG = 7 , cv::IMWRITE_TIFF_COMPRESSION_T85 = 9 , cv::IMWRITE_TIFF_COMPRESSION_T43 = 10 , cv::IMWRITE_TIFF_COMPRESSION_NEXT = 32766 , cv::IMWRITE_TIFF_COMPRESSION_CCITTRLEW = 32771 , cv::IMWRITE_TIFF_COMPRESSION_PACKBITS = 32773 , cv::IMWRITE_TIFF_COMPRESSION_THUNDERSCAN = 32809 , cv::IMWRITE_TIFF_COMPRESSION_IT8CTPAD = 32895 , cv::IMWRITE_TIFF_COMPRESSION_IT8LW = 32896 , cv::IMWRITE_TIFF_COMPRESSION_IT8MP = 32897 , cv::IMWRITE_TIFF_COMPRESSION_IT8BL = 32898 , cv::IMWRITE_TIFF_COMPRESSION_PIXARFILM = 32908 , cv::IMWRITE_TIFF_COMPRESSION_PIXARLOG = 32909 , cv::IMWRITE_TIFF_COMPRESSION_DEFLATE = 32946 , cv::IMWRITE_TIFF_COMPRESSION_ADOBE_DEFLATE = 8 , cv::IMWRITE_TIFF_COMPRESSION_DCS = 32947 , cv::IMWRITE_TIFF_COMPRESSION_JBIG = 34661 , cv::IMWRITE_TIFF_COMPRESSION_SGILOG = 34676 , cv::IMWRITE_TIFF_COMPRESSION_SGILOG24 = 34677 , cv::IMWRITE_TIFF_COMPRESSION_JP2000 = 34712 , cv::IMWRITE_TIFF_COMPRESSION_LERC = 34887 , cv::IMWRITE_TIFF_COMPRESSION_LZMA = 34925 , cv::IMWRITE_TIFF_COMPRESSION_ZSTD = 50000 , cv::IMWRITE_TIFF_COMPRESSION_WEBP = 50001 , cv::IMWRITE_TIFF_COMPRESSION_JXL = 50002 }  enum cv::ImwriteTiffPredictorFlags { cv::IMWRITE_TIFF_PREDICTOR_NONE = 1 , cv::IMWRITE_TIFF_PREDICTOR_HORIZONTAL = 2 , cv::IMWRITE_TIFF_PREDICTOR_FLOATINGPOINT = 3 } "
calib3d,Add: dst[i] = src1[i] + src2[i] Sub: dst[i] = src1[i] - src2[i]
calib3d,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
calib3d,"Functions int hal_ni_add16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_add16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_add32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_add32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_add64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_add8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_add8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8s32f (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8u32f (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height) "
calib3d,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
calib3d,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
calib3d,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
calib3d,This module has been originally developed as a project for Google Summer of Code 2012-2015.
calib3d,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
calib3d,STL namespace.
calib3d,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
calib3d,The distortion-free projective transformation given by a pinhole camera model is shown below.
calib3d,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
calib3d,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
calib3d,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
calib3d,\[p = A P_c.\]
calib3d,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
calib3d,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
calib3d,and thus
calib3d,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
calib3d,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
calib3d,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
calib3d,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
calib3d,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
calib3d,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
calib3d,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
calib3d,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
calib3d,and therefore
calib3d,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
calib3d,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
calib3d,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
calib3d,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
calib3d,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
calib3d,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
calib3d,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
calib3d,with
calib3d,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
calib3d,The following figure illustrates the pinhole camera model.
calib3d,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
calib3d,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
calib3d,where
calib3d,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
calib3d,with
calib3d,\[r^2 = x'^2 + y'^2\]
calib3d,and
calib3d,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
calib3d,if \(Z_c \ne 0\).
calib3d,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
calib3d,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
calib3d,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
calib3d,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
calib3d,where
calib3d,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
calib3d,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
calib3d,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
calib3d,In the functions below the coefficients are passed or returned as
calib3d,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
calib3d,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
calib3d,The functions below use the above model to do the following:
calib3d,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
calib3d,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
calib3d,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
calib3d,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
calib3d,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
calib3d,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
calib3d,if \(W \ne 0\).
calib3d,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
calib3d,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
calib3d,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
calib3d,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
calib3d,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
calib3d,Template Read-Only Sparse Matrix Iterator Class.
calib3d,This is the derived from SparseMatConstIterator class that introduces more convenient operator *() for accessing the current element.
calib3d,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
calib3d,The class defining termination criteria for iterative algorithms.
calib3d,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
calib3d,"ArUco Marker Detection, module functionality was moved to objdetect module"
calib3d,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
calib3d,"Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige."
calib3d,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
calib3d,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
calib3d,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
calib3d,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
calib3d,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
calib3d,"Class for implementing the wrapper which makes detectors and extractors to be affine invariant, described as ASIFT in [312] ."
calib3d,Template class specifying a continuous subsequence (slice) of a sequence.
calib3d,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
calib3d,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
calib3d,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
calib3d,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
calib3d,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
calib3d,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
calib3d,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
calib3d,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
calib3d,XML/YAML/JSON file storage class that encapsulates all the information necessary for writing or reading data to/from a file.
calib3d,Computes weighted sum of two arrays using formula: dst[i] = a * src1[i] + b * src2[i] + c
calib3d,"src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images scalars numbers a, b, and c"
calib3d,"Functions int hal_ni_addWeighted16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, const double scalars[3]) "
calib3d,Hamming norm of a vector
calib3d,"a pointer to vector data n length of a vector cellSize how many bits of the vector will be added and treated as a single bit, can be 1 (standard Hamming distance), 2 or 4 result pointer to result output"
calib3d,Hamming distance between two vectors
calib3d,"a pointer to first vector data b pointer to second vector data n length of vectors cellSize how many bits of the vectors will be added and treated as a single bit, can be 1 (standard Hamming distance), 2 or 4 result pointer to result output"
calib3d,"Functions int hal_ni_normHamming8u (const uchar *a, int n, int cellSize, int *result)  int hal_ni_normHammingDiff8u (const uchar *a, const uchar *b, int n, int cellSize, int *result) "
calib3d,A complex number class.
calib3d,"The template class is similar and compatible with std::complex, however it provides slightly more convenient access to the real and imaginary parts using through the simple field access, as opposite to std::complex::real() and std::complex::imag()."
calib3d,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
calib3d,Performs QR decomposition of \(M\times N\)( \(M>N\)) matrix \(A = Q*R\) and solves matrix equation \(A*X=B\).
calib3d,"src1 pointer to input matrix \(A\) stored in row major order. After finish of work src1 contains upper triangular \(N\times N\) matrix \(R\). Lower triangle of src1 will be filled with vectors of elementary reflectors. See [284] and Lapack's DGEQRF documentation for details. src1_step number of bytes between two consequent rows of matrix \(A\). m number fo rows in matrix \(A\). n number of columns in matrix \(A\). k number of right-hand vectors in \(M\times K\) matrix \(B\). src2 pointer to \(M\times K\) matrix \(B\) which is the right-hand side of system \(A*X=B\). \(B\) stored in row major order. If src2 is null pointer only QR decomposition will be performed. Otherwise system will be solved and src1 will be used as temporary buffer, so after finish of work src2 contains solution \(X\) of system \(A*X=B\). src2_step number of bytes between two consequent rows of matrix \(B\). dst pointer to continiuos \(N\times 1\) array for scalar factors of elementary reflectors. See [284] for details. info indicates success of decomposition. If *info is zero decomposition failed."
calib3d,"Functions int hal_ni_QR32f (float *src1, size_t src1_step, int m, int n, int k, float *src2, size_t src2_step, float *dst, int *info)  int hal_ni_QR64f (double *src1, size_t src1_step, int m, int n, int k, double *src2, size_t src2_step, double *dst, int *info) "
calib3d,Mersenne Twister random number generator.
calib3d,Inspired by http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/CODES/mt19937ar.c
calib3d,Template class for 2D points specified by its coordinates x and y.
calib3d,"An instance of the class is interchangeable with C structures, CvPoint and CvPoint2D32f . There is also a cast operator to convert point coordinates to the specified type. The conversion from floating-point coordinates to integer coordinates is done by rounding. Commonly, the conversion uses this operation for each of the coordinates. Besides the class members listed in the declaration above, the following operations on points are implemented:"
calib3d,"For your convenience, the following type aliases are defined:"
calib3d,Example:
calib3d,Definitions: Let P be a point in 3D of coordinates X in the world reference frame (stored in the matrix X) The coordinate vector of P in the camera reference frame is:
calib3d,\[Xc = R X + T\]
calib3d,"where R is the rotation matrix corresponding to the rotation vector om: R = rodrigues(om); call x, y and z the 3 coordinates of Xc:"
calib3d,\[\begin{array}{l} x = Xc_1 \\ y = Xc_2 \\ z = Xc_3 \end{array} \]
calib3d,The pinhole projection coordinates of P is [a; b] where
calib3d,\[\begin{array}{l} a = x / z \ and \ b = y / z \\ r^2 = a^2 + b^2 \\ \theta = atan(r) \end{array} \]
calib3d,Fisheye distortion:
calib3d,\[\theta_d = \theta (1 + k_1 \theta^2 + k_2 \theta^4 + k_3 \theta^6 + k_4 \theta^8)\]
calib3d,The distorted point coordinates are [x'; y'] where
calib3d,\[\begin{array}{l} x' = (\theta_d / r) a \\ y' = (\theta_d / r) b \end{array} \]
calib3d,"Finally, conversion into pixel coordinates: The final pixel coordinates vector [u; v] where:"
calib3d,\[\begin{array}{l} u = f_x (x' + \alpha y') + c_x \\ v = f_y y' + c_y \end{array} \]
calib3d,Summary: Generic camera model [143] with perspective projection and without distortion correction
calib3d,Namespaces namespace cv::fisheye  The methods in this namespace use a so-called fisheye camera model. 
calib3d,Linear Discriminant Analysis.
calib3d,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
calib3d,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
calib3d,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
calib3d,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
calib3d,"Each class derived from Map implements a motion model, as follows:"
calib3d,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
calib3d,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
calib3d,The classes derived from Mapper are
calib3d,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
calib3d,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
calib3d,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
calib3d,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
calib3d,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
calib3d,Line segment detector class.
calib3d,following the algorithm described at [290] .
calib3d,This module provides storage routines for Hierarchical Data Format objects.
calib3d,Face module changelog Face Recognition with OpenCV
calib3d,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
calib3d,Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor.
calib3d,"described in [229] . The algorithm uses FAST in pyramids to detect stable keypoints, selects the strongest features using FAST or Harris response, finds their orientation using first-order moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or k-tuples) are rotated according to the measured orientation)."
calib3d,The class represents rotated (i.e. not up-right) rectangles on a plane.
calib3d,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
calib3d,The sample below demonstrates how to use RotatedRect:
calib3d,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
calib3d,Classes class cv::plot::Plot2d 
calib3d,"Minimum: dst[i] = min(src1[i], src2[i]) Maximum: dst[i] = max(src1[i], src2[i])"
calib3d,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
calib3d,"Functions int hal_ni_max16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_max16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_max32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_max32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_max64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_max8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_max8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_min16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_min16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_min32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_min32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_min64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_min8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_min8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height) "
calib3d,Classes class cv::quality::QualityBase 
calib3d,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
calib3d,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
calib3d,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
calib3d,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
calib3d,Classes class cv::DenseOpticalFlow  class cv::DISOpticalFlow  DIS optical flow algorithm. More...  class cv::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::KalmanFilter  Kalman filter class. More...  class cv::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More...  class cv::Tracker  Base abstract class for the long-term tracker. More...  class cv::TrackerDaSiamRPN  class cv::TrackerGOTURN  the GOTURN (Generic Object Tracking Using Regression Networks) tracker More...  class cv::TrackerMIL  The MIL algorithm trains a classifier in an online manner to separate the object from the background. More...  class cv::TrackerNano  the Nano tracker is a super lightweight dnn-based general object tracking. More...  class cv::TrackerVit  the VIT tracker is a super lightweight dnn-based general object tracking. More...  class cv::VariationalRefinement  Variational optical flow refinement. More... 
calib3d,Namespaces namespace cv::traits 
calib3d,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
calib3d,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
calib3d,Namespace for all functions is cv::intensity_transform.
calib3d,Classes class cv::LineSegmentDetector  Line segment detector class. More... 
calib3d,Performs singular value decomposition of \(M\times N\)( \(M>N\)) matrix \(A = U*\Sigma*V^T\).
calib3d,"src pointer to input \(M\times N\) matrix \(A\) stored in column major order. After finish of work src will be filled with rows of \(U\) or not modified (depends of flag CV_HAL_SVD_MODIFY_A). src_step number of bytes between two consequent columns of matrix \(A\). w pointer to array for singular values of matrix \(A\) (i. e. first \(N\) diagonal elements of matrix \(\Sigma\)). u pointer to output \(M\times N\) or \(M\times M\) matrix \(U\) (size depends of flags). Pointer must be valid if flag CV_HAL_SVD_MODIFY_A not used. u_step number of bytes between two consequent rows of matrix \(U\). vt pointer to array for \(N\times N\) matrix \(V^T\). vt_step number of bytes between two consequent rows of matrix \(V^T\). m number fo rows in matrix \(A\). n number of columns in matrix \(A\). flags algorithm options (combination of CV_HAL_SVD_FULL_UV, ...)."
calib3d,"Functions int hal_ni_SVD32f (float *src, size_t src_step, float *w, float *u, size_t u_step, float *vt, size_t vt_step, int m, int n, int flags)  int hal_ni_SVD64f (double *src, size_t src_step, double *w, double *u, size_t u_step, double *vt, size_t vt_step, int m, int n, int flags) "
calib3d,Divide: dst[i] = scale * src1[i] / src2[i]
calib3d,src1_data first source image data and step src1_step first source image data and step src2_data second source image data and step src2_step second source image data and step dst_data destination image data and step dst_step destination image data and step width dimensions of the images height dimensions of the images scale additional multiplier
calib3d,"Functions int hal_ni_div16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, double scale) "
calib3d,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
calib3d,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
calib3d,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
calib3d,The class SparseMat represents multi-dimensional sparse numerical arrays.
calib3d,"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:"
calib3d,"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5; int size[5] = {10, 10, 10, 10, 10}; SparseMat sparse_mat(dims, size, CV_32F); for(int i = 0; i < 1000; i++) { int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f; } cout << ""nnz = "" << sparse_mat.nzcount() << endl; cv::SparseMatThe class SparseMat represents multi-dimensional sparse numerical arrays.Definition mat.hpp:2751 cv::SparseMat::sizeconst int * size() constreturns the array of sizes, or NULL if the matrix is not allocated cv::SparseMat::dimsint dims() constreturns the matrix dimensionality CV_32F#define CV_32FDefinition interface.h:78 Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix // and the sum of elements. SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>(); double s = 0; int dims = sparse_mat.dims(); for(; it != it_end; ++it) { // print element indices and the element value const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("": %g\n"", it.value<float>()); s += *it; } printf(""Element sum is %g\n"", s); cv::SparseMatConstIterator_Template Read-Only Sparse Matrix Iterator Class.Definition mat.hpp:3354 cv::SparseMatConstIterator::valueconst _Tp & value() consttemplate method returning the current matrix element cv::SparseMatConstIterator::nodeconst SparseMat::Node * node() constreturns the current node of the sparse matrix. it.node->idx is the current element index cv::SparseMat::Nodesparse matrix node - element of a hash tableDefinition mat.hpp:2776 cv::SparseMat::Node::idxint idx[MAX_DIM]index of the matrix elementDefinition mat.hpp:2782 If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation. Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b) { const SparseMat *_a = &a, *_b = &b; // if b contains less elements than a, // it is faster to iterate through b if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // take the next element from the first matrix float avalue = *it; const Node* anode = it.node(); // and try to find an element with the same index in the second matrix. // since the hash value depends only on the element index, // reuse the hash value stored in the node float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr; } cv::SparseMat::endSparseMatIterator end()returns the sparse matrix iterator at the matrix end cv::SparseMat::nzcountsize_t nzcount() constreturns the number of non-zero elements (=the number of hash table nodes) cv::SparseMat::beginSparseMatIterator begin()returns the sparse matrix iterator at the matrix beginning cv::SparseMat::Node::hashvalsize_t hashvalhash valueDefinition mat.hpp:2778"
calib3d,Read and write video or images sequence with OpenCV.
calib3d,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
calib3d,Bioinspired Module Retina Introduction
calib3d,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
calib3d,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
calib3d,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
calib3d,See detailed overview here: Machine Learning Overview.
calib3d,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
calib3d,Dummy structure storing DFT/DCT context.
calib3d,Users can convert this pointer to any type they want. Initialisation and destruction should be made in Init and Free function implementations correspondingly. Example:
calib3d,core/src/hal_replacement.hpp
calib3d,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
calib3d,A class filters a vector of keypoints.
calib3d,"Because now it is difficult to provide a convenient interface for all usage scenarios of the keypoints filter class, it has only several needed by now static methods."
calib3d,Multiply: dst[i] = scale * src1[i] * src2[i]
calib3d,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images scale additional multiplier
calib3d,"Functions int hal_ni_mul16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8s16s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8u16u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, double scale) "
calib3d,"Class implementing the BRISK keypoint detector and descriptor extractor, described in [159] ."
calib3d,File Storage Node class.
calib3d,"The node is used to store each and every element of the file storage opened for reading. When XML/YAML file is read, it is first parsed and stored in the memory as a hierarchical collection of nodes. Each node can be a ""leaf"" that is contain a single number or a string, or be a collection of other nodes. There can be named collections (mappings) where each element has a name and it is accessed by a name, and ordered collections (sequences) where elements do not have names but rather accessed by index. Type of the file node can be determined using FileNode::type method."
calib3d,"Note that file nodes are only used for navigating file storages opened for reading. When a file storage is opened for writing, no data is stored in memory after it is written."
calib3d,Template class for small matrices whose type and size are known at compilation time.
calib3d,"If you need a more flexible type, use Mat . The elements of the matrix M are accessible using the M(i,j) notation. Most of the common matrix operations (see also MatrixExpressions ) are available. To do an operation on Matx that is not implemented, you can easily convert the matrix to Mat and backwards:"
calib3d,"Except of the plain constructor which takes a list of elements, Matx can be initialized from a C-array:"
calib3d,"In case if C++11 features are available, std::initializer_list can be also used to initialize Matx:"
calib3d,Wrapping class for feature detection using the goodFeaturesToTrack function. :
calib3d,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
calib3d,"src_data array of pointers to source arrays (cn items x len items) [ [B, B, ...], [G, G, ...], [R, R, ...] ] dst_data destination array of interleaved values (len x cn items) [ B, G, R, B, G, R, ...] len number of elements cn number of channels"
calib3d,"Functions int hal_ni_merge16u (const ushort **src_data, ushort *dst_data, int len, int cn)  int hal_ni_merge32s (const int **src_data, int *dst_data, int len, int cn)  int hal_ni_merge64s (const int64 **src_data, int64 *dst_data, int len, int cn)  int hal_ni_merge8u (const uchar **src_data, uchar *dst_data, int len, int cn) "
calib3d,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
calib3d,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
calib3d,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
calib3d,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
calib3d,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
calib3d,Custom array allocator.
calib3d,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
calib3d,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
calib3d,"Template class for 3D points specified by its coordinates x, y and z."
calib3d,"An instance of the class is interchangeable with the C structure CvPoint2D32f . Similarly to Point_ , the coordinates of 3D points can be converted to another type. The vector arithmetic and comparison operations are also supported."
calib3d,The following Point3_<> aliases are available:
calib3d,Wrapping class for feature detection using the FAST method. :
calib3d,Read-Only Sparse Matrix Iterator.
calib3d,Here is how to use the iterator to compute the sum of floating-point sparse matrix elements:
calib3d,"src_data array of interleaved values (len x cn items) [ B, G, R, B, G, R, ...] dst_data array of pointers to destination arrays (cn items x len items) [ [B, B, ...], [G, G, ...], [R, R, ...] ] len number of elements cn number of channels"
calib3d,"Functions int hal_ni_split16u (const ushort *src_data, ushort **dst_data, int len, int cn)  int hal_ni_split32s (const int *src_data, int **dst_data, int len, int cn)  int hal_ni_split64s (const int64 *src_data, int64 **dst_data, int len, int cn)  int hal_ni_split8u (const uchar *src_data, uchar **dst_data, int len, int cn) "
calib3d,Singular Value Decomposition.
calib3d,"Class for computing Singular Value Decomposition of a floating-point matrix. The Singular Value Decomposition is used to solve least-square problems, under-determined linear systems, invert matrices, compute condition numbers, and so on."
calib3d,"If you want to compute a condition number of a matrix or an absolute value of its determinant, you do not need u and vt. You can pass flags=SVD::NO_UV|... . Another flag SVD::FULL_UV indicates that full-size u and vt must be computed, which is not necessary most of the time."
calib3d,"Enumerations enum cv::TemplateMatchModes { cv::TM_SQDIFF = 0 , cv::TM_SQDIFF_NORMED = 1 , cv::TM_CCORR = 2 , cv::TM_CCORR_NORMED = 3 , cv::TM_CCOEFF = 4 , cv::TM_CCOEFF_NORMED = 5 }  type of the template matching operation More... "
calib3d,Template matrix class derived from Mat.
calib3d,"The class Mat_<_Tp> is a thin template wrapper on top of the Mat class. It does not have any extra data fields. Nor this class nor Mat has any virtual methods. Thus, references or pointers to these two classes can be freely but carefully converted one to another. For example:"
calib3d,"While Mat is sufficient in most cases, Mat_ can be more convenient if you use a lot of element access operations and if you know matrix type at the compilation time. Note that Mat::at(int y,int x) and Mat_::operator()(int y,int x) do absolutely the same and run at the same speed, but the latter is certainly shorter:"
calib3d,"To use Mat_ for multi-channel images/matrices, pass Vec as a Mat_ parameter:"
calib3d,Mat_ is fully compatible with C++11 range-based for loop. For example such loop can be used to safely apply look-up table:
calib3d,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
calib3d,"Namespace for all functions is cvv, i.e. cvv::showImage()."
calib3d,Compilation:
calib3d,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
calib3d,See cvv tutorial for a commented example application using cvv.
calib3d,Namespaces namespace cvv::impl 
core,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
core,"This class is used to perform the non-linear non-constrained minimization of a function with known gradient,."
core,"defined on an n-dimensional Euclidean space, using the Nonlinear Conjugate Gradient method. The implementation was done based on the beautifully clear explanatory article [An Introduction to the Conjugate Gradient Method Without the Agonizing Pain](http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) by Jonathan Richard Shewchuk. The method can be seen as an adaptation of a standard Conjugate Gradient method (see, for example http://en.wikipedia.org/wiki/Conjugate_gradient_method) for numerically solving the systems of linear equations."
core,"It should be noted, that this method, although deterministic, is rather a heuristic method and therefore may converge to a local minima, not necessary a global one. What is even more disastrous, most of its behaviour is ruled by gradient, therefore it essentially cannot distinguish between local minima and maxima. Therefore, if it starts sufficiently near to the local maximum, it may converge to it. Another obvious restriction is that it should be possible to compute the gradient of a function at any point, thus it is preferable to have analytic expression for gradient and computational burden should be born by the user."
core,The latter responsibility is accomplished via the getGradient method of a MinProblemSolver::Function interface (which represents function being optimized). This method takes point a point in n-dimensional space (first argument represents the array of coordinates of that point) and compute its gradient (it should be stored in the second argument as an array).
core,Designed for command line parsing.
core,The sample below demonstrates how to use CommandLineParser:
core,"Template ""trait"" class for OpenCV primitive data types."
core,"A primitive OpenCV data type is one of unsigned char, bool, signed char, unsigned short, signed short, int, float, double, or a tuple of values of one of these types, where all the values in the tuple have the same type. Any primitive type from the list can be defined by an identifier in the form CV_<bit-depth>{U|S|F}C(<number_of_channels>), for example: uchar CV_8UC1, 3-element floating-point tuple CV_32FC3, and so on. A universal OpenCV structure that is able to store a single instance of such a primitive data type is Vec. Multiple instances of such a type can be stored in a std::vector, Mat, Mat_, SparseMat, SparseMat_, or any other container that is able to store Vec instances."
core,"The DataType class is basically used to provide a description of such primitive data types without adding any fields or methods to the corresponding classes (and it is actually impossible to add anything to primitive C/C++ data types). This technique is known in C++ as class traits. It is not DataType itself that is used but its specialized versions, such as:"
core,"The main purpose of this class is to convert compilation-time type information to an OpenCV-compatible data type identifier, for example:"
core,"So, such traits are used to tell OpenCV which data type you are working with, even if such a type is not native to OpenCV. For example, the matrix B initialization above is compiled because OpenCV defines the proper specialized template class DataType<complex<_Tp> > . This mechanism is also useful (and used in OpenCV this way) for generic algorithms implementations."
core,opencv2/core/traits.hpp
core,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
core,Provides result of asynchronous operations.
core,These functions are provided for OpenCV-Eigen interoperability. They convert Mat objects to corresponding Eigen::Matrix objects and vice-versa. Consult the Eigen documentation for information about the Matrix template type.
core,Namespaces namespace cv::traits 
core,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
core,This module includes photo processing algorithms
core,"Matrix expression representation This is a list of implemented matrix operations that can be combined in arbitrary complex expressions (here A, B stand for matrices ( Mat ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double )):"
core,"Addition, subtraction, negation: A+B, A-B, A+s, A-s, s+A, s-A, -A Scaling: A*alpha Per-element multiplication and division: A.mul(B), A/B, alpha/A Matrix multiplication: A*B Transposition: A.t() (means AT) Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems: A.inv([method]) (~ A<sup>-1</sup>), A.inv([method])*B (~ X: AX=B) Comparison: A cmpop B, A cmpop alpha, alpha cmpop A, where cmpop is one of >, >=, ==, !=, <=, <. The result of comparison is an 8-bit single channel mask whose elements are set to 255 (if the particular element or pair of elements satisfy the condition) or 0. Bitwise logical operations: A logicop B, A logicop s, s logicop A, ~A, where logicop is one of &, |, ^. Element-wise minimum and maximum: min(A, B), min(A, alpha), max(A, B), max(A, alpha) Element-wise absolute value: abs(A) Cross-product, dot-product: A.cross(B), A.dot(B) Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm, mean, sum, countNonZero, trace, determinant, repeat, and others. Matrix initializers ( Mat::eye(), Mat::zeros(), Mat::ones() ), matrix comma-separated initializers, matrix constructors and operators that extract sub-matrices (see Mat description). Mat_<destination_type>() constructors to cast the result to the proper type. NoteComma-separated initializers and probably some other operations may require additional explicit Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity. Here are examples of matrix expressions: // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD) SVD svd(A); Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t(); // compute the new vector of parameters in the Levenberg-Marquardt algorithm x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err); // sharpen image using ""unsharp mask"" algorithm Mat blurred; double sigma = 1, threshold = 5, amount = 1; GaussianBlur(img, blurred, Size(), sigma, sigma); Mat lowContrastMask = abs(img - blurred) < threshold; Mat sharpened = img*(1+amount) + blurred*(-amount); img.copyTo(sharpened, lowContrastMask); cv::MatExpr::absMatExpr abs(const Mat &m)Calculates an absolute value of each matrix element. cv::MatExpr::invMatExpr inv(int method=DECOMP_LU) const cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::Mat::diagMat diag(int d=0) constExtracts a diagonal from a matrix. cv::Mat::uUMatData * uinteraction with UMatDefinition mat.hpp:2174 cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Mat::tMatExpr t() constTransposes a matrix. cv::SVDSingular Value Decomposition.Definition core.hpp:2739 cv::DECOMP_CHOLESKY@ DECOMP_CHOLESKYDefinition base.hpp:143 cv::SizeSize2i SizeDefinition types.hpp:370 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. cv::thresholddouble threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)Applies a fixed-level threshold to each array element."
core,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
core,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
core,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
core,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
core,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
core,The implemented stitching pipeline is very similar to the one proposed in [41] .
core,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
core,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
core,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
core,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
core,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
core,Template class for a 4-element vector derived from Vec.
core,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
core,This section describes Intel VA-API/OpenCL (CL-VA) interoperability.
core,To enable basic VA interoperability build OpenCV with libva library integration enabled: -DWITH_VA=ON (corresponding dev package should be installed).
core,"To enable advanced CL-VA interoperability support on Intel HW, enable option: -DWITH_VA_INTEL=ON (OpenCL integration should be enabled which is the default setting). Special runtime environment should be set up in order to use this feature: correct combination of libva, OpenCL runtime and media driver should be installed."
core,Check usage example for details: samples/va_intel/va_intel_interop.cpp
core,Namespaces namespace cv::va_intel::ocl 
core,Random Number Generator.
core,"Random number generator. It encapsulates the state (currently, a 64-bit integer) and has methods to return scalar random values and to fill arrays with random values. Currently it supports uniform and Gaussian (normal) distributions. The generator uses Multiply-With-Carry algorithm, introduced by G. Marsaglia ( http://en.wikipedia.org/wiki/Multiply-with-carry ). Gaussian-distribution random numbers are generated using the Ziggurat algorithm ( http://en.wikipedia.org/wiki/Ziggurat_algorithm ), introduced by G. Marsaglia and W. W. Tsang."
core,Matrix read-only iterator.
core,Class passed to an error.
core,This class encapsulates all or almost all necessary information about the error happened in the program. The exception is usually constructed and thrown implicitly via CV_Error and CV_Error_ macros.
core,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
core,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
core,This module includes signal processing algorithms.
core,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
core,Data structure for salient point detectors.
core,"The class instance stores a keypoint, i.e. a point feature found by one of many available keypoint detectors, such as Harris corner detector, FAST, StarDetector, SURF, SIFT etc."
core,"The keypoint is characterized by the 2D position, scale (proportional to the diameter of the neighborhood that needs to be taken into account), orientation and some other parameters. The keypoint neighborhood is then analyzed by another algorithm that builds a descriptor (usually represented as a feature vector). The keypoints representing the same object in different images can then be matched using KDTree or another method."
core,ICP point-to-plane odometry algorithm
core,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
core,This section describes OpenGL interoperability.
core,"To enable OpenGL support, configure OpenCV using CMake with WITH_OPENGL=ON . Currently OpenGL is supported only with WIN32, GTK and Qt backends on Windows and Linux (MacOS and Android are not supported). For GTK-2.0 backend gtkglext-1.0 library is required."
core,"To use OpenGL functionality you should first create OpenGL context (window or frame buffer). You can do this with namedWindow function or with other OpenGL toolkit (GLUT, for example)."
core,Namespaces namespace cv::ogl::ocl 
core,Template class for 2D rectangles.
core,described by the following parameters:
core,"Coordinates of the top-left corner. This is a default interpretation of Rect_::x and Rect_::y in OpenCV. Though, in your algorithms you may count x and y from the bottom-left corner. Rectangle width and height."
core,"OpenCV typically assumes that the top and left boundary of the rectangle are inclusive, while the right and bottom boundaries are not. For example, the method Rect_::contains returns true if"
core,"\[x \leq pt.x < x+width, y \leq pt.y < y+height\]"
core,Virtually every loop over an image ROI in OpenCV (where ROI is specified by Rect_<int> ) is implemented as:
core,"In addition to the class members, the following operations on rectangles are implemented:"
core,"\(\texttt{rect} = \texttt{rect} \pm \texttt{point}\) (shifting a rectangle by a certain offset) \(\texttt{rect} = \texttt{rect} \pm \texttt{size}\) (expanding or shrinking a rectangle by a certain amount) rect += point, rect -= point, rect += size, rect -= size (augmenting operations) rect = rect1 & rect2 (rectangle intersection) rect = rect1 | rect2 (minimum area rectangle containing rect1 and rect2 ) rect &= rect1, rect |= rect1 (and the corresponding augmenting operations) rect == rect1, rect != rect1 (rectangle comparison)"
core,This is an example how the partial ordering on rectangles can be established (rect1 \(\subseteq\) rect2):
core,"For your convenience, the Rect_<> alias is available: cv::Rect"
core,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
core,Base class for parallel data processors.
core,YOUR ATTENTION PLEASE!
core,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
core,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
core,Note for developers: please don't put videoio dependency in G-API because of this file.
core,Dense optical flow algorithms compute motion for each point:
core,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
core,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
core,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
core,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
core,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
core,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
core,This type is very similar to InputArray except that it is used for input/output and output function parameters.
core,"Just like with InputArray, OpenCV users should not care about OutputArray, they just pass Mat, vector<T> etc. to the functions. The same limitation as for InputArray: Do not explicitly create OutputArray instances applies here too."
core,"If you want to make your function polymorphic (i.e. accept different arrays as output parameters), it is also not very difficult. Take the sample above as the reference. Note that _OutputArray::create() needs to be called before _OutputArray::getMat(). This way you guarantee that the output array is properly allocated."
core,"Optional output parameters. If you do not need certain output array to be computed and returned to you, pass cv::noArray(), just like you would in the case of optional input array. At the implementation level, use _OutputArray::needed() to check if certain output array needs to be computed or not."
core,There are several synonyms for OutputArray that are used to assist automatic Python/Java/... wrapper generators:
core,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
core,Template Read-Write Sparse Matrix Iterator Class.
core,This is the derived from cv::SparseMatConstIterator_ class that introduces more convenient operator *() for accessing the current element.
core,The STL-compliant memory Allocator based on cv::fastMalloc() and cv::fastFree()
core,This is a base class for all more or less complex algorithms in OpenCV.
core,"especially for classes of algorithms, for which there can be multiple implementations. The examples are stereo correspondence (for which there are algorithms like block matching, semi-global block matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck etc.)."
core,Here is example of SimpleBlobDetector use in your application via Algorithm interface:
core,n-dimensional dense array class
core,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
core,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
core,"In case of a 2-dimensional array, the above formula is reduced to:"
core,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
core,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
core,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
core,There are many different ways to create a Mat object. The most popular options are listed below:
core,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
core,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
core,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
core,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
core,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
core,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
core,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
core,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
core,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
core,Smart pointer for OpenGL buffer object with reference counting.
core,"Buffer Objects are OpenGL objects that store an array of unformatted memory allocated by the OpenGL context. These can be used to store vertex data, pixel data retrieved from images or the framebuffer, and a variety of other things."
core,ogl::Buffer has interface similar with Mat interface and represents 2D array memory.
core,ogl::Buffer supports memory transfers between host and device and also can be mapped to CUDA memory.
core,Classes class cv::ParallelLoopBody  Base class for parallel data processors. More...  class cv::ParallelLoopBodyLambdaWrapper 
core,Principal Component Analysis.
core,"The class is used to calculate a special basis for a set of vectors. The basis will consist of eigenvectors of the covariance matrix calculated from the input set of vectors. The class PCA can also transform vectors to/from the new coordinate space defined by the basis. Usually, in this new coordinate system, each vector from the original set (and any linear combination of such vectors) can be quite accurately approximated by taking its first few components, corresponding to the eigenvectors of the largest eigenvalues of the covariance matrix. Geometrically it means that you calculate a projection of the vector to a subspace formed by a few eigenvectors corresponding to the dominant eigenvalues of the covariance matrix. And usually such a projection is very close to the original vector. So, you can represent the original vector from a high-dimensional space with a much shorter vector consisting of the projected vector's coordinates in the subspace. Such a transformation is also known as Karhunen-Loeve Transform, or KLT. See http://en.wikipedia.org/wiki/Principal_component_analysis"
core,"The sample below is the function that takes two matrices. The first function stores a set of vectors (a row per vector) that is used to calculate PCA. The second function stores another ""test"" set of vectors (a row per vector). First, these vectors are compressed with PCA, then reconstructed back, and then the reconstruction error norm is computed and printed for each vector. :"
core,Namespaces namespace cv  namespace cv::utils::logging::internal 
core,Namespaces namespace cv::omnidir::internal 
core,Classes struct cv::hal::DCT2D  struct cv::hal::DFT1D  struct cv::hal::DFT2D 
core,Returns result of asynchronous operations.
core,Object has attached asynchronous state. Assignment operator doesn't clone asynchronous state (it is shared between all instances).
core,Result can be fetched via get() method only once.
core,This is the proxy class for passing read-only input arrays into OpenCV functions.
core,It is defined as:
core,"where _InputArray is a class that can be constructed from Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat_<T> >, UMat, std::vector<UMat> or double. It can also be constructed from a matrix expression."
core,"Since this is mostly implementation-level class, and its interface may change in future versions, we do not describe it in details. There are a few key things, though, that should be kept in mind:"
core,"When you see in the reference manual or in OpenCV source code a function that takes InputArray, it means that you can actually pass Mat, Matx, vector<T> etc. (see above the complete list). Optional input arguments: If some of the input arrays may be empty, pass cv::noArray() (or simply cv::Mat() as you probably did before). The class is designed solely for passing parameters. That is, normally you should not declare class members, local and global variables of this type. If you want to design your own function or a class method that can operate of arrays of multiple types, you can use InputArray (or OutputArray) for the respective parameters. Inside a function you should use _InputArray::getMat() method to construct a matrix header for the array (without copying data). _InputArray::kind() can be used to distinguish Mat from vector<> etc., but normally it is not needed."
core,Here is how you can use a function that takes InputArray :
core,"That is, we form an STL vector containing points, and apply in-place affine transformation to the vector using the 2x3 matrix created inline as Matx<float, 2, 3> instance."
core,"Here is how such a function can be implemented (for simplicity, we implement a very specific case of it, according to the assertion statement inside) :"
core,"There is another related type, InputArrayOfArrays, which is currently defined as a synonym for InputArray:"
core,"It denotes function arguments that are either vectors of vectors or vectors of matrices. A separate synonym is needed to generate Python/Java etc. wrappers properly. At the function implementation level their use is similar, but _InputArray::getMat(idx) should be used to get header for the idx-th component of the outer vector and _InputArray::size().area() should be used to find the number of components (vectors/matrices) of the outer vector."
core,"In general, type support is limited to cv::Mat types. Other types are forbidden. But in some cases we need to support passing of custom non-general Mat types, like arrays of cv::KeyPoint, cv::DMatch, etc. This data is not intended to be interpreted as an image data, or processed somehow like regular cv::Mat. To pass such custom type use rawIn() / rawOut() / rawInOut() wrappers. Custom type is wrapped as Mat-compatible CV_8UC<N> values (N = sizeof(T), N <= CV_CN_MAX)."
core,Information Flow algorithm implementaton for alphamatting
core,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
core,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
core,The implementation is based on [7].
core,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
core,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
core,"This class is used to perform the non-linear non-constrained minimization of a function,."
core,"defined on an n-dimensional Euclidean space, using the Nelder-Mead method, also known as downhill simplex method**. The basic idea about the method can be obtained from http://en.wikipedia.org/wiki/Nelder-Mead_method."
core,"It should be noted, that this method, although deterministic, is rather a heuristic and therefore may converge to a local minima, not necessary a global one. It is iterative optimization technique, which at each step uses an information about the values of a function evaluated only at n+1 points, arranged as a simplex in n-dimensional space (hence the second name of the method). At each step new point is chosen to evaluate function at, obtained value is compared with previous ones and based on this information simplex changes it's shape , slowly moving to the local minimum. Thus this method is using only function values to make decision, on contrary to, say, Nonlinear Conjugate Gradient method (which is also implemented in optim)."
core,"Algorithm stops when the number of function evaluations done exceeds termcrit.maxCount, when the function values at the vertices of simplex are within termcrit.epsilon range or simplex becomes so small that it can enclosed in a box with termcrit.epsilon sides, whatever comes first, for some defined by user positive integer termcrit.maxCount and positive non-integer termcrit.epsilon."
core,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
core,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
core,"Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as Mat's). It means that for each pixel location \((x,y)\) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location \((x,y)\). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one."
core,"Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian \(3 \times 3\) filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (""replicated border"" extrapolation method), or assume that all the non-existing pixels are zeros (""constant border"" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see BorderTypes"
core,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
core,Namespace for all functions is cv::img_hash.
core,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
core,"Dual quaternions were introduced to describe rotation together with translation while ordinary quaternions can only describe rotation. It can be used for shortest path pose interpolation, local pose optimization or volumetric deformation. More details can be found"
core,"https://en.wikipedia.org/wiki/Dual_quaternion ""A beginners guide to dual-quaternions: what they are, how they work, and how to use them for 3D character hierarchies"", Ben Kenwright, 2012 ""Dual Quaternions"", Yan-Bin Jia, 2013 ""Geometric Skinning with Approximate Dual Quaternion Blending"", Kavan, 2008 http://rodolphe-vaillant.fr/?e=29"
core,A unit dual quaternion can be classically represented as:
core,"\[ \begin{equation} \begin{split} \sigma &= \left(r+\frac{\epsilon}{2}tr\right)\\ &= [w, x, y, z, w\_, x\_, y\_, z\_] \end{split} \end{equation} \]"
core,"where \(r, t\) represents the rotation (ordinary unit quaternion) and translation (pure ordinary quaternion) respectively."
core,A general dual quaternions which consist of two quaternions is usually represented in form of:
core,\[ \sigma = p + \epsilon q \]
core,"where the introduced dual unit \(\epsilon\) satisfies \(\epsilon^2 = \epsilon^3 =...=0\), and \(p, q\) are quaternions."
core,"Alternatively, dual quaternions can also be interpreted as four components which are all dual numbers:"
core,\[ \sigma = \hat{q}_w + \hat{q}_xi + \hat{q}_yj + \hat{q}_zk \]
core,"If we set \(\hat{q}_x, \hat{q}_y\) and \(\hat{q}_z\) equal to 0, a dual quaternion is transformed to a dual number. see normalize()."
core,"If you want to create a dual quaternion, you can use:"
core,"A point \(v=(x, y, z)\) in form of dual quaternion is \([1+\epsilon v]=[1,0,0,0,0,x,y,z]\). The transformation of a point \(v_1\) to another point \(v_2\) under the dual quaternion \(\sigma\) is"
core,\[ 1 + \epsilon v_2 = \sigma * (1 + \epsilon v_1) * \sigma^{\star} \]
core,where \(\sigma^{\star}=p^*-\epsilon q^*.\)
core,"A line in the \(Pl\ddot{u}cker\) coordinates \((\hat{l}, m)\) defined by the dual quaternion \(l=\hat{l}+\epsilon m\). To transform a line,"
core,"\[l_2 = \sigma * l_1 * \sigma^*,\]"
core,where \(\sigma=r+\frac{\epsilon}{2}rt\) and \(\sigma^*=p^*+\epsilon q^*\).
core,"To extract the Vec<double, 8> or Vec<float, 8>, see toVec();"
core,"To extract the affine transformation matrix, see toMat();"
core,"To extract the instance of Affine3, see toAffine3();"
core,"If two quaternions \(q_0, q_1\) are needed to be interpolated, you can use sclerp()"
core,or dqblend().
core,"With more than two dual quaternions to be blended, you can use generalize linear dual quaternion blending with the corresponding weights, i.e. gdqblend()."
core,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
core,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
core,Class for matching keypoint descriptors.
core,"query descriptor index, train descriptor index, train image index, and distance between descriptors."
core,This modules is to draw UTF-8 strings with freetype/harfbuzz.
core,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
core,Classes class cv::freetype::FreeType2 
core,Matrix read-write iterator.
core,"Enumerations enum cv::KmeansFlags { cv::KMEANS_RANDOM_CENTERS = 0 , cv::KMEANS_PP_CENTERS = 2 , cv::KMEANS_USE_INITIAL_LABELS = 1 }  k-means flags More... "
core,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
core,Read-write Sparse Matrix Iterator.
core,"The class is similar to cv::SparseMatConstIterator, but can be used for in-place modification of the matrix elements."
core,Namespaces namespace NcvCTprep 
core,n-ary multi-dimensional array iterator.
core,"Use the class to implement unary, binary, and, generally, n-ary element-wise operations on multi-dimensional arrays. Some of the arguments of an n-ary function may be continuous arrays, some may be not. It is possible to use conventional MatIterator 's for each array but incrementing all of the iterators after each small operations may be a big overhead. In this case consider using NAryMatIterator to iterate through several matrices simultaneously as long as they have the same geometry (dimensionality and all the dimension sizes are the same). On each iteration it.planes[0], it.planes[1],... will be the slices of the corresponding matrices."
core,The example below illustrates how you can compute a normalized and threshold 3D color histogram:
core,Smart pointer for OpenGL 2D texture memory with reference counting.
core,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
core,"Functions void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1, __m128 &v_a0, __m128 &v_a1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1, __m128 &v_a0, __m128 &v_a1) "
core,This module contains:
core,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
core,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
core,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
core,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
core,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
core,Template class for specifying the size of an image or rectangle.
core,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
core,OpenCV defines the following Size_<> aliases:
core,Wrapper for OpenGL Client-Side Vertex arrays.
core,ogl::Arrays stores vertex data in ogl::Buffer objects.
core,"Drawing functions work with matrices/images of arbitrary depth. The boundaries of the shapes can be rendered with antialiasing (implemented only for 8-bit images for now). All the functions include the parameter color that uses an RGB value (that may be constructed with the Scalar constructor ) for color images and brightness for grayscale images. For color images, the channel ordering is normally Blue, Green, Red. This is what imshow, imread, and imwrite expect. So, if you form a color using the Scalar constructor, it should look like:"
core,"\[\texttt{Scalar} (blue \_ component, green \_ component, red \_ component[, alpha \_ component])\]"
core,"If you are using your own image rendering and I/O functions, you can use any channel ordering. The drawing functions process each channel independently and do not depend on the channel order or even on the used color space. The whole image can be converted from BGR to RGB or to a different color space using cvtColor ."
core,"If a drawn figure is partially or completely outside the image, the drawing functions clip it. Also, many drawing functions can handle pixel coordinates specified with sub-pixel accuracy. This means that the coordinates can be passed as fixed-point numbers encoded as integers. The number of fractional bits is specified by the shift parameter and the real point coordinates are calculated as \(\texttt{Point}(x,y)\rightarrow\texttt{Point2f}(x*2^{-shift},y*2^{-shift})\) . This feature is especially effective when rendering antialiased shapes."
core,Classes class cv::LineIterator  Class for iterating over all pixels on a raster line segment. More... 
core,Comma-separated Matrix Initializer.
core,"The class instances are usually not created explicitly. Instead, they are created on ""matrix << firstValue"" operator."
core,The sample below initializes 2x2 rotation matrix:
core,"Functions void cv::samples::addSamplesDataSearchPath (const cv::String &path)  Override search data path by adding new search location.  void cv::samples::addSamplesDataSearchSubDirectory (const cv::String &subdir)  Append samples search data sub directory.  cv::String cv::samples::findFile (const cv::String &relative_path, bool required=true, bool silentMode=false)  Try to find requested data file.  cv::String cv::samples::findFileOrKeep (const cv::String &relative_path, bool silentMode=false) "
core,"Template class for short numerical vectors, a partial case of Matx."
core,"This template class represents short numerical vectors (of 1, 2, 3, 4 ... elements) on which you can perform basic arithmetical operations, access individual elements using [] operator etc. The vectors are allocated on stack, as opposite to std::valarray, std::vector, cv::Mat etc., which elements are dynamically allocated in the heap."
core,The template takes 2 parameters:
core,_Tp element type cn the number of elements
core,"In addition to the universal notation like Vec<float, 3>, you can use shorter aliases for the most popular specialized variants of Vec, e.g. Vec3f ~ Vec<float, 3>."
core,"It is possible to convert Vec<T,2> to/from Point_, Vec<T,3> to/from Point3_ , and Vec<T,4> to CvScalar or Scalar_. Use operator[] to access the elements of Vec."
core,All the expected vector operations are also implemented:
core,"v1 = v2 + v3 v1 = v2 - v3 v1 = v2 * scale v1 = scale * v2 v1 = -v2 v1 += v2 and other augmenting operations v1 == v2, v1 != v2 norm(v1) (euclidean norm) The Vec class is commonly used to describe pixel types of multi-channel arrays. See Mat for details."
core,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
core,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
core,A helper class for cv::DataType.
core,The class is specialized for each fundamental numerical data type supported by OpenCV. It provides DataDepth<T>::value constant.
core,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
core,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
core,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
core,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
core,"Functions void cv::julia::initJulia (int argc, char **argv) "
core,used to iterate through sequences and mappings.
core,"A standard STL notation, with node.begin(), node.end() denoting the beginning and the end of a sequence, stored in node. See the data reading sample in the beginning of the section."
core,Template sparse n-dimensional array class derived from SparseMat.
core,SparseMat_ is a thin wrapper on top of SparseMat created in the same way as Mat_ . It simplifies notation of some operations:
core,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
core,"SoftFloat is a software implementation of floating-point calculations according to IEEE 754 standard. All calculations are done in integers, that's why they are machine-independent and bit-exact. This library can be useful in accuracy-critical parts like look-up tables generation, tests, etc. OpenCV contains a subset of SoftFloat partially rewritten to C++."
core,"Functions void cv::accumulate (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds an image to the accumulator image.  void cv::accumulateProduct (InputArray src1, InputArray src2, InputOutputArray dst, InputArray mask=noArray())  Adds the per-element product of two input images to the accumulator image.  void cv::accumulateSquare (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds the square of a source image to the accumulator image.  void cv::accumulateWeighted (InputArray src, InputOutputArray dst, double alpha, InputArray mask=noArray())  Updates a running average.  void cv::createHanningWindow (OutputArray dst, Size winSize, int type)  This function computes a Hanning window coefficients in two dimensions.  void cv::divSpectrums (InputArray a, InputArray b, OutputArray c, int flags, bool conjB=false)  Performs the per-element division of the first Fourier spectrum by the second Fourier spectrum.  Point2d cv::phaseCorrelate (InputArray src1, InputArray src2, InputArray window=noArray(), double *response=0)  The function is used to detect translational shifts that occur between two images. "
core,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
core,It provides easy interface to:
core,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
core,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
core,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
core,It is planned to have:
core,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
core,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
core,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
core,Classes class cv::DualQuat< _Tp >  class cv::Quat< _Tp >  class cv::QuatEnum 
core,Manages memory block shared by muliple buffers.
core,"This class allows to allocate one large memory block and split it into several smaller non-overlapping buffers. In safe mode each buffer allocation will be performed independently, this mode allows dynamic memory access instrumentation using valgrind or memory sanitizer."
core,Safe mode can be explicitly switched ON in constructor. It will also be enabled when compiling with memory sanitizer support or in runtime with the environment variable OPENCV_BUFFER_AREA_ALWAYS_SAFE.
core,Example of usage:
core,Functions float32x2_t cv_vrecp_f32 (float32x2_t val)  float32x4_t cv_vrecpq_f32 (float32x4_t val)  int32x2_t cv_vrnd_s32_f32 (float32x2_t v)  uint32x2_t cv_vrnd_u32_f32 (float32x2_t v)  int32x4_t cv_vrndq_s32_f32 (float32x4_t v)  uint32x4_t cv_vrndq_u32_f32 (float32x4_t v)  float32x2_t cv_vrsqrt_f32 (float32x2_t val)  float32x4_t cv_vrsqrtq_f32 (float32x4_t val)  float32x2_t cv_vsqrt_f32 (float32x2_t val)  float32x4_t cv_vsqrtq_f32 (float32x4_t val) 
core,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
core,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
core,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
core,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
core,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
core,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
core,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
core,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
core,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
core,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
core,This module has been originally developed as a project for Google Summer of Code 2012-2015.
core,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
core,Automatically Allocated Buffer Class.
core,"The class is used for temporary buffers in functions and methods. If a temporary buffer is usually small (a few K's of memory), but its size depends on the parameters, it makes sense to create a small fixed-size array on stack and use it if it's large enough. If the required buffer size is larger than the fixed size, another buffer of sufficient size is allocated dynamically and released after the processing. Therefore, in typical cases, when the buffer size is small, there is no overhead associated with malloc()/free(). At the same time, there is no limit on the size of processed data."
core,This is what AutoBuffer does. The template takes 2 parameters - type of the buffer elements and the number of stack-allocated elements. Here is how the class is used:
core,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
core,The distortion-free projective transformation given by a pinhole camera model is shown below.
core,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
core,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
core,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
core,\[p = A P_c.\]
core,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
core,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
core,and thus
core,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
core,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
core,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
core,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
core,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
core,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
core,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
core,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
core,and therefore
core,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
core,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
core,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
core,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
core,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
core,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
core,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
core,with
core,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
core,The following figure illustrates the pinhole camera model.
core,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
core,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
core,where
core,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
core,with
core,\[r^2 = x'^2 + y'^2\]
core,and
core,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
core,if \(Z_c \ne 0\).
core,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
core,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
core,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
core,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
core,where
core,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
core,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
core,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
core,In the functions below the coefficients are passed or returned as
core,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
core,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
core,The functions below use the above model to do the following:
core,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
core,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
core,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
core,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
core,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
core,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
core,if \(W \ne 0\).
core,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
core,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
core,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
core,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
core,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
core,Template Read-Only Sparse Matrix Iterator Class.
core,This is the derived from SparseMatConstIterator class that introduces more convenient operator *() for accessing the current element.
core,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
core,The class defining termination criteria for iterative algorithms.
core,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
core,"ArUco Marker Detection, module functionality was moved to objdetect module"
core,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
core,a Class to measure passing time.
core,"The class computes passing time by counting the number of ticks per second. That is, the following code computes the execution time in seconds:"
core,It is also possible to compute the average time over multiple runs:
core,Simple TLS data class.
core,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
core,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
core,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
core,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
core,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
core,The algorithms in this section minimize or maximize function value within specified constraints or without any constraints.
core,"Classes class cv::ConjGradSolver  This class is used to perform the non-linear non-constrained minimization of a function with known gradient,. More...  class cv::DownhillSolver  This class is used to perform the non-linear non-constrained minimization of a function,. More...  class cv::MinProblemSolver  Basic interface for all solvers. More... "
core,Template class specifying a continuous subsequence (slice) of a sequence.
core,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
core,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
core,Quaternion is a number system that extends the complex numbers. It can be expressed as a rotation in three-dimensional space. A quaternion is generally represented in the form:
core,\[q = w + x\boldsymbol{i} + y\boldsymbol{j} + z\boldsymbol{k}\]
core,"\[q = [w, x, y, z]\]"
core,"\[q = [w, \boldsymbol{v}] \]"
core,"\[q = ||q||[\cos\psi, u_x\sin\psi,u_y\sin\psi, u_z\sin\psi].\]"
core,"\[q = ||q||[\cos\psi, \boldsymbol{u}\sin\psi]\]"
core,"where \(\psi = \frac{\theta}{2}\), \(\theta\) represents rotation angle, \(\boldsymbol{u} = [u_x, u_y, u_z]\) represents normalized rotation axis, and \(||q||\) represents the norm of \(q\)."
core,"A unit quaternion is usually represents rotation, which has the form:"
core,"\[q = [\cos\psi, u_x\sin\psi,u_y\sin\psi, u_z\sin\psi].\]"
core,"To create a quaternion representing the rotation around the axis \(\boldsymbol{u}\) with angle \(\theta\), you can use"
core,You can simply use four same type number to create a quaternion
core,Or use a Vec4d or Vec4f vector.
core,"If you already have a 3x3 rotation matrix R, then you can use"
core,"If you already have a rotation vector rvec which has the form of angle * axis, then you can use"
core,"To extract the rotation matrix from quaternion, see toRotMat3x3()"
core,"To extract the Vec4d or Vec4f, see toVec()"
core,"To extract the rotation vector, see toRotVec()"
core,"If there are two quaternions \(q_0, q_1\) are needed to interpolate, you can use nlerp(), slerp() or spline()"
core,spline can smoothly connect rotations of multiple quaternions
core,Three ways to get an element in Quaternion
core,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
core,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
core,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
core,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
core,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
core,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
core,XML/YAML/JSON file storage class that encapsulates all the information necessary for writing or reading data to/from a file.
core,Classes class cv::AsyncArray  Returns result of asynchronous operations. More...  class cv::AsyncPromise  Provides result of asynchronous operations. More... 
core,API for OpenCV external plugins:
core,HAL accelerators VideoIO camera backends / decoders / encoders Imgcodecs encoders / decoders
core,Plugins are usually built separately or before OpenCV (OpenCV can depend on them - like HAL libraries).
core,Using this approach OpenCV provides some basic low level functionality for external plugins.
core,Classes struct OpenCV_API_Header 
core,A complex number class.
core,"The template class is similar and compatible with std::complex, however it provides slightly more convenient access to the real and imaginary parts using through the simple field access, as opposite to std::complex::real() and std::complex::imag()."
core,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
core,Mersenne Twister random number generator.
core,Inspired by http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/CODES/mt19937ar.c
core,Template class for 2D points specified by its coordinates x and y.
core,"An instance of the class is interchangeable with C structures, CvPoint and CvPoint2D32f . There is also a cast operator to convert point coordinates to the specified type. The conversion from floating-point coordinates to integer coordinates is done by rounding. Commonly, the conversion uses this operation for each of the coordinates. Besides the class members listed in the declaration above, the following operations on points are implemented:"
core,"For your convenience, the following type aliases are defined:"
core,Example:
core,Linear Discriminant Analysis.
core,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
core,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
core,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
core,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
core,"Each class derived from Map implements a motion model, as follows:"
core,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
core,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
core,The classes derived from Mapper are
core,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
core,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
core,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
core,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
core,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
core,This module provides storage routines for Hierarchical Data Format objects.
core,Face module changelog Face Recognition with OpenCV
core,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
core,The class represents rotated (i.e. not up-right) rectangles on a plane.
core,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
core,The sample below demonstrates how to use RotatedRect:
core,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
core,Classes class cv::plot::Plot2d 
core,Classes class cv::quality::QualityBase 
core,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
core,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
core,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
core,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
core,TLS data accumulator with gathering methods.
core,Classes class cv::BufferPoolController  class cv::ocl::Context  class cv::ocl::Device  class cv::ocl::Image2D  class cv::ocl::Kernel  class cv::ocl::KernelArg  class cv::ocl::OpenCLExecutionContext  class cv::ocl::OpenCLExecutionContextScope  class cv::ocl::Platform  class cv::ocl::PlatformInfo  class cv::ocl::Program  class cv::ocl::ProgramSource  class cv::ocl::Queue  class cv::ocl::Timer 
core,Namespaces namespace cv::traits 
core,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
core,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
core,Namespace for all functions is cv::intensity_transform.
core,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
core,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
core,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
core,Affine transform.
core,It represents a 4x4 homogeneous transformation matrix \(T\)
core,\[T = \begin{bmatrix} R & t\\ 0 & 1\\ \end{bmatrix} \]
core,where \(R\) is a 3x3 rotation matrix and \(t\) is a 3x1 translation vector.
core,"You can specify \(R\) either by a 3x3 rotation matrix or by a 3x1 rotation vector, which is converted to a 3x3 rotation matrix by the Rodrigues formula."
core,"To construct a matrix \(T\) representing first rotation around the axis \(r\) with rotation angle \(|r|\) in radian (right hand rule) and then translation by the vector \(t\), you can use"
core,"If you already have the rotation matrix \(R\), then you can use"
core,"To extract the rotation matrix \(R\) from \(T\), use"
core,"To extract the translation vector \(t\) from \(T\), use"
core,"To extract the rotation vector \(r\) from \(T\), use"
core,Note that since the mapping from rotation vectors to rotation matrices is many to one. The returned rotation vector is not necessarily the one you used before to set the matrix.
core,"If you have two transformations \(T = T_1 * T_2\), use"
core,"To get the inverse transform of \(T\), use"
core,The class SparseMat represents multi-dimensional sparse numerical arrays.
core,"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:"
core,"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5; int size[5] = {10, 10, 10, 10, 10}; SparseMat sparse_mat(dims, size, CV_32F); for(int i = 0; i < 1000; i++) { int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f; } cout << ""nnz = "" << sparse_mat.nzcount() << endl; cv::SparseMatThe class SparseMat represents multi-dimensional sparse numerical arrays.Definition mat.hpp:2751 cv::SparseMat::sizeconst int * size() constreturns the array of sizes, or NULL if the matrix is not allocated cv::SparseMat::dimsint dims() constreturns the matrix dimensionality CV_32F#define CV_32FDefinition interface.h:78 Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix // and the sum of elements. SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>(); double s = 0; int dims = sparse_mat.dims(); for(; it != it_end; ++it) { // print element indices and the element value const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("": %g\n"", it.value<float>()); s += *it; } printf(""Element sum is %g\n"", s); cv::SparseMatConstIterator_Template Read-Only Sparse Matrix Iterator Class.Definition mat.hpp:3354 cv::SparseMatConstIterator::valueconst _Tp & value() consttemplate method returning the current matrix element cv::SparseMatConstIterator::nodeconst SparseMat::Node * node() constreturns the current node of the sparse matrix. it.node->idx is the current element index cv::SparseMat::Nodesparse matrix node - element of a hash tableDefinition mat.hpp:2776 cv::SparseMat::Node::idxint idx[MAX_DIM]index of the matrix elementDefinition mat.hpp:2782 If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation. Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b) { const SparseMat *_a = &a, *_b = &b; // if b contains less elements than a, // it is faster to iterate through b if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // take the next element from the first matrix float avalue = *it; const Node* anode = it.node(); // and try to find an element with the same index in the second matrix. // since the hash value depends only on the element index, // reuse the hash value stored in the node float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr; } cv::SparseMat::endSparseMatIterator end()returns the sparse matrix iterator at the matrix end cv::SparseMat::nzcountsize_t nzcount() constreturns the number of non-zero elements (=the number of hash table nodes) cv::SparseMat::beginSparseMatIterator begin()returns the sparse matrix iterator at the matrix beginning cv::SparseMat::Node::hashvalsize_t hashvalhash valueDefinition mat.hpp:2778"
core,Read and write video or images sequence with OpenCV.
core,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
core,Bioinspired Module Retina Introduction
core,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
core,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
core,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
core,See detailed overview here: Machine Learning Overview.
core,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
core,API below is provided to resolve problem of CPU resource over-subscription by multiple thread pools from different multi-threading frameworks. This is common problem for cases when OpenCV compiled threading framework is different from the Users Applications framework.
core,Applications can replace OpenCV parallel_for() backend with own implementation (to reuse Application's thread pool).
core,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
core,"Functions void cv::directx::convertFromD3D10Texture2D (ID3D10Texture2D *pD3D10Texture2D, OutputArray dst)  Converts ID3D10Texture2D to OutputArray.  void cv::directx::convertFromD3D11Texture2D (ID3D11Texture2D *pD3D11Texture2D, OutputArray dst)  Converts ID3D11Texture2D to OutputArray. If input texture format is DXGI_FORMAT_NV12 then data will be upsampled and color-converted to BGR format.  void cv::directx::convertFromDirect3DSurface9 (IDirect3DSurface9 *pDirect3DSurface9, OutputArray dst, void *surfaceSharedHandle=NULL)  Converts IDirect3DSurface9 to OutputArray.  void cv::directx::convertToD3D10Texture2D (InputArray src, ID3D10Texture2D *pD3D10Texture2D)  Converts InputArray to ID3D10Texture2D.  void cv::directx::convertToD3D11Texture2D (InputArray src, ID3D11Texture2D *pD3D11Texture2D)  Converts InputArray to ID3D11Texture2D. If destination texture format is DXGI_FORMAT_NV12 then input UMat expected to be in BGR format and data will be downsampled and color-converted to NV12.  void cv::directx::convertToDirect3DSurface9 (InputArray src, IDirect3DSurface9 *pDirect3DSurface9, void *surfaceSharedHandle=NULL)  Converts InputArray to IDirect3DSurface9.  int cv::directx::getTypeFromD3DFORMAT (const int iD3DFORMAT)  Get OpenCV type from DirectX type.  int cv::directx::getTypeFromDXGI_FORMAT (const int iDXGI_FORMAT)  Get OpenCV type from DirectX type.  Context & cv::directx::ocl::initializeContextFromD3D10Device (ID3D10Device *pD3D10Device)  Creates OpenCL context from D3D10 device.  Context & cv::directx::ocl::initializeContextFromD3D11Device (ID3D11Device *pD3D11Device)  Creates OpenCL context from D3D11 device.  Context & cv::directx::ocl::initializeContextFromDirect3DDevice9 (IDirect3DDevice9 *pDirect3DDevice9)  Creates OpenCL context from Direct3DDevice9 device.  Context & cv::directx::ocl::initializeContextFromDirect3DDevice9Ex (IDirect3DDevice9Ex *pDirect3DDevice9Ex)  Creates OpenCL context from Direct3DDevice9Ex device. "
core,File Storage Node class.
core,"The node is used to store each and every element of the file storage opened for reading. When XML/YAML file is read, it is first parsed and stored in the memory as a hierarchical collection of nodes. Each node can be a ""leaf"" that is contain a single number or a string, or be a collection of other nodes. There can be named collections (mappings) where each element has a name and it is accessed by a name, and ordered collections (sequences) where elements do not have names but rather accessed by index. Type of the file node can be determined using FileNode::type method."
core,"Note that file nodes are only used for navigating file storages opened for reading. When a file storage is opened for writing, no data is stored in memory after it is written."
core,Template class for small matrices whose type and size are known at compilation time.
core,"If you need a more flexible type, use Mat . The elements of the matrix M are accessible using the M(i,j) notation. Most of the common matrix operations (see also MatrixExpressions ) are available. To do an operation on Matx that is not implemented, you can easily convert the matrix to Mat and backwards:"
core,"Except of the plain constructor which takes a list of elements, Matx can be initialized from a C-array:"
core,"In case if C++11 features are available, std::initializer_list can be also used to initialize Matx:"
core,Basic interface for all solvers.
core,TLS container base implementation
core,Don't use directly.
core,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
core,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
core,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
core,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
core,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
core,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
core,Custom array allocator.
core,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
core,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
core,"Template class for 3D points specified by its coordinates x, y and z."
core,"An instance of the class is interchangeable with the C structure CvPoint2D32f . Similarly to Point_ , the coordinates of 3D points can be converted to another type. The vector arithmetic and comparison operations are also supported."
core,The following Point3_<> aliases are available:
core,"""Universal intrinsics"" is a types and functions set intended to simplify vectorization of code on different platforms. Currently a few different SIMD extensions on different architectures are supported. 128 bit registers of various types support is implemented for a wide range of architectures including x86(SSE/SSE2/SSE4.2), ARM(NEON), PowerPC(VSX), MIPS(MSA). 256 bit long registers are supported on x86(AVX2) and 512 bit long registers are supported on x86(AVX512). In case when there is no SIMD extension available during compilation, fallback C++ implementation of intrinsics will be chosen and code will work as expected although it could be slower."
core,Read-Only Sparse Matrix Iterator.
core,Here is how to use the iterator to compute the sum of floating-point sparse matrix elements:
core,Singular Value Decomposition.
core,"Class for computing Singular Value Decomposition of a floating-point matrix. The Singular Value Decomposition is used to solve least-square problems, under-determined linear systems, invert matrices, compute condition numbers, and so on."
core,"If you want to compute a condition number of a matrix or an absolute value of its determinant, you do not need u and vt. You can pass flags=SVD::NO_UV|... . Another flag SVD::FULL_UV indicates that full-size u and vt must be computed, which is not necessary most of the time."
core,"Enumerations enum cv::TemplateMatchModes { cv::TM_SQDIFF = 0 , cv::TM_SQDIFF_NORMED = 1 , cv::TM_CCORR = 2 , cv::TM_CCORR_NORMED = 3 , cv::TM_CCOEFF = 4 , cv::TM_CCOEFF_NORMED = 5 }  type of the template matching operation More... "
core,Template matrix class derived from Mat.
core,"The class Mat_<_Tp> is a thin template wrapper on top of the Mat class. It does not have any extra data fields. Nor this class nor Mat has any virtual methods. Thus, references or pointers to these two classes can be freely but carefully converted one to another. For example:"
core,"While Mat is sufficient in most cases, Mat_ can be more convenient if you use a lot of element access operations and if you know matrix type at the compilation time. Note that Mat::at(int y,int x) and Mat_::operator()(int y,int x) do absolutely the same and run at the same speed, but the latter is certainly shorter:"
core,"To use Mat_ for multi-channel images/matrices, pass Vec as a Mat_ parameter:"
core,Mat_ is fully compatible with C++11 range-based for loop. For example such loop can be used to safely apply look-up table:
core,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
core,"Namespace for all functions is cvv, i.e. cvv::showImage()."
core,Compilation:
core,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
core,See cvv tutorial for a commented example application using cvv.
core,Namespaces namespace cvv::impl 
dnn,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
dnn,Designed for command line parsing.
dnn,The sample below demonstrates how to use CommandLineParser:
dnn,"Template ""trait"" class for OpenCV primitive data types."
dnn,"A primitive OpenCV data type is one of unsigned char, bool, signed char, unsigned short, signed short, int, float, double, or a tuple of values of one of these types, where all the values in the tuple have the same type. Any primitive type from the list can be defined by an identifier in the form CV_<bit-depth>{U|S|F}C(<number_of_channels>), for example: uchar CV_8UC1, 3-element floating-point tuple CV_32FC3, and so on. A universal OpenCV structure that is able to store a single instance of such a primitive data type is Vec. Multiple instances of such a type can be stored in a std::vector, Mat, Mat_, SparseMat, SparseMat_, or any other container that is able to store Vec instances."
dnn,"The DataType class is basically used to provide a description of such primitive data types without adding any fields or methods to the corresponding classes (and it is actually impossible to add anything to primitive C/C++ data types). This technique is known in C++ as class traits. It is not DataType itself that is used but its specialized versions, such as:"
dnn,"The main purpose of this class is to convert compilation-time type information to an OpenCV-compatible data type identifier, for example:"
dnn,"So, such traits are used to tell OpenCV which data type you are working with, even if such a type is not native to OpenCV. For example, the matrix B initialization above is compiled because OpenCV defines the proper specialized template class DataType<complex<_Tp> > . This mechanism is also useful (and used in OpenCV this way) for generic algorithms implementations."
dnn,opencv2/core/traits.hpp
dnn,src source array dst destination array len length of arrays
dnn,"Functions int hal_ni_invSqrt32f (const float *src, float *dst, int len)  int hal_ni_invSqrt64f (const double *src, double *dst, int len) "
dnn,Base storage class for GPU memory with reference counting.
dnn,Its interface matches the Mat interface with the following limitations:
dnn,no arbitrary dimensions support (only 2D) no functions that return references to their data (because references on GPU are not valid for CPU) no expression templates technique support
dnn,Beware that the latter limitation may lead to overloaded matrix operators that cause memory allocations. The GpuMat class is convertible to cuda::PtrStepSz and cuda::PtrStep so it can be passed directly to the kernel.
dnn,"Some member functions are described as a ""Blocking Call"" while some are described as a ""Non-Blocking Call"". Blocking functions are synchronous to host. It is guaranteed that the GPU operation is finished when the function returns. However, non-blocking functions are asynchronous to host. Those functions may return even if the GPU operation is not finished."
dnn,"Compared to their blocking counterpart, non-blocking functions accept Stream as an additional argument. If a non-default stream is passed, the GPU operation may overlap with operations in other streams."
dnn,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
dnn,"InnerProduct, MatMul and Gemm operations are all implemented by Fully Connected Layer. Parameter is_matmul is used to distinguish MatMul and Gemm from InnerProduct."
dnn,This class represents high-level API for classification models.
dnn,"ClassificationModel allows to set params for preprocessing input image. ClassificationModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and return top-1 prediction."
dnn,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
dnn,This module includes photo processing algorithms
dnn,"Matrix expression representation This is a list of implemented matrix operations that can be combined in arbitrary complex expressions (here A, B stand for matrices ( Mat ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double )):"
dnn,"Addition, subtraction, negation: A+B, A-B, A+s, A-s, s+A, s-A, -A Scaling: A*alpha Per-element multiplication and division: A.mul(B), A/B, alpha/A Matrix multiplication: A*B Transposition: A.t() (means AT) Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems: A.inv([method]) (~ A<sup>-1</sup>), A.inv([method])*B (~ X: AX=B) Comparison: A cmpop B, A cmpop alpha, alpha cmpop A, where cmpop is one of >, >=, ==, !=, <=, <. The result of comparison is an 8-bit single channel mask whose elements are set to 255 (if the particular element or pair of elements satisfy the condition) or 0. Bitwise logical operations: A logicop B, A logicop s, s logicop A, ~A, where logicop is one of &, |, ^. Element-wise minimum and maximum: min(A, B), min(A, alpha), max(A, B), max(A, alpha) Element-wise absolute value: abs(A) Cross-product, dot-product: A.cross(B), A.dot(B) Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm, mean, sum, countNonZero, trace, determinant, repeat, and others. Matrix initializers ( Mat::eye(), Mat::zeros(), Mat::ones() ), matrix comma-separated initializers, matrix constructors and operators that extract sub-matrices (see Mat description). Mat_<destination_type>() constructors to cast the result to the proper type. NoteComma-separated initializers and probably some other operations may require additional explicit Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity. Here are examples of matrix expressions: // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD) SVD svd(A); Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t(); // compute the new vector of parameters in the Levenberg-Marquardt algorithm x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err); // sharpen image using ""unsharp mask"" algorithm Mat blurred; double sigma = 1, threshold = 5, amount = 1; GaussianBlur(img, blurred, Size(), sigma, sigma); Mat lowContrastMask = abs(img - blurred) < threshold; Mat sharpened = img*(1+amount) + blurred*(-amount); img.copyTo(sharpened, lowContrastMask); cv::MatExpr::absMatExpr abs(const Mat &m)Calculates an absolute value of each matrix element. cv::MatExpr::invMatExpr inv(int method=DECOMP_LU) const cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::Mat::diagMat diag(int d=0) constExtracts a diagonal from a matrix. cv::Mat::uUMatData * uinteraction with UMatDefinition mat.hpp:2174 cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Mat::tMatExpr t() constTransposes a matrix. cv::SVDSingular Value Decomposition.Definition core.hpp:2739 cv::DECOMP_CHOLESKY@ DECOMP_CHOLESKYDefinition base.hpp:143 cv::SizeSize2i SizeDefinition types.hpp:370 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. cv::thresholddouble threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)Applies a fixed-level threshold to each array element."
dnn,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
dnn,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
dnn,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
dnn,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
dnn,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
dnn,The implemented stitching pipeline is very similar to the one proposed in [41] .
dnn,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
dnn,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
dnn,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
dnn,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
dnn,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
dnn,Template class for a 4-element vector derived from Vec.
dnn,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
dnn,Random Number Generator.
dnn,"Random number generator. It encapsulates the state (currently, a 64-bit integer) and has methods to return scalar random values and to fill arrays with random values. Currently it supports uniform and Gaussian (normal) distributions. The generator uses Multiply-With-Carry algorithm, introduced by G. Marsaglia ( http://en.wikipedia.org/wiki/Multiply-with-carry ). Gaussian-distribution random numbers are generated using the Ziggurat algorithm ( http://en.wikipedia.org/wiki/Ziggurat_algorithm ), introduced by G. Marsaglia and W. W. Tsang."
dnn,Matrix read-only iterator.
dnn,Class passed to an error.
dnn,This class encapsulates all or almost all necessary information about the error happened in the program. The exception is usually constructed and thrown implicitly via CV_Error and CV_Error_ macros.
dnn,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
dnn,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
dnn,Bitwise AND: dst[i] = src1[i] & src2[i] Bitwise OR: dst[i] = src1[i] | src2[i] Bitwise XOR: dst[i] = src1[i] ^ src2[i] Bitwise NOT: dst[i] = !src[i]
dnn,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
dnn,"Functions int hal_ni_and8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_not8u (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_or8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_xor8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height) "
dnn,This module includes signal processing algorithms.
dnn,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
dnn,Data structure for salient point detectors.
dnn,"The class instance stores a keypoint, i.e. a point feature found by one of many available keypoint detectors, such as Harris corner detector, FAST, StarDetector, SURF, SIFT etc."
dnn,"The keypoint is characterized by the 2D position, scale (proportional to the diameter of the neighborhood that needs to be taken into account), orientation and some other parameters. The keypoint neighborhood is then analyzed by another algorithm that builds a descriptor (usually represented as a feature vector). The keypoints representing the same object in different images can then be matched using KDTree or another method."
dnn,ICP point-to-plane odometry algorithm
dnn,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
dnn,Template class for 2D rectangles.
dnn,described by the following parameters:
dnn,"Coordinates of the top-left corner. This is a default interpretation of Rect_::x and Rect_::y in OpenCV. Though, in your algorithms you may count x and y from the bottom-left corner. Rectangle width and height."
dnn,"OpenCV typically assumes that the top and left boundary of the rectangle are inclusive, while the right and bottom boundaries are not. For example, the method Rect_::contains returns true if"
dnn,"\[x \leq pt.x < x+width, y \leq pt.y < y+height\]"
dnn,Virtually every loop over an image ROI in OpenCV (where ROI is specified by Rect_<int> ) is implemented as:
dnn,"In addition to the class members, the following operations on rectangles are implemented:"
dnn,"\(\texttt{rect} = \texttt{rect} \pm \texttt{point}\) (shifting a rectangle by a certain offset) \(\texttt{rect} = \texttt{rect} \pm \texttt{size}\) (expanding or shrinking a rectangle by a certain amount) rect += point, rect -= point, rect += size, rect -= size (augmenting operations) rect = rect1 & rect2 (rectangle intersection) rect = rect1 | rect2 (minimum area rectangle containing rect1 and rect2 ) rect &= rect1, rect |= rect1 (and the corresponding augmenting operations) rect == rect1, rect != rect1 (rectangle comparison)"
dnn,This is an example how the partial ordering on rectangles can be established (rect1 \(\subseteq\) rect2):
dnn,"For your convenience, the Rect_<> alias is available: cv::Rect"
dnn,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
dnn,Base class for parallel data processors.
dnn,YOUR ATTENTION PLEASE!
dnn,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
dnn,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
dnn,Note for developers: please don't put videoio dependency in G-API because of this file.
dnn,Dense optical flow algorithms compute motion for each point:
dnn,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
dnn,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
dnn,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
dnn,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
dnn,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
dnn,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
dnn,This type is very similar to InputArray except that it is used for input/output and output function parameters.
dnn,"Just like with InputArray, OpenCV users should not care about OutputArray, they just pass Mat, vector<T> etc. to the functions. The same limitation as for InputArray: Do not explicitly create OutputArray instances applies here too."
dnn,"If you want to make your function polymorphic (i.e. accept different arrays as output parameters), it is also not very difficult. Take the sample above as the reference. Note that _OutputArray::create() needs to be called before _OutputArray::getMat(). This way you guarantee that the output array is properly allocated."
dnn,"Optional output parameters. If you do not need certain output array to be computed and returned to you, pass cv::noArray(), just like you would in the case of optional input array. At the implementation level, use _OutputArray::needed() to check if certain output array needs to be computed or not."
dnn,There are several synonyms for OutputArray that are used to assist automatic Python/Java/... wrapper generators:
dnn,The function performs generalized matrix multiplication similar to the gemm functions in BLAS level 3: \(D = \alpha*AB+\beta*C\)
dnn,"src1 pointer to input \(M\times N\) matrix \(A\) or \(A^T\) stored in row major order. src1_step number of bytes between two consequent rows of matrix \(A\) or \(A^T\). src2 pointer to input \(N\times K\) matrix \(B\) or \(B^T\) stored in row major order. src2_step number of bytes between two consequent rows of matrix \(B\) or \(B^T\). alpha \(\alpha\) multiplier before \(AB\) src3 pointer to input \(M\times K\) matrix \(C\) or \(C^T\) stored in row major order. src3_step number of bytes between two consequent rows of matrix \(C\) or \(C^T\). beta \(\beta\) multiplier before \(C\) dst pointer to input \(M\times K\) matrix \(D\) stored in row major order. dst_step number of bytes between two consequent rows of matrix \(D\). m number of rows in matrix \(A\) or \(A^T\), equals to number of rows in matrix \(D\) n number of columns in matrix \(A\) or \(A^T\) k number of columns in matrix \(B\) or \(B^T\), equals to number of columns in matrix \(D\) flags algorithm options (combination of CV_HAL_GEMM_1_T, ...)."
dnn,"Functions int hal_ni_gemm32f (const float *src1, size_t src1_step, const float *src2, size_t src2_step, float alpha, const float *src3, size_t src3_step, float beta, float *dst, size_t dst_step, int m, int n, int k, int flags)  int hal_ni_gemm32fc (const float *src1, size_t src1_step, const float *src2, size_t src2_step, float alpha, const float *src3, size_t src3_step, float beta, float *dst, size_t dst_step, int m, int n, int k, int flags)  int hal_ni_gemm64f (const double *src1, size_t src1_step, const double *src2, size_t src2_step, double alpha, const double *src3, size_t src3_step, double beta, double *dst, size_t dst_step, int m, int n, int k, int flags)  int hal_ni_gemm64fc (const double *src1, size_t src1_step, const double *src2, size_t src2_step, double alpha, const double *src3, size_t src3_step, double beta, double *dst, size_t dst_step, int m, int n, int k, int flags) "
dnn,Derivatives of this class encapsulates functions of certain backends.
dnn,GRU recurrent one-layer.
dnn,Accepts input sequence and computes the final hidden state for each element in the batch.
dnn,"input[0] containing the features of the input sequence. input[0] should have shape [T, N, data_dims] where T is sequence length, N is batch size, data_dims is input size output would have shape [T, N, D * hidden_size] where D = 2 if layer is bidirectional otherwise D = 1"
dnn,Depends on the following attributes:
dnn,hidden_size - Number of neurons in the hidden layer direction - RNN could be bidirectional or forward
dnn,The final hidden state \( h_t \) computes by the following formulas:
dnn,\begin{eqnarray*} r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\ z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\ n_t = \tanh(W_{in} x_t + b_{in} + r_t \odot (W_{hn} h_{(t-1)}+ b_{hn})) \\ h_t = (1 - z_t) \odot n_t + z_t \odot h_{(t-1)} \\ \end{eqnarray*}
dnn,"Where \(x_t\) is current input, \(h_{(t-1)}\) is previous or initial hidden state."
dnn,"\(W_{x?}\), \(W_{h?}\) and \(b_{?}\) are learned weights represented as matrices: \(W_{x?} \in R^{N_h \times N_x}\), \(W_{h?} \in R^{N_h \times N_h}\), \(b_? \in R^{N_h}\)."
dnn,\(\odot\) is per-element multiply operation.
dnn,src source array dst destination array len length of arrays
dnn,"Functions int hal_ni_log32f (const float *src, float *dst, int len)  int hal_ni_log64f (const double *src, double *dst, int len) "
dnn,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
dnn,Template Read-Write Sparse Matrix Iterator Class.
dnn,This is the derived from cv::SparseMatConstIterator_ class that introduces more convenient operator *() for accessing the current element.
dnn,The STL-compliant memory Allocator based on cv::fastMalloc() and cv::fastFree()
dnn,This is a base class for all more or less complex algorithms in OpenCV.
dnn,"especially for classes of algorithms, for which there can be multiple implementations. The examples are stereo correspondence (for which there are algorithms like block matching, semi-global block matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck etc.)."
dnn,Here is example of SimpleBlobDetector use in your application via Algorithm interface:
dnn,n-dimensional dense array class
dnn,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
dnn,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
dnn,"In case of a 2-dimensional array, the above formula is reduced to:"
dnn,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
dnn,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
dnn,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
dnn,There are many different ways to create a Mat object. The most popular options are listed below:
dnn,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
dnn,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
dnn,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
dnn,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
dnn,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
dnn,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
dnn,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
dnn,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
dnn,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
dnn,This interface class allows to build new Layers - are building blocks of networks.
dnn,"Each class, derived from Layer, must implement forward() method to compute outputs. Also before using the new layer into networks you must register your layer by using one of LayerFactory macros."
dnn,Principal Component Analysis.
dnn,"The class is used to calculate a special basis for a set of vectors. The basis will consist of eigenvectors of the covariance matrix calculated from the input set of vectors. The class PCA can also transform vectors to/from the new coordinate space defined by the basis. Usually, in this new coordinate system, each vector from the original set (and any linear combination of such vectors) can be quite accurately approximated by taking its first few components, corresponding to the eigenvectors of the largest eigenvalues of the covariance matrix. Geometrically it means that you calculate a projection of the vector to a subspace formed by a few eigenvectors corresponding to the dominant eigenvalues of the covariance matrix. And usually such a projection is very close to the original vector. So, you can represent the original vector from a high-dimensional space with a much shorter vector consisting of the projected vector's coordinates in the subspace. Such a transformation is also known as Karhunen-Loeve Transform, or KLT. See http://en.wikipedia.org/wiki/Principal_component_analysis"
dnn,"The sample below is the function that takes two matrices. The first function stores a set of vectors (a row per vector) that is used to calculate PCA. The second function stores another ""test"" set of vectors (a row per vector). First, these vectors are compressed with PCA, then reconstructed back, and then the reconstruction error norm is computed and printed for each vector. :"
dnn,Namespaces namespace cv::traits 
dnn,Namespaces namespace cv  namespace cv::utils::logging::internal 
dnn,Namespaces namespace cv::omnidir::internal 
dnn,This class is presented high-level API for neural networks.
dnn,"Model allows to set params for preprocessing input image. Model creates net from file with trained weights and config, sets preprocessing input and runs forward pass."
dnn,This class represents high-level API for object detection networks.
dnn,"DetectionModel allows to set params for preprocessing input image. DetectionModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and return result detections. For DetectionModel SSD, Faster R-CNN, YOLO topologies are supported."
dnn,Returns result of asynchronous operations.
dnn,Object has attached asynchronous state. Assignment operator doesn't clone asynchronous state (it is shared between all instances).
dnn,Result can be fetched via get() method only once.
dnn,This is the proxy class for passing read-only input arrays into OpenCV functions.
dnn,It is defined as:
dnn,"where _InputArray is a class that can be constructed from Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat_<T> >, UMat, std::vector<UMat> or double. It can also be constructed from a matrix expression."
dnn,"Since this is mostly implementation-level class, and its interface may change in future versions, we do not describe it in details. There are a few key things, though, that should be kept in mind:"
dnn,"When you see in the reference manual or in OpenCV source code a function that takes InputArray, it means that you can actually pass Mat, Matx, vector<T> etc. (see above the complete list). Optional input arguments: If some of the input arrays may be empty, pass cv::noArray() (or simply cv::Mat() as you probably did before). The class is designed solely for passing parameters. That is, normally you should not declare class members, local and global variables of this type. If you want to design your own function or a class method that can operate of arrays of multiple types, you can use InputArray (or OutputArray) for the respective parameters. Inside a function you should use _InputArray::getMat() method to construct a matrix header for the array (without copying data). _InputArray::kind() can be used to distinguish Mat from vector<> etc., but normally it is not needed."
dnn,Here is how you can use a function that takes InputArray :
dnn,"That is, we form an STL vector containing points, and apply in-place affine transformation to the vector using the 2x3 matrix created inline as Matx<float, 2, 3> instance."
dnn,"Here is how such a function can be implemented (for simplicity, we implement a very specific case of it, according to the assertion statement inside) :"
dnn,"There is another related type, InputArrayOfArrays, which is currently defined as a synonym for InputArray:"
dnn,"It denotes function arguments that are either vectors of vectors or vectors of matrices. A separate synonym is needed to generate Python/Java etc. wrappers properly. At the function implementation level their use is similar, but _InputArray::getMat(idx) should be used to get header for the idx-th component of the outer vector and _InputArray::size().area() should be used to find the number of components (vectors/matrices) of the outer vector."
dnn,"In general, type support is limited to cv::Mat types. Other types are forbidden. But in some cases we need to support passing of custom non-general Mat types, like arrays of cv::KeyPoint, cv::DMatch, etc. This data is not intended to be interpreted as an image data, or processed somehow like regular cv::Mat. To pass such custom type use rawIn() / rawOut() / rawInOut() wrappers. Custom type is wrapped as Mat-compatible CV_8UC<N> values (N = sizeof(T), N <= CV_CN_MAX)."
dnn,Information Flow algorithm implementaton for alphamatting
dnn,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
dnn,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
dnn,The implementation is based on [7].
dnn,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
dnn,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
dnn,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
dnn,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
dnn,Lookup table replacement Table consists of 256 elements of a size from 1 to 8 bytes having 1 channel or src_channels For 8s input type 128 is added to LUT index Destination should have the same element type and number of channels as lookup table elements
dnn,src_data Source image data src_step Source image step src_type Sorce image type lut_data Pointer to lookup table lut_channel_size Size of each channel in bytes lut_channels Number of channels in lookup table dst_data Destination data dst_step Destination step width Width of images height Height of images
dnn,"Functions int hal_ni_lut (const uchar *src_data, size_t src_step, size_t src_type, const uchar *lut_data, size_t lut_channel_size, size_t lut_channels, uchar *dst_data, size_t dst_step, int width, int height) "
dnn,Layer factory allows to create instances of registered layers.
dnn,"Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as Mat's). It means that for each pixel location \((x,y)\) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location \((x,y)\). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one."
dnn,"Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian \(3 \times 3\) filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (""replicated border"" extrapolation method), or assume that all the non-existing pixels are zeros (""constant border"" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see BorderTypes"
dnn,src source array dst destination array len length of arrays
dnn,"Functions int hal_ni_exp32f (const float *src, float *dst, int len)  int hal_ni_exp64f (const double *src, double *dst, int len) "
dnn,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
dnn,Namespace for all functions is cv::img_hash.
dnn,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
dnn,Adds extra values for specific axes.
dnn,"paddings Vector of paddings in format [ pad_before, pad_after, // [0]th dimension pad_before, pad_after, // [1]st dimension ... pad_before, pad_after ] // [n]th dimension that represents number of padded values at every dimension starting from the first one. The rest of dimensions won't be padded. value Value to be padded. Defaults to zero. type Padding type: 'constant', 'reflect' input_dims Torch's parameter. If input_dims is not equal to the actual input dimensionality then the [0]th dimension is considered as a batch dimension and paddings are shifted to a one dimension. Defaults to -1 that means padding corresponding to paddings."
dnn,"This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64."
dnn,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
dnn,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
dnn,Class for matching keypoint descriptors.
dnn,"query descriptor index, train descriptor index, train image index, and distance between descriptors."
dnn,Base class for text detection networks.
dnn,Computes reciprocial: dst[i] = scale / src[i]
dnn,src_data source image data src_step source image step dst_data destination image data dst_step destination image step width width of the images height height of the images scale additional multiplier
dnn,"Functions int hal_ni_recip16s (const short *src_data, size_t src_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip16u (const ushort *src_data, size_t src_step, ushort *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip32f (const float *src_data, size_t src_step, float *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip32s (const int *src_data, size_t src_step, int *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip64f (const double *src_data, size_t src_step, double *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip8s (const schar *src_data, size_t src_step, schar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip8u (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, double scale) "
dnn,This modules is to draw UTF-8 strings with freetype/harfbuzz.
dnn,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
dnn,Classes class cv::freetype::FreeType2 
dnn,Matrix read-write iterator.
dnn,Performs \(LU\) decomposition of square matrix \(A=P*L*U\) (where \(P\) is permutation matrix) and solves matrix equation \(A*X=B\). Function returns the \(sign\) of permutation \(P\) via parameter info.
dnn,"src1 pointer to input matrix \(A\) stored in row major order. After finish of work src1 contains at least \(U\) part of \(LU\) decomposition which is appropriate for determainant calculation: \(det(A)=sign*\prod_{j=1}^{M}a_{jj}\). src1_step number of bytes between two consequent rows of matrix \(A\). m size of square matrix \(A\). src2 pointer to \(M\times N\) matrix \(B\) which is the right-hand side of system \(A*X=B\). \(B\) stored in row major order. If src2 is null pointer only \(LU\) decomposition will be performed. After finish of work src2 contains solution \(X\) of system \(A*X=B\). src2_step number of bytes between two consequent rows of matrix \(B\). n number of right-hand vectors in \(M\times N\) matrix \(B\). info indicates success of decomposition. If *info is equals to zero decomposition failed, othervise *info is equals to \(sign\)."
dnn,"Functions int hal_ni_LU32f (float *src1, size_t src1_step, int m, float *src2, size_t src2_step, int n, int *info)  int hal_ni_LU64f (double *src1, size_t src1_step, int m, double *src2, size_t src2_step, int n, int *info) "
dnn,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
dnn,Read-write Sparse Matrix Iterator.
dnn,"The class is similar to cv::SparseMatConstIterator, but can be used for in-place modification of the matrix elements."
dnn,src source array dst destination array len length of arrays
dnn,"Functions int hal_ni_sqrt32f (const float *src, float *dst, int len)  int hal_ni_sqrt64f (const double *src, double *dst, int len) "
dnn,Namespaces namespace NcvCTprep 
dnn,n-ary multi-dimensional array iterator.
dnn,"Use the class to implement unary, binary, and, generally, n-ary element-wise operations on multi-dimensional arrays. Some of the arguments of an n-ary function may be continuous arrays, some may be not. It is possible to use conventional MatIterator 's for each array but incrementing all of the iterators after each small operations may be a big overhead. In this case consider using NAryMatIterator to iterate through several matrices simultaneously as long as they have the same geometry (dimensionality and all the dimension sizes are the same). On each iteration it.planes[0], it.planes[1],... will be the slices of the corresponding matrices."
dnn,The example below illustrates how you can compute a normalized and threshold 3D color histogram:
dnn,Absolute difference: dst[i] = | src1[i] - src2[i] |
dnn,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
dnn,"Functions int hal_ni_absdiff16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height) "
dnn,"x source X arrays y source Y arrays mag destination magnitude array angle destination angle array len length of arrays angleInDegrees if set to true return angles in degrees, otherwise in radians y source Y arrays x source X arrays dst destination array len length of arrays angleInDegrees if set to true return angles in degrees, otherwise in radians mag source magnitude arrays mag source angle arrays x destination X array y destination Y array len length of arrays angleInDegrees if set to true interpret angles from degrees, otherwise from radians"
dnn,"Functions int hal_ni_cartToPolar32f (const float *x, const float *y, float *mag, float *angle, int len, bool angleInDegrees)  int hal_ni_cartToPolar64f (const double *x, const double *y, double *mag, double *angle, int len, bool angleInDegrees)  int hal_ni_fastAtan32f (const float *y, const float *x, float *dst, int len, bool angleInDegrees)  int hal_ni_fastAtan64f (const double *y, const double *x, double *dst, int len, bool angleInDegrees)  int hal_ni_polarToCart32f (const float *mag, const float *angle, float *x, float *y, int len, bool angleInDegrees)  int hal_ni_polarToCart64f (const double *mag, const double *angle, double *x, double *y, int len, bool angleInDegrees) "
dnn,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
dnn,"Functions void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1, __m128 &v_a0, __m128 &v_a1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1, __m128 &v_a0, __m128 &v_a1) "
dnn,This module contains:
dnn,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
dnn,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
dnn,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
dnn,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
dnn,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
dnn,Template class for specifying the size of an image or rectangle.
dnn,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
dnn,OpenCV defines the following Size_<> aliases:
dnn,Element wise operation on inputs.
dnn,Extra optional parameters:
dnn,"""operation"" as string. Values are ""sum"" (default), ""prod"", ""max"", ""div"", ""min"" ""coeff"" as float array. Specify weights of inputs for SUM operation ""output_channels_mode"" as string. Values are ""same"" (default, all input must have the same layout), ""input_0"", ""input_0_truncate"", ""max_input_channels"""
dnn,"Drawing functions work with matrices/images of arbitrary depth. The boundaries of the shapes can be rendered with antialiasing (implemented only for 8-bit images for now). All the functions include the parameter color that uses an RGB value (that may be constructed with the Scalar constructor ) for color images and brightness for grayscale images. For color images, the channel ordering is normally Blue, Green, Red. This is what imshow, imread, and imwrite expect. So, if you form a color using the Scalar constructor, it should look like:"
dnn,"\[\texttt{Scalar} (blue \_ component, green \_ component, red \_ component[, alpha \_ component])\]"
dnn,"If you are using your own image rendering and I/O functions, you can use any channel ordering. The drawing functions process each channel independently and do not depend on the channel order or even on the used color space. The whole image can be converted from BGR to RGB or to a different color space using cvtColor ."
dnn,"If a drawn figure is partially or completely outside the image, the drawing functions clip it. Also, many drawing functions can handle pixel coordinates specified with sub-pixel accuracy. This means that the coordinates can be passed as fixed-point numbers encoded as integers. The number of fractional bits is specified by the shift parameter and the real point coordinates are calculated as \(\texttt{Point}(x,y)\rightarrow\texttt{Point2f}(x*2^{-shift},y*2^{-shift})\) . This feature is especially effective when rendering antialiased shapes."
dnn,Classes class cv::LineIterator  Class for iterating over all pixels on a raster line segment. More... 
dnn,This subsection of dnn module contains information about built-in layers and their descriptions.
dnn,"Classes listed here, in fact, provides C++ API for creating instances of built-in layers. In addition to this way of layers instantiation, there is a more common factory API (see Utilities for New Layers Registration), it allows to create layers dynamically (by name) and register new ones. You can use both API, but factory API is less convenient for native C++ programming and basically designed for use inside importers (see readNetFromCaffe(), readNetFromTorch(), readNetFromTensorflow())."
dnn,"Built-in layers partially reproduce functionality of corresponding Caffe and Torch7 layers. In particular, the following layers and Caffe importer were tested to reproduce Caffe functionality:"
dnn,"Convolution Deconvolution Pooling InnerProduct TanH, ReLU, Sigmoid, BNLL, Power, AbsVal Softmax Reshape, Flatten, Slice, Split LRN MVN Dropout (since it does nothing on forward pass -))"
dnn,Classes class cv::dnn::AbsLayer  class cv::dnn::AccumLayer  class cv::dnn::AcoshLayer  class cv::dnn::AcosLayer  class cv::dnn::ActivationLayer  class cv::dnn::ActivationLayerInt8  class cv::dnn::ArgLayer  ArgMax/ArgMin layer. More...  class cv::dnn::AsinhLayer  class cv::dnn::AsinLayer  class cv::dnn::AtanhLayer  class cv::dnn::AtanLayer  class cv::dnn::AttentionLayer  class cv::dnn::BaseConvolutionLayer  class cv::dnn::BatchNormLayer  class cv::dnn::BatchNormLayerInt8  class cv::dnn::BlankLayer  class cv::dnn::BNLLLayer  class cv::dnn::CeilLayer  class cv::dnn::CeluLayer  class cv::dnn::ChannelsPReLULayer  class cv::dnn::CompareLayer  class cv::dnn::ConcatLayer  class cv::dnn::ConstLayer  class cv::dnn::ConvolutionLayer  class cv::dnn::ConvolutionLayerInt8  class cv::dnn::CorrelationLayer  class cv::dnn::CoshLayer  class cv::dnn::CosLayer  class cv::dnn::CropAndResizeLayer  class cv::dnn::CropLayer  class cv::dnn::CumSumLayer  class cv::dnn::DataAugmentationLayer  class cv::dnn::DeconvolutionLayer  class cv::dnn::DepthToSpaceLayer  class cv::dnn::DequantizeLayer  class cv::dnn::DetectionOutputLayer  Detection output layer. More...  class cv::dnn::EinsumLayer  This function performs array summation based on the Einstein summation convention. The function allows for concise expressions of various mathematical operations using subscripts. More...  class cv::dnn::EltwiseLayer  Element wise operation on inputs. More...  class cv::dnn::EltwiseLayerInt8  class cv::dnn::ELULayer  class cv::dnn::ErfLayer  class cv::dnn::ExpandLayer  class cv::dnn::ExpLayer  class cv::dnn::FlattenLayer  class cv::dnn::FloorLayer  class cv::dnn::FlowWarpLayer  class cv::dnn::GatherElementsLayer  GatherElements layer GatherElements takes two inputs data and indices of the same rank r >= 1 and an optional attribute axis and works such that: output[i][j][k] = data[index[i][j][k]][j][k] if axis = 0 and r = 3 output[i][j][k] = data[i][index[i][j][k]][k] if axis = 1 and r = 3 output[i][j][k] = data[i][j][index[i][j][k]] if axis = 2 and r = 3. More...  class cv::dnn::GatherLayer  Gather layer. More...  class cv::dnn::GeluApproximationLayer  class cv::dnn::GeluLayer  class cv::dnn::GemmLayer  class cv::dnn::GroupNormLayer  class cv::dnn::GRULayer  GRU recurrent one-layer. More...  class cv::dnn::HardSigmoidLayer  class cv::dnn::HardSwishLayer  class cv::dnn::InnerProductLayer  class cv::dnn::InnerProductLayerInt8  class cv::dnn::InstanceNormLayer  class cv::dnn::InterpLayer  Bilinear resize layer from https://github.com/cdmh/deeplab-public-ver2. More...  class cv::dnn::LayerNormLayer  class cv::dnn::LogLayer  class cv::dnn::LRNLayer  class cv::dnn::LSTMLayer  LSTM recurrent layer. More...  class cv::dnn::MatMulLayer  class cv::dnn::MaxUnpoolLayer  class cv::dnn::MishLayer  class cv::dnn::MVNLayer  class cv::dnn::NaryEltwiseLayer  class cv::dnn::NormalizeBBoxLayer  \( L_p \) - normalization layer. More...  class cv::dnn::NotLayer  class cv::dnn::PaddingLayer  Adds extra values for specific axes. More...  class cv::dnn::PermuteLayer  class cv::dnn::PoolingLayer  class cv::dnn::PoolingLayerInt8  class cv::dnn::PowerLayer  class cv::dnn::PriorBoxLayer  class cv::dnn::ProposalLayer  class cv::dnn::QuantizeLayer  class cv::dnn::ReciprocalLayer  class cv::dnn::ReduceLayer  class cv::dnn::RegionLayer  class cv::dnn::ReLU6Layer  class cv::dnn::ReLULayer  class cv::dnn::ReorgLayer  class cv::dnn::RequantizeLayer  class cv::dnn::ReshapeLayer  class cv::dnn::ResizeLayer  Resize input 4-dimensional blob by nearest neighbor or bilinear strategy. More...  class cv::dnn::RNNLayer  Classical recurrent layer. More...  class cv::dnn::RoundLayer  class cv::dnn::ScaleLayer  class cv::dnn::ScaleLayerInt8  class cv::dnn::ScatterLayer  class cv::dnn::ScatterNDLayer  class cv::dnn::SeluLayer  class cv::dnn::ShiftLayer  class cv::dnn::ShiftLayerInt8  class cv::dnn::ShrinkLayer  class cv::dnn::ShuffleChannelLayer  class cv::dnn::SigmoidLayer  class cv::dnn::SignLayer  class cv::dnn::SinhLayer  class cv::dnn::SinLayer  class cv::dnn::SliceLayer  class cv::dnn::SoftmaxLayer  class cv::dnn::SoftmaxLayerInt8  class cv::dnn::SoftplusLayer  class cv::dnn::SoftsignLayer  class cv::dnn::SpaceToDepthLayer  class cv::dnn::SplitLayer  class cv::dnn::SqrtLayer  class cv::dnn::SwishLayer  class cv::dnn::TanHLayer  class cv::dnn::TanLayer  class cv::dnn::ThresholdedReluLayer  class cv::dnn::TileLayer  class cv::dnn::TopKLayer 
dnn,Comma-separated Matrix Initializer.
dnn,"The class instances are usually not created explicitly. Instead, they are created on ""matrix << firstValue"" operator."
dnn,The sample below initializes 2x2 rotation matrix:
dnn,Compare: dst[i] = src1[i] op src2[i]
dnn,"src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images operation one of (CV_HAL_CMP_EQ, CV_HAL_CMP_GT, ...)"
dnn,"Functions int hal_ni_cmp16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation) "
dnn,"Functions void cv::samples::addSamplesDataSearchPath (const cv::String &path)  Override search data path by adding new search location.  void cv::samples::addSamplesDataSearchSubDirectory (const cv::String &subdir)  Append samples search data sub directory.  cv::String cv::samples::findFile (const cv::String &relative_path, bool required=true, bool silentMode=false)  Try to find requested data file.  cv::String cv::samples::findFileOrKeep (const cv::String &relative_path, bool silentMode=false) "
dnn,"Template class for short numerical vectors, a partial case of Matx."
dnn,"This template class represents short numerical vectors (of 1, 2, 3, 4 ... elements) on which you can perform basic arithmetical operations, access individual elements using [] operator etc. The vectors are allocated on stack, as opposite to std::valarray, std::vector, cv::Mat etc., which elements are dynamically allocated in the heap."
dnn,The template takes 2 parameters:
dnn,_Tp element type cn the number of elements
dnn,"In addition to the universal notation like Vec<float, 3>, you can use shorter aliases for the most popular specialized variants of Vec, e.g. Vec3f ~ Vec<float, 3>."
dnn,"It is possible to convert Vec<T,2> to/from Point_, Vec<T,3> to/from Point3_ , and Vec<T,4> to CvScalar or Scalar_. Use operator[] to access the elements of Vec."
dnn,All the expected vector operations are also implemented:
dnn,"v1 = v2 + v3 v1 = v2 - v3 v1 = v2 * scale v1 = scale * v2 v1 = -v2 v1 += v2 and other augmenting operations v1 == v2, v1 != v2 norm(v1) (euclidean norm) The Vec class is commonly used to describe pixel types of multi-channel arrays. See Mat for details."
dnn,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
dnn,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
dnn,A helper class for cv::DataType.
dnn,The class is specialized for each fundamental numerical data type supported by OpenCV. It provides DataDepth<T>::value constant.
dnn,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
dnn,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
dnn,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
dnn,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
dnn,"Functions void cv::julia::initJulia (int argc, char **argv) "
dnn,Template sparse n-dimensional array class derived from SparseMat.
dnn,SparseMat_ is a thin wrapper on top of SparseMat created in the same way as Mat_ . It simplifies notation of some operations:
dnn,x source X array y source Y array dst destination array len length of arrays
dnn,"Functions int hal_ni_magnitude32f (const float *x, const float *y, float *dst, int len)  int hal_ni_magnitude64f (const double *x, const double *y, double *dst, int len) "
dnn,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
dnn,"SoftFloat is a software implementation of floating-point calculations according to IEEE 754 standard. All calculations are done in integers, that's why they are machine-independent and bit-exact. This library can be useful in accuracy-critical parts like look-up tables generation, tests, etc. OpenCV contains a subset of SoftFloat partially rewritten to C++."
dnn,"Functions void cv::accumulate (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds an image to the accumulator image.  void cv::accumulateProduct (InputArray src1, InputArray src2, InputOutputArray dst, InputArray mask=noArray())  Adds the per-element product of two input images to the accumulator image.  void cv::accumulateSquare (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds the square of a source image to the accumulator image.  void cv::accumulateWeighted (InputArray src, InputOutputArray dst, double alpha, InputArray mask=noArray())  Updates a running average.  void cv::createHanningWindow (OutputArray dst, Size winSize, int type)  This function computes a Hanning window coefficients in two dimensions.  void cv::divSpectrums (InputArray a, InputArray b, OutputArray c, int flags, bool conjB=false)  Performs the per-element division of the first Fourier spectrum by the second Fourier spectrum.  Point2d cv::phaseCorrelate (InputArray src1, InputArray src2, InputArray window=noArray(), double *response=0)  The function is used to detect translational shifts that occur between two images. "
dnn,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
dnn,It provides easy interface to:
dnn,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
dnn,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
dnn,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
dnn,It is planned to have:
dnn,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
dnn,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
dnn,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
dnn,Classes class cv::DualQuat< _Tp >  class cv::Quat< _Tp >  class cv::QuatEnum 
dnn,Performs Cholesky decomposition of matrix \(A = L*L^T\) and solves matrix equation \(A*X=B\).
dnn,src1 pointer to input matrix \(A\) stored in row major order. After finish of work src1 contains lower triangular matrix \(L\). src1_step number of bytes between two consequent rows of matrix \(A\). m size of square matrix \(A\). src2 pointer to \(M\times N\) matrix \(B\) which is the right-hand side of system \(A*X=B\). B stored in row major order. If src2 is null pointer only Cholesky decomposition will be performed. After finish of work src2 contains solution \(X\) of system \(A*X=B\). src2_step number of bytes between two consequent rows of matrix \(B\). n number of right-hand vectors in \(M\times N\) matrix \(B\). info indicates success of decomposition. If *info is false decomposition failed.
dnn,"Functions int hal_ni_Cholesky32f (float *src1, size_t src1_step, int m, float *src2, size_t src2_step, int n, bool *info)  int hal_ni_Cholesky64f (double *src1, size_t src1_step, int m, double *src2, size_t src2_step, int n, bool *info) "
dnn,Manages memory block shared by muliple buffers.
dnn,"This class allows to allocate one large memory block and split it into several smaller non-overlapping buffers. In safe mode each buffer allocation will be performed independently, this mode allows dynamic memory access instrumentation using valgrind or memory sanitizer."
dnn,Safe mode can be explicitly switched ON in constructor. It will also be enabled when compiling with memory sanitizer support or in runtime with the environment variable OPENCV_BUFFER_AREA_ALWAYS_SAFE.
dnn,Example of usage:
dnn,Functions float32x2_t cv_vrecp_f32 (float32x2_t val)  float32x4_t cv_vrecpq_f32 (float32x4_t val)  int32x2_t cv_vrnd_s32_f32 (float32x2_t v)  uint32x2_t cv_vrnd_u32_f32 (float32x2_t v)  int32x4_t cv_vrndq_s32_f32 (float32x4_t v)  uint32x4_t cv_vrndq_u32_f32 (float32x4_t v)  float32x2_t cv_vrsqrt_f32 (float32x2_t val)  float32x4_t cv_vrsqrtq_f32 (float32x4_t val)  float32x2_t cv_vsqrt_f32 (float32x2_t val)  float32x4_t cv_vsqrtq_f32 (float32x4_t val) 
dnn,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
dnn,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
dnn,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
dnn,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
dnn,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
dnn,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
dnn,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
dnn,Add: dst[i] = src1[i] + src2[i] Sub: dst[i] = src1[i] - src2[i]
dnn,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
dnn,"Functions int hal_ni_add16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_add16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_add32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_add32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_add64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_add8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_add8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8s32f (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8u32f (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height) "
dnn,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
dnn,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
dnn,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
dnn,This module has been originally developed as a project for Google Summer of Code 2012-2015.
dnn,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
dnn,Automatically Allocated Buffer Class.
dnn,"The class is used for temporary buffers in functions and methods. If a temporary buffer is usually small (a few K's of memory), but its size depends on the parameters, it makes sense to create a small fixed-size array on stack and use it if it's large enough. If the required buffer size is larger than the fixed size, another buffer of sufficient size is allocated dynamically and released after the processing. Therefore, in typical cases, when the buffer size is small, there is no overhead associated with malloc()/free(). At the same time, there is no limit on the size of processed data."
dnn,This is what AutoBuffer does. The template takes 2 parameters - type of the buffer elements and the number of stack-allocated elements. Here is how the class is used:
dnn,This class represents high-level API for text detection DL networks compatible with EAST model.
dnn,Configurable parameters:
dnn,"(float) confThreshold - used to filter boxes by confidences, default: 0.5f (float) nmsThreshold - used in non maximum suppression, default: 0.0f"
dnn,ArgMax/ArgMin layer.
dnn,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
dnn,The distortion-free projective transformation given by a pinhole camera model is shown below.
dnn,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
dnn,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
dnn,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
dnn,\[p = A P_c.\]
dnn,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
dnn,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
dnn,and thus
dnn,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
dnn,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
dnn,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
dnn,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
dnn,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
dnn,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
dnn,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
dnn,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
dnn,and therefore
dnn,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
dnn,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
dnn,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
dnn,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
dnn,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
dnn,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
dnn,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
dnn,with
dnn,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
dnn,The following figure illustrates the pinhole camera model.
dnn,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
dnn,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
dnn,where
dnn,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
dnn,with
dnn,\[r^2 = x'^2 + y'^2\]
dnn,and
dnn,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
dnn,if \(Z_c \ne 0\).
dnn,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
dnn,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
dnn,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
dnn,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
dnn,where
dnn,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
dnn,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
dnn,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
dnn,In the functions below the coefficients are passed or returned as
dnn,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
dnn,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
dnn,The functions below use the above model to do the following:
dnn,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
dnn,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
dnn,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
dnn,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
dnn,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
dnn,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
dnn,if \(W \ne 0\).
dnn,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
dnn,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
dnn,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
dnn,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
dnn,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
dnn,Template Read-Only Sparse Matrix Iterator Class.
dnn,This is the derived from SparseMatConstIterator class that introduces more convenient operator *() for accessing the current element.
dnn,"This class implements name-value dictionary, values are instances of DictValue."
dnn,Processing params of image to blob.
dnn,It includes all possible image processing operations and corresponding parameters.
dnn,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
dnn,The class defining termination criteria for iterative algorithms.
dnn,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
dnn,"ArUco Marker Detection, module functionality was moved to objdetect module"
dnn,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
dnn,a Class to measure passing time.
dnn,"The class computes passing time by counting the number of ticks per second. That is, the following code computes the execution time in seconds:"
dnn,It is also possible to compute the average time over multiple runs:
dnn,Simple TLS data class.
dnn,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
dnn,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
dnn,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
dnn,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
dnn,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
dnn,Template class specifying a continuous subsequence (slice) of a sequence.
dnn,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
dnn,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
dnn,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
dnn,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
dnn,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
dnn,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
dnn,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
dnn,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
dnn,XML/YAML/JSON file storage class that encapsulates all the information necessary for writing or reading data to/from a file.
dnn,Computes weighted sum of two arrays using formula: dst[i] = a * src1[i] + b * src2[i] + c
dnn,"src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images scalars numbers a, b, and c"
dnn,"Functions int hal_ni_addWeighted16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, const double scalars[3]) "
dnn,Hamming norm of a vector
dnn,"a pointer to vector data n length of a vector cellSize how many bits of the vector will be added and treated as a single bit, can be 1 (standard Hamming distance), 2 or 4 result pointer to result output"
dnn,Hamming distance between two vectors
dnn,"a pointer to first vector data b pointer to second vector data n length of vectors cellSize how many bits of the vectors will be added and treated as a single bit, can be 1 (standard Hamming distance), 2 or 4 result pointer to result output"
dnn,"Functions int hal_ni_normHamming8u (const uchar *a, int n, int cellSize, int *result)  int hal_ni_normHammingDiff8u (const uchar *a, const uchar *b, int n, int cellSize, int *result) "
dnn,Detection output layer.
dnn,"The layer size is: \( (1 \times 1 \times N \times 7) \) where N is [keep_top_k] parameter multiplied by batch size. Each row is: [image_id, label, confidence, xmin, ymin, xmax, ymax] where image_id is the index of image input in the batch."
dnn,This class represents high-level API for segmentation models.
dnn,"SegmentationModel allows to set params for preprocessing input image. SegmentationModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and returns the class prediction for each pixel."
dnn,A complex number class.
dnn,"The template class is similar and compatible with std::complex, however it provides slightly more convenient access to the real and imaginary parts using through the simple field access, as opposite to std::complex::real() and std::complex::imag()."
dnn,This class represents high-level API for text detection DL networks compatible with DB model.
dnn,"Related publications: [167] Paper: https://arxiv.org/abs/1911.08947 For more information about the hyper-parameters setting, please refer to https://github.com/MhLiao/DB"
dnn,Configurable parameters:
dnn,"(float) binaryThreshold - The threshold of the binary map. It is usually set to 0.3. (float) polygonThreshold - The threshold of text polygons. It is usually set to 0.5, 0.6, and 0.7. Default is 0.5f (double) unclipRatio - The unclip ratio of the detected text region, which determines the output size. It is usually set to 2.0. (int) maxCandidates - The max number of the output results."
dnn,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
dnn,This class allows to create and manipulate comprehensive artificial neural networks.
dnn,"Neural network is presented as directed acyclic graph (DAG), where vertices are Layer instances, and edges specify relationships between layers inputs and outputs."
dnn,Each network layer has unique integer id and unique string name inside its network. LayerId can store either layer name or layer id.
dnn,"This class supports reference counting of its instances, i. e. copies point to the same instance."
dnn,Performs QR decomposition of \(M\times N\)( \(M>N\)) matrix \(A = Q*R\) and solves matrix equation \(A*X=B\).
dnn,"src1 pointer to input matrix \(A\) stored in row major order. After finish of work src1 contains upper triangular \(N\times N\) matrix \(R\). Lower triangle of src1 will be filled with vectors of elementary reflectors. See [284] and Lapack's DGEQRF documentation for details. src1_step number of bytes between two consequent rows of matrix \(A\). m number fo rows in matrix \(A\). n number of columns in matrix \(A\). k number of right-hand vectors in \(M\times K\) matrix \(B\). src2 pointer to \(M\times K\) matrix \(B\) which is the right-hand side of system \(A*X=B\). \(B\) stored in row major order. If src2 is null pointer only QR decomposition will be performed. Otherwise system will be solved and src1 will be used as temporary buffer, so after finish of work src2 contains solution \(X\) of system \(A*X=B\). src2_step number of bytes between two consequent rows of matrix \(B\). dst pointer to continiuos \(N\times 1\) array for scalar factors of elementary reflectors. See [284] for details. info indicates success of decomposition. If *info is zero decomposition failed."
dnn,"Functions int hal_ni_QR32f (float *src1, size_t src1_step, int m, int n, int k, float *src2, size_t src2_step, float *dst, int *info)  int hal_ni_QR64f (double *src1, size_t src1_step, int m, int n, int k, double *src2, size_t src2_step, double *dst, int *info) "
dnn,Resize input 4-dimensional blob by nearest neighbor or bilinear strategy.
dnn,Layer is used to support TensorFlow's resize_nearest_neighbor and resize_bilinear ops.
dnn,LSTM recurrent layer.
dnn,Mersenne Twister random number generator.
dnn,Inspired by http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/CODES/mt19937ar.c
dnn,Template class for 2D points specified by its coordinates x and y.
dnn,"An instance of the class is interchangeable with C structures, CvPoint and CvPoint2D32f . There is also a cast operator to convert point coordinates to the specified type. The conversion from floating-point coordinates to integer coordinates is done by rounding. Commonly, the conversion uses this operation for each of the coordinates. Besides the class members listed in the declaration above, the following operations on points are implemented:"
dnn,"For your convenience, the following type aliases are defined:"
dnn,Example:
dnn,Linear Discriminant Analysis.
dnn,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
dnn,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
dnn,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
dnn,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
dnn,"Each class derived from Map implements a motion model, as follows:"
dnn,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
dnn,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
dnn,The classes derived from Mapper are
dnn,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
dnn,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
dnn,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
dnn,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
dnn,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
dnn,This module provides storage routines for Hierarchical Data Format objects.
dnn,Face module changelog Face Recognition with OpenCV
dnn,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
dnn,This class provides all data needed to initialize layer.
dnn,"It includes dictionary with scalar params (which can be read by using Dict interface), blob params blobs and optional meta information: name and type of layer instance."
dnn,Gather layer.
dnn,The class represents rotated (i.e. not up-right) rectangles on a plane.
dnn,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
dnn,The sample below demonstrates how to use RotatedRect:
dnn,This class represents high-level API for keypoints models.
dnn,"KeypointsModel allows to set params for preprocessing input image. KeypointsModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and returns the x and y coordinates of each detected keypoint"
dnn,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
dnn,Classes class cv::plot::Plot2d 
dnn,"Minimum: dst[i] = min(src1[i], src2[i]) Maximum: dst[i] = max(src1[i], src2[i])"
dnn,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
dnn,"Functions int hal_ni_max16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_max16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_max32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_max32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_max64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_max8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_max8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_min16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_min16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_min32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_min32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_min64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_min8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_min8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height) "
dnn,Classes class cv::quality::QualityBase 
dnn,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
dnn,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
dnn,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
dnn,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
dnn,Classes class cv::DenseOpticalFlow  class cv::DISOpticalFlow  DIS optical flow algorithm. More...  class cv::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::KalmanFilter  Kalman filter class. More...  class cv::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More...  class cv::Tracker  Base abstract class for the long-term tracker. More...  class cv::TrackerDaSiamRPN  class cv::TrackerGOTURN  the GOTURN (Generic Object Tracking Using Regression Networks) tracker More...  class cv::TrackerMIL  The MIL algorithm trains a classifier in an online manner to separate the object from the background. More...  class cv::TrackerNano  the Nano tracker is a super lightweight dnn-based general object tracking. More...  class cv::TrackerVit  the VIT tracker is a super lightweight dnn-based general object tracking. More...  class cv::VariationalRefinement  Variational optical flow refinement. More... 
dnn,TLS data accumulator with gathering methods.
dnn,Namespaces namespace cv::traits 
dnn,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
dnn,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
dnn,\( L_p \) - normalization layer.
dnn,"p Normalization factor. The most common p = 1 for \( L_1 \) - normalization or p = 2 for \( L_2 \) - normalization or a custom one. eps Parameter \( \epsilon \) to prevent a division by zero. across_spatial If true, normalize an input across all non-batch dimensions. Otherwise normalize an every channel separately."
dnn,Across spatial:
dnn,"\[ norm = \sqrt[p]{\epsilon + \sum_{x, y, c} |src(x, y, c)|^p } \\ dst(x, y, c) = \frac{ src(x, y, c) }{norm} \]"
dnn,Channel wise normalization:
dnn,"\[ norm(c) = \sqrt[p]{\epsilon + \sum_{x, y} |src(x, y, c)|^p } \\ dst(x, y, c) = \frac{ src(x, y, c) }{norm(c)} \]"
dnn,"Where x, y - spatial coordinates, c - channel."
dnn,"An every sample in the batch is normalized separately. Optionally, output is scaled by the trained parameters."
dnn,Namespace for all functions is cv::intensity_transform.
dnn,This function performs array summation based on the Einstein summation convention. The function allows for concise expressions of various mathematical operations using subscripts.
dnn,"By default, the labels are placed in alphabetical order at the end of the output. For example: if c = einsum(""i,j"", a, b), then c[i,j] == a[i]*b[j]. However, if c = einsum(""j,i"", a, b), then c[i,j] = a[j]*b[i]. Alternatively, you can control the output order or prevent an axis from being summed/force an axis to be summed by providing indices for the output. For example: diag(a) -> einsum(""ii->i"", a) sum(a, axis=0) -> einsum(""i...->"", a) Subscripts at the beginning and end may be specified by putting an ellipsis ""..."" in the middle. For instance, the function einsum(""i...i"", a) takes the diagonal of the first and last dimensions of the operand, and einsum(""ij...,jk...->ik..."") performs the matrix product using the first two indices of each operand instead of the last two. When there is only one operand, no axes being summed, and no output parameter, this function returns a view into the operand instead of creating a copy."
dnn,Performs singular value decomposition of \(M\times N\)( \(M>N\)) matrix \(A = U*\Sigma*V^T\).
dnn,"src pointer to input \(M\times N\) matrix \(A\) stored in column major order. After finish of work src will be filled with rows of \(U\) or not modified (depends of flag CV_HAL_SVD_MODIFY_A). src_step number of bytes between two consequent columns of matrix \(A\). w pointer to array for singular values of matrix \(A\) (i. e. first \(N\) diagonal elements of matrix \(\Sigma\)). u pointer to output \(M\times N\) or \(M\times M\) matrix \(U\) (size depends of flags). Pointer must be valid if flag CV_HAL_SVD_MODIFY_A not used. u_step number of bytes between two consequent rows of matrix \(U\). vt pointer to array for \(N\times N\) matrix \(V^T\). vt_step number of bytes between two consequent rows of matrix \(V^T\). m number fo rows in matrix \(A\). n number of columns in matrix \(A\). flags algorithm options (combination of CV_HAL_SVD_FULL_UV, ...)."
dnn,"Functions int hal_ni_SVD32f (float *src, size_t src_step, float *w, float *u, size_t u_step, float *vt, size_t vt_step, int m, int n, int flags)  int hal_ni_SVD64f (double *src, size_t src_step, double *w, double *u, size_t u_step, double *vt, size_t vt_step, int m, int n, int flags) "
dnn,Divide: dst[i] = scale * src1[i] / src2[i]
dnn,src1_data first source image data and step src1_step first source image data and step src2_data second source image data and step src2_step second source image data and step dst_data destination image data and step dst_step destination image data and step width dimensions of the images height dimensions of the images scale additional multiplier
dnn,"Functions int hal_ni_div16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, double scale) "
dnn,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
dnn,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
dnn,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
dnn,The class SparseMat represents multi-dimensional sparse numerical arrays.
dnn,"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:"
dnn,"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5; int size[5] = {10, 10, 10, 10, 10}; SparseMat sparse_mat(dims, size, CV_32F); for(int i = 0; i < 1000; i++) { int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f; } cout << ""nnz = "" << sparse_mat.nzcount() << endl; cv::SparseMatThe class SparseMat represents multi-dimensional sparse numerical arrays.Definition mat.hpp:2751 cv::SparseMat::sizeconst int * size() constreturns the array of sizes, or NULL if the matrix is not allocated cv::SparseMat::dimsint dims() constreturns the matrix dimensionality CV_32F#define CV_32FDefinition interface.h:78 Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix // and the sum of elements. SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>(); double s = 0; int dims = sparse_mat.dims(); for(; it != it_end; ++it) { // print element indices and the element value const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("": %g\n"", it.value<float>()); s += *it; } printf(""Element sum is %g\n"", s); cv::SparseMatConstIterator_Template Read-Only Sparse Matrix Iterator Class.Definition mat.hpp:3354 cv::SparseMatConstIterator::valueconst _Tp & value() consttemplate method returning the current matrix element cv::SparseMatConstIterator::nodeconst SparseMat::Node * node() constreturns the current node of the sparse matrix. it.node->idx is the current element index cv::SparseMat::Nodesparse matrix node - element of a hash tableDefinition mat.hpp:2776 cv::SparseMat::Node::idxint idx[MAX_DIM]index of the matrix elementDefinition mat.hpp:2782 If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation. Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b) { const SparseMat *_a = &a, *_b = &b; // if b contains less elements than a, // it is faster to iterate through b if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // take the next element from the first matrix float avalue = *it; const Node* anode = it.node(); // and try to find an element with the same index in the second matrix. // since the hash value depends only on the element index, // reuse the hash value stored in the node float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr; } cv::SparseMat::endSparseMatIterator end()returns the sparse matrix iterator at the matrix end cv::SparseMat::nzcountsize_t nzcount() constreturns the number of non-zero elements (=the number of hash table nodes) cv::SparseMat::beginSparseMatIterator begin()returns the sparse matrix iterator at the matrix beginning cv::SparseMat::Node::hashvalsize_t hashvalhash valueDefinition mat.hpp:2778"
dnn,Read and write video or images sequence with OpenCV.
dnn,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
dnn,Bioinspired Module Retina Introduction
dnn,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
dnn,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
dnn,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
dnn,See detailed overview here: Machine Learning Overview.
dnn,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
dnn,Classical recurrent layer.
dnn,Accepts two inputs \(x_t\) and \(h_{t-1}\) and compute two outputs \(o_t\) and \(h_t\).
dnn,input: should contain packed input \(x_t\). output: should contain output \(o_t\) (and \(h_t\) if setProduceHiddenOutput() is set to true).
dnn,"input[0] should have shape [T, N, data_dims] where T and N is number of timestamps and number of independent samples of \(x_t\) respectively."
dnn,"output[0] will have shape [T, N, \(N_o\)], where \(N_o\) is number of rows in \( W_{xo} \) matrix."
dnn,"If setProduceHiddenOutput() is set to true then output[1] will contain a Mat with shape [T, N, \(N_h\)], where \(N_h\) is number of rows in \( W_{hh} \) matrix."
dnn,Permute channels of 4-dimensional input blob.
dnn,group Number of groups to split input channels and pick in turns into output blob.
dnn,\[ groupSize = \frac{number\ of\ channels}{group} \]
dnn,"\[ output(n, c, h, w) = input(n, groupSize \times (c \% group) + \lfloor \frac{c}{group} \rfloor, h, w) \]"
dnn,Read more at https://arxiv.org/pdf/1707.01083.pdf
dnn,Dummy structure storing DFT/DCT context.
dnn,Users can convert this pointer to any type they want. Initialisation and destruction should be made in Init and Free function implementations correspondingly. Example:
dnn,core/src/hal_replacement.hpp
dnn,Bilinear resize layer from https://github.com/cdmh/deeplab-public-ver2.
dnn,It differs from ResizeLayer in output shape and resize scales computations.
dnn,Slice layer has several modes:
dnn,[in] axis Axis of split operation [in] slice_point Array of split points
dnn,begin Vector of start indices size Vector of sizes
dnn,axis Axis of split operation
dnn,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
dnn,Multiply: dst[i] = scale * src1[i] * src2[i]
dnn,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images scale additional multiplier
dnn,"Functions int hal_ni_mul16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8s16s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8u16u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, double scale) "
dnn,File Storage Node class.
dnn,"The node is used to store each and every element of the file storage opened for reading. When XML/YAML file is read, it is first parsed and stored in the memory as a hierarchical collection of nodes. Each node can be a ""leaf"" that is contain a single number or a string, or be a collection of other nodes. There can be named collections (mappings) where each element has a name and it is accessed by a name, and ordered collections (sequences) where elements do not have names but rather accessed by index. Type of the file node can be determined using FileNode::type method."
dnn,"Note that file nodes are only used for navigating file storages opened for reading. When a file storage is opened for writing, no data is stored in memory after it is written."
dnn,Template class for small matrices whose type and size are known at compilation time.
dnn,"If you need a more flexible type, use Mat . The elements of the matrix M are accessible using the M(i,j) notation. Most of the common matrix operations (see also MatrixExpressions ) are available. To do an operation on Matx that is not implemented, you can easily convert the matrix to Mat and backwards:"
dnn,"Except of the plain constructor which takes a list of elements, Matx can be initialized from a C-array:"
dnn,"In case if C++11 features are available, std::initializer_list can be also used to initialize Matx:"
dnn,This class represents high-level API for text recognition networks.
dnn,"TextRecognitionModel allows to set params for preprocessing input image. TextRecognitionModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and return recognition result. For TextRecognitionModel, CRNN-CTC is supported."
dnn,Constant layer produces the same data blob at an every forward pass.
dnn,TLS container base implementation
dnn,Don't use directly.
dnn,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
dnn,"src_data array of pointers to source arrays (cn items x len items) [ [B, B, ...], [G, G, ...], [R, R, ...] ] dst_data destination array of interleaved values (len x cn items) [ B, G, R, B, G, R, ...] len number of elements cn number of channels"
dnn,"Functions int hal_ni_merge16u (const ushort **src_data, ushort *dst_data, int len, int cn)  int hal_ni_merge32s (const int **src_data, int *dst_data, int len, int cn)  int hal_ni_merge64s (const int64 **src_data, int64 *dst_data, int len, int cn)  int hal_ni_merge8u (const uchar **src_data, uchar *dst_data, int len, int cn) "
dnn,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
dnn,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
dnn,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
dnn,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
dnn,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
dnn,Custom array allocator.
dnn,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
dnn,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
dnn,"Template class for 3D points specified by its coordinates x, y and z."
dnn,"An instance of the class is interchangeable with the C structure CvPoint2D32f . Similarly to Point_ , the coordinates of 3D points can be converted to another type. The vector arithmetic and comparison operations are also supported."
dnn,The following Point3_<> aliases are available:
dnn,Derivatives of this class wraps cv::Mat for different backends and targets.
dnn,Classes class cv::dnn::LayerFactory  Layer factory allows to create instances of registered layers. More... 
dnn,Read-Only Sparse Matrix Iterator.
dnn,Here is how to use the iterator to compute the sum of floating-point sparse matrix elements:
dnn,"src_data array of interleaved values (len x cn items) [ B, G, R, B, G, R, ...] dst_data array of pointers to destination arrays (cn items x len items) [ [B, B, ...], [G, G, ...], [R, R, ...] ] len number of elements cn number of channels"
dnn,"Functions int hal_ni_split16u (const ushort *src_data, ushort **dst_data, int len, int cn)  int hal_ni_split32s (const int *src_data, int **dst_data, int len, int cn)  int hal_ni_split64s (const int64 *src_data, int64 **dst_data, int len, int cn)  int hal_ni_split8u (const uchar *src_data, uchar **dst_data, int len, int cn) "
dnn,Singular Value Decomposition.
dnn,"Class for computing Singular Value Decomposition of a floating-point matrix. The Singular Value Decomposition is used to solve least-square problems, under-determined linear systems, invert matrices, compute condition numbers, and so on."
dnn,"If you want to compute a condition number of a matrix or an absolute value of its determinant, you do not need u and vt. You can pass flags=SVD::NO_UV|... . Another flag SVD::FULL_UV indicates that full-size u and vt must be computed, which is not necessary most of the time."
dnn,GatherElements layer GatherElements takes two inputs data and indices of the same rank r >= 1 and an optional attribute axis and works such that: output[i][j][k] = data[index[i][j][k]][j][k] if axis = 0 and r = 3 output[i][j][k] = data[i][index[i][j][k]][k] if axis = 1 and r = 3 output[i][j][k] = data[i][j][index[i][j][k]] if axis = 2 and r = 3.
dnn,"Gather, on the other hand, takes a data tensor of rank r >= 1, and indices tensor of rank q, and works such that: it gathers the enteries along axis dimension of the input data indexed by indices and concatenates them in an output tensor of rank q + (r - 1) e.g. If axis = 0, let k = indices[i_{0}, ..., i_{q-1}] then output[i_{0}, ..., i_{q-1}, j_{0}, ..., j_{r-2}] = input[k , j_{0}, ..., j_{r-2}]:"
dnn,"Enumerations enum cv::TemplateMatchModes { cv::TM_SQDIFF = 0 , cv::TM_SQDIFF_NORMED = 1 , cv::TM_CCORR = 2 , cv::TM_CCORR_NORMED = 3 , cv::TM_CCOEFF = 4 , cv::TM_CCOEFF_NORMED = 5 }  type of the template matching operation More... "
dnn,Template matrix class derived from Mat.
dnn,"The class Mat_<_Tp> is a thin template wrapper on top of the Mat class. It does not have any extra data fields. Nor this class nor Mat has any virtual methods. Thus, references or pointers to these two classes can be freely but carefully converted one to another. For example:"
dnn,"While Mat is sufficient in most cases, Mat_ can be more convenient if you use a lot of element access operations and if you know matrix type at the compilation time. Note that Mat::at(int y,int x) and Mat_::operator()(int y,int x) do absolutely the same and run at the same speed, but the latter is certainly shorter:"
dnn,"To use Mat_ for multi-channel images/matrices, pass Vec as a Mat_ parameter:"
dnn,Mat_ is fully compatible with C++11 range-based for loop. For example such loop can be used to safely apply look-up table:
dnn,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
dnn,"Namespace for all functions is cvv, i.e. cvv::showImage()."
dnn,Compilation:
dnn,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
dnn,See cvv tutorial for a commented example application using cvv.
dnn,Namespaces namespace cvv::impl 
features2d,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
features2d,Abstract base class for 2D image feature detectors and descriptor extractors.
features2d,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
features2d,Class for extracting blobs from an image. :
features2d,The class implements a simple algorithm for extracting blobs from an image:
features2d,This class performs several filtrations of returned blobs. You should set filterBy* to true/false to turn on/off corresponding filtration. Available filtrations:
features2d,"By color. This filter compares the intensity of a binary image at the center of a blob to blobColor. If they differ, the blob is filtered out. Use blobColor = 0 to extract dark blobs and blobColor = 255 to extract light blobs. By area. Extracted blobs have an area between minArea (inclusive) and maxArea (exclusive). By circularity. Extracted blobs have circularity ( \(\frac{4*\pi*Area}{perimeter * perimeter}\)) between minCircularity (inclusive) and maxCircularity (exclusive). By ratio of the minimum inertia to maximum inertia. Extracted blobs have this ratio between minInertiaRatio (inclusive) and maxInertiaRatio (exclusive). By convexity. Extracted blobs have convexity (area / area of blob convex hull) between minConvexity (inclusive) and maxConvexity (exclusive)."
features2d,Default values of parameters are tuned to extract dark circular blobs.
features2d,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
features2d,This module includes photo processing algorithms
features2d,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
features2d,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
features2d,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
features2d,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
features2d,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
features2d,The implemented stitching pipeline is very similar to the one proposed in [41] .
features2d,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
features2d,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
features2d,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
features2d,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
features2d,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
features2d,Template class for a 4-element vector derived from Vec.
features2d,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
features2d,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
features2d,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
features2d,This module includes signal processing algorithms.
features2d,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
features2d,Data structure for salient point detectors.
features2d,"The class instance stores a keypoint, i.e. a point feature found by one of many available keypoint detectors, such as Harris corner detector, FAST, StarDetector, SURF, SIFT etc."
features2d,"The keypoint is characterized by the 2D position, scale (proportional to the diameter of the neighborhood that needs to be taken into account), orientation and some other parameters. The keypoint neighborhood is then analyzed by another algorithm that builds a descriptor (usually represented as a feature vector). The keypoints representing the same object in different images can then be matched using KDTree or another method."
features2d,ICP point-to-plane odometry algorithm
features2d,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
features2d,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
features2d,Class to compute an image descriptor using the bag of visual words.
features2d,Such a computation consists of the following steps:
features2d,Dense optical flow algorithms compute motion for each point:
features2d,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
features2d,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
features2d,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
features2d,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
features2d,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
features2d,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
features2d,Maximally stable extremal region extractor.
features2d,The class encapsulates all the parameters of the MSER extraction algorithm (see wiki article).
features2d,"there are two different implementation of MSER: one for grey image, one for color image the grey image algorithm is taken from: [208] ; the paper claims to be faster than union-find method; it actually get 1.5~2m/s on my centrino L7200 1.2GHz laptop. the color image algorithm is taken from: [94] ; it should be much slower than grey image method ( 3~4 times ) (Python) A complete example showing the use of the MSER detector can be found at samples/python/mser.py"
features2d,"Class implementing the KAZE keypoint detector and descriptor extractor, described in [9] ."
features2d,n-dimensional dense array class
features2d,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
features2d,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
features2d,"In case of a 2-dimensional array, the above formula is reduced to:"
features2d,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
features2d,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
features2d,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
features2d,There are many different ways to create a Mat object. The most popular options are listed below:
features2d,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
features2d,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
features2d,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
features2d,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
features2d,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
features2d,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
features2d,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
features2d,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
features2d,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
features2d,Brute-force descriptor matcher.
features2d,"For each descriptor in the first set, this matcher finds the closest descriptor in the second set by trying each one. This descriptor matcher supports masking permissible matches of descriptor sets."
features2d,Namespaces namespace cv::omnidir::internal 
features2d,Information Flow algorithm implementaton for alphamatting
features2d,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
features2d,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
features2d,The implementation is based on [7].
features2d,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
features2d,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
features2d,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
features2d,"Enumerations enum struct cv::DrawMatchesFlags { cv::DrawMatchesFlags::DEFAULT = 0 , cv::DrawMatchesFlags::DRAW_OVER_OUTIMG = 1 , cv::DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS = 2 , cv::DrawMatchesFlags::DRAW_RICH_KEYPOINTS = 4 } "
features2d,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
features2d,kmeans -based class to train visual vocabulary using the bag of visual words approach. :
features2d,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
features2d,Namespace for all functions is cv::img_hash.
features2d,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
features2d,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
features2d,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
features2d,Class for matching keypoint descriptors.
features2d,"query descriptor index, train descriptor index, train image index, and distance between descriptors."
features2d,This modules is to draw UTF-8 strings with freetype/harfbuzz.
features2d,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
features2d,Classes class cv::freetype::FreeType2 
features2d,Abstract base class for training the bag of visual words vocabulary from a set of descriptors.
features2d,"For details, see, for example, Visual Categorization with Bags of Keypoints by Gabriella Csurka, Christopher R. Dance, Lixin Fan, Jutta Willamowski, Cedric Bray, 2004. :"
features2d,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
features2d,"Classes struct cv::Accumulator< T >  struct cv::Accumulator< char >  struct cv::Accumulator< short >  struct cv::Accumulator< unsigned char >  struct cv::Accumulator< unsigned short >  class cv::AffineFeature  Class for implementing the wrapper which makes detectors and extractors to be affine invariant, described as ASIFT in [312] . More...  class cv::AgastFeatureDetector  Wrapping class for feature detection using the AGAST method. : More...  class cv::AKAZE  Class implementing the AKAZE keypoint detector and descriptor extractor, described in [10]. More...  class cv::BRISK  Class implementing the BRISK keypoint detector and descriptor extractor, described in [159] . More...  class cv::FastFeatureDetector  Wrapping class for feature detection using the FAST method. : More...  class cv::Feature2D  Abstract base class for 2D image feature detectors and descriptor extractors. More...  class cv::GFTTDetector  Wrapping class for feature detection using the goodFeaturesToTrack function. : More...  class cv::KAZE  Class implementing the KAZE keypoint detector and descriptor extractor, described in [9] . More...  class cv::KeyPointsFilter  A class filters a vector of keypoints. More...  struct cv::L1< T >  struct cv::L2< T >  class cv::MSER  Maximally stable extremal region extractor. More...  class cv::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More...  class cv::SIFT  Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe [174] . More...  class cv::SimpleBlobDetector  Class for extracting blobs from an image. : More...  struct cv::SL2< T > "
features2d,Namespaces namespace NcvCTprep 
features2d,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
features2d,This module contains:
features2d,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
features2d,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
features2d,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
features2d,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
features2d,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
features2d,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
features2d,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
features2d,Wrapping class for feature detection using the AGAST method. :
features2d,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
features2d,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
features2d,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
features2d,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
features2d,"Functions void cv::julia::initJulia (int argc, char **argv) "
features2d,Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe [174] .
features2d,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
features2d,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
features2d,It provides easy interface to:
features2d,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
features2d,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
features2d,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
features2d,It is planned to have:
features2d,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
features2d,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
features2d,"Class implementing the AKAZE keypoint detector and descriptor extractor, described in [10]."
features2d,AKAZE descriptors can only be used with KAZE or AKAZE keypoints. This class is thread-safe.
features2d,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
features2d,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
features2d,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
features2d,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
features2d,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
features2d,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
features2d,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
features2d,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
features2d,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
features2d,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
features2d,This module has been originally developed as a project for Google Summer of Code 2012-2015.
features2d,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
features2d,Matchers of keypoint descriptors in OpenCV have wrappers with a common interface that enables you to easily switch between different algorithms solving the same problem. This section is devoted to matching descriptors that are represented as vectors in a multidimensional space. All objects that implement vector descriptor matchers inherit the DescriptorMatcher interface.
features2d,Classes class cv::BFMatcher  Brute-force descriptor matcher. More...  class cv::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::FlannBasedMatcher  Flann-based descriptor matcher. More... 
features2d,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
features2d,The distortion-free projective transformation given by a pinhole camera model is shown below.
features2d,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
features2d,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
features2d,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
features2d,\[p = A P_c.\]
features2d,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
features2d,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
features2d,and thus
features2d,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
features2d,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
features2d,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
features2d,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
features2d,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
features2d,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
features2d,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
features2d,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
features2d,and therefore
features2d,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
features2d,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
features2d,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
features2d,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
features2d,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
features2d,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
features2d,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
features2d,with
features2d,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
features2d,The following figure illustrates the pinhole camera model.
features2d,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
features2d,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
features2d,where
features2d,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
features2d,with
features2d,\[r^2 = x'^2 + y'^2\]
features2d,and
features2d,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
features2d,if \(Z_c \ne 0\).
features2d,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
features2d,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
features2d,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
features2d,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
features2d,where
features2d,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
features2d,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
features2d,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
features2d,In the functions below the coefficients are passed or returned as
features2d,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
features2d,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
features2d,The functions below use the above model to do the following:
features2d,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
features2d,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
features2d,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
features2d,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
features2d,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
features2d,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
features2d,if \(W \ne 0\).
features2d,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
features2d,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
features2d,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
features2d,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
features2d,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
features2d,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
features2d,"ArUco Marker Detection, module functionality was moved to objdetect module"
features2d,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
features2d,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
features2d,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
features2d,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
features2d,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
features2d,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
features2d,"Class for implementing the wrapper which makes detectors and extractors to be affine invariant, described as ASIFT in [312] ."
features2d,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
features2d,Abstract base class for matching keypoint descriptors.
features2d,It has two groups of match methods: for matching descriptors of an image with another image or with an image set.
features2d,Classes struct cvhalKeyPoint 
features2d,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
features2d,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
features2d,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
features2d,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
features2d,"Each class derived from Map implements a motion model, as follows:"
features2d,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
features2d,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
features2d,The classes derived from Mapper are
features2d,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
features2d,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
features2d,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
features2d,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
features2d,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
features2d,This module provides storage routines for Hierarchical Data Format objects.
features2d,Face module changelog Face Recognition with OpenCV
features2d,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
features2d,Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor.
features2d,"described in [229] . The algorithm uses FAST in pyramids to detect stable keypoints, selects the strongest features using FAST or Harris response, finds their orientation using first-order moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or k-tuples) are rotated according to the measured orientation)."
features2d,Classes class cv::plot::Plot2d 
features2d,Classes class cv::quality::QualityBase 
features2d,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
features2d,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
features2d,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
features2d,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
features2d,Namespaces namespace cv::traits 
features2d,Flann-based descriptor matcher.
features2d,"This matcher trains cv::flann::Index on a train descriptor collection and calls its nearest search methods to find the best matches. So, this matcher may be faster when matching a large train collection than the brute force matcher. FlannBasedMatcher does not support masking permissible matches of descriptor sets because flann::Index does not support this. :"
features2d,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
features2d,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
features2d,Namespace for all functions is cv::intensity_transform.
features2d,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
features2d,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
features2d,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
features2d,Read and write video or images sequence with OpenCV.
features2d,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
features2d,Bioinspired Module Retina Introduction
features2d,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
features2d,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
features2d,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
features2d,See detailed overview here: Machine Learning Overview.
features2d,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
features2d,This section describes approaches based on local 2D features and used to categorize objects.
features2d,Classes class cv::BOWImgDescriptorExtractor  Class to compute an image descriptor using the bag of visual words. More...  class cv::BOWKMeansTrainer  kmeans -based class to train visual vocabulary using the bag of visual words approach. : More...  class cv::BOWTrainer  Abstract base class for training the bag of visual words vocabulary from a set of descriptors. More... 
features2d,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
features2d,A class filters a vector of keypoints.
features2d,"Because now it is difficult to provide a convenient interface for all usage scenarios of the keypoints filter class, it has only several needed by now static methods."
features2d,"Class implementing the BRISK keypoint detector and descriptor extractor, described in [159] ."
features2d,Wrapping class for feature detection using the goodFeaturesToTrack function. :
features2d,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
features2d,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
features2d,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
features2d,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
features2d,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
features2d,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
features2d,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
features2d,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
features2d,Wrapping class for feature detection using the FAST method. :
features2d,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
features2d,"Namespace for all functions is cvv, i.e. cvv::showImage()."
features2d,Compilation:
features2d,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
features2d,See cvv tutorial for a commented example application using cvv.
features2d,Namespaces namespace cvv::impl 
flann,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
flann,"Template ""trait"" class for OpenCV primitive data types."
flann,"A primitive OpenCV data type is one of unsigned char, bool, signed char, unsigned short, signed short, int, float, double, or a tuple of values of one of these types, where all the values in the tuple have the same type. Any primitive type from the list can be defined by an identifier in the form CV_<bit-depth>{U|S|F}C(<number_of_channels>), for example: uchar CV_8UC1, 3-element floating-point tuple CV_32FC3, and so on. A universal OpenCV structure that is able to store a single instance of such a primitive data type is Vec. Multiple instances of such a type can be stored in a std::vector, Mat, Mat_, SparseMat, SparseMat_, or any other container that is able to store Vec instances."
flann,"The DataType class is basically used to provide a description of such primitive data types without adding any fields or methods to the corresponding classes (and it is actually impossible to add anything to primitive C/C++ data types). This technique is known in C++ as class traits. It is not DataType itself that is used but its specialized versions, such as:"
flann,"The main purpose of this class is to convert compilation-time type information to an OpenCV-compatible data type identifier, for example:"
flann,"So, such traits are used to tell OpenCV which data type you are working with, even if such a type is not native to OpenCV. For example, the matrix B initialization above is compiled because OpenCV defines the proper specialized template class DataType<complex<_Tp> > . This mechanism is also useful (and used in OpenCV this way) for generic algorithms implementations."
flann,opencv2/core/traits.hpp
flann,Base storage class for GPU memory with reference counting.
flann,Its interface matches the Mat interface with the following limitations:
flann,no arbitrary dimensions support (only 2D) no functions that return references to their data (because references on GPU are not valid for CPU) no expression templates technique support
flann,Beware that the latter limitation may lead to overloaded matrix operators that cause memory allocations. The GpuMat class is convertible to cuda::PtrStepSz and cuda::PtrStep so it can be passed directly to the kernel.
flann,"Some member functions are described as a ""Blocking Call"" while some are described as a ""Non-Blocking Call"". Blocking functions are synchronous to host. It is guaranteed that the GPU operation is finished when the function returns. However, non-blocking functions are asynchronous to host. Those functions may return even if the GPU operation is not finished."
flann,"Compared to their blocking counterpart, non-blocking functions accept Stream as an additional argument. If a non-default stream is passed, the GPU operation may overlap with operations in other streams."
flann,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
flann,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
flann,This module includes photo processing algorithms
flann,"Matrix expression representation This is a list of implemented matrix operations that can be combined in arbitrary complex expressions (here A, B stand for matrices ( Mat ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double )):"
flann,"Addition, subtraction, negation: A+B, A-B, A+s, A-s, s+A, s-A, -A Scaling: A*alpha Per-element multiplication and division: A.mul(B), A/B, alpha/A Matrix multiplication: A*B Transposition: A.t() (means AT) Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems: A.inv([method]) (~ A<sup>-1</sup>), A.inv([method])*B (~ X: AX=B) Comparison: A cmpop B, A cmpop alpha, alpha cmpop A, where cmpop is one of >, >=, ==, !=, <=, <. The result of comparison is an 8-bit single channel mask whose elements are set to 255 (if the particular element or pair of elements satisfy the condition) or 0. Bitwise logical operations: A logicop B, A logicop s, s logicop A, ~A, where logicop is one of &, |, ^. Element-wise minimum and maximum: min(A, B), min(A, alpha), max(A, B), max(A, alpha) Element-wise absolute value: abs(A) Cross-product, dot-product: A.cross(B), A.dot(B) Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm, mean, sum, countNonZero, trace, determinant, repeat, and others. Matrix initializers ( Mat::eye(), Mat::zeros(), Mat::ones() ), matrix comma-separated initializers, matrix constructors and operators that extract sub-matrices (see Mat description). Mat_<destination_type>() constructors to cast the result to the proper type. NoteComma-separated initializers and probably some other operations may require additional explicit Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity. Here are examples of matrix expressions: // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD) SVD svd(A); Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t(); // compute the new vector of parameters in the Levenberg-Marquardt algorithm x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err); // sharpen image using ""unsharp mask"" algorithm Mat blurred; double sigma = 1, threshold = 5, amount = 1; GaussianBlur(img, blurred, Size(), sigma, sigma); Mat lowContrastMask = abs(img - blurred) < threshold; Mat sharpened = img*(1+amount) + blurred*(-amount); img.copyTo(sharpened, lowContrastMask); cv::MatExpr::absMatExpr abs(const Mat &m)Calculates an absolute value of each matrix element. cv::MatExpr::invMatExpr inv(int method=DECOMP_LU) const cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::Mat::diagMat diag(int d=0) constExtracts a diagonal from a matrix. cv::Mat::uUMatData * uinteraction with UMatDefinition mat.hpp:2174 cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Mat::tMatExpr t() constTransposes a matrix. cv::SVDSingular Value Decomposition.Definition core.hpp:2739 cv::DECOMP_CHOLESKY@ DECOMP_CHOLESKYDefinition base.hpp:143 cv::SizeSize2i SizeDefinition types.hpp:370 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. cv::thresholddouble threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)Applies a fixed-level threshold to each array element."
flann,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
flann,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
flann,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
flann,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
flann,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
flann,The implemented stitching pipeline is very similar to the one proposed in [41] .
flann,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
flann,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
flann,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
flann,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
flann,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
flann,Template class for a 4-element vector derived from Vec.
flann,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
flann,Matrix read-only iterator.
flann,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
flann,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
flann,This module includes signal processing algorithms.
flann,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
flann,ICP point-to-plane odometry algorithm
flann,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
flann,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
flann,YOUR ATTENTION PLEASE!
flann,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
flann,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
flann,Note for developers: please don't put videoio dependency in G-API because of this file.
flann,Dense optical flow algorithms compute motion for each point:
flann,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
flann,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
flann,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
flann,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
flann,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
flann,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
flann,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
flann,n-dimensional dense array class
flann,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
flann,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
flann,"In case of a 2-dimensional array, the above formula is reduced to:"
flann,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
flann,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
flann,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
flann,There are many different ways to create a Mat object. The most popular options are listed below:
flann,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
flann,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
flann,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
flann,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
flann,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
flann,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
flann,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
flann,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
flann,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
flann,Namespaces namespace cv::omnidir::internal 
flann,Information Flow algorithm implementaton for alphamatting
flann,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
flann,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
flann,The implementation is based on [7].
flann,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
flann,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
flann,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
flann,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
flann,"Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as Mat's). It means that for each pixel location \((x,y)\) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location \((x,y)\). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one."
flann,"Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian \(3 \times 3\) filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (""replicated border"" extrapolation method), or assume that all the non-existing pixels are zeros (""constant border"" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see BorderTypes"
flann,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
flann,Namespace for all functions is cv::img_hash.
flann,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
flann,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
flann,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
flann,This modules is to draw UTF-8 strings with freetype/harfbuzz.
flann,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
flann,Classes class cv::freetype::FreeType2 
flann,Matrix read-write iterator.
flann,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
flann,Namespaces namespace NcvCTprep 
flann,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
flann,This module contains:
flann,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
flann,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
flann,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
flann,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
flann,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
flann,Template class for specifying the size of an image or rectangle.
flann,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
flann,OpenCV defines the following Size_<> aliases:
flann,Comma-separated Matrix Initializer.
flann,"The class instances are usually not created explicitly. Instead, they are created on ""matrix << firstValue"" operator."
flann,The sample below initializes 2x2 rotation matrix:
flann,"Template class for short numerical vectors, a partial case of Matx."
flann,"This template class represents short numerical vectors (of 1, 2, 3, 4 ... elements) on which you can perform basic arithmetical operations, access individual elements using [] operator etc. The vectors are allocated on stack, as opposite to std::valarray, std::vector, cv::Mat etc., which elements are dynamically allocated in the heap."
flann,The template takes 2 parameters:
flann,_Tp element type cn the number of elements
flann,"In addition to the universal notation like Vec<float, 3>, you can use shorter aliases for the most popular specialized variants of Vec, e.g. Vec3f ~ Vec<float, 3>."
flann,"It is possible to convert Vec<T,2> to/from Point_, Vec<T,3> to/from Point3_ , and Vec<T,4> to CvScalar or Scalar_. Use operator[] to access the elements of Vec."
flann,All the expected vector operations are also implemented:
flann,"v1 = v2 + v3 v1 = v2 - v3 v1 = v2 * scale v1 = scale * v2 v1 = -v2 v1 += v2 and other augmenting operations v1 == v2, v1 != v2 norm(v1) (euclidean norm) The Vec class is commonly used to describe pixel types of multi-channel arrays. See Mat for details."
flann,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
flann,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
flann,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
flann,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
flann,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
flann,"Functions void cv::julia::initJulia (int argc, char **argv) "
flann,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
flann,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
flann,It provides easy interface to:
flann,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
flann,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
flann,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
flann,It is planned to have:
flann,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
flann,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
flann,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
flann,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
flann,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
flann,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
flann,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
flann,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
flann,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
flann,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
flann,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
flann,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
flann,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
flann,This module has been originally developed as a project for Google Summer of Code 2012-2015.
flann,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
flann,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
flann,The distortion-free projective transformation given by a pinhole camera model is shown below.
flann,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
flann,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
flann,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
flann,\[p = A P_c.\]
flann,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
flann,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
flann,and thus
flann,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
flann,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
flann,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
flann,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
flann,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
flann,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
flann,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
flann,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
flann,and therefore
flann,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
flann,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
flann,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
flann,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
flann,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
flann,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
flann,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
flann,with
flann,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
flann,The following figure illustrates the pinhole camera model.
flann,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
flann,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
flann,where
flann,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
flann,with
flann,\[r^2 = x'^2 + y'^2\]
flann,and
flann,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
flann,if \(Z_c \ne 0\).
flann,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
flann,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
flann,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
flann,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
flann,where
flann,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
flann,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
flann,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
flann,In the functions below the coefficients are passed or returned as
flann,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
flann,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
flann,The functions below use the above model to do the following:
flann,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
flann,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
flann,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
flann,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
flann,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
flann,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
flann,if \(W \ne 0\).
flann,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
flann,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
flann,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
flann,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
flann,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
flann,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
flann,"ArUco Marker Detection, module functionality was moved to objdetect module"
flann,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
flann,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
flann,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
flann,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
flann,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
flann,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
flann,Template class specifying a continuous subsequence (slice) of a sequence.
flann,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
flann,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
flann,The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built.
flann,Distance functor specifies the metric to be used to calculate the distance between two points. There are several Distance functors that are readily available:
flann,"cv::cvflann::L2_Simple - Squared Euclidean distance functor. This is the simpler, unrolled version. This is preferable for very low dimensionality data (eg 3D points)"
flann,"cv::flann::L2 - Squared Euclidean distance functor, optimized version."
flann,"cv::flann::L1 - Manhattan distance functor, optimized version."
flann,cv::flann::MinkowskiDistance - The Minkowski distance functor. This is highly optimised with loop unrolling. The computation of squared root at the end is omitted for efficiency.
flann,"cv::flann::MaxDistance - The max distance functor. It computes the maximum distance between two vectors. This distance is not a valid kdtree distance, it's not dimensionwise additive."
flann,cv::flann::HammingLUT - Hamming distance functor. It counts the bit differences between two strings using a lookup table implementation.
flann,"cv::flann::Hamming - Hamming distance functor. Population count is performed using library calls, if available. Lookup table implementation is used as a fallback."
flann,cv::flann::Hamming2 - Hamming distance functor. Population count is implemented in 12 arithmetic operations (one of which is multiplication).
flann,"cv::flann::DNAmmingLUT - Adaptation of the Hamming distance functor to DNA comparison. As the four bases A, C, G, T of the DNA (or A, G, C, U for RNA) can be coded on 2 bits, it counts the bits pairs differences between two sequences using a lookup table implementation."
flann,cv::flann::DNAmming2 - Adaptation of the Hamming distance functor to DNA comparison. Bases differences count are vectorised thanks to arithmetic operations using standard registers (AVX2 and AVX-512 should come in a near future).
flann,cv::flann::HistIntersectionDistance - The histogram intersection distance functor.
flann,cv::flann::HellingerDistance - The Hellinger distance functor.
flann,cv::flann::ChiSquareDistance - The chi-square distance functor.
flann,cv::flann::KL_Divergence - The Kullback-Leibler divergence functor.
flann,"Although the provided implementations cover a vast range of cases, it is also possible to use a custom implementation. The distance functor is a class whose operator() computes the distance between two features. If the distance is also a kd-tree compatible distance, it should also provide an accum_dist() method that computes the distance between individual feature dimensions."
flann,"In addition to operator() and accum_dist(), a distance functor should also define the ElementType and the ResultType as the types of the elements it operates on and the type of the result it computes. If a distance functor can be used as a kd-tree distance (meaning that the full distance between a pair of features can be accumulated from the partial distances between the individual dimensions) a typedef is_kdtree_distance should be present inside the distance functor. If the distance is not a kd-tree distance, but it's a distance in a vector space (the individual dimensions of the elements it operates on can be accessed independently) a typedef is_vector_space_distance should be defined inside the functor. If neither typedef is defined, the distance is assumed to be a metric distance and will only be used with indexes operating on generic metric distances."
flann,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
flann,Template class for 2D points specified by its coordinates x and y.
flann,"An instance of the class is interchangeable with C structures, CvPoint and CvPoint2D32f . There is also a cast operator to convert point coordinates to the specified type. The conversion from floating-point coordinates to integer coordinates is done by rounding. Commonly, the conversion uses this operation for each of the coordinates. Besides the class members listed in the declaration above, the following operations on points are implemented:"
flann,"For your convenience, the following type aliases are defined:"
flann,Example:
flann,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
flann,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
flann,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
flann,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
flann,"Each class derived from Map implements a motion model, as follows:"
flann,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
flann,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
flann,The classes derived from Mapper are
flann,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
flann,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
flann,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
flann,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
flann,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
flann,This module provides storage routines for Hierarchical Data Format objects.
flann,Face module changelog Face Recognition with OpenCV
flann,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
flann,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
flann,Classes class cv::plot::Plot2d 
flann,Classes class cv::quality::QualityBase 
flann,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
flann,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
flann,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
flann,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
flann,Namespaces namespace cv::traits 
flann,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
flann,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
flann,Namespace for all functions is cv::intensity_transform.
flann,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
flann,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
flann,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
flann,The class SparseMat represents multi-dimensional sparse numerical arrays.
flann,"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:"
flann,"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5; int size[5] = {10, 10, 10, 10, 10}; SparseMat sparse_mat(dims, size, CV_32F); for(int i = 0; i < 1000; i++) { int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f; } cout << ""nnz = "" << sparse_mat.nzcount() << endl; cv::SparseMatThe class SparseMat represents multi-dimensional sparse numerical arrays.Definition mat.hpp:2751 cv::SparseMat::sizeconst int * size() constreturns the array of sizes, or NULL if the matrix is not allocated cv::SparseMat::dimsint dims() constreturns the matrix dimensionality CV_32F#define CV_32FDefinition interface.h:78 Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix // and the sum of elements. SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>(); double s = 0; int dims = sparse_mat.dims(); for(; it != it_end; ++it) { // print element indices and the element value const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("": %g\n"", it.value<float>()); s += *it; } printf(""Element sum is %g\n"", s); cv::SparseMatConstIterator_Template Read-Only Sparse Matrix Iterator Class.Definition mat.hpp:3354 cv::SparseMatConstIterator::valueconst _Tp & value() consttemplate method returning the current matrix element cv::SparseMatConstIterator::nodeconst SparseMat::Node * node() constreturns the current node of the sparse matrix. it.node->idx is the current element index cv::SparseMat::Nodesparse matrix node - element of a hash tableDefinition mat.hpp:2776 cv::SparseMat::Node::idxint idx[MAX_DIM]index of the matrix elementDefinition mat.hpp:2782 If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation. Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b) { const SparseMat *_a = &a, *_b = &b; // if b contains less elements than a, // it is faster to iterate through b if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // take the next element from the first matrix float avalue = *it; const Node* anode = it.node(); // and try to find an element with the same index in the second matrix. // since the hash value depends only on the element index, // reuse the hash value stored in the node float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr; } cv::SparseMat::endSparseMatIterator end()returns the sparse matrix iterator at the matrix end cv::SparseMat::nzcountsize_t nzcount() constreturns the number of non-zero elements (=the number of hash table nodes) cv::SparseMat::beginSparseMatIterator begin()returns the sparse matrix iterator at the matrix beginning cv::SparseMat::Node::hashvalsize_t hashvalhash valueDefinition mat.hpp:2778"
flann,Read and write video or images sequence with OpenCV.
flann,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
flann,Bioinspired Module Retina Introduction
flann,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
flann,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
flann,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
flann,See detailed overview here: Machine Learning Overview.
flann,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
flann,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
flann,Template class for small matrices whose type and size are known at compilation time.
flann,"If you need a more flexible type, use Mat . The elements of the matrix M are accessible using the M(i,j) notation. Most of the common matrix operations (see also MatrixExpressions ) are available. To do an operation on Matx that is not implemented, you can easily convert the matrix to Mat and backwards:"
flann,"Except of the plain constructor which takes a list of elements, Matx can be initialized from a C-array:"
flann,"In case if C++11 features are available, std::initializer_list can be also used to initialize Matx:"
flann,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
flann,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
flann,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
flann,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
flann,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
flann,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
flann,Custom array allocator.
flann,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
flann,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
flann,"Template class for 3D points specified by its coordinates x, y and z."
flann,"An instance of the class is interchangeable with the C structure CvPoint2D32f . Similarly to Point_ , the coordinates of 3D points can be converted to another type. The vector arithmetic and comparison operations are also supported."
flann,The following Point3_<> aliases are available:
flann,Template matrix class derived from Mat.
flann,"The class Mat_<_Tp> is a thin template wrapper on top of the Mat class. It does not have any extra data fields. Nor this class nor Mat has any virtual methods. Thus, references or pointers to these two classes can be freely but carefully converted one to another. For example:"
flann,"While Mat is sufficient in most cases, Mat_ can be more convenient if you use a lot of element access operations and if you know matrix type at the compilation time. Note that Mat::at(int y,int x) and Mat_::operator()(int y,int x) do absolutely the same and run at the same speed, but the latter is certainly shorter:"
flann,"To use Mat_ for multi-channel images/matrices, pass Vec as a Mat_ parameter:"
flann,Mat_ is fully compatible with C++11 range-based for loop. For example such loop can be used to safely apply look-up table:
flann,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
flann,"Namespace for all functions is cvv, i.e. cvv::showImage()."
flann,Compilation:
flann,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
flann,See cvv tutorial for a commented example application using cvv.
flann,Namespaces namespace cvv::impl 
gapi,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
gapi,This structure represents a mosaicing operation.
gapi,Mosaicing is a very basic method to obfuscate regions in the image.
gapi,Designed for command line parsing.
gapi,The sample below demonstrates how to use CommandLineParser:
gapi,"Template ""trait"" class for OpenCV primitive data types."
gapi,"A primitive OpenCV data type is one of unsigned char, bool, signed char, unsigned short, signed short, int, float, double, or a tuple of values of one of these types, where all the values in the tuple have the same type. Any primitive type from the list can be defined by an identifier in the form CV_<bit-depth>{U|S|F}C(<number_of_channels>), for example: uchar CV_8UC1, 3-element floating-point tuple CV_32FC3, and so on. A universal OpenCV structure that is able to store a single instance of such a primitive data type is Vec. Multiple instances of such a type can be stored in a std::vector, Mat, Mat_, SparseMat, SparseMat_, or any other container that is able to store Vec instances."
gapi,"The DataType class is basically used to provide a description of such primitive data types without adding any fields or methods to the corresponding classes (and it is actually impossible to add anything to primitive C/C++ data types). This technique is known in C++ as class traits. It is not DataType itself that is used but its specialized versions, such as:"
gapi,"The main purpose of this class is to convert compilation-time type information to an OpenCV-compatible data type identifier, for example:"
gapi,"So, such traits are used to tell OpenCV which data type you are working with, even if such a type is not native to OpenCV. For example, the matrix B initialization above is compiled because OpenCV defines the proper specialized template class DataType<complex<_Tp> > . This mechanism is also useful (and used in OpenCV this way) for generic algorithms implementations."
gapi,opencv2/core/traits.hpp
gapi,"Functions void cv::gapi::wip::draw::render (cv::Mat &bgr, const Prims &prims, cv::GCompileArgs &&args={})  The function renders on the input image passed drawing primitivies.  void cv::gapi::wip::draw::render (cv::Mat &y_plane, cv::Mat &uv_plane, const Prims &prims, cv::GCompileArgs &&args={})  The function renders on two NV12 planes passed drawing primitivies.  void cv::gapi::wip::draw::render (cv::MediaFrame &frame, const Prims &prims, cv::GCompileArgs &&args={})  The function renders on the input media frame passed drawing primitivies.  GMat cv::gapi::wip::draw::render3ch (const GMat &src, const GArray< Prim > &prims)  Renders on 3 channels input.  GFrame cv::gapi::wip::draw::renderFrame (const GFrame &m_frame, const GArray< Prim > &prims)  Renders Media Frame.  GMat2 cv::gapi::wip::draw::renderNV12 (const GMat &y, const GMat &uv, const GArray< Prim > &prims)  Renders on two planes. "
gapi,Base storage class for GPU memory with reference counting.
gapi,Its interface matches the Mat interface with the following limitations:
gapi,no arbitrary dimensions support (only 2D) no functions that return references to their data (because references on GPU are not valid for CPU) no expression templates technique support
gapi,Beware that the latter limitation may lead to overloaded matrix operators that cause memory allocations. The GpuMat class is convertible to cuda::PtrStepSz and cuda::PtrStep so it can be passed directly to the kernel.
gapi,"Some member functions are described as a ""Blocking Call"" while some are described as a ""Non-Blocking Call"". Blocking functions are synchronous to host. It is guaranteed that the GPU operation is finished when the function returns. However, non-blocking functions are asynchronous to host. Those functions may return even if the GPU operation is not finished."
gapi,"Compared to their blocking counterpart, non-blocking functions accept Stream as an additional argument. If a non-default stream is passed, the GPU operation may overlap with operations in other streams."
gapi,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
gapi,"Functions GMat cv::gapi::absDiff (const GMat &src1, const GMat &src2)  Calculates the per-element absolute difference between two matrices.  GMat cv::gapi::absDiffC (const GMat &src, const GScalar &c)  Calculates absolute value of matrix elements.  GMat cv::gapi::addWeighted (const GMat &src1, double alpha, const GMat &src2, double beta, double gamma, int ddepth=-1)  Calculates the weighted sum of two matrices.  GOpaque< int > cv::gapi::countNonZero (const GMat &src)  Counts non-zero array elements.  GMat cv::gapi::inRange (const GMat &src, const GScalar &threshLow, const GScalar &threshUp)  Applies a range-level threshold to each matrix element.  std::tuple< GMat, GMat > cv::gapi::integral (const GMat &src, int sdepth=-1, int sqdepth=-1)  Calculates the integral of an image.  GMat cv::gapi::max (const GMat &src1, const GMat &src2)  Calculates per-element maximum of two matrices.  GMat cv::gapi::min (const GMat &src1, const GMat &src2)  Calculates per-element minimum of two matrices.  GScalar cv::gapi::normInf (const GMat &src)  Calculates the absolute infinite norm of a matrix.  GScalar cv::gapi::normL1 (const GMat &src)  Calculates the absolute L1 norm of a matrix.  GScalar cv::gapi::normL2 (const GMat &src)  Calculates the absolute L2 norm of a matrix.  GScalar cv::gapi::sum (const GMat &src)  Calculates sum of all matrix elements.  std::tuple< GMat, GScalar > cv::gapi::threshold (const GMat &src, const GScalar &maxval, int type)  GMat cv::gapi::threshold (const GMat &src, const GScalar &thresh, const GScalar &maxval, int type)  Applies a fixed-level threshold to each matrix element. "
gapi,"Enumerations enum cv::MouseEventFlags { cv::EVENT_FLAG_LBUTTON = 1 , cv::EVENT_FLAG_RBUTTON = 2 , cv::EVENT_FLAG_MBUTTON = 4 , cv::EVENT_FLAG_CTRLKEY = 8 , cv::EVENT_FLAG_SHIFTKEY = 16 , cv::EVENT_FLAG_ALTKEY = 32 }  Mouse Event Flags see cv::MouseCallback. More...  enum cv::MouseEventTypes { cv::EVENT_MOUSEMOVE = 0 , cv::EVENT_LBUTTONDOWN = 1 , cv::EVENT_RBUTTONDOWN = 2 , cv::EVENT_MBUTTONDOWN = 3 , cv::EVENT_LBUTTONUP = 4 , cv::EVENT_RBUTTONUP = 5 , cv::EVENT_MBUTTONUP = 6 , cv::EVENT_LBUTTONDBLCLK = 7 , cv::EVENT_RBUTTONDBLCLK = 8 , cv::EVENT_MBUTTONDBLCLK = 9 , cv::EVENT_MOUSEWHEEL = 10 , cv::EVENT_MOUSEHWHEEL = 11 }  Mouse Events see cv::MouseCallback. More...  enum cv::WindowFlags { cv::WINDOW_NORMAL = 0x00000000 , cv::WINDOW_AUTOSIZE = 0x00000001 , cv::WINDOW_OPENGL = 0x00001000 , cv::WINDOW_FULLSCREEN = 1 , cv::WINDOW_FREERATIO = 0x00000100 , cv::WINDOW_KEEPRATIO = 0x00000000 , cv::WINDOW_GUI_EXPANDED =0x00000000 , cv::WINDOW_GUI_NORMAL = 0x00000010 }  Flags for cv::namedWindow. More...  enum cv::WindowPropertyFlags { cv::WND_PROP_FULLSCREEN = 0 , cv::WND_PROP_AUTOSIZE = 1 , cv::WND_PROP_ASPECT_RATIO = 2 , cv::WND_PROP_OPENGL = 3 , cv::WND_PROP_VISIBLE = 4 , cv::WND_PROP_TOPMOST = 5 , cv::WND_PROP_VSYNC = 6 }  Flags for cv::setWindowProperty / cv::getWindowProperty. More... "
gapi,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
gapi,This module includes photo processing algorithms
gapi,"Matrix expression representation This is a list of implemented matrix operations that can be combined in arbitrary complex expressions (here A, B stand for matrices ( Mat ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double )):"
gapi,"Addition, subtraction, negation: A+B, A-B, A+s, A-s, s+A, s-A, -A Scaling: A*alpha Per-element multiplication and division: A.mul(B), A/B, alpha/A Matrix multiplication: A*B Transposition: A.t() (means AT) Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems: A.inv([method]) (~ A<sup>-1</sup>), A.inv([method])*B (~ X: AX=B) Comparison: A cmpop B, A cmpop alpha, alpha cmpop A, where cmpop is one of >, >=, ==, !=, <=, <. The result of comparison is an 8-bit single channel mask whose elements are set to 255 (if the particular element or pair of elements satisfy the condition) or 0. Bitwise logical operations: A logicop B, A logicop s, s logicop A, ~A, where logicop is one of &, |, ^. Element-wise minimum and maximum: min(A, B), min(A, alpha), max(A, B), max(A, alpha) Element-wise absolute value: abs(A) Cross-product, dot-product: A.cross(B), A.dot(B) Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm, mean, sum, countNonZero, trace, determinant, repeat, and others. Matrix initializers ( Mat::eye(), Mat::zeros(), Mat::ones() ), matrix comma-separated initializers, matrix constructors and operators that extract sub-matrices (see Mat description). Mat_<destination_type>() constructors to cast the result to the proper type. NoteComma-separated initializers and probably some other operations may require additional explicit Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity. Here are examples of matrix expressions: // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD) SVD svd(A); Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t(); // compute the new vector of parameters in the Levenberg-Marquardt algorithm x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err); // sharpen image using ""unsharp mask"" algorithm Mat blurred; double sigma = 1, threshold = 5, amount = 1; GaussianBlur(img, blurred, Size(), sigma, sigma); Mat lowContrastMask = abs(img - blurred) < threshold; Mat sharpened = img*(1+amount) + blurred*(-amount); img.copyTo(sharpened, lowContrastMask); cv::MatExpr::absMatExpr abs(const Mat &m)Calculates an absolute value of each matrix element. cv::MatExpr::invMatExpr inv(int method=DECOMP_LU) const cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::Mat::diagMat diag(int d=0) constExtracts a diagonal from a matrix. cv::Mat::uUMatData * uinteraction with UMatDefinition mat.hpp:2174 cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Mat::tMatExpr t() constTransposes a matrix. cv::SVDSingular Value Decomposition.Definition core.hpp:2739 cv::DECOMP_CHOLESKY@ DECOMP_CHOLESKYDefinition base.hpp:143 cv::SizeSize2i SizeDefinition types.hpp:370 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. cv::thresholddouble threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)Applies a fixed-level threshold to each array element."
gapi,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
gapi,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
gapi,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
gapi,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
gapi,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
gapi,The implemented stitching pipeline is very similar to the one proposed in [41] .
gapi,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
gapi,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
gapi,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
gapi,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
gapi,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
gapi,Represents a computation (graph) compiled for streaming.
gapi,"This class represents a product of graph compilation (calling cv::GComputation::compileStreaming()). Objects of this class actually do stream processing, and the whole pipeline execution complexity is incapsulated into objects of this class. Execution model has two levels: at the very top, the execution of a heterogeneous graph is aggressively pipelined; at the very bottom the execution of every internal block is determined by its associated backend. Backends are selected based on kernel packages passed via compilation arguments ( see G-API Graph Compilation Arguments, GNetworkPackage, GKernelPackage for details)."
gapi,"GStreamingCompiled objects have a ""player"" semantics there are methods like start() and stop(). GStreamingCompiled has a full control over a videostream and so is stateful. You need to specify the input stream data using setSource() and then call start() to actually start processing. After that, use pull() or try_pull() to obtain next processed data frame from the graph in a blocking or non-blocking way, respectively."
gapi,Currently a single GStreamingCompiled can process only one video streat at time. Produce multiple GStreamingCompiled objects to run the same graph on multiple video streams.
gapi,Template class for a 4-element vector derived from Vec.
gapi,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
gapi,gapi_math
gapi,"Functions GMat cv::gapi::bitwise_and (const GMat &src1, const GMat &src2)  computes bitwise conjunction of the two matrixes (src1 & src2) Calculates the per-element bit-wise logical conjunction of two matrices of the same size.  GMat cv::gapi::bitwise_and (const GMat &src1, const GScalar &src2)  GMat cv::gapi::bitwise_not (const GMat &src)  Inverts every bit of an array.  GMat cv::gapi::bitwise_or (const GMat &src1, const GMat &src2)  computes bitwise disjunction of the two matrixes (src1 | src2) Calculates the per-element bit-wise logical disjunction of two matrices of the same size.  GMat cv::gapi::bitwise_or (const GMat &src1, const GScalar &src2)  GMat cv::gapi::bitwise_xor (const GMat &src1, const GMat &src2)  computes bitwise logical ""exclusive or"" of the two matrixes (src1 ^ src2) Calculates the per-element bit-wise logical ""exclusive or"" of two matrices of the same size.  GMat cv::gapi::bitwise_xor (const GMat &src1, const GScalar &src2)  GMat cv::gapi::cmpEQ (const GMat &src1, const GMat &src2)  Performs the per-element comparison of two matrices checking if elements from first matrix are equal to elements in second.  GMat cv::gapi::cmpEQ (const GMat &src1, const GScalar &src2)  GMat cv::gapi::cmpGE (const GMat &src1, const GMat &src2)  Performs the per-element comparison of two matrices checking if elements from first matrix are greater or equal compare to elements in second.  GMat cv::gapi::cmpGE (const GMat &src1, const GScalar &src2)  GMat cv::gapi::cmpGT (const GMat &src1, const GMat &src2)  Performs the per-element comparison of two matrices checking if elements from first matrix are greater compare to elements in second.  GMat cv::gapi::cmpGT (const GMat &src1, const GScalar &src2)  GMat cv::gapi::cmpLE (const GMat &src1, const GMat &src2)  Performs the per-element comparison of two matrices checking if elements from first matrix are less or equal compare to elements in second.  GMat cv::gapi::cmpLE (const GMat &src1, const GScalar &src2)  GMat cv::gapi::cmpLT (const GMat &src1, const GMat &src2)  Performs the per-element comparison of two matrices checking if elements from first matrix are less than elements in second.  GMat cv::gapi::cmpLT (const GMat &src1, const GScalar &src2)  GMat cv::gapi::cmpNE (const GMat &src1, const GMat &src2)  Performs the per-element comparison of two matrices checking if elements from first matrix are not equal to elements in second.  GMat cv::gapi::cmpNE (const GMat &src1, const GScalar &src2)  GMat cv::gapi::select (const GMat &src1, const GMat &src2, const GMat &mask)  Select values from either first or second of input matrices by given mask. The function set to the output matrix either the value from the first input matrix if corresponding value of mask matrix is 255, or value from the second input matrix (if value of mask matrix set to 0). "
gapi,Random Number Generator.
gapi,"Random number generator. It encapsulates the state (currently, a 64-bit integer) and has methods to return scalar random values and to fill arrays with random values. Currently it supports uniform and Gaussian (normal) distributions. The generator uses Multiply-With-Carry algorithm, introduced by G. Marsaglia ( http://en.wikipedia.org/wiki/Multiply-with-carry ). Gaussian-distribution random numbers are generated using the Ziggurat algorithm ( http://en.wikipedia.org/wiki/Ziggurat_algorithm ), introduced by G. Marsaglia and W. W. Tsang."
gapi,Matrix read-only iterator.
gapi,Class passed to an error.
gapi,This class encapsulates all or almost all necessary information about the error happened in the program. The exception is usually constructed and thrown implicitly via CV_Error and CV_Error_ macros.
gapi,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
gapi,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
gapi,This module includes signal processing algorithms.
gapi,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
gapi,"Represents a compiled computation (graph). Can only be used with image / data formats & resolutions it was compiled for, with some exceptions."
gapi,"This class represents a product of graph compilation (calling cv::GComputation::compile()). Objects of this class actually do data processing, and graph execution is incapsulated into objects of this class. Execution model itself depends on kernels and backends which were using during the compilation, see G-API Graph Compilation Arguments for details."
gapi,"In a general case, GCompiled objects can be applied to data only in that formats/resolutions they were compiled for (see G-API Metadata Descriptors). However, if the underlying backends allow, a compiled object can be reshaped to handle data (images) of different resolution, though formats and types must remain the same."
gapi,GCompiled is very similar to std::function<> in its semantics running it looks like a function call in the user code.
gapi,"At the moment, GCompiled objects are not reentrant generally, the objects are stateful since graph execution itself is a stateful process and this state is now maintained in GCompiled's own memory (not on the process stack)."
gapi,"At the same time, two different GCompiled objects produced from the single cv::GComputation are completely independent and can be used concurrently."
gapi,ICP point-to-plane odometry algorithm
gapi,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
gapi,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
gapi,Base class for parallel data processors.
gapi,YOUR ATTENTION PLEASE!
gapi,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
gapi,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
gapi,Note for developers: please don't put videoio dependency in G-API because of this file.
gapi,Dense optical flow algorithms compute motion for each point:
gapi,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
gapi,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
gapi,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
gapi,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
gapi,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
gapi,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
gapi,QtFont available only for Qt. See cv::fontQt.
gapi,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
gapi,"Functions GOpaque< Rect > cv::gapi::boundingRect (const GArray< Point2f > &src)  GOpaque< Rect > cv::gapi::boundingRect (const GArray< Point2i > &src)  GOpaque< Rect > cv::gapi::boundingRect (const GMat &src)  Calculates the up-right bounding rectangle of a point set or non-zero pixels of gray-scale image.  GArray< GArray< Point > > cv::gapi::findContours (const GMat &src, const RetrievalModes mode, const ContourApproximationModes method)  GArray< GArray< Point > > cv::gapi::findContours (const GMat &src, const RetrievalModes mode, const ContourApproximationModes method, const GOpaque< Point > &offset)  Finds contours in a binary image.  std::tuple< GArray< GArray< Point > >, GArray< Vec4i > > cv::gapi::findContoursH (const GMat &src, const RetrievalModes mode, const ContourApproximationModes method)  std::tuple< GArray< GArray< Point > >, GArray< Vec4i > > cv::gapi::findContoursH (const GMat &src, const RetrievalModes mode, const ContourApproximationModes method, const GOpaque< Point > &offset)  Finds contours and their hierarchy in a binary image.  GOpaque< Vec4f > cv::gapi::fitLine2D (const GArray< Point2d > &src, const DistanceTypes distType, const double param=0., const double reps=0., const double aeps=0.)  GOpaque< Vec4f > cv::gapi::fitLine2D (const GArray< Point2f > &src, const DistanceTypes distType, const double param=0., const double reps=0., const double aeps=0.)  GOpaque< Vec4f > cv::gapi::fitLine2D (const GArray< Point2i > &src, const DistanceTypes distType, const double param=0., const double reps=0., const double aeps=0.)  GOpaque< Vec4f > cv::gapi::fitLine2D (const GMat &src, const DistanceTypes distType, const double param=0., const double reps=0., const double aeps=0.)  Fits a line to a 2D point set.  GOpaque< Vec6f > cv::gapi::fitLine3D (const GArray< Point3d > &src, const DistanceTypes distType, const double param=0., const double reps=0., const double aeps=0.)  GOpaque< Vec6f > cv::gapi::fitLine3D (const GArray< Point3f > &src, const DistanceTypes distType, const double param=0., const double reps=0., const double aeps=0.)  GOpaque< Vec6f > cv::gapi::fitLine3D (const GArray< Point3i > &src, const DistanceTypes distType, const double param=0., const double reps=0., const double aeps=0.)  GOpaque< Vec6f > cv::gapi::fitLine3D (const GMat &src, const DistanceTypes distType, const double param=0., const double reps=0., const double aeps=0.)  Fits a line to a 3D point set. "
gapi,Namespaces namespace cv::gapi 
gapi,The STL-compliant memory Allocator based on cv::fastMalloc() and cv::fastFree()
gapi,cv::GArray<T> template class represents a list of objects of class T in the graph.
gapi,"cv::GArray<T> describes a functional relationship between operations consuming and producing arrays of objects of class T. The primary purpose of cv::GArray<T> is to represent a dynamic list of objects where the size of the list is not known at the graph construction or compile time. Examples include: corner and feature detectors (cv::GArray<cv::Point>), object detection and tracking results (cv::GArray<cv::Rect>). Programmers can use their own types with cv::GArray<T> in the custom operations."
gapi,"Similar to cv::GScalar, cv::GArray<T> may be value-initialized in this case a graph-constant value is associated with the object."
gapi,"GArray<T> is a virtual counterpart of std::vector<T>, which is usually used to represent the GArray<T> data in G-API during the execution."
gapi,This is a base class for all more or less complex algorithms in OpenCV.
gapi,"especially for classes of algorithms, for which there can be multiple implementations. The examples are stereo correspondence (for which there are algorithms like block matching, semi-global block matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck etc.)."
gapi,Here is example of SimpleBlobDetector use in your application via Algorithm interface:
gapi,n-dimensional dense array class
gapi,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
gapi,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
gapi,"In case of a 2-dimensional array, the above formula is reduced to:"
gapi,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
gapi,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
gapi,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
gapi,There are many different ways to create a Mat object. The most popular options are listed below:
gapi,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
gapi,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
gapi,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
gapi,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
gapi,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
gapi,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
gapi,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
gapi,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
gapi,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
gapi,Smart pointer for OpenGL buffer object with reference counting.
gapi,"Buffer Objects are OpenGL objects that store an array of unformatted memory allocated by the OpenGL context. These can be used to store vertex data, pixel data retrieved from images or the framebuffer, and a variety of other things."
gapi,ogl::Buffer has interface similar with Mat interface and represents 2D array memory.
gapi,ogl::Buffer supports memory transfers between host and device and also can be mapped to CUDA memory.
gapi,Namespaces namespace cv::traits 
gapi,Namespaces namespace cv  namespace cv::utils::logging::internal 
gapi,Namespaces namespace cv::omnidir::internal 
gapi,"Enumerations enum cv::VideoCaptureAPIs { cv::CAP_ANY = 0 , cv::CAP_VFW = 200 , cv::CAP_V4L = 200 , cv::CAP_V4L2 = CAP_V4L , cv::CAP_FIREWIRE = 300 , cv::CAP_FIREWARE = CAP_FIREWIRE , cv::CAP_IEEE1394 = CAP_FIREWIRE , cv::CAP_DC1394 = CAP_FIREWIRE , cv::CAP_CMU1394 = CAP_FIREWIRE , cv::CAP_QT = 500 , cv::CAP_UNICAP = 600 , cv::CAP_DSHOW = 700 , cv::CAP_PVAPI = 800 , cv::CAP_OPENNI = 900 , cv::CAP_OPENNI_ASUS = 910 , cv::CAP_ANDROID = 1000 , cv::CAP_XIAPI = 1100 , cv::CAP_AVFOUNDATION = 1200 , cv::CAP_GIGANETIX = 1300 , cv::CAP_MSMF = 1400 , cv::CAP_WINRT = 1410 , cv::CAP_INTELPERC = 1500 , cv::CAP_REALSENSE = 1500 , cv::CAP_OPENNI2 = 1600 , cv::CAP_OPENNI2_ASUS = 1610 , cv::CAP_OPENNI2_ASTRA = 1620 , cv::CAP_GPHOTO2 = 1700 , cv::CAP_GSTREAMER = 1800 , cv::CAP_FFMPEG = 1900 , cv::CAP_IMAGES = 2000 , cv::CAP_ARAVIS = 2100 , cv::CAP_OPENCV_MJPEG = 2200 , cv::CAP_INTEL_MFX = 2300 , cv::CAP_XINE = 2400 , cv::CAP_UEYE = 2500 , cv::CAP_OBSENSOR = 2600 }  cv::VideoCapture API backends identifier. More...  enum cv::VideoCaptureProperties { cv::CAP_PROP_POS_MSEC =0 , cv::CAP_PROP_POS_FRAMES =1 , cv::CAP_PROP_POS_AVI_RATIO =2 , cv::CAP_PROP_FRAME_WIDTH =3 , cv::CAP_PROP_FRAME_HEIGHT =4 , cv::CAP_PROP_FPS =5 , cv::CAP_PROP_FOURCC =6 , cv::CAP_PROP_FRAME_COUNT =7 , cv::CAP_PROP_FORMAT =8 , cv::CAP_PROP_MODE =9 , cv::CAP_PROP_BRIGHTNESS =10 , cv::CAP_PROP_CONTRAST =11 , cv::CAP_PROP_SATURATION =12 , cv::CAP_PROP_HUE =13 , cv::CAP_PROP_GAIN =14 , cv::CAP_PROP_EXPOSURE =15 , cv::CAP_PROP_CONVERT_RGB =16 , cv::CAP_PROP_WHITE_BALANCE_BLUE_U =17 , cv::CAP_PROP_RECTIFICATION =18 , cv::CAP_PROP_MONOCHROME =19 , cv::CAP_PROP_SHARPNESS =20 , cv::CAP_PROP_AUTO_EXPOSURE =21 , cv::CAP_PROP_GAMMA =22 , cv::CAP_PROP_TEMPERATURE =23 , cv::CAP_PROP_TRIGGER =24 , cv::CAP_PROP_TRIGGER_DELAY =25 , cv::CAP_PROP_WHITE_BALANCE_RED_V =26 , cv::CAP_PROP_ZOOM =27 , cv::CAP_PROP_FOCUS =28 , cv::CAP_PROP_GUID =29 , cv::CAP_PROP_ISO_SPEED =30 , cv::CAP_PROP_BACKLIGHT =32 , cv::CAP_PROP_PAN =33 , cv::CAP_PROP_TILT =34 , cv::CAP_PROP_ROLL =35 , cv::CAP_PROP_IRIS =36 , cv::CAP_PROP_SETTINGS =37 , cv::CAP_PROP_BUFFERSIZE =38 , cv::CAP_PROP_AUTOFOCUS =39 , cv::CAP_PROP_SAR_NUM =40 , cv::CAP_PROP_SAR_DEN =41 , cv::CAP_PROP_BACKEND =42 , cv::CAP_PROP_CHANNEL =43 , cv::CAP_PROP_AUTO_WB =44 , cv::CAP_PROP_WB_TEMPERATURE =45 , cv::CAP_PROP_CODEC_PIXEL_FORMAT =46 , cv::CAP_PROP_BITRATE =47 , cv::CAP_PROP_ORIENTATION_META =48 , cv::CAP_PROP_ORIENTATION_AUTO =49 , cv::CAP_PROP_HW_ACCELERATION =50 , cv::CAP_PROP_HW_DEVICE =51 , cv::CAP_PROP_HW_ACCELERATION_USE_OPENCL =52 , cv::CAP_PROP_OPEN_TIMEOUT_MSEC =53 , cv::CAP_PROP_READ_TIMEOUT_MSEC =54 , cv::CAP_PROP_STREAM_OPEN_TIME_USEC =55 , cv::CAP_PROP_VIDEO_TOTAL_CHANNELS = 56 , cv::CAP_PROP_VIDEO_STREAM = 57 , cv::CAP_PROP_AUDIO_STREAM = 58 , cv::CAP_PROP_AUDIO_POS = 59 , cv::CAP_PROP_AUDIO_SHIFT_NSEC = 60 , cv::CAP_PROP_AUDIO_DATA_DEPTH = 61 , cv::CAP_PROP_AUDIO_SAMPLES_PER_SECOND = 62 , cv::CAP_PROP_AUDIO_BASE_INDEX = 63 , cv::CAP_PROP_AUDIO_TOTAL_CHANNELS = 64 , cv::CAP_PROP_AUDIO_TOTAL_STREAMS = 65 , cv::CAP_PROP_AUDIO_SYNCHRONIZE = 66 , cv::CAP_PROP_LRF_HAS_KEY_FRAME = 67 , cv::CAP_PROP_CODEC_EXTRADATA_INDEX = 68 , cv::CAP_PROP_FRAME_TYPE = 69 , cv::CAP_PROP_N_THREADS = 70 , cv::CAP_PROP_PTS = 71 , cv::CAP_PROP_DTS_DELAY = 72 }  cv::VideoCapture generic properties identifier. More...  enum cv::VideoWriterProperties { cv::VIDEOWRITER_PROP_QUALITY = 1 , cv::VIDEOWRITER_PROP_FRAMEBYTES = 2 , cv::VIDEOWRITER_PROP_NSTRIPES = 3 , cv::VIDEOWRITER_PROP_IS_COLOR = 4 , cv::VIDEOWRITER_PROP_DEPTH = 5 , cv::VIDEOWRITER_PROP_HW_ACCELERATION = 6 , cv::VIDEOWRITER_PROP_HW_DEVICE = 7 , cv::VIDEOWRITER_PROP_HW_ACCELERATION_USE_OPENCL = 8 , cv::VIDEOWRITER_PROP_RAW_VIDEO = 9 , cv::VIDEOWRITER_PROP_KEY_INTERVAL = 10 , cv::VIDEOWRITER_PROP_KEY_FLAG = 11 , cv::VIDEOWRITER_PROP_PTS = 12 , cv::VIDEOWRITER_PROP_DTS_DELAY = 13 }  cv::VideoWriter generic properties identifier. More... "
gapi,Information Flow algorithm implementaton for alphamatting
gapi,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
gapi,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
gapi,The implementation is based on [7].
gapi,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
gapi,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
gapi,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
gapi,"Functions GMat cv::gapi::BackgroundSubtractor (const GMat &src, const cv::gapi::video::BackgroundSubtractorParams &bsParams)  Gaussian Mixture-based or K-nearest neighbours-based Background/Foreground Segmentation Algorithm. The operation generates a foreground mask.  std::tuple< GArray< GMat >, GScalar > cv::gapi::buildOpticalFlowPyramid (const GMat &img, const Size &winSize, const GScalar &maxLevel, bool withDerivatives=true, int pyrBorder=BORDER_REFLECT_101, int derivBorder=BORDER_CONSTANT, bool tryReuseInputImage=true)  Constructs the image pyramid which can be passed to calcOpticalFlowPyrLK.  std::tuple< GArray< Point2f >, GArray< uchar >, GArray< float > > cv::gapi::calcOpticalFlowPyrLK (const GArray< GMat > &prevPyr, const GArray< GMat > &nextPyr, const GArray< Point2f > &prevPts, const GArray< Point2f > &predPts, const Size &winSize=Size(21, 21), const GScalar &maxLevel=3, const TermCriteria &criteria=TermCriteria(TermCriteria::COUNT|TermCriteria::EPS, 30, 0.01), int flags=0, double minEigThresh=1e-4)  std::tuple< GArray< Point2f >, GArray< uchar >, GArray< float > > cv::gapi::calcOpticalFlowPyrLK (const GMat &prevImg, const GMat &nextImg, const GArray< Point2f > &prevPts, const GArray< Point2f > &predPts, const Size &winSize=Size(21, 21), const GScalar &maxLevel=3, const TermCriteria &criteria=TermCriteria(TermCriteria::COUNT|TermCriteria::EPS, 30, 0.01), int flags=0, double minEigThresh=1e-4)  Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with pyramids.  GMat cv::gapi::KalmanFilter (const GMat &measurement, const GOpaque< bool > &haveMeasurement, const cv::gapi::KalmanParams &kfParams)  GMat cv::gapi::KalmanFilter (const GMat &measurement, const GOpaque< bool > &haveMeasurement, const GMat &control, const cv::gapi::KalmanParams &kfParams)  Standard Kalman filter algorithm http://en.wikipedia.org/wiki/Kalman_filter. "
gapi,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
gapi,Structure for the Background Subtractor operation's initialization parameters.
gapi,"Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as Mat's). It means that for each pixel location \((x,y)\) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location \((x,y)\). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one."
gapi,"Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian \(3 \times 3\) filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (""replicated border"" extrapolation method), or assume that all the non-existing pixels are zeros (""constant border"" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see BorderTypes"
gapi,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
gapi,Namespace for all functions is cv::img_hash.
gapi,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
gapi,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
gapi,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
gapi,This modules is to draw UTF-8 strings with freetype/harfbuzz.
gapi,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
gapi,Classes class cv::freetype::FreeType2 
gapi,Matrix read-write iterator.
gapi,cv::MediaFrame class represents an image/media frame obtained from an external source.
gapi,cv::MediaFrame represents image data as specified in cv::MediaFormat. cv::MediaFrame is designed to be a thin wrapper over some external memory of buffer; the class itself provides an uniform interface over such types of memory. cv::MediaFrame wraps data from a camera driver or from a media codec and provides an abstraction layer over this memory to G-API. MediaFrame defines a compact interface to access and manage the underlying data; the implementation is fully defined by the associated Adapter (which is usually user-defined).
gapi,"Enumerations enum cv::KmeansFlags { cv::KMEANS_RANDOM_CENTERS = 0 , cv::KMEANS_PP_CENTERS = 2 , cv::KMEANS_USE_INITIAL_LABELS = 1 }  k-means flags More... "
gapi,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
gapi,Namespaces namespace NcvCTprep 
gapi,Smart pointer for OpenGL 2D texture memory with reference counting.
gapi,"Functions GMat cv::gapi::Canny (const GMat &image, double threshold1, double threshold2, int apertureSize=3, bool L2gradient=false)  Finds edges in an image using the Canny algorithm.  GArray< Point2f > cv::gapi::goodFeaturesToTrack (const GMat &image, int maxCorners, double qualityLevel, double minDistance, const Mat &mask=Mat(), int blockSize=3, bool useHarrisDetector=false, double k=0.04)  Determines strong corners on an image. "
gapi,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
gapi,"Functions void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1, __m128 &v_a0, __m128 &v_a1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1, __m128 &v_a0, __m128 &v_a1) "
gapi,This module contains:
gapi,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
gapi,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
gapi,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
gapi,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
gapi,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
gapi,"This figure explains new functionality implemented with WinRT GUI. The new GUI provides an Image control, and a slider panel. Slider panel holds trackbars attached to it."
gapi,Sliders are attached below the image control. Every new slider is added below the previous one.
gapi,See below the example used to generate the figure:
gapi,Functions void cv::winrt_initContainer (::Windows::UI::Xaml::Controls::Panel^ container)  Initializes container component that will be used to hold generated window content. 
gapi,This structure represents a rectangle to draw.
gapi,Parameters match cv::rectangle().
gapi,Template class for specifying the size of an image or rectangle.
gapi,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
gapi,OpenCV defines the following Size_<> aliases:
gapi,"Drawing functions work with matrices/images of arbitrary depth. The boundaries of the shapes can be rendered with antialiasing (implemented only for 8-bit images for now). All the functions include the parameter color that uses an RGB value (that may be constructed with the Scalar constructor ) for color images and brightness for grayscale images. For color images, the channel ordering is normally Blue, Green, Red. This is what imshow, imread, and imwrite expect. So, if you form a color using the Scalar constructor, it should look like:"
gapi,"\[\texttt{Scalar} (blue \_ component, green \_ component, red \_ component[, alpha \_ component])\]"
gapi,"If you are using your own image rendering and I/O functions, you can use any channel ordering. The drawing functions process each channel independently and do not depend on the channel order or even on the used color space. The whole image can be converted from BGR to RGB or to a different color space using cvtColor ."
gapi,"If a drawn figure is partially or completely outside the image, the drawing functions clip it. Also, many drawing functions can handle pixel coordinates specified with sub-pixel accuracy. This means that the coordinates can be passed as fixed-point numbers encoded as integers. The number of fractional bits is specified by the shift parameter and the real point coordinates are calculated as \(\texttt{Point}(x,y)\rightarrow\texttt{Point2f}(x*2^{-shift},y*2^{-shift})\) . This feature is especially effective when rendering antialiased shapes."
gapi,Classes class cv::LineIterator  Class for iterating over all pixels on a raster line segment. More... 
gapi,gapi_colorconvert
gapi,"Functions GMat cv::gapi::concatHor (const GMat &src1, const GMat &src2)  Applies horizontal concatenation to given matrices.  GMat cv::gapi::concatHor (const std::vector< GMat > &v)  GMat cv::gapi::concatVert (const GMat &src1, const GMat &src2)  Applies vertical concatenation to given matrices.  GMat cv::gapi::concatVert (const std::vector< GMat > &v)  GMat cv::gapi::convertTo (const GMat &src, int rdepth, double alpha=1, double beta=0)  Converts a matrix to another data depth with optional scaling.  GFrame cv::gapi::copy (const GFrame &in)  Makes a copy of the input frame. Note that this copy may be not real (no actual data copied). Use this function to maintain graph contracts, e.g when graph's input needs to be passed directly to output, like in Streaming mode.  GMat cv::gapi::copy (const GMat &in)  Makes a copy of the input image. Note that this copy may be not real (no actual data copied). Use this function to maintain graph contracts, e.g when graph's input needs to be passed directly to output, like in Streaming mode.  GMat cv::gapi::crop (const GMat &src, const Rect &rect)  Crops a 2D matrix.  GMat cv::gapi::flip (const GMat &src, int flipCode)  Flips a 2D matrix around vertical, horizontal, or both axes.  GMat cv::gapi::LUT (const GMat &src, const Mat &lut)  Performs a look-up table transform of a matrix.  GMat cv::gapi::merge3 (const GMat &src1, const GMat &src2, const GMat &src3)  Creates one 3-channel matrix out of 3 single-channel ones.  GMat cv::gapi::merge4 (const GMat &src1, const GMat &src2, const GMat &src3, const GMat &src4)  Creates one 4-channel matrix out of 4 single-channel ones.  GMat cv::gapi::normalize (const GMat &src, double alpha, double beta, int norm_type, int ddepth=-1)  Normalizes the norm or value range of an array.  GMat cv::gapi::remap (const GMat &src, const Mat &map1, const Mat &map2, int interpolation, int borderMode=BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies a generic geometrical transformation to an image.  GMat cv::gapi::resize (const GMat &src, const Size &dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR)  Resizes an image.  GMatP cv::gapi::resizeP (const GMatP &src, const Size &dsize, int interpolation=cv::INTER_LINEAR)  Resizes a planar image.  std::tuple< GMat, GMat, GMat > cv::gapi::split3 (const GMat &src)  Divides a 3-channel matrix into 3 single-channel matrices.  std::tuple< GMat, GMat, GMat, GMat > cv::gapi::split4 (const GMat &src)  Divides a 4-channel matrix into 4 single-channel matrices.  GMat cv::gapi::warpAffine (const GMat &src, const Mat &M, const Size &dsize, int flags=cv::INTER_LINEAR, int borderMode=cv::BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies an affine transformation to an image.  GMat cv::gapi::warpPerspective (const GMat &src, const Mat &M, const Size &dsize, int flags=cv::INTER_LINEAR, int borderMode=cv::BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies a perspective transformation to an image. "
gapi,Comma-separated Matrix Initializer.
gapi,"The class instances are usually not created explicitly. Instead, they are created on ""matrix << firstValue"" operator."
gapi,The sample below initializes 2x2 rotation matrix:
gapi,"Functions void cv::samples::addSamplesDataSearchPath (const cv::String &path)  Override search data path by adding new search location.  void cv::samples::addSamplesDataSearchSubDirectory (const cv::String &subdir)  Append samples search data sub directory.  cv::String cv::samples::findFile (const cv::String &relative_path, bool required=true, bool silentMode=false)  Try to find requested data file.  cv::String cv::samples::findFileOrKeep (const cv::String &relative_path, bool silentMode=false) "
gapi,"Template class for short numerical vectors, a partial case of Matx."
gapi,"This template class represents short numerical vectors (of 1, 2, 3, 4 ... elements) on which you can perform basic arithmetical operations, access individual elements using [] operator etc. The vectors are allocated on stack, as opposite to std::valarray, std::vector, cv::Mat etc., which elements are dynamically allocated in the heap."
gapi,The template takes 2 parameters:
gapi,_Tp element type cn the number of elements
gapi,"In addition to the universal notation like Vec<float, 3>, you can use shorter aliases for the most popular specialized variants of Vec, e.g. Vec3f ~ Vec<float, 3>."
gapi,"It is possible to convert Vec<T,2> to/from Point_, Vec<T,3> to/from Point3_ , and Vec<T,4> to CvScalar or Scalar_. Use operator[] to access the elements of Vec."
gapi,All the expected vector operations are also implemented:
gapi,"v1 = v2 + v3 v1 = v2 - v3 v1 = v2 * scale v1 = scale * v2 v1 = -v2 v1 += v2 and other augmenting operations v1 == v2, v1 != v2 norm(v1) (euclidean norm) The Vec class is commonly used to describe pixel types of multi-channel arrays. See Mat for details."
gapi,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
gapi,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
gapi,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
gapi,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
gapi,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
gapi,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
gapi,"Functions void cv::julia::initJulia (int argc, char **argv) "
gapi,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
gapi,"SoftFloat is a software implementation of floating-point calculations according to IEEE 754 standard. All calculations are done in integers, that's why they are machine-independent and bit-exact. This library can be useful in accuracy-critical parts like look-up tables generation, tests, etc. OpenCV contains a subset of SoftFloat partially rewritten to C++."
gapi,Structure for the Kalman filter's initialization parameters.
gapi,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
gapi,It provides easy interface to:
gapi,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
gapi,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
gapi,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
gapi,It is planned to have:
gapi,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
gapi,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
gapi,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
gapi,Manages memory block shared by muliple buffers.
gapi,"This class allows to allocate one large memory block and split it into several smaller non-overlapping buffers. In safe mode each buffer allocation will be performed independently, this mode allows dynamic memory access instrumentation using valgrind or memory sanitizer."
gapi,Safe mode can be explicitly switched ON in constructor. It will also be enabled when compiling with memory sanitizer support or in runtime with the environment variable OPENCV_BUFFER_AREA_ALWAYS_SAFE.
gapi,Example of usage:
gapi,Functions float32x2_t cv_vrecp_f32 (float32x2_t val)  float32x4_t cv_vrecpq_f32 (float32x4_t val)  int32x2_t cv_vrnd_s32_f32 (float32x2_t v)  uint32x2_t cv_vrnd_u32_f32 (float32x2_t v)  int32x4_t cv_vrndq_s32_f32 (float32x4_t v)  uint32x4_t cv_vrndq_u32_f32 (float32x4_t v)  float32x2_t cv_vrsqrt_f32 (float32x2_t val)  float32x4_t cv_vrsqrtq_f32 (float32x4_t val)  float32x2_t cv_vsqrt_f32 (float32x2_t val)  float32x4_t cv_vsqrtq_f32 (float32x4_t val) 
gapi,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
gapi,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
gapi,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
gapi,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
gapi,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
gapi,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
gapi,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
gapi,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
gapi,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
gapi,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
gapi,This module has been originally developed as a project for Google Summer of Code 2012-2015.
gapi,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
gapi,STL namespace.
gapi,Automatically Allocated Buffer Class.
gapi,"The class is used for temporary buffers in functions and methods. If a temporary buffer is usually small (a few K's of memory), but its size depends on the parameters, it makes sense to create a small fixed-size array on stack and use it if it's large enough. If the required buffer size is larger than the fixed size, another buffer of sufficient size is allocated dynamically and released after the processing. Therefore, in typical cases, when the buffer size is small, there is no overhead associated with malloc()/free(). At the same time, there is no limit on the size of processed data."
gapi,This is what AutoBuffer does. The template takes 2 parameters - type of the buffer elements and the number of stack-allocated elements. Here is how the class is used:
gapi,G-API classes for constructed and compiled graphs.
gapi,Namespaces namespace cv::detail  namespace cv::gapi 
gapi,"Class for video capturing from video files, image sequences or cameras."
gapi,The class provides C++ API for capturing video from cameras or for reading video files and image sequences.
gapi,Here is how the class can be used:
gapi,(C++) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/cpp/videocapture_starter.cpp (Python) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/python/video.py (Python) A multi threaded video processing sample can be found at OPENCV_SOURCE_CODE/samples/python/video_threaded.py (Python) VideoCapture sample showcasing some features of the Video4Linux2 backend OPENCV_SOURCE_CODE/samples/python/video_v4l2.py
gapi,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
gapi,The distortion-free projective transformation given by a pinhole camera model is shown below.
gapi,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
gapi,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
gapi,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
gapi,\[p = A P_c.\]
gapi,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
gapi,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
gapi,and thus
gapi,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
gapi,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
gapi,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
gapi,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
gapi,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
gapi,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
gapi,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
gapi,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
gapi,and therefore
gapi,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
gapi,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
gapi,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
gapi,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
gapi,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
gapi,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
gapi,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
gapi,with
gapi,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
gapi,The following figure illustrates the pinhole camera model.
gapi,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
gapi,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
gapi,where
gapi,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
gapi,with
gapi,\[r^2 = x'^2 + y'^2\]
gapi,and
gapi,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
gapi,if \(Z_c \ne 0\).
gapi,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
gapi,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
gapi,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
gapi,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
gapi,where
gapi,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
gapi,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
gapi,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
gapi,In the functions below the coefficients are passed or returned as
gapi,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
gapi,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
gapi,The functions below use the above model to do the following:
gapi,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
gapi,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
gapi,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
gapi,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
gapi,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
gapi,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
gapi,if \(W \ne 0\).
gapi,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
gapi,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
gapi,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
gapi,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
gapi,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
gapi,Compilation arguments: data structures controlling the compilation process.
gapi,"G-API comes with a number of graph compilation options which can be passed to cv::GComputation::apply() or cv::GComputation::compile(). Known compilation options are listed in this page, while extra backends may introduce their own compilation options (G-API transparently accepts everything which can be passed to cv::compile_args(), it depends on underlying backends if an option would be interpreted or not)."
gapi,"For example, if an example computation is executed like this:"
gapi,Extra parameter specifying which kernels to compile with can be passed like this:
gapi,Namespaces namespace cv::gapi 
gapi,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
gapi,The class defining termination criteria for iterative algorithms.
gapi,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
gapi,"ArUco Marker Detection, module functionality was moved to objdetect module"
gapi,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
gapi,a Class to measure passing time.
gapi,"The class computes passing time by counting the number of ticks per second. That is, the following code computes the execution time in seconds:"
gapi,It is also possible to compute the average time over multiple runs:
gapi,This namespace contains G-API Operation Types for OpenCV Core module functionality.
gapi,GFrame class represents an image or media frame in the graph.
gapi,"GFrame doesn't store any data itself, instead it describes a functional relationship between operations consuming and producing GFrame objects."
gapi,"GFrame is introduced to handle various media formats (e.g., NV12 or I420) under the same type. Various image formats may differ in the number of planes (e.g. two for NV12, three for I420) and the pixel layout inside. GFrame type allows to handle these media formats in the graph uniformly the graph structure will not change if the media format changes, e.g. a different camera or decoder is used with the same graph. G-API provides a number of operations which operate directly on GFrame, like infer<>() or renderFrame(); these operations are expected to handle different media formats inside. There is also a number of accessor operations like BGR(), Y(), UV() these operations provide access to frame's data in the familiar cv::GMat form, which can be used with the majority of the existing G-API operations. These accessor functions may perform color space conversion on the fly if the image format of the GFrame they are applied to differs from the operation's semantic (e.g. the BGR() accessor is called on an NV12 image frame)."
gapi,GFrame is a virtual counterpart of cv::MediaFrame.
gapi,GScalar class represents cv::Scalar data in the graph.
gapi,"GScalar may be associated with a cv::Scalar value, which becomes its constant value bound in graph compile time. cv::GScalar describes a functional relationship between operations consuming and producing GScalar objects."
gapi,"GScalar is a virtual counterpart of cv::Scalar, which is usually used to represent the GScalar data in G-API during the execution."
gapi,opencv2/gapi/gscalar.hpp
gapi,Simple TLS data class.
gapi,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
gapi,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
gapi,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
gapi,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
gapi,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
gapi,"GComputation class represents a captured computation graph. GComputation objects form boundaries for expression code user writes with G-API, allowing to compile and execute it."
gapi,"G-API computations are defined with input/output data objects. G-API will track automatically which operations connect specified outputs to the inputs, forming up a call graph to be executed. The below example expresses calculation of Sobel operator for edge detection ( \(G = \sqrt{G_x^2 + G_y^2}\)):"
gapi,Full pipeline can be now captured with this object declaration:
gapi,Input/output data objects on which a call graph should be reconstructed are passed using special wrappers cv::GIn and cv::GOut. G-API will track automatically which operations form a path from inputs to outputs and build the execution graph appropriately.
gapi,"Note that cv::GComputation doesn't take ownership on data objects it is defined. Moreover, multiple GComputation objects may be defined on the same expressions, e.g. a smaller pipeline which expects that image gradients are already pre-calculated may be defined like this:"
gapi,"The resulting graph would expect two inputs and produce one output. In this case, it doesn't matter if gx/gy data objects are results of cv::gapi::Sobel operators G-API will stop unrolling expressions and building the underlying graph one reaching this data objects."
gapi,"The way how GComputation is defined is important as its definition specifies graph protocol the way how the graph should be used. Protocol is defined by number of inputs, number of outputs, and shapes of inputs and outputs."
gapi,"In the above example, sobelEdge expects one Mat on input and produces one Mat; while sobelEdgeSub expects two Mats on input and produces one Mat. GComputation's protocol defines how other computation methods should be used cv::GComputation::compile() and cv::GComputation::apply(). For example, if a graph is defined on two GMat inputs, two cv::Mat objects have to be passed to apply() for execution. GComputation checks protocol correctness in runtime so passing a different number of objects in apply() or passing cv::Scalar instead of cv::Mat there would compile well as a C++ source but raise an exception in run-time. G-API also comes with a typed wrapper cv::GComputationT<> which introduces this type-checking in compile-time."
gapi,"cv::GComputation itself is a thin object which just captures what the graph is. The compiled graph (which actually process data) is represented by class GCompiled. Use compile() method to generate a compiled graph with given compile options. cv::GComputation can also be used to process data with implicit graph compilation on-the-fly, see apply() for details."
gapi,"GComputation is a reference-counted object once defined, all its copies will refer to the same instance."
gapi,Template class specifying a continuous subsequence (slice) of a sequence.
gapi,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
gapi,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
gapi,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
gapi,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
gapi,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
gapi,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
gapi,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
gapi,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
gapi,"Functions GMat cv::gapi::bilateralFilter (const GMat &src, int d, double sigmaColor, double sigmaSpace, int borderType=BORDER_DEFAULT)  Applies the bilateral filter to an image.  GMat cv::gapi::blur (const GMat &src, const Size &ksize, const Point &anchor=Point(-1,-1), int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Blurs an image using the normalized box filter.  GMat cv::gapi::boxFilter (const GMat &src, int dtype, const Size &ksize, const Point &anchor=Point(-1,-1), bool normalize=true, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Blurs an image using the box filter.  GMat cv::gapi::dilate (const GMat &src, const Mat &kernel, const Point &anchor=Point(-1,-1), int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Dilates an image by using a specific structuring element.  GMat cv::gapi::dilate3x3 (const GMat &src, int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Dilates an image by using 3 by 3 rectangular structuring element.  GMat cv::gapi::erode (const GMat &src, const Mat &kernel, const Point &anchor=Point(-1,-1), int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Erodes an image by using a specific structuring element.  GMat cv::gapi::erode3x3 (const GMat &src, int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Erodes an image by using 3 by 3 rectangular structuring element.  GMat cv::gapi::filter2D (const GMat &src, int ddepth, const Mat &kernel, const Point &anchor=Point(-1,-1), const Scalar &delta=Scalar(0), int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Convolves an image with the kernel.  GMat cv::gapi::gaussianBlur (const GMat &src, const Size &ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Blurs an image using a Gaussian filter.  GMat cv::gapi::Laplacian (const GMat &src, int ddepth, int ksize=1, double scale=1, double delta=0, int borderType=BORDER_DEFAULT)  Calculates the Laplacian of an image.  GMat cv::gapi::medianBlur (const GMat &src, int ksize)  Blurs an image using the median filter.  GMat cv::gapi::morphologyEx (const GMat &src, const MorphTypes op, const Mat &kernel, const Point &anchor=Point(-1,-1), const int iterations=1, const BorderTypes borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Performs advanced morphological transformations.  GMat cv::gapi::sepFilter (const GMat &src, int ddepth, const Mat &kernelX, const Mat &kernelY, const Point &anchor, const Scalar &delta, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Applies a separable linear filter to a matrix(image).  GMat cv::gapi::Sobel (const GMat &src, int ddepth, int dx, int dy, int ksize=3, double scale=1, double delta=0, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.  std::tuple< GMat, GMat > cv::gapi::SobelXY (const GMat &src, int ddepth, int order, int ksize=3, double scale=1, double delta=0, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator. "
gapi,"Functions GMat cv::gapi::add (const GMat &src1, const GMat &src2, int ddepth=-1)  Calculates the per-element sum of two matrices.  GMat cv::gapi::addC (const GMat &src1, const GScalar &c, int ddepth=-1)  Calculates the per-element sum of matrix and given scalar.  GMat cv::gapi::addC (const GScalar &c, const GMat &src1, int ddepth=-1)  This is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts.  std::tuple< GMat, GMat > cv::gapi::cartToPolar (const GMat &x, const GMat &y, bool angleInDegrees=false)  Calculates the magnitude and angle of 2D vectors.  GMat cv::gapi::div (const GMat &src1, const GMat &src2, double scale, int ddepth=-1)  Performs per-element division of two matrices.  GMat cv::gapi::divC (const GMat &src, const GScalar &divisor, double scale, int ddepth=-1)  Divides matrix by scalar.  GMat cv::gapi::divRC (const GScalar &divident, const GMat &src, double scale, int ddepth=-1)  Divides scalar by matrix.  GMat cv::gapi::mask (const GMat &src, const GMat &mask)  Applies a mask to a matrix.  GScalar cv::gapi::mean (const GMat &src)  Calculates an average (mean) of matrix elements.  GMat cv::gapi::mul (const GMat &src1, const GMat &src2, double scale=1.0, int ddepth=-1)  Calculates the per-element scaled product of two matrices.  GMat cv::gapi::mulC (const GMat &src, const GScalar &multiplier, int ddepth=-1)  This is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts.  GMat cv::gapi::mulC (const GMat &src, double multiplier, int ddepth=-1)  Multiplies matrix by scalar.  GMat cv::gapi::mulC (const GScalar &multiplier, const GMat &src, int ddepth=-1)  This is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts.  GMat cv::gapi::phase (const GMat &x, const GMat &y, bool angleInDegrees=false)  Calculates the rotation angle of 2D vectors.  std::tuple< GMat, GMat > cv::gapi::polarToCart (const GMat &magnitude, const GMat &angle, bool angleInDegrees=false)  Calculates x and y coordinates of 2D vectors from their magnitude and angle.  GMat cv::gapi::sqrt (const GMat &src)  Calculates a square root of array elements.  GMat cv::gapi::sub (const GMat &src1, const GMat &src2, int ddepth=-1)  Calculates the per-element difference between two matrices.  GMat cv::gapi::subC (const GMat &src, const GScalar &c, int ddepth=-1)  Calculates the per-element difference between matrix and given scalar.  GMat cv::gapi::subRC (const GScalar &c, const GMat &src, int ddepth=-1)  Calculates the per-element difference between given scalar and the matrix. "
gapi,G-API functions and classes for serialization and deserialization.
gapi,Namespaces namespace cv::gapi  namespace cv::gapi::s11n  This namespace contains G-API serialization and deserialization functions and data structures. 
gapi,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
gapi,Template class for 2D points specified by its coordinates x and y.
gapi,"An instance of the class is interchangeable with C structures, CvPoint and CvPoint2D32f . There is also a cast operator to convert point coordinates to the specified type. The conversion from floating-point coordinates to integer coordinates is done by rounding. Commonly, the conversion uses this operation for each of the coordinates. Besides the class members listed in the declaration above, the following operations on points are implemented:"
gapi,"For your convenience, the following type aliases are defined:"
gapi,Example:
gapi,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
gapi,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
gapi,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
gapi,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
gapi,"Each class derived from Map implements a motion model, as follows:"
gapi,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
gapi,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
gapi,The classes derived from Mapper are
gapi,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
gapi,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
gapi,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
gapi,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
gapi,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
gapi,This module provides storage routines for Hierarchical Data Format objects.
gapi,Face module changelog Face Recognition with OpenCV
gapi,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
gapi,The class represents rotated (i.e. not up-right) rectangles on a plane.
gapi,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
gapi,The sample below demonstrates how to use RotatedRect:
gapi,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
gapi,G-API data objects used to build G-API expressions.
gapi,These objects do not own any particular data (except compile-time associated values like with cv::GScalar or cv::GArray<T>) and are used only to construct graphs.
gapi,Every graph in G-API starts and ends with data objects.
gapi,"Once constructed and compiled, G-API operates with regular host-side data instead. Refer to the below table to find the mapping between G-API and regular data types when passing input and output data structures to G-API:"
gapi,"G-API data type I/O data type cv::GMat cv::Mat, cv::UMat, cv::RMat cv::GScalar cv::Scalar cv::GArray<T> std::vector<T> cv::GOpaque<T> T cv::GFrame cv::MediaFrame"
gapi,Classes class cv::GArray< T >  cv::GArray<T> template class represents a list of objects of class T in the graph. More...  class cv::GFrame  GFrame class represents an image or media frame in the graph. More...  class cv::GMat  GMat class represents image or tensor data in the graph. More...  class cv::GMatP  class cv::GOpaque< T >  cv::GOpaque<T> template class represents an object of class T in the graph. More...  class cv::GScalar  GScalar class represents cv::Scalar data in the graph. More... 
gapi,Classes class cv::plot::Plot2d 
gapi,This class is a typed wrapper over a regular GComputation.
gapi,"std::function<>-like template parameter specifies the graph signature so methods so the object's constructor, methods like apply() and the derived GCompiledT::operator() also become typed."
gapi,"There is no need to use cv::gin() or cv::gout() modifiers with objects of this class. Instead, all input arguments are followed by all output arguments in the order from the template argument signature."
gapi,Refer to the following example. Regular (untyped) code is written this way:
gapi,Here:
gapi,"cv::GComputation object is created with a lambda constructor where it is defined as a two-input, one-output graph. Its method apply() in fact takes arbitrary number of arguments (as vectors) so user can pass wrong number of inputs/outputs here. C++ compiler wouldn't notice that since the cv::GComputation API is polymorphic, and only a run-time error will be generated."
gapi,Now the same code written with typed API:
gapi,The key difference is:
gapi,Now the constructor lambda must take parameters and must return values as defined in the GComputationT<> signature. Its method apply() does not require any extra specifiers to separate input arguments from the output ones A GCompiledT (compilation product) takes input/output arguments with no extra specifiers as well.
gapi,opencv2/gapi/gtyped.hpp
gapi,G-API backends available in this OpenCV version.
gapi,G-API backends play a corner stone role in G-API execution stack. Every backend is hardware-oriented and thus can run its kernels efficiently on the target platform.
gapi,"Backends are usually ""black boxes"" for G-API users on the API side, all backends are represented as different objects of the same class cv::gapi::GBackend. User can manipulate with backends by specifying which kernels to use."
gapi,Functions cv::gapi::GBackend cv::gapi::cpu::backend ()  Get a reference to CPU (OpenCV) backend.  cv::gapi::GBackend cv::gapi::fluid::backend ()  Get a reference to Fluid backend.  cv::gapi::GBackend cv::gapi::ocl::backend ()  Get a reference to OCL backend. 
gapi,Classes class cv::quality::QualityBase 
gapi,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
gapi,"Functions GMat cv::gapi::BayerGR2RGB (const GMat &src_gr)  Converts an image from BayerGR color space to RGB. The function converts an input image from BayerGR color space to RGB. The conventional ranges for G, R, and B channel values are 0 to 255.  GMat cv::gapi::BGR2Gray (const GMat &src)  Converts an image from BGR color space to gray-scaled.  GMat cv::gapi::BGR2I420 (const GMat &src)  Converts an image from BGR color space to I420 color space.  GMat cv::gapi::BGR2LUV (const GMat &src)  Converts an image from BGR color space to LUV color space.  GMat cv::gapi::BGR2RGB (const GMat &src)  Converts an image from BGR color space to RGB color space.  GMat cv::gapi::BGR2YUV (const GMat &src)  Converts an image from BGR color space to YUV color space.  GMat cv::gapi::I4202BGR (const GMat &src)  Converts an image from I420 color space to BGR color space.  GMat cv::gapi::I4202RGB (const GMat &src)  Converts an image from I420 color space to BGR color space.  GMat cv::gapi::LUV2BGR (const GMat &src)  Converts an image from LUV color space to BGR color space.  GMat cv::gapi::NV12toBGR (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to BGR. The function converts an input image from NV12 color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMatP cv::gapi::NV12toBGRp (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to BGR. The function converts an input image from NV12 color space to BGR. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMat cv::gapi::NV12toGray (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to gray-scaled. The function converts an input image from NV12 color space to gray-scaled. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMat cv::gapi::NV12toRGB (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to RGB. The function converts an input image from NV12 color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMatP cv::gapi::NV12toRGBp (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to RGB. The function converts an input image from NV12 color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMat cv::gapi::RGB2Gray (const GMat &src)  Converts an image from RGB color space to gray-scaled.  GMat cv::gapi::RGB2Gray (const GMat &src, float rY, float gY, float bY)  GMat cv::gapi::RGB2HSV (const GMat &src)  Converts an image from RGB color space to HSV. The function converts an input image from RGB color space to HSV. The conventional ranges for R, G, and B channel values are 0 to 255.  GMat cv::gapi::RGB2I420 (const GMat &src)  Converts an image from RGB color space to I420 color space.  GMat cv::gapi::RGB2Lab (const GMat &src)  Converts an image from RGB color space to Lab color space.  GMat cv::gapi::RGB2YUV (const GMat &src)  Converts an image from RGB color space to YUV color space.  GMat cv::gapi::RGB2YUV422 (const GMat &src)  Converts an image from RGB color space to YUV422. The function converts an input image from RGB color space to YUV422. The conventional ranges for R, G, and B channel values are 0 to 255.  GMat cv::gapi::YUV2BGR (const GMat &src)  Converts an image from YUV color space to BGR color space.  GMat cv::gapi::YUV2RGB (const GMat &src)  Converts an image from YUV color space to RGB. The function converts an input image from YUV color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255. "
gapi,"This figure explains new functionality implemented with Qt* GUI. The new GUI provides a statusbar, a toolbar, and a control panel. The control panel can have trackbars and buttonbars attached to it. If you cannot see the control panel, press Ctrl+P or right-click any Qt window and select Display properties window."
gapi,"To attach a trackbar, the window name parameter must be NULL. To attach a buttonbar, a button must be created. If the last bar attached to the control panel is a buttonbar, the new button is added to the right of the last button. If the last bar attached to the control panel is a trackbar, or the control panel is empty, a new buttonbar is created. Then, a new button is attached to it."
gapi,See below the example used to generate the figure:
gapi,Classes struct cv::QtFont  QtFont available only for Qt. See cv::fontQt. More... 
gapi,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
gapi,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
gapi,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
gapi,TLS data accumulator with gathering methods.
gapi,Namespaces namespace cv::traits 
gapi,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
gapi,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
gapi,"Hardware acceleration support enum cv::VideoAccelerationType { cv::VIDEO_ACCELERATION_NONE = 0 , cv::VIDEO_ACCELERATION_ANY = 1 , cv::VIDEO_ACCELERATION_D3D11 = 2 , cv::VIDEO_ACCELERATION_VAAPI = 3 , cv::VIDEO_ACCELERATION_MFX = 4 }  Video Acceleration type. More... "
gapi,Namespace for all functions is cv::intensity_transform.
gapi,Classes class cv::LineSegmentDetector  Line segment detector class. More... 
gapi,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
gapi,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
gapi,Kalman filter class.
gapi,"The class implements a standard Kalman filter http://en.wikipedia.org/wiki/Kalman_filter, [297] . However, you can modify transitionMatrix, controlMatrix, and measurementMatrix to get an extended Kalman filter functionality."
gapi,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
gapi,Video writer class.
gapi,The class provides C++ API for writing video files or image sequences.
gapi,The class SparseMat represents multi-dimensional sparse numerical arrays.
gapi,"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:"
gapi,"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5; int size[5] = {10, 10, 10, 10, 10}; SparseMat sparse_mat(dims, size, CV_32F); for(int i = 0; i < 1000; i++) { int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f; } cout << ""nnz = "" << sparse_mat.nzcount() << endl; cv::SparseMatThe class SparseMat represents multi-dimensional sparse numerical arrays.Definition mat.hpp:2751 cv::SparseMat::sizeconst int * size() constreturns the array of sizes, or NULL if the matrix is not allocated cv::SparseMat::dimsint dims() constreturns the matrix dimensionality CV_32F#define CV_32FDefinition interface.h:78 Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix // and the sum of elements. SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>(); double s = 0; int dims = sparse_mat.dims(); for(; it != it_end; ++it) { // print element indices and the element value const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("": %g\n"", it.value<float>()); s += *it; } printf(""Element sum is %g\n"", s); cv::SparseMatConstIterator_Template Read-Only Sparse Matrix Iterator Class.Definition mat.hpp:3354 cv::SparseMatConstIterator::valueconst _Tp & value() consttemplate method returning the current matrix element cv::SparseMatConstIterator::nodeconst SparseMat::Node * node() constreturns the current node of the sparse matrix. it.node->idx is the current element index cv::SparseMat::Nodesparse matrix node - element of a hash tableDefinition mat.hpp:2776 cv::SparseMat::Node::idxint idx[MAX_DIM]index of the matrix elementDefinition mat.hpp:2782 If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation. Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b) { const SparseMat *_a = &a, *_b = &b; // if b contains less elements than a, // it is faster to iterate through b if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // take the next element from the first matrix float avalue = *it; const Node* anode = it.node(); // and try to find an element with the same index in the second matrix. // since the hash value depends only on the element index, // reuse the hash value stored in the node float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr; } cv::SparseMat::endSparseMatIterator end()returns the sparse matrix iterator at the matrix end cv::SparseMat::nzcountsize_t nzcount() constreturns the number of non-zero elements (=the number of hash table nodes) cv::SparseMat::beginSparseMatIterator begin()returns the sparse matrix iterator at the matrix beginning cv::SparseMat::Node::hashvalsize_t hashvalhash valueDefinition mat.hpp:2778"
gapi,Read and write video or images sequence with OpenCV.
gapi,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
gapi,Bioinspired Module Retina Introduction
gapi,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
gapi,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
gapi,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
gapi,See detailed overview here: Machine Learning Overview.
gapi,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
gapi,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
gapi,Classes struct cv::gapi::wip::draw::Circle  This structure represents a circle to draw. More...  struct cv::gapi::wip::draw::FText  This structure represents a text string to draw using FreeType renderer. More...  struct cv::gapi::wip::draw::Image  This structure represents an image to draw. More...  struct cv::gapi::wip::draw::Line  This structure represents a line to draw. More...  struct cv::gapi::wip::draw::Mosaic  This structure represents a mosaicing operation. More...  struct cv::gapi::wip::draw::Poly  This structure represents a polygon to draw. More...  struct cv::gapi::wip::draw::Rect  This structure represents a rectangle to draw. More...  struct cv::gapi::wip::draw::Text  This structure represents a text string to draw. More... 
gapi,Template class for small matrices whose type and size are known at compilation time.
gapi,"If you need a more flexible type, use Mat . The elements of the matrix M are accessible using the M(i,j) notation. Most of the common matrix operations (see also MatrixExpressions ) are available. To do an operation on Matx that is not implemented, you can easily convert the matrix to Mat and backwards:"
gapi,"Except of the plain constructor which takes a list of elements, Matx can be initialized from a C-array:"
gapi,"In case if C++11 features are available, std::initializer_list can be also used to initialize Matx:"
gapi,This namespace contains G-API Operation Types for OpenCV ImgProc module functionality.
gapi,cv::GOpaque<T> template class represents an object of class T in the graph.
gapi,"cv::GOpaque<T> describes a functional relationship between operations consuming and producing object of class T. cv::GOpaque<T> is designed to extend G-API with user-defined data types, which are often required with user-defined operations. G-API can't apply any optimizations to user-defined types since these types are opaque to the framework. However, there is a number of G-API operations declared with cv::GOpaque<T> as a return type, e.g. cv::gapi::streaming::timestamp() or cv::gapi::streaming::size()."
gapi,TLS container base implementation
gapi,Don't use directly.
gapi,"Functions void cv::imshow (const String &winname, const ogl::Texture2D &tex)  Displays OpenGL 2D texture in the specified window.  void cv::setOpenGlContext (const String &winname)  Sets the specified window as current OpenGL context.  void cv::setOpenGlDrawCallback (const String &winname, OpenGlDrawCallback onOpenGlDraw, void *userdata=0)  Sets a callback function to be called to draw on top of displayed image.  void cv::updateWindow (const String &winname)  Force window to redraw its context and call draw callback ( See cv::setOpenGlDrawCallback ). "
gapi,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
gapi,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
gapi,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
gapi,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
gapi,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
gapi,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
gapi,Custom array allocator.
gapi,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
gapi,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
gapi,"This namespace contains G-API functions, structures, and symbols related to the Streaming execution mode."
gapi,"Some of the operations defined in this namespace (e.g. size(), BGR(), etc.) can be used in the traditional execution mode too."
gapi,"Template class for 3D points specified by its coordinates x, y and z."
gapi,"An instance of the class is interchangeable with the C structure CvPoint2D32f . Similarly to Point_ , the coordinates of 3D points can be converted to another type. The vector arithmetic and comparison operations are also supported."
gapi,The following Point3_<> aliases are available:
gapi,GMat class represents image or tensor data in the graph.
gapi,"GMat doesn't store any data itself, instead it describes a functional relationship between operations consuming and producing GMat objects."
gapi,"GMat is a virtual counterpart of Mat and UMat, but it doesn't mean G-API use Mat or UMat objects internally to represent GMat objects the internal data representation may be backend-specific or optimized out at all."
gapi,Functions for in-graph drawing.
gapi,"G-API can do some in-graph drawing with a generic operations and a set of rendering primitives. In contrast with traditional OpenCV, in G-API user need to form a rendering list of primitives to draw. This list can be built manually or generated within a graph. This list is passed to special operations or functions where all primitives are interpreted and applied to the image."
gapi,"For example, in a complex pipeline a list of detected objects can be translated in-graph to a list of cv::gapi::wip::draw::Rect primitives to highlight those with bounding boxes, or a list of detected faces can be translated in-graph to a list of cv::gapi::wip::draw::Mosaic primitives to hide sensitive content or protect privacy."
gapi,"Like any other operations, rendering in G-API can be reimplemented by different backends. Currently only an OpenCV-based backend is available."
gapi,"In addition to the graph-level operations, there are also regular (immediate) OpenCV-like functions are available see cv::gapi::wip::draw::render(). These functions are just wrappers over regular G-API and build the rendering graphs on the fly, so take compilation arguments as parameters."
gapi,"Currently this API is more machine-oriented than human-oriented. The main purpose is to translate a set of domain-specific objects to a list of primitives to draw. For example, in order to generate a picture like this:"
gapi,Rendering list needs to be generated as follows:
gapi,Template matrix class derived from Mat.
gapi,"The class Mat_<_Tp> is a thin template wrapper on top of the Mat class. It does not have any extra data fields. Nor this class nor Mat has any virtual methods. Thus, references or pointers to these two classes can be freely but carefully converted one to another. For example:"
gapi,"While Mat is sufficient in most cases, Mat_ can be more convenient if you use a lot of element access operations and if you know matrix type at the compilation time. Note that Mat::at(int y,int x) and Mat_::operator()(int y,int x) do absolutely the same and run at the same speed, but the latter is certainly shorter:"
gapi,"To use Mat_ for multi-channel images/matrices, pass Vec as a Mat_ parameter:"
gapi,Mat_ is fully compatible with C++11 range-based for loop. For example such loop can be used to safely apply look-up table:
gapi,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
gapi,"Namespace for all functions is cvv, i.e. cvv::showImage()."
gapi,Compilation:
gapi,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
gapi,See cvv tutorial for a commented example application using cvv.
gapi,Namespaces namespace cvv::impl 
highgui,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
highgui,"Template ""trait"" class for OpenCV primitive data types."
highgui,"A primitive OpenCV data type is one of unsigned char, bool, signed char, unsigned short, signed short, int, float, double, or a tuple of values of one of these types, where all the values in the tuple have the same type. Any primitive type from the list can be defined by an identifier in the form CV_<bit-depth>{U|S|F}C(<number_of_channels>), for example: uchar CV_8UC1, 3-element floating-point tuple CV_32FC3, and so on. A universal OpenCV structure that is able to store a single instance of such a primitive data type is Vec. Multiple instances of such a type can be stored in a std::vector, Mat, Mat_, SparseMat, SparseMat_, or any other container that is able to store Vec instances."
highgui,"The DataType class is basically used to provide a description of such primitive data types without adding any fields or methods to the corresponding classes (and it is actually impossible to add anything to primitive C/C++ data types). This technique is known in C++ as class traits. It is not DataType itself that is used but its specialized versions, such as:"
highgui,"The main purpose of this class is to convert compilation-time type information to an OpenCV-compatible data type identifier, for example:"
highgui,"So, such traits are used to tell OpenCV which data type you are working with, even if such a type is not native to OpenCV. For example, the matrix B initialization above is compiled because OpenCV defines the proper specialized template class DataType<complex<_Tp> > . This mechanism is also useful (and used in OpenCV this way) for generic algorithms implementations."
highgui,opencv2/core/traits.hpp
highgui,Class with reference counting wrapping special memory type allocation functions from CUDA.
highgui,Its interface is also Mat-like but with additional memory type parameters.
highgui,"PAGE_LOCKED sets a page locked memory type used commonly for fast and asynchronous uploading/downloading data from/to GPU. SHARED specifies a zero copy memory allocation that enables mapping the host memory to GPU address space, if supported. WRITE_COMBINED sets the write combined buffer that is not cached by CPU. Such buffers are used to supply GPU with data when GPU only reads it. The advantage is a better CPU cache utilization."
highgui,Base storage class for GPU memory with reference counting.
highgui,Its interface matches the Mat interface with the following limitations:
highgui,no arbitrary dimensions support (only 2D) no functions that return references to their data (because references on GPU are not valid for CPU) no expression templates technique support
highgui,Beware that the latter limitation may lead to overloaded matrix operators that cause memory allocations. The GpuMat class is convertible to cuda::PtrStepSz and cuda::PtrStep so it can be passed directly to the kernel.
highgui,"Some member functions are described as a ""Blocking Call"" while some are described as a ""Non-Blocking Call"". Blocking functions are synchronous to host. It is guaranteed that the GPU operation is finished when the function returns. However, non-blocking functions are asynchronous to host. Those functions may return even if the GPU operation is not finished."
highgui,"Compared to their blocking counterpart, non-blocking functions accept Stream as an additional argument. If a non-default stream is passed, the GPU operation may overlap with operations in other streams."
highgui,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
highgui,"Enumerations enum cv::MouseEventFlags { cv::EVENT_FLAG_LBUTTON = 1 , cv::EVENT_FLAG_RBUTTON = 2 , cv::EVENT_FLAG_MBUTTON = 4 , cv::EVENT_FLAG_CTRLKEY = 8 , cv::EVENT_FLAG_SHIFTKEY = 16 , cv::EVENT_FLAG_ALTKEY = 32 }  Mouse Event Flags see cv::MouseCallback. More...  enum cv::MouseEventTypes { cv::EVENT_MOUSEMOVE = 0 , cv::EVENT_LBUTTONDOWN = 1 , cv::EVENT_RBUTTONDOWN = 2 , cv::EVENT_MBUTTONDOWN = 3 , cv::EVENT_LBUTTONUP = 4 , cv::EVENT_RBUTTONUP = 5 , cv::EVENT_MBUTTONUP = 6 , cv::EVENT_LBUTTONDBLCLK = 7 , cv::EVENT_RBUTTONDBLCLK = 8 , cv::EVENT_MBUTTONDBLCLK = 9 , cv::EVENT_MOUSEWHEEL = 10 , cv::EVENT_MOUSEHWHEEL = 11 }  Mouse Events see cv::MouseCallback. More...  enum cv::WindowFlags { cv::WINDOW_NORMAL = 0x00000000 , cv::WINDOW_AUTOSIZE = 0x00000001 , cv::WINDOW_OPENGL = 0x00001000 , cv::WINDOW_FULLSCREEN = 1 , cv::WINDOW_FREERATIO = 0x00000100 , cv::WINDOW_KEEPRATIO = 0x00000000 , cv::WINDOW_GUI_EXPANDED =0x00000000 , cv::WINDOW_GUI_NORMAL = 0x00000010 }  Flags for cv::namedWindow. More...  enum cv::WindowPropertyFlags { cv::WND_PROP_FULLSCREEN = 0 , cv::WND_PROP_AUTOSIZE = 1 , cv::WND_PROP_ASPECT_RATIO = 2 , cv::WND_PROP_OPENGL = 3 , cv::WND_PROP_VISIBLE = 4 , cv::WND_PROP_TOPMOST = 5 , cv::WND_PROP_VSYNC = 6 }  Flags for cv::setWindowProperty / cv::getWindowProperty. More... "
highgui,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
highgui,This module includes photo processing algorithms
highgui,"Matrix expression representation This is a list of implemented matrix operations that can be combined in arbitrary complex expressions (here A, B stand for matrices ( Mat ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double )):"
highgui,"Addition, subtraction, negation: A+B, A-B, A+s, A-s, s+A, s-A, -A Scaling: A*alpha Per-element multiplication and division: A.mul(B), A/B, alpha/A Matrix multiplication: A*B Transposition: A.t() (means AT) Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems: A.inv([method]) (~ A<sup>-1</sup>), A.inv([method])*B (~ X: AX=B) Comparison: A cmpop B, A cmpop alpha, alpha cmpop A, where cmpop is one of >, >=, ==, !=, <=, <. The result of comparison is an 8-bit single channel mask whose elements are set to 255 (if the particular element or pair of elements satisfy the condition) or 0. Bitwise logical operations: A logicop B, A logicop s, s logicop A, ~A, where logicop is one of &, |, ^. Element-wise minimum and maximum: min(A, B), min(A, alpha), max(A, B), max(A, alpha) Element-wise absolute value: abs(A) Cross-product, dot-product: A.cross(B), A.dot(B) Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm, mean, sum, countNonZero, trace, determinant, repeat, and others. Matrix initializers ( Mat::eye(), Mat::zeros(), Mat::ones() ), matrix comma-separated initializers, matrix constructors and operators that extract sub-matrices (see Mat description). Mat_<destination_type>() constructors to cast the result to the proper type. NoteComma-separated initializers and probably some other operations may require additional explicit Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity. Here are examples of matrix expressions: // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD) SVD svd(A); Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t(); // compute the new vector of parameters in the Levenberg-Marquardt algorithm x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err); // sharpen image using ""unsharp mask"" algorithm Mat blurred; double sigma = 1, threshold = 5, amount = 1; GaussianBlur(img, blurred, Size(), sigma, sigma); Mat lowContrastMask = abs(img - blurred) < threshold; Mat sharpened = img*(1+amount) + blurred*(-amount); img.copyTo(sharpened, lowContrastMask); cv::MatExpr::absMatExpr abs(const Mat &m)Calculates an absolute value of each matrix element. cv::MatExpr::invMatExpr inv(int method=DECOMP_LU) const cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::Mat::diagMat diag(int d=0) constExtracts a diagonal from a matrix. cv::Mat::uUMatData * uinteraction with UMatDefinition mat.hpp:2174 cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Mat::tMatExpr t() constTransposes a matrix. cv::SVDSingular Value Decomposition.Definition core.hpp:2739 cv::DECOMP_CHOLESKY@ DECOMP_CHOLESKYDefinition base.hpp:143 cv::SizeSize2i SizeDefinition types.hpp:370 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. cv::thresholddouble threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)Applies a fixed-level threshold to each array element."
highgui,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
highgui,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
highgui,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
highgui,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
highgui,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
highgui,The implemented stitching pipeline is very similar to the one proposed in [41] .
highgui,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
highgui,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
highgui,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
highgui,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
highgui,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
highgui,Template class for a 4-element vector derived from Vec.
highgui,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
highgui,Matrix read-only iterator.
highgui,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
highgui,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
highgui,This module includes signal processing algorithms.
highgui,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
highgui,Data structure for salient point detectors.
highgui,"The class instance stores a keypoint, i.e. a point feature found by one of many available keypoint detectors, such as Harris corner detector, FAST, StarDetector, SURF, SIFT etc."
highgui,"The keypoint is characterized by the 2D position, scale (proportional to the diameter of the neighborhood that needs to be taken into account), orientation and some other parameters. The keypoint neighborhood is then analyzed by another algorithm that builds a descriptor (usually represented as a feature vector). The keypoints representing the same object in different images can then be matched using KDTree or another method."
highgui,ICP point-to-plane odometry algorithm
highgui,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
highgui,This section describes OpenGL interoperability.
highgui,"To enable OpenGL support, configure OpenCV using CMake with WITH_OPENGL=ON . Currently OpenGL is supported only with WIN32, GTK and Qt backends on Windows and Linux (MacOS and Android are not supported). For GTK-2.0 backend gtkglext-1.0 library is required."
highgui,"To use OpenGL functionality you should first create OpenGL context (window or frame buffer). You can do this with namedWindow function or with other OpenGL toolkit (GLUT, for example)."
highgui,Namespaces namespace cv::ogl::ocl 
highgui,Template class for 2D rectangles.
highgui,described by the following parameters:
highgui,"Coordinates of the top-left corner. This is a default interpretation of Rect_::x and Rect_::y in OpenCV. Though, in your algorithms you may count x and y from the bottom-left corner. Rectangle width and height."
highgui,"OpenCV typically assumes that the top and left boundary of the rectangle are inclusive, while the right and bottom boundaries are not. For example, the method Rect_::contains returns true if"
highgui,"\[x \leq pt.x < x+width, y \leq pt.y < y+height\]"
highgui,Virtually every loop over an image ROI in OpenCV (where ROI is specified by Rect_<int> ) is implemented as:
highgui,"In addition to the class members, the following operations on rectangles are implemented:"
highgui,"\(\texttt{rect} = \texttt{rect} \pm \texttt{point}\) (shifting a rectangle by a certain offset) \(\texttt{rect} = \texttt{rect} \pm \texttt{size}\) (expanding or shrinking a rectangle by a certain amount) rect += point, rect -= point, rect += size, rect -= size (augmenting operations) rect = rect1 & rect2 (rectangle intersection) rect = rect1 | rect2 (minimum area rectangle containing rect1 and rect2 ) rect &= rect1, rect |= rect1 (and the corresponding augmenting operations) rect == rect1, rect != rect1 (rectangle comparison)"
highgui,This is an example how the partial ordering on rectangles can be established (rect1 \(\subseteq\) rect2):
highgui,"For your convenience, the Rect_<> alias is available: cv::Rect"
highgui,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
highgui,YOUR ATTENTION PLEASE!
highgui,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
highgui,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
highgui,Note for developers: please don't put videoio dependency in G-API because of this file.
highgui,Dense optical flow algorithms compute motion for each point:
highgui,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
highgui,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
highgui,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
highgui,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
highgui,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
highgui,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
highgui,This type is very similar to InputArray except that it is used for input/output and output function parameters.
highgui,"Just like with InputArray, OpenCV users should not care about OutputArray, they just pass Mat, vector<T> etc. to the functions. The same limitation as for InputArray: Do not explicitly create OutputArray instances applies here too."
highgui,"If you want to make your function polymorphic (i.e. accept different arrays as output parameters), it is also not very difficult. Take the sample above as the reference. Note that _OutputArray::create() needs to be called before _OutputArray::getMat(). This way you guarantee that the output array is properly allocated."
highgui,"Optional output parameters. If you do not need certain output array to be computed and returned to you, pass cv::noArray(), just like you would in the case of optional input array. At the implementation level, use _OutputArray::needed() to check if certain output array needs to be computed or not."
highgui,There are several synonyms for OutputArray that are used to assist automatic Python/Java/... wrapper generators:
highgui,QtFont available only for Qt. See cv::fontQt.
highgui,Template Read-Write Sparse Matrix Iterator Class.
highgui,This is the derived from cv::SparseMatConstIterator_ class that introduces more convenient operator *() for accessing the current element.
highgui,This is a base class for all more or less complex algorithms in OpenCV.
highgui,"especially for classes of algorithms, for which there can be multiple implementations. The examples are stereo correspondence (for which there are algorithms like block matching, semi-global block matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck etc.)."
highgui,Here is example of SimpleBlobDetector use in your application via Algorithm interface:
highgui,n-dimensional dense array class
highgui,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
highgui,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
highgui,"In case of a 2-dimensional array, the above formula is reduced to:"
highgui,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
highgui,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
highgui,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
highgui,There are many different ways to create a Mat object. The most popular options are listed below:
highgui,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
highgui,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
highgui,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
highgui,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
highgui,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
highgui,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
highgui,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
highgui,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
highgui,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
highgui,Smart pointer for OpenGL buffer object with reference counting.
highgui,"Buffer Objects are OpenGL objects that store an array of unformatted memory allocated by the OpenGL context. These can be used to store vertex data, pixel data retrieved from images or the framebuffer, and a variety of other things."
highgui,ogl::Buffer has interface similar with Mat interface and represents 2D array memory.
highgui,ogl::Buffer supports memory transfers between host and device and also can be mapped to CUDA memory.
highgui,Namespaces namespace cv::omnidir::internal 
highgui,This is the proxy class for passing read-only input arrays into OpenCV functions.
highgui,It is defined as:
highgui,"where _InputArray is a class that can be constructed from Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat_<T> >, UMat, std::vector<UMat> or double. It can also be constructed from a matrix expression."
highgui,"Since this is mostly implementation-level class, and its interface may change in future versions, we do not describe it in details. There are a few key things, though, that should be kept in mind:"
highgui,"When you see in the reference manual or in OpenCV source code a function that takes InputArray, it means that you can actually pass Mat, Matx, vector<T> etc. (see above the complete list). Optional input arguments: If some of the input arrays may be empty, pass cv::noArray() (or simply cv::Mat() as you probably did before). The class is designed solely for passing parameters. That is, normally you should not declare class members, local and global variables of this type. If you want to design your own function or a class method that can operate of arrays of multiple types, you can use InputArray (or OutputArray) for the respective parameters. Inside a function you should use _InputArray::getMat() method to construct a matrix header for the array (without copying data). _InputArray::kind() can be used to distinguish Mat from vector<> etc., but normally it is not needed."
highgui,Here is how you can use a function that takes InputArray :
highgui,"That is, we form an STL vector containing points, and apply in-place affine transformation to the vector using the 2x3 matrix created inline as Matx<float, 2, 3> instance."
highgui,"Here is how such a function can be implemented (for simplicity, we implement a very specific case of it, according to the assertion statement inside) :"
highgui,"There is another related type, InputArrayOfArrays, which is currently defined as a synonym for InputArray:"
highgui,"It denotes function arguments that are either vectors of vectors or vectors of matrices. A separate synonym is needed to generate Python/Java etc. wrappers properly. At the function implementation level their use is similar, but _InputArray::getMat(idx) should be used to get header for the idx-th component of the outer vector and _InputArray::size().area() should be used to find the number of components (vectors/matrices) of the outer vector."
highgui,"In general, type support is limited to cv::Mat types. Other types are forbidden. But in some cases we need to support passing of custom non-general Mat types, like arrays of cv::KeyPoint, cv::DMatch, etc. This data is not intended to be interpreted as an image data, or processed somehow like regular cv::Mat. To pass such custom type use rawIn() / rawOut() / rawInOut() wrappers. Custom type is wrapped as Mat-compatible CV_8UC<N> values (N = sizeof(T), N <= CV_CN_MAX)."
highgui,Information Flow algorithm implementaton for alphamatting
highgui,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
highgui,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
highgui,The implementation is based on [7].
highgui,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
highgui,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
highgui,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
highgui,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
highgui,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
highgui,Namespace for all functions is cv::img_hash.
highgui,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
highgui,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
highgui,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
highgui,Class for matching keypoint descriptors.
highgui,"query descriptor index, train descriptor index, train image index, and distance between descriptors."
highgui,This modules is to draw UTF-8 strings with freetype/harfbuzz.
highgui,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
highgui,Classes class cv::freetype::FreeType2 
highgui,Matrix read-write iterator.
highgui,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
highgui,Read-write Sparse Matrix Iterator.
highgui,"The class is similar to cv::SparseMatConstIterator, but can be used for in-place modification of the matrix elements."
highgui,Namespaces namespace NcvCTprep 
highgui,n-ary multi-dimensional array iterator.
highgui,"Use the class to implement unary, binary, and, generally, n-ary element-wise operations on multi-dimensional arrays. Some of the arguments of an n-ary function may be continuous arrays, some may be not. It is possible to use conventional MatIterator 's for each array but incrementing all of the iterators after each small operations may be a big overhead. In this case consider using NAryMatIterator to iterate through several matrices simultaneously as long as they have the same geometry (dimensionality and all the dimension sizes are the same). On each iteration it.planes[0], it.planes[1],... will be the slices of the corresponding matrices."
highgui,The example below illustrates how you can compute a normalized and threshold 3D color histogram:
highgui,Smart pointer for OpenGL 2D texture memory with reference counting.
highgui,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
highgui,This module contains:
highgui,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
highgui,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
highgui,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
highgui,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
highgui,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
highgui,"This figure explains new functionality implemented with WinRT GUI. The new GUI provides an Image control, and a slider panel. Slider panel holds trackbars attached to it."
highgui,Sliders are attached below the image control. Every new slider is added below the previous one.
highgui,See below the example used to generate the figure:
highgui,Functions void cv::winrt_initContainer (::Windows::UI::Xaml::Controls::Panel^ container)  Initializes container component that will be used to hold generated window content. 
highgui,Template class for specifying the size of an image or rectangle.
highgui,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
highgui,OpenCV defines the following Size_<> aliases:
highgui,Comma-separated Matrix Initializer.
highgui,"The class instances are usually not created explicitly. Instead, they are created on ""matrix << firstValue"" operator."
highgui,The sample below initializes 2x2 rotation matrix:
highgui,"Template class for short numerical vectors, a partial case of Matx."
highgui,"This template class represents short numerical vectors (of 1, 2, 3, 4 ... elements) on which you can perform basic arithmetical operations, access individual elements using [] operator etc. The vectors are allocated on stack, as opposite to std::valarray, std::vector, cv::Mat etc., which elements are dynamically allocated in the heap."
highgui,The template takes 2 parameters:
highgui,_Tp element type cn the number of elements
highgui,"In addition to the universal notation like Vec<float, 3>, you can use shorter aliases for the most popular specialized variants of Vec, e.g. Vec3f ~ Vec<float, 3>."
highgui,"It is possible to convert Vec<T,2> to/from Point_, Vec<T,3> to/from Point3_ , and Vec<T,4> to CvScalar or Scalar_. Use operator[] to access the elements of Vec."
highgui,All the expected vector operations are also implemented:
highgui,"v1 = v2 + v3 v1 = v2 - v3 v1 = v2 * scale v1 = scale * v2 v1 = -v2 v1 += v2 and other augmenting operations v1 == v2, v1 != v2 norm(v1) (euclidean norm) The Vec class is commonly used to describe pixel types of multi-channel arrays. See Mat for details."
highgui,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
highgui,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
highgui,A helper class for cv::DataType.
highgui,The class is specialized for each fundamental numerical data type supported by OpenCV. It provides DataDepth<T>::value constant.
highgui,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
highgui,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
highgui,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
highgui,"Functions void cv::julia::initJulia (int argc, char **argv) "
highgui,Template sparse n-dimensional array class derived from SparseMat.
highgui,SparseMat_ is a thin wrapper on top of SparseMat created in the same way as Mat_ . It simplifies notation of some operations:
highgui,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
highgui,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
highgui,It provides easy interface to:
highgui,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
highgui,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
highgui,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
highgui,It is planned to have:
highgui,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
highgui,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
highgui,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
highgui,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
highgui,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
highgui,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
highgui,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
highgui,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
highgui,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
highgui,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
highgui,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
highgui,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
highgui,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
highgui,This module has been originally developed as a project for Google Summer of Code 2012-2015.
highgui,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
highgui,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
highgui,The distortion-free projective transformation given by a pinhole camera model is shown below.
highgui,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
highgui,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
highgui,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
highgui,\[p = A P_c.\]
highgui,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
highgui,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
highgui,and thus
highgui,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
highgui,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
highgui,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
highgui,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
highgui,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
highgui,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
highgui,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
highgui,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
highgui,and therefore
highgui,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
highgui,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
highgui,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
highgui,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
highgui,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
highgui,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
highgui,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
highgui,with
highgui,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
highgui,The following figure illustrates the pinhole camera model.
highgui,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
highgui,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
highgui,where
highgui,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
highgui,with
highgui,\[r^2 = x'^2 + y'^2\]
highgui,and
highgui,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
highgui,if \(Z_c \ne 0\).
highgui,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
highgui,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
highgui,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
highgui,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
highgui,where
highgui,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
highgui,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
highgui,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
highgui,In the functions below the coefficients are passed or returned as
highgui,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
highgui,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
highgui,The functions below use the above model to do the following:
highgui,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
highgui,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
highgui,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
highgui,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
highgui,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
highgui,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
highgui,if \(W \ne 0\).
highgui,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
highgui,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
highgui,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
highgui,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
highgui,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
highgui,Template Read-Only Sparse Matrix Iterator Class.
highgui,This is the derived from SparseMatConstIterator class that introduces more convenient operator *() for accessing the current element.
highgui,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
highgui,Classes class cv::cuda::BufferPool  BufferPool for use with CUDA streams. More...  class cv::cuda::Event  struct cv::cuda::EventAccessor  Class that enables getting cudaEvent_t from cuda::Event. More...  struct cv::cuda::GpuData  class cv::cuda::GpuMat  Base storage class for GPU memory with reference counting. More...  class cv::cuda::GpuMatND  class cv::cuda::HostMem  Class with reference counting wrapping special memory type allocation functions from CUDA. More...  class cv::cuda::Stream  This class encapsulates a queue of asynchronous calls. More...  struct cv::cuda::StreamAccessor  Class that enables getting cudaStream_t from cuda::Stream. More... 
highgui,The class defining termination criteria for iterative algorithms.
highgui,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
highgui,"ArUco Marker Detection, module functionality was moved to objdetect module"
highgui,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
highgui,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
highgui,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
highgui,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
highgui,This class encapsulates a queue of asynchronous calls.
highgui,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
highgui,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
highgui,Template class specifying a continuous subsequence (slice) of a sequence.
highgui,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
highgui,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
highgui,A complex number class.
highgui,"The template class is similar and compatible with std::complex, however it provides slightly more convenient access to the real and imaginary parts using through the simple field access, as opposite to std::complex::real() and std::complex::imag()."
highgui,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
highgui,Template class for 2D points specified by its coordinates x and y.
highgui,"An instance of the class is interchangeable with C structures, CvPoint and CvPoint2D32f . There is also a cast operator to convert point coordinates to the specified type. The conversion from floating-point coordinates to integer coordinates is done by rounding. Commonly, the conversion uses this operation for each of the coordinates. Besides the class members listed in the declaration above, the following operations on points are implemented:"
highgui,"For your convenience, the following type aliases are defined:"
highgui,Example:
highgui,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
highgui,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
highgui,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
highgui,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
highgui,"Each class derived from Map implements a motion model, as follows:"
highgui,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
highgui,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
highgui,The classes derived from Mapper are
highgui,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
highgui,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
highgui,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
highgui,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
highgui,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
highgui,This module provides storage routines for Hierarchical Data Format objects.
highgui,Face module changelog Face Recognition with OpenCV
highgui,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
highgui,The class represents rotated (i.e. not up-right) rectangles on a plane.
highgui,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
highgui,The sample below demonstrates how to use RotatedRect:
highgui,Classes class cv::plot::Plot2d 
highgui,Classes class cv::quality::QualityBase 
highgui,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
highgui,"This figure explains new functionality implemented with Qt* GUI. The new GUI provides a statusbar, a toolbar, and a control panel. The control panel can have trackbars and buttonbars attached to it. If you cannot see the control panel, press Ctrl+P or right-click any Qt window and select Display properties window."
highgui,"To attach a trackbar, the window name parameter must be NULL. To attach a buttonbar, a button must be created. If the last bar attached to the control panel is a buttonbar, the new button is added to the right of the last button. If the last bar attached to the control panel is a trackbar, or the control panel is empty, a new buttonbar is created. Then, a new button is attached to it."
highgui,See below the example used to generate the figure:
highgui,Classes struct cv::QtFont  QtFont available only for Qt. See cv::fontQt. More... 
highgui,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
highgui,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
highgui,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
highgui,Namespaces namespace cv::traits 
highgui,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
highgui,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
highgui,Namespace for all functions is cv::intensity_transform.
highgui,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
highgui,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
highgui,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
highgui,The class SparseMat represents multi-dimensional sparse numerical arrays.
highgui,"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:"
highgui,"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5; int size[5] = {10, 10, 10, 10, 10}; SparseMat sparse_mat(dims, size, CV_32F); for(int i = 0; i < 1000; i++) { int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f; } cout << ""nnz = "" << sparse_mat.nzcount() << endl; cv::SparseMatThe class SparseMat represents multi-dimensional sparse numerical arrays.Definition mat.hpp:2751 cv::SparseMat::sizeconst int * size() constreturns the array of sizes, or NULL if the matrix is not allocated cv::SparseMat::dimsint dims() constreturns the matrix dimensionality CV_32F#define CV_32FDefinition interface.h:78 Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix // and the sum of elements. SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>(); double s = 0; int dims = sparse_mat.dims(); for(; it != it_end; ++it) { // print element indices and the element value const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("": %g\n"", it.value<float>()); s += *it; } printf(""Element sum is %g\n"", s); cv::SparseMatConstIterator_Template Read-Only Sparse Matrix Iterator Class.Definition mat.hpp:3354 cv::SparseMatConstIterator::valueconst _Tp & value() consttemplate method returning the current matrix element cv::SparseMatConstIterator::nodeconst SparseMat::Node * node() constreturns the current node of the sparse matrix. it.node->idx is the current element index cv::SparseMat::Nodesparse matrix node - element of a hash tableDefinition mat.hpp:2776 cv::SparseMat::Node::idxint idx[MAX_DIM]index of the matrix elementDefinition mat.hpp:2782 If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation. Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b) { const SparseMat *_a = &a, *_b = &b; // if b contains less elements than a, // it is faster to iterate through b if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // take the next element from the first matrix float avalue = *it; const Node* anode = it.node(); // and try to find an element with the same index in the second matrix. // since the hash value depends only on the element index, // reuse the hash value stored in the node float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr; } cv::SparseMat::endSparseMatIterator end()returns the sparse matrix iterator at the matrix end cv::SparseMat::nzcountsize_t nzcount() constreturns the number of non-zero elements (=the number of hash table nodes) cv::SparseMat::beginSparseMatIterator begin()returns the sparse matrix iterator at the matrix beginning cv::SparseMat::Node::hashvalsize_t hashvalhash valueDefinition mat.hpp:2778"
highgui,Read and write video or images sequence with OpenCV.
highgui,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
highgui,Bioinspired Module Retina Introduction
highgui,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
highgui,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
highgui,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
highgui,See detailed overview here: Machine Learning Overview.
highgui,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
highgui,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
highgui,Template class for small matrices whose type and size are known at compilation time.
highgui,"If you need a more flexible type, use Mat . The elements of the matrix M are accessible using the M(i,j) notation. Most of the common matrix operations (see also MatrixExpressions ) are available. To do an operation on Matx that is not implemented, you can easily convert the matrix to Mat and backwards:"
highgui,"Except of the plain constructor which takes a list of elements, Matx can be initialized from a C-array:"
highgui,"In case if C++11 features are available, std::initializer_list can be also used to initialize Matx:"
highgui,"Functions void cv::imshow (const String &winname, const ogl::Texture2D &tex)  Displays OpenGL 2D texture in the specified window.  void cv::setOpenGlContext (const String &winname)  Sets the specified window as current OpenGL context.  void cv::setOpenGlDrawCallback (const String &winname, OpenGlDrawCallback onOpenGlDraw, void *userdata=0)  Sets a callback function to be called to draw on top of displayed image.  void cv::updateWindow (const String &winname)  Force window to redraw its context and call draw callback ( See cv::setOpenGlDrawCallback ). "
highgui,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
highgui,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
highgui,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
highgui,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
highgui,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
highgui,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
highgui,Custom array allocator.
highgui,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
highgui,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
highgui,"Template class for 3D points specified by its coordinates x, y and z."
highgui,"An instance of the class is interchangeable with the C structure CvPoint2D32f . Similarly to Point_ , the coordinates of 3D points can be converted to another type. The vector arithmetic and comparison operations are also supported."
highgui,The following Point3_<> aliases are available:
highgui,Read-Only Sparse Matrix Iterator.
highgui,Here is how to use the iterator to compute the sum of floating-point sparse matrix elements:
highgui,Template matrix class derived from Mat.
highgui,"The class Mat_<_Tp> is a thin template wrapper on top of the Mat class. It does not have any extra data fields. Nor this class nor Mat has any virtual methods. Thus, references or pointers to these two classes can be freely but carefully converted one to another. For example:"
highgui,"While Mat is sufficient in most cases, Mat_ can be more convenient if you use a lot of element access operations and if you know matrix type at the compilation time. Note that Mat::at(int y,int x) and Mat_::operator()(int y,int x) do absolutely the same and run at the same speed, but the latter is certainly shorter:"
highgui,"To use Mat_ for multi-channel images/matrices, pass Vec as a Mat_ parameter:"
highgui,Mat_ is fully compatible with C++11 range-based for loop. For example such loop can be used to safely apply look-up table:
highgui,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
highgui,"Namespace for all functions is cvv, i.e. cvv::showImage()."
highgui,Compilation:
highgui,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
highgui,See cvv tutorial for a commented example application using cvv.
highgui,Namespaces namespace cvv::impl 
imgcodecs,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
imgcodecs,Base abstract class for the long-term tracker.
imgcodecs,Abstract base class for 2D image feature detectors and descriptor extractors.
imgcodecs,An EMD based cost extraction. :
imgcodecs,"This class is used to perform the non-linear non-constrained minimization of a function with known gradient,."
imgcodecs,"defined on an n-dimensional Euclidean space, using the Nonlinear Conjugate Gradient method. The implementation was done based on the beautifully clear explanatory article [An Introduction to the Conjugate Gradient Method Without the Agonizing Pain](http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf) by Jonathan Richard Shewchuk. The method can be seen as an adaptation of a standard Conjugate Gradient method (see, for example http://en.wikipedia.org/wiki/Conjugate_gradient_method) for numerically solving the systems of linear equations."
imgcodecs,"It should be noted, that this method, although deterministic, is rather a heuristic method and therefore may converge to a local minima, not necessary a global one. What is even more disastrous, most of its behaviour is ruled by gradient, therefore it essentially cannot distinguish between local minima and maxima. Therefore, if it starts sufficiently near to the local maximum, it may converge to it. Another obvious restriction is that it should be possible to compute the gradient of a function at any point, thus it is preferable to have analytic expression for gradient and computational burden should be born by the user."
imgcodecs,The latter responsibility is accomplished via the getGradient method of a MinProblemSolver::Function interface (which represents function being optimized). This method takes point a point in n-dimensional space (first argument represents the array of coordinates of that point) and compute its gradient (it should be stored in the second argument as an array).
imgcodecs,The MIL algorithm trains a classifier in an online manner to separate the object from the background.
imgcodecs,Multiple Instance Learning avoids the drift problem for a robust tracking. The implementation is based on [14] .
imgcodecs,Original code can be found here http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml
imgcodecs,Designed for command line parsing.
imgcodecs,The sample below demonstrates how to use CommandLineParser:
imgcodecs,"Template ""trait"" class for OpenCV primitive data types."
imgcodecs,"A primitive OpenCV data type is one of unsigned char, bool, signed char, unsigned short, signed short, int, float, double, or a tuple of values of one of these types, where all the values in the tuple have the same type. Any primitive type from the list can be defined by an identifier in the form CV_<bit-depth>{U|S|F}C(<number_of_channels>), for example: uchar CV_8UC1, 3-element floating-point tuple CV_32FC3, and so on. A universal OpenCV structure that is able to store a single instance of such a primitive data type is Vec. Multiple instances of such a type can be stored in a std::vector, Mat, Mat_, SparseMat, SparseMat_, or any other container that is able to store Vec instances."
imgcodecs,"The DataType class is basically used to provide a description of such primitive data types without adding any fields or methods to the corresponding classes (and it is actually impossible to add anything to primitive C/C++ data types). This technique is known in C++ as class traits. It is not DataType itself that is used but its specialized versions, such as:"
imgcodecs,"The main purpose of this class is to convert compilation-time type information to an OpenCV-compatible data type identifier, for example:"
imgcodecs,"So, such traits are used to tell OpenCV which data type you are working with, even if such a type is not native to OpenCV. For example, the matrix B initialization above is compiled because OpenCV defines the proper specialized template class DataType<complex<_Tp> > . This mechanism is also useful (and used in OpenCV this way) for generic algorithms implementations."
imgcodecs,opencv2/core/traits.hpp
imgcodecs,Plane warper factory class.
imgcodecs,src source array dst destination array len length of arrays
imgcodecs,"Functions int hal_ni_invSqrt32f (const float *src, float *dst, int len)  int hal_ni_invSqrt64f (const double *src, double *dst, int len) "
imgcodecs,Base storage class for GPU memory with reference counting.
imgcodecs,Its interface matches the Mat interface with the following limitations:
imgcodecs,no arbitrary dimensions support (only 2D) no functions that return references to their data (because references on GPU are not valid for CPU) no expression templates technique support
imgcodecs,Beware that the latter limitation may lead to overloaded matrix operators that cause memory allocations. The GpuMat class is convertible to cuda::PtrStepSz and cuda::PtrStep so it can be passed directly to the kernel.
imgcodecs,"Some member functions are described as a ""Blocking Call"" while some are described as a ""Non-Blocking Call"". Blocking functions are synchronous to host. It is guaranteed that the GPU operation is finished when the function returns. However, non-blocking functions are asynchronous to host. Those functions may return even if the GPU operation is not finished."
imgcodecs,"Compared to their blocking counterpart, non-blocking functions accept Stream as an additional argument. If a non-default stream is passed, the GPU operation may overlap with operations in other streams."
imgcodecs,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
imgcodecs,Provides result of asynchronous operations.
imgcodecs,Class for extracting blobs from an image. :
imgcodecs,The class implements a simple algorithm for extracting blobs from an image:
imgcodecs,This class performs several filtrations of returned blobs. You should set filterBy* to true/false to turn on/off corresponding filtration. Available filtrations:
imgcodecs,"By color. This filter compares the intensity of a binary image at the center of a blob to blobColor. If they differ, the blob is filtered out. Use blobColor = 0 to extract dark blobs and blobColor = 255 to extract light blobs. By area. Extracted blobs have an area between minArea (inclusive) and maxArea (exclusive). By circularity. Extracted blobs have circularity ( \(\frac{4*\pi*Area}{perimeter * perimeter}\)) between minCircularity (inclusive) and maxCircularity (exclusive). By ratio of the minimum inertia to maximum inertia. Extracted blobs have this ratio between minInertiaRatio (inclusive) and maxInertiaRatio (exclusive). By convexity. Extracted blobs have convexity (area / area of blob convex hull) between minConvexity (inclusive) and maxConvexity (exclusive)."
imgcodecs,Default values of parameters are tuned to extract dark circular blobs.
imgcodecs,"Enumerations enum cv::MouseEventFlags { cv::EVENT_FLAG_LBUTTON = 1 , cv::EVENT_FLAG_RBUTTON = 2 , cv::EVENT_FLAG_MBUTTON = 4 , cv::EVENT_FLAG_CTRLKEY = 8 , cv::EVENT_FLAG_SHIFTKEY = 16 , cv::EVENT_FLAG_ALTKEY = 32 }  Mouse Event Flags see cv::MouseCallback. More...  enum cv::MouseEventTypes { cv::EVENT_MOUSEMOVE = 0 , cv::EVENT_LBUTTONDOWN = 1 , cv::EVENT_RBUTTONDOWN = 2 , cv::EVENT_MBUTTONDOWN = 3 , cv::EVENT_LBUTTONUP = 4 , cv::EVENT_RBUTTONUP = 5 , cv::EVENT_MBUTTONUP = 6 , cv::EVENT_LBUTTONDBLCLK = 7 , cv::EVENT_RBUTTONDBLCLK = 8 , cv::EVENT_MBUTTONDBLCLK = 9 , cv::EVENT_MOUSEWHEEL = 10 , cv::EVENT_MOUSEHWHEEL = 11 }  Mouse Events see cv::MouseCallback. More...  enum cv::WindowFlags { cv::WINDOW_NORMAL = 0x00000000 , cv::WINDOW_AUTOSIZE = 0x00000001 , cv::WINDOW_OPENGL = 0x00001000 , cv::WINDOW_FULLSCREEN = 1 , cv::WINDOW_FREERATIO = 0x00000100 , cv::WINDOW_KEEPRATIO = 0x00000000 , cv::WINDOW_GUI_EXPANDED =0x00000000 , cv::WINDOW_GUI_NORMAL = 0x00000010 }  Flags for cv::namedWindow. More...  enum cv::WindowPropertyFlags { cv::WND_PROP_FULLSCREEN = 0 , cv::WND_PROP_AUTOSIZE = 1 , cv::WND_PROP_ASPECT_RATIO = 2 , cv::WND_PROP_OPENGL = 3 , cv::WND_PROP_VISIBLE = 4 , cv::WND_PROP_TOPMOST = 5 , cv::WND_PROP_VSYNC = 6 }  Flags for cv::setWindowProperty / cv::getWindowProperty. More... "
imgcodecs,These functions are provided for OpenCV-Eigen interoperability. They convert Mat objects to corresponding Eigen::Matrix objects and vice-versa. Consult the Eigen documentation for information about the Matrix template type.
imgcodecs,Namespaces namespace cv::traits 
imgcodecs,A simple Hausdorff distance measure between shapes defined by contours.
imgcodecs,"according to the paper ""Comparing Images using the Hausdorff distance."" by D.P. Huttenlocher, G.A. Klanderman, and W.J. Rucklidge. (PAMI 1993). :"
imgcodecs,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
imgcodecs,This is a global tonemapping operator that models human visual system.
imgcodecs,"Mapping function is controlled by adaptation parameter, that is computed using light adaptation and color adaptation."
imgcodecs,For more information see [224] .
imgcodecs,This module includes photo processing algorithms
imgcodecs,"Matrix expression representation This is a list of implemented matrix operations that can be combined in arbitrary complex expressions (here A, B stand for matrices ( Mat ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double )):"
imgcodecs,"Addition, subtraction, negation: A+B, A-B, A+s, A-s, s+A, s-A, -A Scaling: A*alpha Per-element multiplication and division: A.mul(B), A/B, alpha/A Matrix multiplication: A*B Transposition: A.t() (means AT) Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems: A.inv([method]) (~ A<sup>-1</sup>), A.inv([method])*B (~ X: AX=B) Comparison: A cmpop B, A cmpop alpha, alpha cmpop A, where cmpop is one of >, >=, ==, !=, <=, <. The result of comparison is an 8-bit single channel mask whose elements are set to 255 (if the particular element or pair of elements satisfy the condition) or 0. Bitwise logical operations: A logicop B, A logicop s, s logicop A, ~A, where logicop is one of &, |, ^. Element-wise minimum and maximum: min(A, B), min(A, alpha), max(A, B), max(A, alpha) Element-wise absolute value: abs(A) Cross-product, dot-product: A.cross(B), A.dot(B) Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm, mean, sum, countNonZero, trace, determinant, repeat, and others. Matrix initializers ( Mat::eye(), Mat::zeros(), Mat::ones() ), matrix comma-separated initializers, matrix constructors and operators that extract sub-matrices (see Mat description). Mat_<destination_type>() constructors to cast the result to the proper type. NoteComma-separated initializers and probably some other operations may require additional explicit Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity. Here are examples of matrix expressions: // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD) SVD svd(A); Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t(); // compute the new vector of parameters in the Levenberg-Marquardt algorithm x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err); // sharpen image using ""unsharp mask"" algorithm Mat blurred; double sigma = 1, threshold = 5, amount = 1; GaussianBlur(img, blurred, Size(), sigma, sigma); Mat lowContrastMask = abs(img - blurred) < threshold; Mat sharpened = img*(1+amount) + blurred*(-amount); img.copyTo(sharpened, lowContrastMask); cv::MatExpr::absMatExpr abs(const Mat &m)Calculates an absolute value of each matrix element. cv::MatExpr::invMatExpr inv(int method=DECOMP_LU) const cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::Mat::diagMat diag(int d=0) constExtracts a diagonal from a matrix. cv::Mat::uUMatData * uinteraction with UMatDefinition mat.hpp:2174 cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Mat::tMatExpr t() constTransposes a matrix. cv::SVDSingular Value Decomposition.Definition core.hpp:2739 cv::DECOMP_CHOLESKY@ DECOMP_CHOLESKYDefinition base.hpp:143 cv::SizeSize2i SizeDefinition types.hpp:370 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. cv::thresholddouble threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)Applies a fixed-level threshold to each array element."
imgcodecs,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
imgcodecs,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
imgcodecs,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
imgcodecs,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
imgcodecs,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
imgcodecs,The implemented stitching pipeline is very similar to the one proposed in [41] .
imgcodecs,"This class is used for grouping object candidates detected by Cascade Classifier, HOG etc."
imgcodecs,instance of the class is to be passed to cv::partition
imgcodecs,"Functions void cv::denoise_TVL1 (const std::vector< Mat > &observations, Mat &result, double lambda=1.0, int niters=30)  Primal-dual algorithm is an algorithm for solving special types of variational problems (that is, finding a function to minimize some functional). As the image denoising, in particular, may be seen as the variational problem, primal-dual algorithm then can be used to perform denoising and this is exactly what is implemented.  void cv::cuda::fastNlMeansDenoising (const GpuMat &src, GpuMat &dst, float h, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  void cv::fastNlMeansDenoising (InputArray src, OutputArray dst, const std::vector< float > &h, int templateWindowSize=7, int searchWindowSize=21, int normType=NORM_L2)  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/ with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::cuda::fastNlMeansDenoising (InputArray src, OutputArray dst, float h, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::fastNlMeansDenoising (InputArray src, OutputArray dst, float h=3, int templateWindowSize=7, int searchWindowSize=21)  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/ with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::cuda::fastNlMeansDenoisingColored (const GpuMat &src, GpuMat &dst, float h_luminance, float photo_render, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  void cv::fastNlMeansDenoisingColored (InputArray src, OutputArray dst, float h=3, float hColor=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoising function for colored images.  void cv::cuda::fastNlMeansDenoisingColored (InputArray src, OutputArray dst, float h_luminance, float photo_render, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  Modification of fastNlMeansDenoising function for colored images.  void cv::fastNlMeansDenoisingColoredMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, float h=3, float hColor=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoisingMulti function for colored images sequences.  void cv::fastNlMeansDenoisingMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, const std::vector< float > &h, int templateWindowSize=7, int searchWindowSize=21, int normType=NORM_L2)  Modification of fastNlMeansDenoising function for images sequence where consecutive images have been captured in small period of time. For example video. This version of the function is for grayscale images or for manual manipulation with colorspaces. See [44] for more details (open access here).  void cv::fastNlMeansDenoisingMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, float h=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoising function for images sequence where consecutive images have been captured in small period of time. For example video. This version of the function is for grayscale images or for manual manipulation with colorspaces. See [44] for more details (open access here).  void cv::cuda::nonLocalMeans (const GpuMat &src, GpuMat &dst, float h, int search_window=21, int block_size=7, int borderMode=BORDER_DEFAULT, Stream &stream=Stream::Null())  void cv::cuda::nonLocalMeans (InputArray src, OutputArray dst, float h, int search_window=21, int block_size=7, int borderMode=BORDER_DEFAULT, Stream &stream=Stream::Null())  Performs pure non local means denoising without any simplification, and thus it is not fast. "
imgcodecs,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
imgcodecs,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
imgcodecs,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
imgcodecs,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
imgcodecs,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
imgcodecs,Represents a computation (graph) compiled for streaming.
imgcodecs,"This class represents a product of graph compilation (calling cv::GComputation::compileStreaming()). Objects of this class actually do stream processing, and the whole pipeline execution complexity is incapsulated into objects of this class. Execution model has two levels: at the very top, the execution of a heterogeneous graph is aggressively pipelined; at the very bottom the execution of every internal block is determined by its associated backend. Backends are selected based on kernel packages passed via compilation arguments ( see G-API Graph Compilation Arguments, GNetworkPackage, GKernelPackage for details)."
imgcodecs,"GStreamingCompiled objects have a ""player"" semantics there are methods like start() and stop(). GStreamingCompiled has a full control over a videostream and so is stateful. You need to specify the input stream data using setSource() and then call start() to actually start processing. After that, use pull() or try_pull() to obtain next processed data frame from the graph in a blocking or non-blocking way, respectively."
imgcodecs,Currently a single GStreamingCompiled can process only one video streat at time. Produce multiple GStreamingCompiled objects to run the same graph on multiple video streams.
imgcodecs,Template class for a 4-element vector derived from Vec.
imgcodecs,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
imgcodecs,Random Number Generator.
imgcodecs,"Random number generator. It encapsulates the state (currently, a 64-bit integer) and has methods to return scalar random values and to fill arrays with random values. Currently it supports uniform and Gaussian (normal) distributions. The generator uses Multiply-With-Carry algorithm, introduced by G. Marsaglia ( http://en.wikipedia.org/wiki/Multiply-with-carry ). Gaussian-distribution random numbers are generated using the Ziggurat algorithm ( http://en.wikipedia.org/wiki/Ziggurat_algorithm ), introduced by G. Marsaglia and W. W. Tsang."
imgcodecs,Matrix read-only iterator.
imgcodecs,Cascade classifier class for object detection.
imgcodecs,Class passed to an error.
imgcodecs,This class encapsulates all or almost all necessary information about the error happened in the program. The exception is usually constructed and thrown implicitly via CV_Error and CV_Error_ macros.
imgcodecs,The methods in this namespace use a so-called fisheye camera model.
imgcodecs,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
imgcodecs,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
imgcodecs,Bitwise AND: dst[i] = src1[i] & src2[i] Bitwise OR: dst[i] = src1[i] | src2[i] Bitwise XOR: dst[i] = src1[i] ^ src2[i] Bitwise NOT: dst[i] = !src[i]
imgcodecs,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
imgcodecs,"Functions int hal_ni_and8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_not8u (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_or8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_xor8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height) "
imgcodecs,This module includes signal processing algorithms.
imgcodecs,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
imgcodecs,Data structure for salient point detectors.
imgcodecs,"The class instance stores a keypoint, i.e. a point feature found by one of many available keypoint detectors, such as Harris corner detector, FAST, StarDetector, SURF, SIFT etc."
imgcodecs,"The keypoint is characterized by the 2D position, scale (proportional to the diameter of the neighborhood that needs to be taken into account), orientation and some other parameters. The keypoint neighborhood is then analyzed by another algorithm that builds a descriptor (usually represented as a feature vector). The keypoints representing the same object in different images can then be matched using KDTree or another method."
imgcodecs,"Represents a compiled computation (graph). Can only be used with image / data formats & resolutions it was compiled for, with some exceptions."
imgcodecs,"This class represents a product of graph compilation (calling cv::GComputation::compile()). Objects of this class actually do data processing, and graph execution is incapsulated into objects of this class. Execution model itself depends on kernels and backends which were using during the compilation, see G-API Graph Compilation Arguments for details."
imgcodecs,"In a general case, GCompiled objects can be applied to data only in that formats/resolutions they were compiled for (see G-API Metadata Descriptors). However, if the underlying backends allow, a compiled object can be reshaped to handle data (images) of different resolution, though formats and types must remain the same."
imgcodecs,GCompiled is very similar to std::function<> in its semantics running it looks like a function call in the user code.
imgcodecs,"At the moment, GCompiled objects are not reentrant generally, the objects are stateful since graph execution itself is a stateful process and this state is now maintained in GCompiled's own memory (not on the process stack)."
imgcodecs,"At the same time, two different GCompiled objects produced from the single cv::GComputation are completely independent and can be used concurrently."
imgcodecs,ICP point-to-plane odometry algorithm
imgcodecs,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
imgcodecs,Template class for 2D rectangles.
imgcodecs,described by the following parameters:
imgcodecs,"Coordinates of the top-left corner. This is a default interpretation of Rect_::x and Rect_::y in OpenCV. Though, in your algorithms you may count x and y from the bottom-left corner. Rectangle width and height."
imgcodecs,"OpenCV typically assumes that the top and left boundary of the rectangle are inclusive, while the right and bottom boundaries are not. For example, the method Rect_::contains returns true if"
imgcodecs,"\[x \leq pt.x < x+width, y \leq pt.y < y+height\]"
imgcodecs,Virtually every loop over an image ROI in OpenCV (where ROI is specified by Rect_<int> ) is implemented as:
imgcodecs,"In addition to the class members, the following operations on rectangles are implemented:"
imgcodecs,"\(\texttt{rect} = \texttt{rect} \pm \texttt{point}\) (shifting a rectangle by a certain offset) \(\texttt{rect} = \texttt{rect} \pm \texttt{size}\) (expanding or shrinking a rectangle by a certain amount) rect += point, rect -= point, rect += size, rect -= size (augmenting operations) rect = rect1 & rect2 (rectangle intersection) rect = rect1 | rect2 (minimum area rectangle containing rect1 and rect2 ) rect &= rect1, rect |= rect1 (and the corresponding augmenting operations) rect == rect1, rect != rect1 (rectangle comparison)"
imgcodecs,This is an example how the partial ordering on rectangles can be established (rect1 \(\subseteq\) rect2):
imgcodecs,"For your convenience, the Rect_<> alias is available: cv::Rect"
imgcodecs,Variational optical flow refinement.
imgcodecs,"This class implements variational refinement of the input flow field, i.e. it uses input flow to initialize the minimization of the following functional: \(E(U) = \int_{\Omega} \delta \Psi(E_I) + \gamma \Psi(E_G) + \alpha \Psi(E_S) \), where \(E_I,E_G,E_S\) are color constancy, gradient constancy and smoothness terms respectively. \(\Psi(s^2)=\sqrt{s^2+\epsilon^2}\) is a robust penalizer to limit the influence of outliers. A complete formulation and a description of the minimization procedure can be found in [42]"
imgcodecs,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
imgcodecs,Class to compute an image descriptor using the bag of visual words.
imgcodecs,Such a computation consists of the following steps:
imgcodecs,The base class for stereo correspondence algorithms.
imgcodecs,Base class for parallel data processors.
imgcodecs,YOUR ATTENTION PLEASE!
imgcodecs,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
imgcodecs,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
imgcodecs,Note for developers: please don't put videoio dependency in G-API because of this file.
imgcodecs,Dense optical flow algorithms compute motion for each point:
imgcodecs,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
imgcodecs,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
imgcodecs,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
imgcodecs,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
imgcodecs,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
imgcodecs,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
imgcodecs,The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows:
imgcodecs,"By default, the algorithm is single-pass, which means that you consider only 5 directions instead of 8. Set mode=StereoSGBM::MODE_HH in createStereoSGBM to run the full variant of the algorithm but beware that it may consume a lot of memory. The algorithm matches blocks, not individual pixels. Though, setting blockSize=1 reduces the blocks to single pixels. Mutual information cost function is not implemented. Instead, a simpler Birchfield-Tomasi sub-pixel metric from [28] is used. Though, the color images are supported as well. Some pre- and post- processing steps from K. Konolige algorithm StereoBM are included, for example: pre-filtering (StereoBM::PREFILTER_XSOBEL type) and post-filtering (uniqueness check, quadratic interpolation and speckle filtering)."
imgcodecs,(Python) An example illustrating the use of the StereoSGBM matching algorithm can be found at opencv_source_code/samples/python/stereo_match.py
imgcodecs,High level image stitcher.
imgcodecs,"It's possible to use this class without being aware of the entire stitching pipeline. However, to be able to achieve higher stitching stability and quality of the final images at least being familiar with the theory is recommended."
imgcodecs,A basic example on image stitching can be found at opencv_source_code/samples/cpp/stitching.cpp A basic example on image stitching in Python can be found at opencv_source_code/samples/python/stitching.py A detailed example on image stitching can be found at opencv_source_code/samples/cpp/stitching_detailed.cpp
imgcodecs,"Functions void CGImageToMat (const CGImageRef image, cv::Mat &m, bool alphaExist=false)  CGImageRef MatToCGImage (const cv::Mat &image) CF_RETURNS_RETAINED  UIImage * MatToUIImage (const cv::Mat &image)  void UIImageToMat (const UIImage *image, cv::Mat &m, bool alphaExist=false) "
imgcodecs,This type is very similar to InputArray except that it is used for input/output and output function parameters.
imgcodecs,"Just like with InputArray, OpenCV users should not care about OutputArray, they just pass Mat, vector<T> etc. to the functions. The same limitation as for InputArray: Do not explicitly create OutputArray instances applies here too."
imgcodecs,"If you want to make your function polymorphic (i.e. accept different arrays as output parameters), it is also not very difficult. Take the sample above as the reference. Note that _OutputArray::create() needs to be called before _OutputArray::getMat(). This way you guarantee that the output array is properly allocated."
imgcodecs,"Optional output parameters. If you do not need certain output array to be computed and returned to you, pass cv::noArray(), just like you would in the case of optional input array. At the implementation level, use _OutputArray::needed() to check if certain output array needs to be computed or not."
imgcodecs,There are several synonyms for OutputArray that are used to assist automatic Python/Java/... wrapper generators:
imgcodecs,the CSRT tracker
imgcodecs,The implementation is based on [177] Discriminative Correlation Filter with Channel and Spatial Reliability
imgcodecs,The function performs generalized matrix multiplication similar to the gemm functions in BLAS level 3: \(D = \alpha*AB+\beta*C\)
imgcodecs,"src1 pointer to input \(M\times N\) matrix \(A\) or \(A^T\) stored in row major order. src1_step number of bytes between two consequent rows of matrix \(A\) or \(A^T\). src2 pointer to input \(N\times K\) matrix \(B\) or \(B^T\) stored in row major order. src2_step number of bytes between two consequent rows of matrix \(B\) or \(B^T\). alpha \(\alpha\) multiplier before \(AB\) src3 pointer to input \(M\times K\) matrix \(C\) or \(C^T\) stored in row major order. src3_step number of bytes between two consequent rows of matrix \(C\) or \(C^T\). beta \(\beta\) multiplier before \(C\) dst pointer to input \(M\times K\) matrix \(D\) stored in row major order. dst_step number of bytes between two consequent rows of matrix \(D\). m number of rows in matrix \(A\) or \(A^T\), equals to number of rows in matrix \(D\) n number of columns in matrix \(A\) or \(A^T\) k number of columns in matrix \(B\) or \(B^T\), equals to number of columns in matrix \(D\) flags algorithm options (combination of CV_HAL_GEMM_1_T, ...)."
imgcodecs,"Functions int hal_ni_gemm32f (const float *src1, size_t src1_step, const float *src2, size_t src2_step, float alpha, const float *src3, size_t src3_step, float beta, float *dst, size_t dst_step, int m, int n, int k, int flags)  int hal_ni_gemm32fc (const float *src1, size_t src1_step, const float *src2, size_t src2_step, float alpha, const float *src3, size_t src3_step, float beta, float *dst, size_t dst_step, int m, int n, int k, int flags)  int hal_ni_gemm64f (const double *src1, size_t src1_step, const double *src2, size_t src2_step, double alpha, const double *src3, size_t src3_step, double beta, double *dst, size_t dst_step, int m, int n, int k, int flags)  int hal_ni_gemm64fc (const double *src1, size_t src1_step, const double *src2, size_t src2_step, double alpha, const double *src3, size_t src3_step, double beta, double *dst, size_t dst_step, int m, int n, int k, int flags) "
imgcodecs,QtFont available only for Qt. See cv::fontQt.
imgcodecs,src source array dst destination array len length of arrays
imgcodecs,"Functions int hal_ni_log32f (const float *src, float *dst, int len)  int hal_ni_log64f (const double *src, double *dst, int len) "
imgcodecs,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
imgcodecs,the KCF (Kernelized Correlation Filter) tracker
imgcodecs,"KCF is a novel tracking framework that utilizes properties of circulant matrix to enhance the processing speed. This tracking method is an implementation of [123] which is extended to KCF with color-names features ([64]). The original paper of KCF is available at http://www.robots.ox.ac.uk/~joao/publications/henriques_tpami2015.pdf as well as the matlab implementation. For more information about KCF with color-names features, please refer to http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html."
imgcodecs,Namespaces namespace cv::gapi 
imgcodecs,finds arbitrary template in the grayscale image using Generalized Hough Transform
imgcodecs,"Detects position, translation and rotation [114] ."
imgcodecs,Template Read-Write Sparse Matrix Iterator Class.
imgcodecs,This is the derived from cv::SparseMatConstIterator_ class that introduces more convenient operator *() for accessing the current element.
imgcodecs,Maximally stable extremal region extractor.
imgcodecs,The class encapsulates all the parameters of the MSER extraction algorithm (see wiki article).
imgcodecs,"there are two different implementation of MSER: one for grey image, one for color image the grey image algorithm is taken from: [208] ; the paper claims to be faster than union-find method; it actually get 1.5~2m/s on my centrino L7200 1.2GHz laptop. the color image algorithm is taken from: [94] ; it should be much slower than grey image method ( 3~4 times ) (Python) A complete example showing the use of the MSER detector can be found at samples/python/mser.py"
imgcodecs,Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. This algorithm uses all image pixels.
imgcodecs,For more information see [227] .
imgcodecs,struct for detection region of interest (ROI)
imgcodecs,Spherical warper factory class.
imgcodecs,The STL-compliant memory Allocator based on cv::fastMalloc() and cv::fastFree()
imgcodecs,cv::GArray<T> template class represents a list of objects of class T in the graph.
imgcodecs,"cv::GArray<T> describes a functional relationship between operations consuming and producing arrays of objects of class T. The primary purpose of cv::GArray<T> is to represent a dynamic list of objects where the size of the list is not known at the graph construction or compile time. Examples include: corner and feature detectors (cv::GArray<cv::Point>), object detection and tracking results (cv::GArray<cv::Rect>). Programmers can use their own types with cv::GArray<T> in the custom operations."
imgcodecs,"Similar to cv::GScalar, cv::GArray<T> may be value-initialized in this case a graph-constant value is associated with the object."
imgcodecs,"GArray<T> is a virtual counterpart of std::vector<T>, which is usually used to represent the GArray<T> data in G-API during the execution."
imgcodecs,This is a base class for all more or less complex algorithms in OpenCV.
imgcodecs,"especially for classes of algorithms, for which there can be multiple implementations. The examples are stereo correspondence (for which there are algorithms like block matching, semi-global block matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck etc.)."
imgcodecs,Here is example of SimpleBlobDetector use in your application via Algorithm interface:
imgcodecs,Classes class cv::segmentation::IntelligentScissorsMB  Intelligent Scissors image segmentation. More... 
imgcodecs,"The human perception isn't built for observing fine changes in grayscale images. Human eyes are more sensitive to observing changes between colors, so you often need to recolor your grayscale images to get a clue about them. OpenCV now comes with various colormaps to enhance the visualization in your computer vision application."
imgcodecs,"In OpenCV you only need applyColorMap to apply a colormap on a given image. The following sample code reads the path to an image from command line, applies a Jet colormap on it and shows the result:"
imgcodecs,"Enumerations enum cv::ColormapTypes { cv::COLORMAP_AUTUMN = 0 , cv::COLORMAP_BONE = 1 , cv::COLORMAP_JET = 2 , cv::COLORMAP_WINTER = 3 , cv::COLORMAP_RAINBOW = 4 , cv::COLORMAP_OCEAN = 5 , cv::COLORMAP_SUMMER = 6 , cv::COLORMAP_SPRING = 7 , cv::COLORMAP_COOL = 8 , cv::COLORMAP_HSV = 9 , cv::COLORMAP_PINK = 10 , cv::COLORMAP_HOT = 11 , cv::COLORMAP_PARULA = 12 , cv::COLORMAP_MAGMA = 13 , cv::COLORMAP_INFERNO = 14 , cv::COLORMAP_PLASMA = 15 , cv::COLORMAP_VIRIDIS = 16 , cv::COLORMAP_CIVIDIS = 17 , cv::COLORMAP_TWILIGHT = 18 , cv::COLORMAP_TWILIGHT_SHIFTED = 19 , cv::COLORMAP_TURBO = 20 , cv::COLORMAP_DEEPGREEN = 21 }  GNU Octave/MATLAB equivalent colormaps. More... "
imgcodecs,"Class implementing the KAZE keypoint detector and descriptor extractor, described in [9] ."
imgcodecs,n-dimensional dense array class
imgcodecs,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
imgcodecs,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
imgcodecs,"In case of a 2-dimensional array, the above formula is reduced to:"
imgcodecs,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
imgcodecs,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
imgcodecs,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
imgcodecs,There are many different ways to create a Mat object. The most popular options are listed below:
imgcodecs,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
imgcodecs,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
imgcodecs,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
imgcodecs,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
imgcodecs,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
imgcodecs,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
imgcodecs,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
imgcodecs,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
imgcodecs,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
imgcodecs,"Levenberg-Marquardt solver. Starting with the specified vector of parameters it optimizes the target vector criteria ""err"" (finds local minima of each target vector component absolute value)."
imgcodecs,"When needed, it calls user-provided callback."
imgcodecs,Classes class cv::ParallelLoopBody  Base class for parallel data processors. More...  class cv::ParallelLoopBodyLambdaWrapper 
imgcodecs,Principal Component Analysis.
imgcodecs,"The class is used to calculate a special basis for a set of vectors. The basis will consist of eigenvectors of the covariance matrix calculated from the input set of vectors. The class PCA can also transform vectors to/from the new coordinate space defined by the basis. Usually, in this new coordinate system, each vector from the original set (and any linear combination of such vectors) can be quite accurately approximated by taking its first few components, corresponding to the eigenvectors of the largest eigenvalues of the covariance matrix. Geometrically it means that you calculate a projection of the vector to a subspace formed by a few eigenvectors corresponding to the dominant eigenvalues of the covariance matrix. And usually such a projection is very close to the original vector. So, you can represent the original vector from a high-dimensional space with a much shorter vector consisting of the projected vector's coordinates in the subspace. Such a transformation is also known as Karhunen-Loeve Transform, or KLT. See http://en.wikipedia.org/wiki/Principal_component_analysis"
imgcodecs,"The sample below is the function that takes two matrices. The first function stores a set of vectors (a row per vector) that is used to calculate PCA. The second function stores another ""test"" set of vectors (a row per vector). First, these vectors are compressed with PCA, then reconstructed back, and then the reconstruction error norm is computed and printed for each vector. :"
imgcodecs,Brute-force descriptor matcher.
imgcodecs,"For each descriptor in the first set, this matcher finds the closest descriptor in the second set by trying each one. This descriptor matcher supports masking permissible matches of descriptor sets."
imgcodecs,Namespaces namespace cv::traits 
imgcodecs,Namespaces namespace cv  namespace cv::utils::logging::internal 
imgcodecs,Namespaces namespace cv::omnidir::internal 
imgcodecs,"Enumerations enum cv::VideoCaptureAPIs { cv::CAP_ANY = 0 , cv::CAP_VFW = 200 , cv::CAP_V4L = 200 , cv::CAP_V4L2 = CAP_V4L , cv::CAP_FIREWIRE = 300 , cv::CAP_FIREWARE = CAP_FIREWIRE , cv::CAP_IEEE1394 = CAP_FIREWIRE , cv::CAP_DC1394 = CAP_FIREWIRE , cv::CAP_CMU1394 = CAP_FIREWIRE , cv::CAP_QT = 500 , cv::CAP_UNICAP = 600 , cv::CAP_DSHOW = 700 , cv::CAP_PVAPI = 800 , cv::CAP_OPENNI = 900 , cv::CAP_OPENNI_ASUS = 910 , cv::CAP_ANDROID = 1000 , cv::CAP_XIAPI = 1100 , cv::CAP_AVFOUNDATION = 1200 , cv::CAP_GIGANETIX = 1300 , cv::CAP_MSMF = 1400 , cv::CAP_WINRT = 1410 , cv::CAP_INTELPERC = 1500 , cv::CAP_REALSENSE = 1500 , cv::CAP_OPENNI2 = 1600 , cv::CAP_OPENNI2_ASUS = 1610 , cv::CAP_OPENNI2_ASTRA = 1620 , cv::CAP_GPHOTO2 = 1700 , cv::CAP_GSTREAMER = 1800 , cv::CAP_FFMPEG = 1900 , cv::CAP_IMAGES = 2000 , cv::CAP_ARAVIS = 2100 , cv::CAP_OPENCV_MJPEG = 2200 , cv::CAP_INTEL_MFX = 2300 , cv::CAP_XINE = 2400 , cv::CAP_UEYE = 2500 , cv::CAP_OBSENSOR = 2600 }  cv::VideoCapture API backends identifier. More...  enum cv::VideoCaptureProperties { cv::CAP_PROP_POS_MSEC =0 , cv::CAP_PROP_POS_FRAMES =1 , cv::CAP_PROP_POS_AVI_RATIO =2 , cv::CAP_PROP_FRAME_WIDTH =3 , cv::CAP_PROP_FRAME_HEIGHT =4 , cv::CAP_PROP_FPS =5 , cv::CAP_PROP_FOURCC =6 , cv::CAP_PROP_FRAME_COUNT =7 , cv::CAP_PROP_FORMAT =8 , cv::CAP_PROP_MODE =9 , cv::CAP_PROP_BRIGHTNESS =10 , cv::CAP_PROP_CONTRAST =11 , cv::CAP_PROP_SATURATION =12 , cv::CAP_PROP_HUE =13 , cv::CAP_PROP_GAIN =14 , cv::CAP_PROP_EXPOSURE =15 , cv::CAP_PROP_CONVERT_RGB =16 , cv::CAP_PROP_WHITE_BALANCE_BLUE_U =17 , cv::CAP_PROP_RECTIFICATION =18 , cv::CAP_PROP_MONOCHROME =19 , cv::CAP_PROP_SHARPNESS =20 , cv::CAP_PROP_AUTO_EXPOSURE =21 , cv::CAP_PROP_GAMMA =22 , cv::CAP_PROP_TEMPERATURE =23 , cv::CAP_PROP_TRIGGER =24 , cv::CAP_PROP_TRIGGER_DELAY =25 , cv::CAP_PROP_WHITE_BALANCE_RED_V =26 , cv::CAP_PROP_ZOOM =27 , cv::CAP_PROP_FOCUS =28 , cv::CAP_PROP_GUID =29 , cv::CAP_PROP_ISO_SPEED =30 , cv::CAP_PROP_BACKLIGHT =32 , cv::CAP_PROP_PAN =33 , cv::CAP_PROP_TILT =34 , cv::CAP_PROP_ROLL =35 , cv::CAP_PROP_IRIS =36 , cv::CAP_PROP_SETTINGS =37 , cv::CAP_PROP_BUFFERSIZE =38 , cv::CAP_PROP_AUTOFOCUS =39 , cv::CAP_PROP_SAR_NUM =40 , cv::CAP_PROP_SAR_DEN =41 , cv::CAP_PROP_BACKEND =42 , cv::CAP_PROP_CHANNEL =43 , cv::CAP_PROP_AUTO_WB =44 , cv::CAP_PROP_WB_TEMPERATURE =45 , cv::CAP_PROP_CODEC_PIXEL_FORMAT =46 , cv::CAP_PROP_BITRATE =47 , cv::CAP_PROP_ORIENTATION_META =48 , cv::CAP_PROP_ORIENTATION_AUTO =49 , cv::CAP_PROP_HW_ACCELERATION =50 , cv::CAP_PROP_HW_DEVICE =51 , cv::CAP_PROP_HW_ACCELERATION_USE_OPENCL =52 , cv::CAP_PROP_OPEN_TIMEOUT_MSEC =53 , cv::CAP_PROP_READ_TIMEOUT_MSEC =54 , cv::CAP_PROP_STREAM_OPEN_TIME_USEC =55 , cv::CAP_PROP_VIDEO_TOTAL_CHANNELS = 56 , cv::CAP_PROP_VIDEO_STREAM = 57 , cv::CAP_PROP_AUDIO_STREAM = 58 , cv::CAP_PROP_AUDIO_POS = 59 , cv::CAP_PROP_AUDIO_SHIFT_NSEC = 60 , cv::CAP_PROP_AUDIO_DATA_DEPTH = 61 , cv::CAP_PROP_AUDIO_SAMPLES_PER_SECOND = 62 , cv::CAP_PROP_AUDIO_BASE_INDEX = 63 , cv::CAP_PROP_AUDIO_TOTAL_CHANNELS = 64 , cv::CAP_PROP_AUDIO_TOTAL_STREAMS = 65 , cv::CAP_PROP_AUDIO_SYNCHRONIZE = 66 , cv::CAP_PROP_LRF_HAS_KEY_FRAME = 67 , cv::CAP_PROP_CODEC_EXTRADATA_INDEX = 68 , cv::CAP_PROP_FRAME_TYPE = 69 , cv::CAP_PROP_N_THREADS = 70 , cv::CAP_PROP_PTS = 71 , cv::CAP_PROP_DTS_DELAY = 72 }  cv::VideoCapture generic properties identifier. More...  enum cv::VideoWriterProperties { cv::VIDEOWRITER_PROP_QUALITY = 1 , cv::VIDEOWRITER_PROP_FRAMEBYTES = 2 , cv::VIDEOWRITER_PROP_NSTRIPES = 3 , cv::VIDEOWRITER_PROP_IS_COLOR = 4 , cv::VIDEOWRITER_PROP_DEPTH = 5 , cv::VIDEOWRITER_PROP_HW_ACCELERATION = 6 , cv::VIDEOWRITER_PROP_HW_DEVICE = 7 , cv::VIDEOWRITER_PROP_HW_ACCELERATION_USE_OPENCL = 8 , cv::VIDEOWRITER_PROP_RAW_VIDEO = 9 , cv::VIDEOWRITER_PROP_KEY_INTERVAL = 10 , cv::VIDEOWRITER_PROP_KEY_FLAG = 11 , cv::VIDEOWRITER_PROP_PTS = 12 , cv::VIDEOWRITER_PROP_DTS_DELAY = 13 }  cv::VideoWriter generic properties identifier. More... "
imgcodecs,Returns result of asynchronous operations.
imgcodecs,Object has attached asynchronous state. Assignment operator doesn't clone asynchronous state (it is shared between all instances).
imgcodecs,Result can be fetched via get() method only once.
imgcodecs,This is the proxy class for passing read-only input arrays into OpenCV functions.
imgcodecs,It is defined as:
imgcodecs,"where _InputArray is a class that can be constructed from Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat_<T> >, UMat, std::vector<UMat> or double. It can also be constructed from a matrix expression."
imgcodecs,"Since this is mostly implementation-level class, and its interface may change in future versions, we do not describe it in details. There are a few key things, though, that should be kept in mind:"
imgcodecs,"When you see in the reference manual or in OpenCV source code a function that takes InputArray, it means that you can actually pass Mat, Matx, vector<T> etc. (see above the complete list). Optional input arguments: If some of the input arrays may be empty, pass cv::noArray() (or simply cv::Mat() as you probably did before). The class is designed solely for passing parameters. That is, normally you should not declare class members, local and global variables of this type. If you want to design your own function or a class method that can operate of arrays of multiple types, you can use InputArray (or OutputArray) for the respective parameters. Inside a function you should use _InputArray::getMat() method to construct a matrix header for the array (without copying data). _InputArray::kind() can be used to distinguish Mat from vector<> etc., but normally it is not needed."
imgcodecs,Here is how you can use a function that takes InputArray :
imgcodecs,"That is, we form an STL vector containing points, and apply in-place affine transformation to the vector using the 2x3 matrix created inline as Matx<float, 2, 3> instance."
imgcodecs,"Here is how such a function can be implemented (for simplicity, we implement a very specific case of it, according to the assertion statement inside) :"
imgcodecs,"There is another related type, InputArrayOfArrays, which is currently defined as a synonym for InputArray:"
imgcodecs,"It denotes function arguments that are either vectors of vectors or vectors of matrices. A separate synonym is needed to generate Python/Java etc. wrappers properly. At the function implementation level their use is similar, but _InputArray::getMat(idx) should be used to get header for the idx-th component of the outer vector and _InputArray::size().area() should be used to find the number of components (vectors/matrices) of the outer vector."
imgcodecs,"In general, type support is limited to cv::Mat types. Other types are forbidden. But in some cases we need to support passing of custom non-general Mat types, like arrays of cv::KeyPoint, cv::DMatch, etc. This data is not intended to be interpreted as an image data, or processed somehow like regular cv::Mat. To pass such custom type use rawIn() / rawOut() / rawInOut() wrappers. Custom type is wrapped as Mat-compatible CV_8UC<N> values (N = sizeof(T), N <= CV_CN_MAX)."
imgcodecs,Useful links:
imgcodecs,http://www.cse.cuhk.edu.hk/leojia/projects/color2gray/index.html
imgcodecs,"Functions void cv::decolor (InputArray src, OutputArray grayscale, OutputArray color_boost)  Transforms a color image to a grayscale image. It is a basic tool in digital printing, stylized black-and-white photograph rendering, and in many single channel image processing applications [176] . "
imgcodecs,A container class for heterogeneous kernel implementation collections and graph transformations.
imgcodecs,"GKernelPackage is a special container class which stores kernel implementations and graph transformations. Objects of this class are created and passed to cv::GComputation::compile() to specify which kernels to use and which transformations to apply in the compiled graph. GKernelPackage may contain kernels of different backends, e.g. be heterogeneous."
imgcodecs,The most easy way to create a kernel package is to use function cv::gapi::kernels(). This template functions takes kernel implementations in form of type list (variadic template) and generates a kernel package atop of that.
imgcodecs,"Kernel packages can be also generated programmatically, starting with an empty package (created with the default constructor) and then by populating it with kernels via call to GKernelPackage::include(). Note this method is also a template one since G-API kernel and transformation implementations are types, not objects."
imgcodecs,"Finally, two kernel packages can be combined into a new one with function cv::gapi::combine()."
imgcodecs,Information Flow algorithm implementaton for alphamatting
imgcodecs,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
imgcodecs,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
imgcodecs,The implementation is based on [7].
imgcodecs,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
imgcodecs,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
imgcodecs,"This class is used to perform the non-linear non-constrained minimization of a function,."
imgcodecs,"defined on an n-dimensional Euclidean space, using the Nelder-Mead method, also known as downhill simplex method**. The basic idea about the method can be obtained from http://en.wikipedia.org/wiki/Nelder-Mead_method."
imgcodecs,"It should be noted, that this method, although deterministic, is rather a heuristic and therefore may converge to a local minima, not necessary a global one. It is iterative optimization technique, which at each step uses an information about the values of a function evaluated only at n+1 points, arranged as a simplex in n-dimensional space (hence the second name of the method). At each step new point is chosen to evaluate function at, obtained value is compared with previous ones and based on this information simplex changes it's shape , slowly moving to the local minimum. Thus this method is using only function values to make decision, on contrary to, say, Nonlinear Conjugate Gradient method (which is also implemented in optim)."
imgcodecs,"Algorithm stops when the number of function evaluations done exceeds termcrit.maxCount, when the function values at the vertices of simplex are within termcrit.epsilon range or simplex becomes so small that it can enclosed in a box with termcrit.epsilon sides, whatever comes first, for some defined by user positive integer termcrit.maxCount and positive non-integer termcrit.epsilon."
imgcodecs,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
imgcodecs,"Enumerations enum struct cv::DrawMatchesFlags { cv::DrawMatchesFlags::DEFAULT = 0 , cv::DrawMatchesFlags::DRAW_OVER_OUTIMG = 1 , cv::DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS = 2 , cv::DrawMatchesFlags::DRAW_RICH_KEYPOINTS = 4 } "
imgcodecs,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
imgcodecs,Lookup table replacement Table consists of 256 elements of a size from 1 to 8 bytes having 1 channel or src_channels For 8s input type 128 is added to LUT index Destination should have the same element type and number of channels as lookup table elements
imgcodecs,src_data Source image data src_step Source image step src_type Sorce image type lut_data Pointer to lookup table lut_channel_size Size of each channel in bytes lut_channels Number of channels in lookup table dst_data Destination data dst_step Destination step width Width of images height Height of images
imgcodecs,"Functions int hal_ni_lut (const uchar *src_data, size_t src_step, size_t src_type, const uchar *lut_data, size_t lut_channel_size, size_t lut_channels, uchar *dst_data, size_t dst_step, int width, int height) "
imgcodecs,kmeans -based class to train visual vocabulary using the bag of visual words approach. :
imgcodecs,"Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as Mat's). It means that for each pixel location \((x,y)\) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location \((x,y)\). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one."
imgcodecs,"Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian \(3 \times 3\) filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (""replicated border"" extrapolation method), or assume that all the non-existing pixels are zeros (""constant border"" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see BorderTypes"
imgcodecs,src source array dst destination array len length of arrays
imgcodecs,"Functions int hal_ni_exp32f (const float *src, float *dst, int len)  int hal_ni_exp64f (const double *src, double *dst, int len) "
imgcodecs,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
imgcodecs,Namespace for all functions is cv::img_hash.
imgcodecs,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
imgcodecs,"This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64."
imgcodecs,"Dual quaternions were introduced to describe rotation together with translation while ordinary quaternions can only describe rotation. It can be used for shortest path pose interpolation, local pose optimization or volumetric deformation. More details can be found"
imgcodecs,"https://en.wikipedia.org/wiki/Dual_quaternion ""A beginners guide to dual-quaternions: what they are, how they work, and how to use them for 3D character hierarchies"", Ben Kenwright, 2012 ""Dual Quaternions"", Yan-Bin Jia, 2013 ""Geometric Skinning with Approximate Dual Quaternion Blending"", Kavan, 2008 http://rodolphe-vaillant.fr/?e=29"
imgcodecs,A unit dual quaternion can be classically represented as:
imgcodecs,"\[ \begin{equation} \begin{split} \sigma &= \left(r+\frac{\epsilon}{2}tr\right)\\ &= [w, x, y, z, w\_, x\_, y\_, z\_] \end{split} \end{equation} \]"
imgcodecs,"where \(r, t\) represents the rotation (ordinary unit quaternion) and translation (pure ordinary quaternion) respectively."
imgcodecs,A general dual quaternions which consist of two quaternions is usually represented in form of:
imgcodecs,\[ \sigma = p + \epsilon q \]
imgcodecs,"where the introduced dual unit \(\epsilon\) satisfies \(\epsilon^2 = \epsilon^3 =...=0\), and \(p, q\) are quaternions."
imgcodecs,"Alternatively, dual quaternions can also be interpreted as four components which are all dual numbers:"
imgcodecs,\[ \sigma = \hat{q}_w + \hat{q}_xi + \hat{q}_yj + \hat{q}_zk \]
imgcodecs,"If we set \(\hat{q}_x, \hat{q}_y\) and \(\hat{q}_z\) equal to 0, a dual quaternion is transformed to a dual number. see normalize()."
imgcodecs,"If you want to create a dual quaternion, you can use:"
imgcodecs,"A point \(v=(x, y, z)\) in form of dual quaternion is \([1+\epsilon v]=[1,0,0,0,0,x,y,z]\). The transformation of a point \(v_1\) to another point \(v_2\) under the dual quaternion \(\sigma\) is"
imgcodecs,\[ 1 + \epsilon v_2 = \sigma * (1 + \epsilon v_1) * \sigma^{\star} \]
imgcodecs,where \(\sigma^{\star}=p^*-\epsilon q^*.\)
imgcodecs,"A line in the \(Pl\ddot{u}cker\) coordinates \((\hat{l}, m)\) defined by the dual quaternion \(l=\hat{l}+\epsilon m\). To transform a line,"
imgcodecs,"\[l_2 = \sigma * l_1 * \sigma^*,\]"
imgcodecs,where \(\sigma=r+\frac{\epsilon}{2}rt\) and \(\sigma^*=p^*+\epsilon q^*\).
imgcodecs,"To extract the Vec<double, 8> or Vec<float, 8>, see toVec();"
imgcodecs,"To extract the affine transformation matrix, see toMat();"
imgcodecs,"To extract the instance of Affine3, see toAffine3();"
imgcodecs,"If two quaternions \(q_0, q_1\) are needed to be interpolated, you can use sclerp()"
imgcodecs,or dqblend().
imgcodecs,"With more than two dual quaternions to be blended, you can use generalize linear dual quaternion blending with the corresponding weights, i.e. gdqblend()."
imgcodecs,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
imgcodecs,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
imgcodecs,Affine warper factory class.
imgcodecs,Class for matching keypoint descriptors.
imgcodecs,"query descriptor index, train descriptor index, train image index, and distance between descriptors."
imgcodecs,Computes reciprocial: dst[i] = scale / src[i]
imgcodecs,src_data source image data src_step source image step dst_data destination image data dst_step destination image step width width of the images height height of the images scale additional multiplier
imgcodecs,"Functions int hal_ni_recip16s (const short *src_data, size_t src_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip16u (const ushort *src_data, size_t src_step, ushort *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip32f (const float *src_data, size_t src_step, float *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip32s (const int *src_data, size_t src_step, int *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip64f (const double *src_data, size_t src_step, double *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip8s (const schar *src_data, size_t src_step, schar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_recip8u (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, double scale) "
imgcodecs,The base class for algorithms that align images of the same scene with different exposures.
imgcodecs,This modules is to draw UTF-8 strings with freetype/harfbuzz.
imgcodecs,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
imgcodecs,Classes class cv::freetype::FreeType2 
imgcodecs,Represents an arbitrary compilation argument.
imgcodecs,"Any value can be wrapped into cv::GCompileArg, but only known ones (to G-API or its backends) can be interpreted correctly."
imgcodecs,"Normally objects of this class shouldn't be created manually, use cv::compile_args() function which automatically wraps everything passed in (a variadic template parameter pack) into a vector of cv::GCompileArg objects."
imgcodecs,Matrix read-write iterator.
imgcodecs,Abstract base class for training the bag of visual words vocabulary from a set of descriptors.
imgcodecs,"For details, see, for example, Visual Categorization with Bags of Keypoints by Gabriella Csurka, Christopher R. Dance, Lixin Fan, Jutta Willamowski, Cedric Bray, 2004. :"
imgcodecs,Interface for modalities that plug into the LINE template matching representation.
imgcodecs,cv::MediaFrame class represents an image/media frame obtained from an external source.
imgcodecs,cv::MediaFrame represents image data as specified in cv::MediaFormat. cv::MediaFrame is designed to be a thin wrapper over some external memory of buffer; the class itself provides an uniform interface over such types of memory. cv::MediaFrame wraps data from a camera driver or from a media codec and provides an abstraction layer over this memory to G-API. MediaFrame defines a compact interface to access and manage the underlying data; the implementation is fully defined by the associated Adapter (which is usually user-defined).
imgcodecs,Implementation of HOG (Histogram of Oriented Gradients) descriptor and object detector.
imgcodecs,the HOG descriptor algorithm introduced by Navneet Dalal and Bill Triggs [63] .
imgcodecs,useful links:
imgcodecs,https://hal.inria.fr/inria-00548512/document/
imgcodecs,https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients
imgcodecs,https://software.intel.com/en-us/ipp-dev-reference-histogram-of-oriented-gradients-hog-descriptor
imgcodecs,http://www.learnopencv.com/histogram-of-oriented-gradients
imgcodecs,http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial
imgcodecs,"Enumerations enum cv::KmeansFlags { cv::KMEANS_RANDOM_CENTERS = 0 , cv::KMEANS_PP_CENTERS = 2 , cv::KMEANS_USE_INITIAL_LABELS = 1 }  k-means flags More... "
imgcodecs,Performs \(LU\) decomposition of square matrix \(A=P*L*U\) (where \(P\) is permutation matrix) and solves matrix equation \(A*X=B\). Function returns the \(sign\) of permutation \(P\) via parameter info.
imgcodecs,"src1 pointer to input matrix \(A\) stored in row major order. After finish of work src1 contains at least \(U\) part of \(LU\) decomposition which is appropriate for determainant calculation: \(det(A)=sign*\prod_{j=1}^{M}a_{jj}\). src1_step number of bytes between two consequent rows of matrix \(A\). m size of square matrix \(A\). src2 pointer to \(M\times N\) matrix \(B\) which is the right-hand side of system \(A*X=B\). \(B\) stored in row major order. If src2 is null pointer only \(LU\) decomposition will be performed. After finish of work src2 contains solution \(X\) of system \(A*X=B\). src2_step number of bytes between two consequent rows of matrix \(B\). n number of right-hand vectors in \(M\times N\) matrix \(B\). info indicates success of decomposition. If *info is equals to zero decomposition failed, othervise *info is equals to \(sign\)."
imgcodecs,"Functions int hal_ni_LU32f (float *src1, size_t src1_step, int m, float *src2, size_t src2_step, int n, int *info)  int hal_ni_LU64f (double *src1, size_t src1_step, int m, double *src2, size_t src2_step, int n, int *info) "
imgcodecs,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
imgcodecs,"Classes struct cv::Accumulator< T >  struct cv::Accumulator< char >  struct cv::Accumulator< short >  struct cv::Accumulator< unsigned char >  struct cv::Accumulator< unsigned short >  class cv::AffineFeature  Class for implementing the wrapper which makes detectors and extractors to be affine invariant, described as ASIFT in [312] . More...  class cv::AgastFeatureDetector  Wrapping class for feature detection using the AGAST method. : More...  class cv::AKAZE  Class implementing the AKAZE keypoint detector and descriptor extractor, described in [10]. More...  class cv::BRISK  Class implementing the BRISK keypoint detector and descriptor extractor, described in [159] . More...  class cv::FastFeatureDetector  Wrapping class for feature detection using the FAST method. : More...  class cv::Feature2D  Abstract base class for 2D image feature detectors and descriptor extractors. More...  class cv::GFTTDetector  Wrapping class for feature detection using the goodFeaturesToTrack function. : More...  class cv::KAZE  Class implementing the KAZE keypoint detector and descriptor extractor, described in [9] . More...  class cv::KeyPointsFilter  A class filters a vector of keypoints. More...  struct cv::L1< T >  struct cv::L2< T >  class cv::MSER  Maximally stable extremal region extractor. More...  class cv::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More...  class cv::SIFT  Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe [174] . More...  class cv::SimpleBlobDetector  Class for extracting blobs from an image. : More...  struct cv::SL2< T > "
imgcodecs,Read-write Sparse Matrix Iterator.
imgcodecs,"The class is similar to cv::SparseMatConstIterator, but can be used for in-place modification of the matrix elements."
imgcodecs,src source array dst destination array len length of arrays
imgcodecs,"Functions int hal_ni_sqrt32f (const float *src, float *dst, int len)  int hal_ni_sqrt64f (const double *src, double *dst, int len) "
imgcodecs,Namespaces namespace NcvCTprep 
imgcodecs,n-ary multi-dimensional array iterator.
imgcodecs,"Use the class to implement unary, binary, and, generally, n-ary element-wise operations on multi-dimensional arrays. Some of the arguments of an n-ary function may be continuous arrays, some may be not. It is possible to use conventional MatIterator 's for each array but incrementing all of the iterators after each small operations may be a big overhead. In this case consider using NAryMatIterator to iterate through several matrices simultaneously as long as they have the same geometry (dimensionality and all the dimension sizes are the same). On each iteration it.planes[0], it.planes[1],... will be the slices of the corresponding matrices."
imgcodecs,The example below illustrates how you can compute a normalized and threshold 3D color histogram:
imgcodecs,Smart pointer for OpenGL 2D texture memory with reference counting.
imgcodecs,Absolute difference: dst[i] = | src1[i] - src2[i] |
imgcodecs,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
imgcodecs,"Functions int hal_ni_absdiff16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_absdiff8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height) "
imgcodecs,"x source X arrays y source Y arrays mag destination magnitude array angle destination angle array len length of arrays angleInDegrees if set to true return angles in degrees, otherwise in radians y source Y arrays x source X arrays dst destination array len length of arrays angleInDegrees if set to true return angles in degrees, otherwise in radians mag source magnitude arrays mag source angle arrays x destination X array y destination Y array len length of arrays angleInDegrees if set to true interpret angles from degrees, otherwise from radians"
imgcodecs,"Functions int hal_ni_cartToPolar32f (const float *x, const float *y, float *mag, float *angle, int len, bool angleInDegrees)  int hal_ni_cartToPolar64f (const double *x, const double *y, double *mag, double *angle, int len, bool angleInDegrees)  int hal_ni_fastAtan32f (const float *y, const float *x, float *dst, int len, bool angleInDegrees)  int hal_ni_fastAtan64f (const double *y, const double *x, double *dst, int len, bool angleInDegrees)  int hal_ni_polarToCart32f (const float *mag, const float *angle, float *x, float *y, int len, bool angleInDegrees)  int hal_ni_polarToCart64f (const double *mag, const double *angle, double *x, double *y, int len, bool angleInDegrees) "
imgcodecs,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
imgcodecs,"Functions void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_deinterleave_epi16 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_deinterleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1)  void _mm_deinterleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1, __m128 &v_a0, __m128 &v_a1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1)  void _mm_interleave_epi8 (__m128i &v_r0, __m128i &v_r1, __m128i &v_g0, __m128i &v_g1, __m128i &v_b0, __m128i &v_b1, __m128i &v_a0, __m128i &v_a1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1)  void _mm_interleave_ps (__m128 &v_r0, __m128 &v_r1, __m128 &v_g0, __m128 &v_g1, __m128 &v_b0, __m128 &v_b1, __m128 &v_a0, __m128 &v_a1) "
imgcodecs,This structure forces Fluid backend to generate multiple parallel output regions in the graph. These regions execute in parallel.
imgcodecs,This feature may be deprecated in the future releases.
imgcodecs,Ask G-API to use threaded executor when cv::GComputation is compiled via cv::GComputation::compile method.
imgcodecs,Specifies a number of threads that should be used by executor.
imgcodecs,This module contains:
imgcodecs,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
imgcodecs,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
imgcodecs,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
imgcodecs,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
imgcodecs,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
imgcodecs,"This figure explains new functionality implemented with WinRT GUI. The new GUI provides an Image control, and a slider panel. Slider panel holds trackbars attached to it."
imgcodecs,Sliders are attached below the image control. Every new slider is added below the previous one.
imgcodecs,See below the example used to generate the figure:
imgcodecs,Functions void cv::winrt_initContainer (::Windows::UI::Xaml::Controls::Panel^ container)  Initializes container component that will be used to hold generated window content. 
imgcodecs,Template class for specifying the size of an image or rectangle.
imgcodecs,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
imgcodecs,OpenCV defines the following Size_<> aliases:
imgcodecs,The base class algorithms that can merge exposure sequence to a single image.
imgcodecs,"Drawing functions work with matrices/images of arbitrary depth. The boundaries of the shapes can be rendered with antialiasing (implemented only for 8-bit images for now). All the functions include the parameter color that uses an RGB value (that may be constructed with the Scalar constructor ) for color images and brightness for grayscale images. For color images, the channel ordering is normally Blue, Green, Red. This is what imshow, imread, and imwrite expect. So, if you form a color using the Scalar constructor, it should look like:"
imgcodecs,"\[\texttt{Scalar} (blue \_ component, green \_ component, red \_ component[, alpha \_ component])\]"
imgcodecs,"If you are using your own image rendering and I/O functions, you can use any channel ordering. The drawing functions process each channel independently and do not depend on the channel order or even on the used color space. The whole image can be converted from BGR to RGB or to a different color space using cvtColor ."
imgcodecs,"If a drawn figure is partially or completely outside the image, the drawing functions clip it. Also, many drawing functions can handle pixel coordinates specified with sub-pixel accuracy. This means that the coordinates can be passed as fixed-point numbers encoded as integers. The number of fractional bits is specified by the shift parameter and the real point coordinates are calculated as \(\texttt{Point}(x,y)\rightarrow\texttt{Point2f}(x*2^{-shift},y*2^{-shift})\) . This feature is especially effective when rendering antialiased shapes."
imgcodecs,Classes class cv::LineIterator  Class for iterating over all pixels on a raster line segment. More... 
imgcodecs,gapi_colorconvert
imgcodecs,"Functions GMat cv::gapi::concatHor (const GMat &src1, const GMat &src2)  Applies horizontal concatenation to given matrices.  GMat cv::gapi::concatHor (const std::vector< GMat > &v)  GMat cv::gapi::concatVert (const GMat &src1, const GMat &src2)  Applies vertical concatenation to given matrices.  GMat cv::gapi::concatVert (const std::vector< GMat > &v)  GMat cv::gapi::convertTo (const GMat &src, int rdepth, double alpha=1, double beta=0)  Converts a matrix to another data depth with optional scaling.  GFrame cv::gapi::copy (const GFrame &in)  Makes a copy of the input frame. Note that this copy may be not real (no actual data copied). Use this function to maintain graph contracts, e.g when graph's input needs to be passed directly to output, like in Streaming mode.  GMat cv::gapi::copy (const GMat &in)  Makes a copy of the input image. Note that this copy may be not real (no actual data copied). Use this function to maintain graph contracts, e.g when graph's input needs to be passed directly to output, like in Streaming mode.  GMat cv::gapi::crop (const GMat &src, const Rect &rect)  Crops a 2D matrix.  GMat cv::gapi::flip (const GMat &src, int flipCode)  Flips a 2D matrix around vertical, horizontal, or both axes.  GMat cv::gapi::LUT (const GMat &src, const Mat &lut)  Performs a look-up table transform of a matrix.  GMat cv::gapi::merge3 (const GMat &src1, const GMat &src2, const GMat &src3)  Creates one 3-channel matrix out of 3 single-channel ones.  GMat cv::gapi::merge4 (const GMat &src1, const GMat &src2, const GMat &src3, const GMat &src4)  Creates one 4-channel matrix out of 4 single-channel ones.  GMat cv::gapi::normalize (const GMat &src, double alpha, double beta, int norm_type, int ddepth=-1)  Normalizes the norm or value range of an array.  GMat cv::gapi::remap (const GMat &src, const Mat &map1, const Mat &map2, int interpolation, int borderMode=BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies a generic geometrical transformation to an image.  GMat cv::gapi::resize (const GMat &src, const Size &dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR)  Resizes an image.  GMatP cv::gapi::resizeP (const GMatP &src, const Size &dsize, int interpolation=cv::INTER_LINEAR)  Resizes a planar image.  std::tuple< GMat, GMat, GMat > cv::gapi::split3 (const GMat &src)  Divides a 3-channel matrix into 3 single-channel matrices.  std::tuple< GMat, GMat, GMat, GMat > cv::gapi::split4 (const GMat &src)  Divides a 4-channel matrix into 4 single-channel matrices.  GMat cv::gapi::warpAffine (const GMat &src, const Mat &M, const Size &dsize, int flags=cv::INTER_LINEAR, int borderMode=cv::BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies an affine transformation to an image.  GMat cv::gapi::warpPerspective (const GMat &src, const Mat &M, const Size &dsize, int flags=cv::INTER_LINEAR, int borderMode=cv::BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies a perspective transformation to an image. "
imgcodecs,Comma-separated Matrix Initializer.
imgcodecs,"The class instances are usually not created explicitly. Instead, they are created on ""matrix << firstValue"" operator."
imgcodecs,The sample below initializes 2x2 rotation matrix:
imgcodecs,Compare: dst[i] = src1[i] op src2[i]
imgcodecs,"src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images operation one of (CV_HAL_CMP_EQ, CV_HAL_CMP_GT, ...)"
imgcodecs,"Functions int hal_ni_cmp16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation)  int hal_ni_cmp8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, int operation) "
imgcodecs,Base class for Contrast Limited Adaptive Histogram Equalization.
imgcodecs,"Functions void cv::samples::addSamplesDataSearchPath (const cv::String &path)  Override search data path by adding new search location.  void cv::samples::addSamplesDataSearchSubDirectory (const cv::String &subdir)  Append samples search data sub directory.  cv::String cv::samples::findFile (const cv::String &relative_path, bool required=true, bool silentMode=false)  Try to find requested data file.  cv::String cv::samples::findFileOrKeep (const cv::String &relative_path, bool silentMode=false) "
imgcodecs,Classes class cv::CLAHE  Base class for Contrast Limited Adaptive Histogram Equalization. More... 
imgcodecs,"Template class for short numerical vectors, a partial case of Matx."
imgcodecs,"This template class represents short numerical vectors (of 1, 2, 3, 4 ... elements) on which you can perform basic arithmetical operations, access individual elements using [] operator etc. The vectors are allocated on stack, as opposite to std::valarray, std::vector, cv::Mat etc., which elements are dynamically allocated in the heap."
imgcodecs,The template takes 2 parameters:
imgcodecs,_Tp element type cn the number of elements
imgcodecs,"In addition to the universal notation like Vec<float, 3>, you can use shorter aliases for the most popular specialized variants of Vec, e.g. Vec3f ~ Vec<float, 3>."
imgcodecs,"It is possible to convert Vec<T,2> to/from Point_, Vec<T,3> to/from Point3_ , and Vec<T,4> to CvScalar or Scalar_. Use operator[] to access the elements of Vec."
imgcodecs,All the expected vector operations are also implemented:
imgcodecs,"v1 = v2 + v3 v1 = v2 - v3 v1 = v2 * scale v1 = scale * v2 v1 = -v2 v1 += v2 and other augmenting operations v1 == v2, v1 != v2 norm(v1) (euclidean norm) The Vec class is commonly used to describe pixel types of multi-channel arrays. See Mat for details."
imgcodecs,"This section describes high dynamic range imaging algorithms namely tonemapping, exposure alignment, camera calibration with multiple exposures and exposure fusion."
imgcodecs,"Classes class cv::AlignExposures  The base class for algorithms that align images of the same scene with different exposures. More...  class cv::AlignMTB  This algorithm converts images to median threshold bitmaps (1 for pixels brighter than median luminance and 0 otherwise) and than aligns the resulting bitmaps using bit operations. More...  class cv::CalibrateCRF  The base class for camera response calibration algorithms. More...  class cv::CalibrateDebevec  Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. Objective function is constructed using pixel values on the same position in all images, extra term is added to make the result smoother. More...  class cv::CalibrateRobertson  Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. This algorithm uses all image pixels. More...  class cv::MergeDebevec  The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response. More...  class cv::MergeExposures  The base class algorithms that can merge exposure sequence to a single image. More...  class cv::MergeMertens  Pixels are weighted using contrast, saturation and well-exposedness measures, than images are combined using laplacian pyramids. More...  class cv::MergeRobertson  The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response. More...  class cv::Tonemap  Base class for tonemapping algorithms - tools that are used to map HDR image to 8-bit range. More...  class cv::TonemapDrago  Adaptive logarithmic mapping is a fast global tonemapping algorithm that scales the image in logarithmic domain. More...  class cv::TonemapMantiuk  This algorithm transforms image to contrast using gradients on all levels of gaussian pyramid, transforms contrast values to HVS response and scales the response. After this the image is reconstructed from new contrast values. More...  class cv::TonemapReinhard  This is a global tonemapping operator that models human visual system. More... "
imgcodecs,Class used for calculating a sparse optical flow.
imgcodecs,The class can calculate an optical flow for a sparse feature set using the iterative Lucas-Kanade method with pyramids.
imgcodecs,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
imgcodecs,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
imgcodecs,A helper class for cv::DataType.
imgcodecs,The class is specialized for each fundamental numerical data type supported by OpenCV. It provides DataDepth<T>::value constant.
imgcodecs,Wrapping class for feature detection using the AGAST method. :
imgcodecs,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
imgcodecs,Image warper factories base class.
imgcodecs,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
imgcodecs,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
imgcodecs,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
imgcodecs,"Functions void cv::julia::initJulia (int argc, char **argv) "
imgcodecs,the GOTURN (Generic Object Tracking Using Regression Networks) tracker
imgcodecs,"GOTURN ([122]) is kind of trackers based on Convolutional Neural Networks (CNN). While taking all advantages of CNN trackers, GOTURN is much faster due to offline training without online fine-tuning nature. GOTURN tracker addresses the problem of single target tracking: given a bounding box label of an object in the first frame of the video, we track that object through the rest of the video. NOTE: Current method of GOTURN does not handle occlusions; however, it is fairly robust to viewpoint changes, lighting changes, and deformations. Inputs of GOTURN are two RGB patches representing Target and Search patches resized to 227x227. Outputs of GOTURN are predicted bounding box coordinates, relative to Search patch coordinate system, in format X1,Y1,X2,Y2. Original paper is here: http://davheld.github.io/GOTURN/GOTURN.pdf As long as original authors implementation: https://github.com/davheld/GOTURN#train-the-tracker Implementation of training algorithm is placed in separately here due to 3d-party dependencies: https://github.com/Auron-X/GOTURN_Training_Toolkit GOTURN architecture goturn.prototxt and trained model goturn.caffemodel are accessible on opencv_extra GitHub repository."
imgcodecs,used to iterate through sequences and mappings.
imgcodecs,"A standard STL notation, with node.begin(), node.end() denoting the beginning and the end of a sequence, stored in node. See the data reading sample in the beginning of the section."
imgcodecs,The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response.
imgcodecs,For more information see [227] .
imgcodecs,Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe [174] .
imgcodecs,Represents a modality operating over an image pyramid.
imgcodecs,Gaussian Mixture-based Background/Foreground Segmentation Algorithm.
imgcodecs,The class implements the Gaussian mixture model background subtraction described in [325] and [324] .
imgcodecs,Template sparse n-dimensional array class derived from SparseMat.
imgcodecs,SparseMat_ is a thin wrapper on top of SparseMat created in the same way as Mat_ . It simplifies notation of some operations:
imgcodecs,the inpainting algorithm
imgcodecs,"Enumerations enum { cv::INPAINT_NS = 0 , cv::INPAINT_TELEA = 1 } "
imgcodecs,x source X array y source Y array dst destination array len length of arrays
imgcodecs,"Functions int hal_ni_magnitude32f (const float *x, const float *y, float *dst, int len)  int hal_ni_magnitude64f (const double *x, const double *y, double *dst, int len) "
imgcodecs,This algorithm converts images to median threshold bitmaps (1 for pixels brighter than median luminance and 0 otherwise) and than aligns the resulting bitmaps using bit operations.
imgcodecs,"It is invariant to exposure, so exposure values and camera response are not necessary."
imgcodecs,In this implementation new image regions are filled with zeros.
imgcodecs,For more information see [295] .
imgcodecs,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
imgcodecs,"SoftFloat is a software implementation of floating-point calculations according to IEEE 754 standard. All calculations are done in integers, that's why they are machine-independent and bit-exact. This library can be useful in accuracy-critical parts like look-up tables generation, tests, etc. OpenCV contains a subset of SoftFloat partially rewritten to C++."
imgcodecs,finds arbitrary template in the grayscale image using Generalized Hough Transform
imgcodecs,"Pixels are weighted using contrast, saturation and well-exposedness measures, than images are combined using laplacian pyramids."
imgcodecs,"The resulting image weight is constructed as weighted average of contrast, saturation and well-exposedness measures."
imgcodecs,"The resulting image doesn't require tonemapping and can be converted to 8-bit image by multiplying by 255, but it's recommended to apply gamma correction and/or linear tonemapping."
imgcodecs,For more information see [189] .
imgcodecs,"Functions void cv::accumulate (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds an image to the accumulator image.  void cv::accumulateProduct (InputArray src1, InputArray src2, InputOutputArray dst, InputArray mask=noArray())  Adds the per-element product of two input images to the accumulator image.  void cv::accumulateSquare (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds the square of a source image to the accumulator image.  void cv::accumulateWeighted (InputArray src, InputOutputArray dst, double alpha, InputArray mask=noArray())  Updates a running average.  void cv::createHanningWindow (OutputArray dst, Size winSize, int type)  This function computes a Hanning window coefficients in two dimensions.  void cv::divSpectrums (InputArray a, InputArray b, OutputArray c, int flags, bool conjB=false)  Performs the per-element division of the first Fourier spectrum by the second Fourier spectrum.  Point2d cv::phaseCorrelate (InputArray src1, InputArray src2, InputArray window=noArray(), double *response=0)  The function is used to detect translational shifts that occur between two images. "
imgcodecs,Base class for background/foreground segmentation. :
imgcodecs,The class is only used to define the common interface for the whole family of background/foreground segmentation algorithms.
imgcodecs,Cylindrical warper factory class.
imgcodecs,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
imgcodecs,It provides easy interface to:
imgcodecs,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
imgcodecs,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
imgcodecs,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
imgcodecs,It is planned to have:
imgcodecs,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
imgcodecs,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
imgcodecs,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
imgcodecs,Classes class cv::DualQuat< _Tp >  class cv::Quat< _Tp >  class cv::QuatEnum 
imgcodecs,Performs Cholesky decomposition of matrix \(A = L*L^T\) and solves matrix equation \(A*X=B\).
imgcodecs,src1 pointer to input matrix \(A\) stored in row major order. After finish of work src1 contains lower triangular matrix \(L\). src1_step number of bytes between two consequent rows of matrix \(A\). m size of square matrix \(A\). src2 pointer to \(M\times N\) matrix \(B\) which is the right-hand side of system \(A*X=B\). B stored in row major order. If src2 is null pointer only Cholesky decomposition will be performed. After finish of work src2 contains solution \(X\) of system \(A*X=B\). src2_step number of bytes between two consequent rows of matrix \(B\). n number of right-hand vectors in \(M\times N\) matrix \(B\). info indicates success of decomposition. If *info is false decomposition failed.
imgcodecs,"Functions int hal_ni_Cholesky32f (float *src1, size_t src1_step, int m, float *src2, size_t src2_step, int n, bool *info)  int hal_ni_Cholesky64f (double *src1, size_t src1_step, int m, double *src2, size_t src2_step, int n, bool *info) "
imgcodecs,struct returned by cv::moments
imgcodecs,The spatial moments \(\texttt{Moments::m}_{ji}\) are computed as:
imgcodecs,"\[\texttt{m} _{ji}= \sum _{x,y} \left ( \texttt{array} (x,y) \cdot x^j \cdot y^i \right )\]"
imgcodecs,The central moments \(\texttt{Moments::mu}_{ji}\) are computed as:
imgcodecs,"\[\texttt{mu} _{ji}= \sum _{x,y} \left ( \texttt{array} (x,y) \cdot (x - \bar{x} )^j \cdot (y - \bar{y} )^i \right )\]"
imgcodecs,"where \((\bar{x}, \bar{y})\) is the mass center:"
imgcodecs,"\[\bar{x} = \frac{\texttt{m}_{10}}{\texttt{m}_{00}} , \; \bar{y} = \frac{\texttt{m}_{01}}{\texttt{m}_{00}}\]"
imgcodecs,The normalized central moments \(\texttt{Moments::nu}_{ij}\) are computed as:
imgcodecs,\[\texttt{nu} _{ji}= \frac{\texttt{mu}_{ji}}{\texttt{m}_{00}^{(i+j)/2+1}} .\]
imgcodecs,"The moments of a contour are defined in the same way but computed using the Green's formula (see http://en.wikipedia.org/wiki/Green_theorem). So, due to a limited raster resolution, the moments computed for a contour are slightly different from the moments computed for the same rasterized contour."
imgcodecs,Manages memory block shared by muliple buffers.
imgcodecs,"This class allows to allocate one large memory block and split it into several smaller non-overlapping buffers. In safe mode each buffer allocation will be performed independently, this mode allows dynamic memory access instrumentation using valgrind or memory sanitizer."
imgcodecs,Safe mode can be explicitly switched ON in constructor. It will also be enabled when compiling with memory sanitizer support or in runtime with the environment variable OPENCV_BUFFER_AREA_ALWAYS_SAFE.
imgcodecs,Example of usage:
imgcodecs,"Class implementing the AKAZE keypoint detector and descriptor extractor, described in [10]."
imgcodecs,AKAZE descriptors can only be used with KAZE or AKAZE keypoints. This class is thread-safe.
imgcodecs,Functions float32x2_t cv_vrecp_f32 (float32x2_t val)  float32x4_t cv_vrecpq_f32 (float32x4_t val)  int32x2_t cv_vrnd_s32_f32 (float32x2_t v)  uint32x2_t cv_vrnd_u32_f32 (float32x2_t v)  int32x4_t cv_vrndq_s32_f32 (float32x4_t v)  uint32x4_t cv_vrndq_u32_f32 (float32x4_t v)  float32x2_t cv_vrsqrt_f32 (float32x2_t val)  float32x4_t cv_vrsqrtq_f32 (float32x4_t val)  float32x2_t cv_vsqrt_f32 (float32x2_t val)  float32x4_t cv_vsqrtq_f32 (float32x4_t val) 
imgcodecs,This structure allows to customize the way how Fluid executes parallel regions.
imgcodecs,"For example, user can utilize his own threading runtime via this parameter. The parallel_for member functor is called by the Fluid runtime with the following arguments:"
imgcodecs,size Size of the parallel range to process f A function which should be called for every integer index in this range by the specified parallel_for implementation.
imgcodecs,This feature may be deprecated in the future releases.
imgcodecs,Base class for tonemapping algorithms - tools that are used to map HDR image to 8-bit range.
imgcodecs,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
imgcodecs,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
imgcodecs,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
imgcodecs,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
imgcodecs,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
imgcodecs,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
imgcodecs,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
imgcodecs,the Nano tracker is a super lightweight dnn-based general object tracking.
imgcodecs,"Nano tracker is much faster and extremely lightweight due to special model structure, the whole model size is about 1.9 MB. Nano tracker needs two models: one for feature extraction (backbone) and the another for localization (neckhead). Model download link: https://github.com/HonglinChu/SiamTrackers/tree/master/NanoTrack/models/nanotrackv2 Original repo is here: https://github.com/HonglinChu/NanoTrack Author: HongLinChu, 16284.nosp@m.6434.nosp@m.5@qq..nosp@m.com"
imgcodecs,"Enumerations enum cv::ImreadModes { cv::IMREAD_UNCHANGED = -1 , cv::IMREAD_GRAYSCALE = 0 , cv::IMREAD_COLOR_BGR = 1 , cv::IMREAD_COLOR = 1 , cv::IMREAD_ANYDEPTH = 2 , cv::IMREAD_ANYCOLOR = 4 , cv::IMREAD_LOAD_GDAL = 8 , cv::IMREAD_REDUCED_GRAYSCALE_2 = 16 , cv::IMREAD_REDUCED_COLOR_2 = 17 , cv::IMREAD_REDUCED_GRAYSCALE_4 = 32 , cv::IMREAD_REDUCED_COLOR_4 = 33 , cv::IMREAD_REDUCED_GRAYSCALE_8 = 64 , cv::IMREAD_REDUCED_COLOR_8 = 65 , cv::IMREAD_IGNORE_ORIENTATION = 128 , cv::IMREAD_COLOR_RGB = 256 }  Imread flags. More...  enum cv::ImwriteEXRCompressionFlags { cv::IMWRITE_EXR_COMPRESSION_NO = 0 , cv::IMWRITE_EXR_COMPRESSION_RLE = 1 , cv::IMWRITE_EXR_COMPRESSION_ZIPS = 2 , cv::IMWRITE_EXR_COMPRESSION_ZIP = 3 , cv::IMWRITE_EXR_COMPRESSION_PIZ = 4 , cv::IMWRITE_EXR_COMPRESSION_PXR24 = 5 , cv::IMWRITE_EXR_COMPRESSION_B44 = 6 , cv::IMWRITE_EXR_COMPRESSION_B44A = 7 , cv::IMWRITE_EXR_COMPRESSION_DWAA = 8 , cv::IMWRITE_EXR_COMPRESSION_DWAB = 9 }  enum cv::ImwriteEXRTypeFlags { cv::IMWRITE_EXR_TYPE_HALF = 1 , cv::IMWRITE_EXR_TYPE_FLOAT = 2 }  enum cv::ImwriteFlags { cv::IMWRITE_JPEG_QUALITY = 1 , cv::IMWRITE_JPEG_PROGRESSIVE = 2 , cv::IMWRITE_JPEG_OPTIMIZE = 3 , cv::IMWRITE_JPEG_RST_INTERVAL = 4 , cv::IMWRITE_JPEG_LUMA_QUALITY = 5 , cv::IMWRITE_JPEG_CHROMA_QUALITY = 6 , cv::IMWRITE_JPEG_SAMPLING_FACTOR = 7 , cv::IMWRITE_PNG_COMPRESSION = 16 , cv::IMWRITE_PNG_STRATEGY = 17 , cv::IMWRITE_PNG_BILEVEL = 18 , cv::IMWRITE_PXM_BINARY = 32 , cv::IMWRITE_EXR_TYPE = (3 << 4) + 0 , cv::IMWRITE_EXR_COMPRESSION = (3 << 4) + 1 , cv::IMWRITE_EXR_DWA_COMPRESSION_LEVEL = (3 << 4) + 2 , cv::IMWRITE_WEBP_QUALITY = 64 , cv::IMWRITE_HDR_COMPRESSION = (5 << 4) + 0 , cv::IMWRITE_PAM_TUPLETYPE = 128 , cv::IMWRITE_TIFF_RESUNIT = 256 , cv::IMWRITE_TIFF_XDPI = 257 , cv::IMWRITE_TIFF_YDPI = 258 , cv::IMWRITE_TIFF_COMPRESSION = 259 , cv::IMWRITE_TIFF_ROWSPERSTRIP = 278 , cv::IMWRITE_TIFF_PREDICTOR = 317 , cv::IMWRITE_JPEG2000_COMPRESSION_X1000 = 272 , cv::IMWRITE_AVIF_QUALITY = 512 , cv::IMWRITE_AVIF_DEPTH = 513 , cv::IMWRITE_AVIF_SPEED = 514 }  Imwrite flags. More...  enum cv::ImwriteHDRCompressionFlags { cv::IMWRITE_HDR_COMPRESSION_NONE = 0 , cv::IMWRITE_HDR_COMPRESSION_RLE = 1 }  Imwrite HDR specific values for IMWRITE_HDR_COMPRESSION parameter key. More...  enum cv::ImwriteJPEGSamplingFactorParams { cv::IMWRITE_JPEG_SAMPLING_FACTOR_411 = 0x411111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_420 = 0x221111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_422 = 0x211111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_440 = 0x121111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_444 = 0x111111 }  enum cv::ImwritePAMFlags { cv::IMWRITE_PAM_FORMAT_NULL = 0 , cv::IMWRITE_PAM_FORMAT_BLACKANDWHITE = 1 , cv::IMWRITE_PAM_FORMAT_GRAYSCALE = 2 , cv::IMWRITE_PAM_FORMAT_GRAYSCALE_ALPHA = 3 , cv::IMWRITE_PAM_FORMAT_RGB = 4 , cv::IMWRITE_PAM_FORMAT_RGB_ALPHA = 5 }  Imwrite PAM specific tupletype flags used to define the 'TUPLETYPE' field of a PAM file. More...  enum cv::ImwritePNGFlags { cv::IMWRITE_PNG_STRATEGY_DEFAULT = 0 , cv::IMWRITE_PNG_STRATEGY_FILTERED = 1 , cv::IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY = 2 , cv::IMWRITE_PNG_STRATEGY_RLE = 3 , cv::IMWRITE_PNG_STRATEGY_FIXED = 4 }  Imwrite PNG specific flags used to tune the compression algorithm. More...  enum cv::ImwriteTiffCompressionFlags { cv::IMWRITE_TIFF_COMPRESSION_NONE = 1 , cv::IMWRITE_TIFF_COMPRESSION_CCITTRLE = 2 , cv::IMWRITE_TIFF_COMPRESSION_CCITTFAX3 = 3 , cv::IMWRITE_TIFF_COMPRESSION_CCITT_T4 = 3 , cv::IMWRITE_TIFF_COMPRESSION_CCITTFAX4 = 4 , cv::IMWRITE_TIFF_COMPRESSION_CCITT_T6 = 4 , cv::IMWRITE_TIFF_COMPRESSION_LZW = 5 , cv::IMWRITE_TIFF_COMPRESSION_OJPEG = 6 , cv::IMWRITE_TIFF_COMPRESSION_JPEG = 7 , cv::IMWRITE_TIFF_COMPRESSION_T85 = 9 , cv::IMWRITE_TIFF_COMPRESSION_T43 = 10 , cv::IMWRITE_TIFF_COMPRESSION_NEXT = 32766 , cv::IMWRITE_TIFF_COMPRESSION_CCITTRLEW = 32771 , cv::IMWRITE_TIFF_COMPRESSION_PACKBITS = 32773 , cv::IMWRITE_TIFF_COMPRESSION_THUNDERSCAN = 32809 , cv::IMWRITE_TIFF_COMPRESSION_IT8CTPAD = 32895 , cv::IMWRITE_TIFF_COMPRESSION_IT8LW = 32896 , cv::IMWRITE_TIFF_COMPRESSION_IT8MP = 32897 , cv::IMWRITE_TIFF_COMPRESSION_IT8BL = 32898 , cv::IMWRITE_TIFF_COMPRESSION_PIXARFILM = 32908 , cv::IMWRITE_TIFF_COMPRESSION_PIXARLOG = 32909 , cv::IMWRITE_TIFF_COMPRESSION_DEFLATE = 32946 , cv::IMWRITE_TIFF_COMPRESSION_ADOBE_DEFLATE = 8 , cv::IMWRITE_TIFF_COMPRESSION_DCS = 32947 , cv::IMWRITE_TIFF_COMPRESSION_JBIG = 34661 , cv::IMWRITE_TIFF_COMPRESSION_SGILOG = 34676 , cv::IMWRITE_TIFF_COMPRESSION_SGILOG24 = 34677 , cv::IMWRITE_TIFF_COMPRESSION_JP2000 = 34712 , cv::IMWRITE_TIFF_COMPRESSION_LERC = 34887 , cv::IMWRITE_TIFF_COMPRESSION_LZMA = 34925 , cv::IMWRITE_TIFF_COMPRESSION_ZSTD = 50000 , cv::IMWRITE_TIFF_COMPRESSION_WEBP = 50001 , cv::IMWRITE_TIFF_COMPRESSION_JXL = 50002 }  enum cv::ImwriteTiffPredictorFlags { cv::IMWRITE_TIFF_PREDICTOR_NONE = 1 , cv::IMWRITE_TIFF_PREDICTOR_HORIZONTAL = 2 , cv::IMWRITE_TIFF_PREDICTOR_FLOATINGPOINT = 3 } "
imgcodecs,Add: dst[i] = src1[i] + src2[i] Sub: dst[i] = src1[i] - src2[i]
imgcodecs,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
imgcodecs,"Functions int hal_ni_add16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_add16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_add32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_add32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_add64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_add8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_add8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8s32f (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_sub8u32f (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height) "
imgcodecs,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
imgcodecs,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
imgcodecs,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
imgcodecs,This module has been originally developed as a project for Google Summer of Code 2012-2015.
imgcodecs,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
imgcodecs,STL namespace.
imgcodecs,Automatically Allocated Buffer Class.
imgcodecs,"The class is used for temporary buffers in functions and methods. If a temporary buffer is usually small (a few K's of memory), but its size depends on the parameters, it makes sense to create a small fixed-size array on stack and use it if it's large enough. If the required buffer size is larger than the fixed size, another buffer of sufficient size is allocated dynamically and released after the processing. Therefore, in typical cases, when the buffer size is small, there is no overhead associated with malloc()/free(). At the same time, there is no limit on the size of processed data."
imgcodecs,This is what AutoBuffer does. The template takes 2 parameters - type of the buffer elements and the number of stack-allocated elements. Here is how the class is used:
imgcodecs,Implementation of the Shape Context descriptor and matching algorithm.
imgcodecs,"proposed by Belongie et al. in ""Shape Matching and Object Recognition Using Shape Contexts"" (PAMI 2002). This implementation is packaged in a generic scheme, in order to allow you the implementation of the common variations of the original pipeline."
imgcodecs,"Class for video capturing from video files, image sequences or cameras."
imgcodecs,The class provides C++ API for capturing video from cameras or for reading video files and image sequences.
imgcodecs,Here is how the class can be used:
imgcodecs,(C++) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/cpp/videocapture_starter.cpp (Python) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/python/video.py (Python) A multi threaded video processing sample can be found at OPENCV_SOURCE_CODE/samples/python/video_threaded.py (Python) VideoCapture sample showcasing some features of the Video4Linux2 backend OPENCV_SOURCE_CODE/samples/python/video_v4l2.py
imgcodecs,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
imgcodecs,The distortion-free projective transformation given by a pinhole camera model is shown below.
imgcodecs,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
imgcodecs,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
imgcodecs,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
imgcodecs,\[p = A P_c.\]
imgcodecs,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
imgcodecs,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
imgcodecs,and thus
imgcodecs,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
imgcodecs,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
imgcodecs,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
imgcodecs,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
imgcodecs,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
imgcodecs,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
imgcodecs,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
imgcodecs,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
imgcodecs,and therefore
imgcodecs,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
imgcodecs,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
imgcodecs,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
imgcodecs,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
imgcodecs,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
imgcodecs,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
imgcodecs,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
imgcodecs,with
imgcodecs,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
imgcodecs,The following figure illustrates the pinhole camera model.
imgcodecs,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
imgcodecs,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
imgcodecs,where
imgcodecs,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
imgcodecs,with
imgcodecs,\[r^2 = x'^2 + y'^2\]
imgcodecs,and
imgcodecs,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
imgcodecs,if \(Z_c \ne 0\).
imgcodecs,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
imgcodecs,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
imgcodecs,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
imgcodecs,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
imgcodecs,where
imgcodecs,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
imgcodecs,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
imgcodecs,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
imgcodecs,In the functions below the coefficients are passed or returned as
imgcodecs,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
imgcodecs,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
imgcodecs,The functions below use the above model to do the following:
imgcodecs,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
imgcodecs,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
imgcodecs,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
imgcodecs,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
imgcodecs,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
imgcodecs,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
imgcodecs,if \(W \ne 0\).
imgcodecs,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
imgcodecs,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
imgcodecs,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
imgcodecs,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
imgcodecs,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
imgcodecs,This structure allows to control the output image region which Fluid backend will produce in the graph.
imgcodecs,"This feature is useful for external tiling and parallelism, but will be deprecated in the future releases."
imgcodecs,the VIT tracker is a super lightweight dnn-based general object tracking.
imgcodecs,"VIT tracker is much faster and extremely lightweight due to special model structure, the model file is about 767KB. Model download link: https://github.com/opencv/opencv_zoo/tree/main/models/object_tracking_vittrack Author: PengyuLiu, 18729.nosp@m.1850.nosp@m.7@qq..nosp@m.com"
imgcodecs,Template Read-Only Sparse Matrix Iterator Class.
imgcodecs,This is the derived from SparseMatConstIterator class that introduces more convenient operator *() for accessing the current element.
imgcodecs,Compilation arguments: data structures controlling the compilation process.
imgcodecs,"G-API comes with a number of graph compilation options which can be passed to cv::GComputation::apply() or cv::GComputation::compile(). Known compilation options are listed in this page, while extra backends may introduce their own compilation options (G-API transparently accepts everything which can be passed to cv::compile_args(), it depends on underlying backends if an option would be interpreted or not)."
imgcodecs,"For example, if an example computation is executed like this:"
imgcodecs,Extra parameter specifying which kernels to compile with can be passed like this:
imgcodecs,Namespaces namespace cv::gapi 
imgcodecs,Ask G-API to dump compiled graph in Graphviz format under the given file name.
imgcodecs,Specifies a graph dump path (path to .dot file to be generated). G-API will dump a .dot file under specified path during a compilation process if this flag is passed.
imgcodecs,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
imgcodecs,Extra G-API data structures used to pass input/output data to the graph for processing.
imgcodecs,Classes class cv::MediaFrame::IAdapter  An interface class for MediaFrame data adapters. More...  class cv::MediaFrame  cv::MediaFrame class represents an image/media frame obtained from an external source. More...  class cv::RMat  class cv::MediaFrame::View  Provides access to the MediaFrame's underlying data. More... 
imgcodecs,The class defining termination criteria for iterative algorithms.
imgcodecs,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
imgcodecs,"ArUco Marker Detection, module functionality was moved to objdetect module"
imgcodecs,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
imgcodecs,a Class to measure passing time.
imgcodecs,"The class computes passing time by counting the number of ticks per second. That is, the following code computes the execution time in seconds:"
imgcodecs,It is also possible to compute the average time over multiple runs:
imgcodecs,GFrame class represents an image or media frame in the graph.
imgcodecs,"GFrame doesn't store any data itself, instead it describes a functional relationship between operations consuming and producing GFrame objects."
imgcodecs,"GFrame is introduced to handle various media formats (e.g., NV12 or I420) under the same type. Various image formats may differ in the number of planes (e.g. two for NV12, three for I420) and the pixel layout inside. GFrame type allows to handle these media formats in the graph uniformly the graph structure will not change if the media format changes, e.g. a different camera or decoder is used with the same graph. G-API provides a number of operations which operate directly on GFrame, like infer<>() or renderFrame(); these operations are expected to handle different media formats inside. There is also a number of accessor operations like BGR(), Y(), UV() these operations provide access to frame's data in the familiar cv::GMat form, which can be used with the majority of the existing G-API operations. These accessor functions may perform color space conversion on the fly if the image format of the GFrame they are applied to differs from the operation's semantic (e.g. the BGR() accessor is called on an NV12 image frame)."
imgcodecs,GFrame is a virtual counterpart of cv::MediaFrame.
imgcodecs,GScalar class represents cv::Scalar data in the graph.
imgcodecs,"GScalar may be associated with a cv::Scalar value, which becomes its constant value bound in graph compile time. cv::GScalar describes a functional relationship between operations consuming and producing GScalar objects."
imgcodecs,"GScalar is a virtual counterpart of cv::Scalar, which is usually used to represent the GScalar data in G-API during the execution."
imgcodecs,opencv2/gapi/gscalar.hpp
imgcodecs,Simple TLS data class.
imgcodecs,"Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige."
imgcodecs,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
imgcodecs,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
imgcodecs,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
imgcodecs,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
imgcodecs,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
imgcodecs,"GComputation class represents a captured computation graph. GComputation objects form boundaries for expression code user writes with G-API, allowing to compile and execute it."
imgcodecs,"G-API computations are defined with input/output data objects. G-API will track automatically which operations connect specified outputs to the inputs, forming up a call graph to be executed. The below example expresses calculation of Sobel operator for edge detection ( \(G = \sqrt{G_x^2 + G_y^2}\)):"
imgcodecs,Full pipeline can be now captured with this object declaration:
imgcodecs,Input/output data objects on which a call graph should be reconstructed are passed using special wrappers cv::GIn and cv::GOut. G-API will track automatically which operations form a path from inputs to outputs and build the execution graph appropriately.
imgcodecs,"Note that cv::GComputation doesn't take ownership on data objects it is defined. Moreover, multiple GComputation objects may be defined on the same expressions, e.g. a smaller pipeline which expects that image gradients are already pre-calculated may be defined like this:"
imgcodecs,"The resulting graph would expect two inputs and produce one output. In this case, it doesn't matter if gx/gy data objects are results of cv::gapi::Sobel operators G-API will stop unrolling expressions and building the underlying graph one reaching this data objects."
imgcodecs,"The way how GComputation is defined is important as its definition specifies graph protocol the way how the graph should be used. Protocol is defined by number of inputs, number of outputs, and shapes of inputs and outputs."
imgcodecs,"In the above example, sobelEdge expects one Mat on input and produces one Mat; while sobelEdgeSub expects two Mats on input and produces one Mat. GComputation's protocol defines how other computation methods should be used cv::GComputation::compile() and cv::GComputation::apply(). For example, if a graph is defined on two GMat inputs, two cv::Mat objects have to be passed to apply() for execution. GComputation checks protocol correctness in runtime so passing a different number of objects in apply() or passing cv::Scalar instead of cv::Mat there would compile well as a C++ source but raise an exception in run-time. G-API also comes with a typed wrapper cv::GComputationT<> which introduces this type-checking in compile-time."
imgcodecs,"cv::GComputation itself is a thin object which just captures what the graph is. The compiled graph (which actually process data) is represented by class GCompiled. Use compile() method to generate a compiled graph with given compile options. cv::GComputation can also be used to process data with implicit graph compilation on-the-fly, see apply() for details."
imgcodecs,"GComputation is a reference-counted object once defined, all its copies will refer to the same instance."
imgcodecs,The algorithms in this section minimize or maximize function value within specified constraints or without any constraints.
imgcodecs,"Classes class cv::ConjGradSolver  This class is used to perform the non-linear non-constrained minimization of a function with known gradient,. More...  class cv::DownhillSolver  This class is used to perform the non-linear non-constrained minimization of a function,. More...  class cv::MinProblemSolver  Basic interface for all solvers. More... "
imgcodecs,Abstract streaming pipeline source.
imgcodecs,Implement this interface if you want customize the way how data is streaming into GStreamingCompiled.
imgcodecs,Objects implementing this interface can be passed to GStreamingCompiled using setSource() with cv::gin(). Regular compiled graphs (GCompiled) don't support input objects of this type.
imgcodecs,"Default cv::VideoCapture-based implementation is available, see cv::gapi::wip::GCaptureSource."
imgcodecs,DNN-based face recognizer.
imgcodecs,model download link: https://github.com/opencv/opencv_zoo/tree/master/models/face_recognition_sface
imgcodecs,"Class for implementing the wrapper which makes detectors and extractors to be affine invariant, described as ASIFT in [312] ."
imgcodecs,"Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. Objective function is constructed using pixel values on the same position in all images, extra term is added to make the result smoother."
imgcodecs,For more information see [68] .
imgcodecs,Template class specifying a continuous subsequence (slice) of a sequence.
imgcodecs,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
imgcodecs,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
imgcodecs,Quaternion is a number system that extends the complex numbers. It can be expressed as a rotation in three-dimensional space. A quaternion is generally represented in the form:
imgcodecs,\[q = w + x\boldsymbol{i} + y\boldsymbol{j} + z\boldsymbol{k}\]
imgcodecs,"\[q = [w, x, y, z]\]"
imgcodecs,"\[q = [w, \boldsymbol{v}] \]"
imgcodecs,"\[q = ||q||[\cos\psi, u_x\sin\psi,u_y\sin\psi, u_z\sin\psi].\]"
imgcodecs,"\[q = ||q||[\cos\psi, \boldsymbol{u}\sin\psi]\]"
imgcodecs,"where \(\psi = \frac{\theta}{2}\), \(\theta\) represents rotation angle, \(\boldsymbol{u} = [u_x, u_y, u_z]\) represents normalized rotation axis, and \(||q||\) represents the norm of \(q\)."
imgcodecs,"A unit quaternion is usually represents rotation, which has the form:"
imgcodecs,"\[q = [\cos\psi, u_x\sin\psi,u_y\sin\psi, u_z\sin\psi].\]"
imgcodecs,"To create a quaternion representing the rotation around the axis \(\boldsymbol{u}\) with angle \(\theta\), you can use"
imgcodecs,You can simply use four same type number to create a quaternion
imgcodecs,Or use a Vec4d or Vec4f vector.
imgcodecs,"If you already have a 3x3 rotation matrix R, then you can use"
imgcodecs,"If you already have a rotation vector rvec which has the form of angle * axis, then you can use"
imgcodecs,"To extract the rotation matrix from quaternion, see toRotMat3x3()"
imgcodecs,"To extract the Vec4d or Vec4f, see toVec()"
imgcodecs,"To extract the rotation vector, see toRotVec()"
imgcodecs,"If there are two quaternions \(q_0, q_1\) are needed to interpolate, you can use nlerp(), slerp() or spline()"
imgcodecs,spline can smoothly connect rotations of multiple quaternions
imgcodecs,Three ways to get an element in Quaternion
imgcodecs,Adaptive logarithmic mapping is a fast global tonemapping algorithm that scales the image in logarithmic domain.
imgcodecs,"Since it's a global operator the same function is applied to all the pixels, it is controlled by the bias parameter."
imgcodecs,Optional saturation enhancement is possible as described in [84] .
imgcodecs,For more information see [71] .
imgcodecs,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
imgcodecs,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
imgcodecs,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
imgcodecs,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
imgcodecs,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
imgcodecs,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
imgcodecs,XML/YAML/JSON file storage class that encapsulates all the information necessary for writing or reading data to/from a file.
imgcodecs,An EMD-L1 based cost extraction. :
imgcodecs,opencv2/shape/hist_cost.hpp
imgcodecs,Computes weighted sum of two arrays using formula: dst[i] = a * src1[i] + b * src2[i] + c
imgcodecs,"src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images scalars numbers a, b, and c"
imgcodecs,"Functions int hal_ni_addWeighted16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height, const double scalars[3])  int hal_ni_addWeighted8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, const double scalars[3]) "
imgcodecs,Hamming norm of a vector
imgcodecs,"a pointer to vector data n length of a vector cellSize how many bits of the vector will be added and treated as a single bit, can be 1 (standard Hamming distance), 2 or 4 result pointer to result output"
imgcodecs,Hamming distance between two vectors
imgcodecs,"a pointer to first vector data b pointer to second vector data n length of vectors cellSize how many bits of the vectors will be added and treated as a single bit, can be 1 (standard Hamming distance), 2 or 4 result pointer to result output"
imgcodecs,"Functions int hal_ni_normHamming8u (const uchar *a, int n, int cellSize, int *result)  int hal_ni_normHammingDiff8u (const uchar *a, const uchar *b, int n, int cellSize, int *result) "
imgcodecs,To read multi-page images on demand.
imgcodecs,The ImageCollection class provides iterator API to read multi-page images on demand. Create iterator to the collection of the images and iterate over the collection. Decode the necessary page with operator*.
imgcodecs,"The performance of page decoding is O(1) if collection is increment sequentially. If the user wants to access random page, then the time Complexity is O(n) because the collection has to be reinitialized every time in order to go to the correct page. However, the intermediate pages are not decoded during the process, so typically it's quite fast. This is required because multi-page codecs does not support going backwards. After decoding the one page, it is stored inside the collection cache. Hence, trying to get Mat object from already decoded page is O(1). If you need memory, you can use .releaseCache() method to release cached index. The space complexity is O(n) if all pages are decoded into memory. The user is able to decode and release images on demand."
imgcodecs,A complex number class.
imgcodecs,"The template class is similar and compatible with std::complex, however it provides slightly more convenient access to the real and imaginary parts using through the simple field access, as opposite to std::complex::real() and std::complex::imag()."
imgcodecs,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
imgcodecs,Performs QR decomposition of \(M\times N\)( \(M>N\)) matrix \(A = Q*R\) and solves matrix equation \(A*X=B\).
imgcodecs,"src1 pointer to input matrix \(A\) stored in row major order. After finish of work src1 contains upper triangular \(N\times N\) matrix \(R\). Lower triangle of src1 will be filled with vectors of elementary reflectors. See [284] and Lapack's DGEQRF documentation for details. src1_step number of bytes between two consequent rows of matrix \(A\). m number fo rows in matrix \(A\). n number of columns in matrix \(A\). k number of right-hand vectors in \(M\times K\) matrix \(B\). src2 pointer to \(M\times K\) matrix \(B\) which is the right-hand side of system \(A*X=B\). \(B\) stored in row major order. If src2 is null pointer only QR decomposition will be performed. Otherwise system will be solved and src1 will be used as temporary buffer, so after finish of work src2 contains solution \(X\) of system \(A*X=B\). src2_step number of bytes between two consequent rows of matrix \(B\). dst pointer to continiuos \(N\times 1\) array for scalar factors of elementary reflectors. See [284] for details. info indicates success of decomposition. If *info is zero decomposition failed."
imgcodecs,"Functions int hal_ni_QR32f (float *src1, size_t src1_step, int m, int n, int k, float *src2, size_t src2_step, float *dst, int *info)  int hal_ni_QR64f (double *src1, size_t src1_step, int m, int n, int k, double *src2, size_t src2_step, double *dst, int *info) "
imgcodecs,Abstract base class for matching keypoint descriptors.
imgcodecs,It has two groups of match methods: for matching descriptors of an image with another image or with an image set.
imgcodecs,Mersenne Twister random number generator.
imgcodecs,Inspired by http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/CODES/mt19937ar.c
imgcodecs,Template class for 2D points specified by its coordinates x and y.
imgcodecs,"An instance of the class is interchangeable with C structures, CvPoint and CvPoint2D32f . There is also a cast operator to convert point coordinates to the specified type. The conversion from floating-point coordinates to integer coordinates is done by rounding. Commonly, the conversion uses this operation for each of the coordinates. Besides the class members listed in the declaration above, the following operations on points are implemented:"
imgcodecs,"For your convenience, the following type aliases are defined:"
imgcodecs,Example:
imgcodecs,Linear Discriminant Analysis.
imgcodecs,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
imgcodecs,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
imgcodecs,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
imgcodecs,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
imgcodecs,"Each class derived from Map implements a motion model, as follows:"
imgcodecs,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
imgcodecs,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
imgcodecs,The classes derived from Mapper are
imgcodecs,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
imgcodecs,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
imgcodecs,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
imgcodecs,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
imgcodecs,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
imgcodecs,Line segment detector class.
imgcodecs,following the algorithm described at [290] .
imgcodecs,This module provides storage routines for Hierarchical Data Format objects.
imgcodecs,Face module changelog Face Recognition with OpenCV
imgcodecs,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
imgcodecs,K-nearest neighbours - based Background/Foreground Segmentation Algorithm.
imgcodecs,The class implements the K-nearest neighbours background subtraction described in [324] . Very efficient if number of foreground pixels is low.
imgcodecs,Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor.
imgcodecs,"described in [229] . The algorithm uses FAST in pyramids to detect stable keypoints, selects the strongest features using FAST or Harris response, finds their orientation using first-order moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or k-tuples) are rotated according to the measured orientation)."
imgcodecs,The class represents rotated (i.e. not up-right) rectangles on a plane.
imgcodecs,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
imgcodecs,The sample below demonstrates how to use RotatedRect:
imgcodecs,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
imgcodecs,Classes class cv::plot::Plot2d 
imgcodecs,This class is a typed wrapper over a regular GComputation.
imgcodecs,"std::function<>-like template parameter specifies the graph signature so methods so the object's constructor, methods like apply() and the derived GCompiledT::operator() also become typed."
imgcodecs,"There is no need to use cv::gin() or cv::gout() modifiers with objects of this class. Instead, all input arguments are followed by all output arguments in the order from the template argument signature."
imgcodecs,Refer to the following example. Regular (untyped) code is written this way:
imgcodecs,Here:
imgcodecs,"cv::GComputation object is created with a lambda constructor where it is defined as a two-input, one-output graph. Its method apply() in fact takes arbitrary number of arguments (as vectors) so user can pass wrong number of inputs/outputs here. C++ compiler wouldn't notice that since the cv::GComputation API is polymorphic, and only a run-time error will be generated."
imgcodecs,Now the same code written with typed API:
imgcodecs,The key difference is:
imgcodecs,Now the constructor lambda must take parameters and must return values as defined in the GComputationT<> signature. Its method apply() does not require any extra specifiers to separate input arguments from the output ones A GCompiledT (compilation product) takes input/output arguments with no extra specifiers as well.
imgcodecs,opencv2/gapi/gtyped.hpp
imgcodecs,Definition of the transformation.
imgcodecs,"occupied in the paper ""Principal Warps: Thin-Plate Splines and Decomposition of Deformations"", by F.L. Bookstein (PAMI 1989). :"
imgcodecs,"Minimum: dst[i] = min(src1[i], src2[i]) Maximum: dst[i] = max(src1[i], src2[i])"
imgcodecs,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images
imgcodecs,"Functions int hal_ni_max16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_max16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_max32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_max32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_max64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_max8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_max8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height)  int hal_ni_min16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height)  int hal_ni_min16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height)  int hal_ni_min32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height)  int hal_ni_min32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height)  int hal_ni_min64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height)  int hal_ni_min8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height)  int hal_ni_min8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height) "
imgcodecs,Classes class cv::quality::QualityBase 
imgcodecs,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
imgcodecs,finds arbitrary template in the grayscale image using Generalized Hough Transform
imgcodecs,Detects position only without translation and rotation [15] .
imgcodecs,"This figure explains new functionality implemented with Qt* GUI. The new GUI provides a statusbar, a toolbar, and a control panel. The control panel can have trackbars and buttonbars attached to it. If you cannot see the control panel, press Ctrl+P or right-click any Qt window and select Display properties window."
imgcodecs,"To attach a trackbar, the window name parameter must be NULL. To attach a buttonbar, a button must be created. If the last bar attached to the control panel is a buttonbar, the new button is added to the right of the last button. If the last bar attached to the control panel is a trackbar, or the control panel is empty, a new buttonbar is created. Then, a new button is attached to it."
imgcodecs,See below the example used to generate the figure:
imgcodecs,Classes struct cv::QtFont  QtFont available only for Qt. See cv::fontQt. More... 
imgcodecs,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
imgcodecs,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
imgcodecs,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
imgcodecs,Classes class cv::DenseOpticalFlow  class cv::DISOpticalFlow  DIS optical flow algorithm. More...  class cv::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::KalmanFilter  Kalman filter class. More...  class cv::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More...  class cv::Tracker  Base abstract class for the long-term tracker. More...  class cv::TrackerDaSiamRPN  class cv::TrackerGOTURN  the GOTURN (Generic Object Tracking Using Regression Networks) tracker More...  class cv::TrackerMIL  The MIL algorithm trains a classifier in an online manner to separate the object from the background. More...  class cv::TrackerNano  the Nano tracker is a super lightweight dnn-based general object tracking. More...  class cv::TrackerVit  the VIT tracker is a super lightweight dnn-based general object tracking. More...  class cv::VariationalRefinement  Variational optical flow refinement. More... 
imgcodecs,TLS data accumulator with gathering methods.
imgcodecs,"Functions void CGImageToMat (const CGImageRef image, cv::Mat &m, bool alphaExist=false)  CGImageRef MatToCGImage (const cv::Mat &image) CF_RETURNS_RETAINED  NSImage * MatToNSImage (const cv::Mat &image)  void NSImageToMat (const NSImage *image, cv::Mat &m, bool alphaExist=false) "
imgcodecs,Wrapper class for the OpenCV Affine Transformation algorithm. :
imgcodecs,Namespaces namespace cv::traits 
imgcodecs,Abstract base class for shape distance algorithms.
imgcodecs,Class for iterating over all pixels on a raster line segment.
imgcodecs,"The class LineIterator is used to get each pixel of a raster line connecting two specified points. It can be treated as a versatile implementation of the Bresenham algorithm where you can stop at each pixel and do some extra processing, for example, grab pixel values along the line or draw a line with an effect (for example, with XOR operation)."
imgcodecs,The number of pixels along the line is stored in LineIterator::count. The method LineIterator::pos returns the current position in the image:
imgcodecs,Flann-based descriptor matcher.
imgcodecs,"This matcher trains cv::flann::Index on a train descriptor collection and calls its nearest search methods to find the best matches. So, this matcher may be faster when matching a large train collection than the brute force matcher. FlannBasedMatcher does not support masking permissible matches of descriptor sets because flann::Index does not support this. :"
imgcodecs,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
imgcodecs,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
imgcodecs,"Hardware acceleration support enum cv::VideoAccelerationType { cv::VIDEO_ACCELERATION_NONE = 0 , cv::VIDEO_ACCELERATION_ANY = 1 , cv::VIDEO_ACCELERATION_D3D11 = 2 , cv::VIDEO_ACCELERATION_VAAPI = 3 , cv::VIDEO_ACCELERATION_MFX = 4 }  Video Acceleration type. More... "
imgcodecs,Namespace for all functions is cv::intensity_transform.
imgcodecs,"Enumerations enum { cv::OPEN_CAMERA = 300 , cv::CLOSE_CAMERA , cv::UPDATE_IMAGE_ELEMENT , cv::SHOW_TRACKBAR } "
imgcodecs,Classes class cv::LineSegmentDetector  Line segment detector class. More... 
imgcodecs,Performs singular value decomposition of \(M\times N\)( \(M>N\)) matrix \(A = U*\Sigma*V^T\).
imgcodecs,"src pointer to input \(M\times N\) matrix \(A\) stored in column major order. After finish of work src will be filled with rows of \(U\) or not modified (depends of flag CV_HAL_SVD_MODIFY_A). src_step number of bytes between two consequent columns of matrix \(A\). w pointer to array for singular values of matrix \(A\) (i. e. first \(N\) diagonal elements of matrix \(\Sigma\)). u pointer to output \(M\times N\) or \(M\times M\) matrix \(U\) (size depends of flags). Pointer must be valid if flag CV_HAL_SVD_MODIFY_A not used. u_step number of bytes between two consequent rows of matrix \(U\). vt pointer to array for \(N\times N\) matrix \(V^T\). vt_step number of bytes between two consequent rows of matrix \(V^T\). m number fo rows in matrix \(A\). n number of columns in matrix \(A\). flags algorithm options (combination of CV_HAL_SVD_FULL_UV, ...)."
imgcodecs,"Functions int hal_ni_SVD32f (float *src, size_t src_step, float *w, float *u, size_t u_step, float *vt, size_t vt_step, int m, int n, int flags)  int hal_ni_SVD64f (double *src, size_t src_step, double *w, double *u, size_t u_step, double *vt, size_t vt_step, int m, int n, int flags) "
imgcodecs,Divide: dst[i] = scale * src1[i] / src2[i]
imgcodecs,src1_data first source image data and step src1_step first source image data and step src2_data second source image data and step src2_step second source image data and step dst_data destination image data and step dst_step destination image data and step width dimensions of the images height dimensions of the images scale additional multiplier
imgcodecs,"Functions int hal_ni_div16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_div8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, double scale) "
imgcodecs,An Chi based cost extraction. :
imgcodecs,opencv2/shape/hist_cost.hpp
imgcodecs,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
imgcodecs,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
imgcodecs,Kalman filter class.
imgcodecs,"The class implements a standard Kalman filter http://en.wikipedia.org/wiki/Kalman_filter, [297] . However, you can modify transitionMatrix, controlMatrix, and measurementMatrix to get an extended Kalman filter functionality."
imgcodecs,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
imgcodecs,Affine transform.
imgcodecs,It represents a 4x4 homogeneous transformation matrix \(T\)
imgcodecs,\[T = \begin{bmatrix} R & t\\ 0 & 1\\ \end{bmatrix} \]
imgcodecs,where \(R\) is a 3x3 rotation matrix and \(t\) is a 3x1 translation vector.
imgcodecs,"You can specify \(R\) either by a 3x3 rotation matrix or by a 3x1 rotation vector, which is converted to a 3x3 rotation matrix by the Rodrigues formula."
imgcodecs,"To construct a matrix \(T\) representing first rotation around the axis \(r\) with rotation angle \(|r|\) in radian (right hand rule) and then translation by the vector \(t\), you can use"
imgcodecs,"If you already have the rotation matrix \(R\), then you can use"
imgcodecs,"To extract the rotation matrix \(R\) from \(T\), use"
imgcodecs,"To extract the translation vector \(t\) from \(T\), use"
imgcodecs,"To extract the rotation vector \(r\) from \(T\), use"
imgcodecs,Note that since the mapping from rotation vectors to rotation matrices is many to one. The returned rotation vector is not necessarily the one you used before to set the matrix.
imgcodecs,"If you have two transformations \(T = T_1 * T_2\), use"
imgcodecs,"To get the inverse transform of \(T\), use"
imgcodecs,Video writer class.
imgcodecs,The class provides C++ API for writing video files or image sequences.
imgcodecs,The class SparseMat represents multi-dimensional sparse numerical arrays.
imgcodecs,"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:"
imgcodecs,"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5; int size[5] = {10, 10, 10, 10, 10}; SparseMat sparse_mat(dims, size, CV_32F); for(int i = 0; i < 1000; i++) { int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f; } cout << ""nnz = "" << sparse_mat.nzcount() << endl; cv::SparseMatThe class SparseMat represents multi-dimensional sparse numerical arrays.Definition mat.hpp:2751 cv::SparseMat::sizeconst int * size() constreturns the array of sizes, or NULL if the matrix is not allocated cv::SparseMat::dimsint dims() constreturns the matrix dimensionality CV_32F#define CV_32FDefinition interface.h:78 Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix // and the sum of elements. SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>(); double s = 0; int dims = sparse_mat.dims(); for(; it != it_end; ++it) { // print element indices and the element value const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("": %g\n"", it.value<float>()); s += *it; } printf(""Element sum is %g\n"", s); cv::SparseMatConstIterator_Template Read-Only Sparse Matrix Iterator Class.Definition mat.hpp:3354 cv::SparseMatConstIterator::valueconst _Tp & value() consttemplate method returning the current matrix element cv::SparseMatConstIterator::nodeconst SparseMat::Node * node() constreturns the current node of the sparse matrix. it.node->idx is the current element index cv::SparseMat::Nodesparse matrix node - element of a hash tableDefinition mat.hpp:2776 cv::SparseMat::Node::idxint idx[MAX_DIM]index of the matrix elementDefinition mat.hpp:2782 If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation. Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b) { const SparseMat *_a = &a, *_b = &b; // if b contains less elements than a, // it is faster to iterate through b if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // take the next element from the first matrix float avalue = *it; const Node* anode = it.node(); // and try to find an element with the same index in the second matrix. // since the hash value depends only on the element index, // reuse the hash value stored in the node float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr; } cv::SparseMat::endSparseMatIterator end()returns the sparse matrix iterator at the matrix end cv::SparseMat::nzcountsize_t nzcount() constreturns the number of non-zero elements (=the number of hash table nodes) cv::SparseMat::beginSparseMatIterator begin()returns the sparse matrix iterator at the matrix beginning cv::SparseMat::Node::hashvalsize_t hashvalhash valueDefinition mat.hpp:2778"
imgcodecs,The base class for camera response calibration algorithms.
imgcodecs,Read and write video or images sequence with OpenCV.
imgcodecs,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
imgcodecs,Bioinspired Module Retina Introduction
imgcodecs,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
imgcodecs,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
imgcodecs,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
imgcodecs,See detailed overview here: Machine Learning Overview.
imgcodecs,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
imgcodecs,Base interface for sparse optical flow algorithms.
imgcodecs,Dummy structure storing DFT/DCT context.
imgcodecs,Users can convert this pointer to any type they want. Initialisation and destruction should be made in Init and Free function implementations correspondingly. Example:
imgcodecs,core/src/hal_replacement.hpp
imgcodecs,The object detector described below has been initially proposed by Paul Viola [285] and improved by Rainer Lienhart [168] .
imgcodecs,"First, a classifier (namely a cascade of boosted classifiers working with haar-like features) is trained with a few hundred sample views of a particular object (i.e., a face or a car), called positive examples, that are scaled to the same size (say, 20x20), and negative examples - arbitrary images of the same size."
imgcodecs,"After a classifier is trained, it can be applied to a region of interest (of the same size as used during the training) in an input image. The classifier outputs a ""1"" if the region is likely to show the object (i.e., face/car), and ""0"" otherwise. To search for the object in the whole image one can move the search window across the image and check every location using the classifier. The classifier is designed so that it can be easily ""resized"" in order to be able to find the objects of interest at different sizes, which is more efficient than resizing the image itself. So, to find an object of an unknown size in the image the scan procedure should be done several times at different scales."
imgcodecs,"The word ""cascade"" in the classifier name means that the resultant classifier consists of several simpler classifiers (stages) that are applied subsequently to a region of interest until at some stage the candidate is rejected or all the stages are passed. The word ""boosted"" means that the classifiers at every stage of the cascade are complex themselves and they are built out of basic classifiers using one of four different boosting techniques (weighted voting). Currently Discrete Adaboost, Real Adaboost, Gentle Adaboost and Logitboost are supported. The basic classifiers are decision-tree classifiers with at least 2 leaves. Haar-like features are the input to the basic classifiers, and are calculated as described below. The current algorithm uses the following Haar-like features:"
imgcodecs,"The feature used in a particular classifier is specified by its shape (1a, 2b etc.), position within the region of interest and the scale (this scale is not the same as the scale used at the detection stage, though these two scales are multiplied). For example, in the case of the third line feature (2c) the response is calculated as the difference between the sum of image pixels under the rectangle covering the whole feature (including the two white stripes and the black stripe in the middle) and the sum of the image pixels under the black stripe multiplied by 3 in order to compensate for the differences in the size of areas. The sums of pixel values over a rectangular regions are calculated rapidly using integral images (see below and the integral description)."
imgcodecs,Check the corresponding tutorial for more details.
imgcodecs,The following reference is for the detection part only. There is a separate application called opencv_traincascade that can train a cascade of boosted classifiers from a set of samples.
imgcodecs,Classes class cv::BaseCascadeClassifier  class cv::CascadeClassifier  Cascade classifier class for object detection. More...  struct cv::DefaultDeleter< CvHaarClassifierCascade >  class cv::DetectionBasedTracker 
imgcodecs,DIS optical flow algorithm.
imgcodecs,"This class implements the Dense Inverse Search (DIS) optical flow algorithm. More details about the algorithm can be found at [151] . Includes three presets with preselected parameters to provide reasonable trade-off between speed and quality. However, even the slowest preset is still relatively fast, use DeepFlow if you need better quality and don't care about speed."
imgcodecs,"This implementation includes several additional features compared to the algorithm described in the paper, including spatial propagation of flow vectors (getUseSpatialPropagation), as well as an option to utilize an initial flow approximation passed to calc (which is, essentially, temporal propagation, if the previous frame's flow field is passed)."
imgcodecs,"This algorithm transforms image to contrast using gradients on all levels of gaussian pyramid, transforms contrast values to HVS response and scales the response. After this the image is reconstructed from new contrast values."
imgcodecs,For more information see [182] .
imgcodecs,Class computing a dense optical flow using the Gunnar Farneback's algorithm.
imgcodecs,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
imgcodecs,A class filters a vector of keypoints.
imgcodecs,"Because now it is difficult to provide a convenient interface for all usage scenarios of the keypoints filter class, it has only several needed by now static methods."
imgcodecs,Multiply: dst[i] = scale * src1[i] * src2[i]
imgcodecs,src1_data first source image data src1_step first source image step src2_data second source image data src2_step second source image step dst_data destination image data dst_step destination image step width width of the images height height of the images scale additional multiplier
imgcodecs,"Functions int hal_ni_mul16s (const short *src1_data, size_t src1_step, const short *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul16u (const ushort *src1_data, size_t src1_step, const ushort *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul32f (const float *src1_data, size_t src1_step, const float *src2_data, size_t src2_step, float *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul32s (const int *src1_data, size_t src1_step, const int *src2_data, size_t src2_step, int *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul64f (const double *src1_data, size_t src1_step, const double *src2_data, size_t src2_step, double *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, schar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8s16s (const schar *src1_data, size_t src1_step, const schar *src2_data, size_t src2_step, short *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, uchar *dst_data, size_t dst_step, int width, int height, double scale)  int hal_ni_mul8u16u (const uchar *src1_data, size_t src1_step, const uchar *src2_data, size_t src2_step, ushort *dst_data, size_t dst_step, int width, int height, double scale) "
imgcodecs,"Class implementing the BRISK keypoint detector and descriptor extractor, described in [159] ."
imgcodecs,File Storage Node class.
imgcodecs,"The node is used to store each and every element of the file storage opened for reading. When XML/YAML file is read, it is first parsed and stored in the memory as a hierarchical collection of nodes. Each node can be a ""leaf"" that is contain a single number or a string, or be a collection of other nodes. There can be named collections (mappings) where each element has a name and it is accessed by a name, and ordered collections (sequences) where elements do not have names but rather accessed by index. Type of the file node can be determined using FileNode::type method."
imgcodecs,"Note that file nodes are only used for navigating file storages opened for reading. When a file storage is opened for writing, no data is stored in memory after it is written."
imgcodecs,Template class for small matrices whose type and size are known at compilation time.
imgcodecs,"If you need a more flexible type, use Mat . The elements of the matrix M are accessible using the M(i,j) notation. Most of the common matrix operations (see also MatrixExpressions ) are available. To do an operation on Matx that is not implemented, you can easily convert the matrix to Mat and backwards:"
imgcodecs,"Except of the plain constructor which takes a list of elements, Matx can be initialized from a C-array:"
imgcodecs,"In case if C++11 features are available, std::initializer_list can be also used to initialize Matx:"
imgcodecs,"Classes class cv::GraphicalCodeDetector  class cv::SimilarRects  This class is used for grouping object candidates detected by Cascade Classifier, HOG etc. More... "
imgcodecs,Classes class cv::BackgroundSubtractor  Base class for background/foreground segmentation. : More...  class cv::BackgroundSubtractorKNN  K-nearest neighbours - based Background/Foreground Segmentation Algorithm. More...  class cv::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
imgcodecs,cv::GOpaque<T> template class represents an object of class T in the graph.
imgcodecs,"cv::GOpaque<T> describes a functional relationship between operations consuming and producing object of class T. cv::GOpaque<T> is designed to extend G-API with user-defined data types, which are often required with user-defined operations. G-API can't apply any optimizations to user-defined types since these types are opaque to the framework. However, there is a number of G-API operations declared with cv::GOpaque<T> as a return type, e.g. cv::gapi::streaming::timestamp() or cv::gapi::streaming::size()."
imgcodecs,Basic interface for all solvers.
imgcodecs,DNN-based face detector.
imgcodecs,model download link: https://github.com/opencv/opencv_zoo/tree/master/models/face_detection_yunet
imgcodecs,Wrapping class for feature detection using the goodFeaturesToTrack function. :
imgcodecs,TLS container base implementation
imgcodecs,Don't use directly.
imgcodecs,"Functions void cv::imshow (const String &winname, const ogl::Texture2D &tex)  Displays OpenGL 2D texture in the specified window.  void cv::setOpenGlContext (const String &winname)  Sets the specified window as current OpenGL context.  void cv::setOpenGlDrawCallback (const String &winname, OpenGlDrawCallback onOpenGlDraw, void *userdata=0)  Sets a callback function to be called to draw on top of displayed image.  void cv::updateWindow (const String &winname)  Force window to redraw its context and call draw callback ( See cv::setOpenGlDrawCallback ). "
imgcodecs,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
imgcodecs,"src_data array of pointers to source arrays (cn items x len items) [ [B, B, ...], [G, G, ...], [R, R, ...] ] dst_data destination array of interleaved values (len x cn items) [ B, G, R, B, G, R, ...] len number of elements cn number of channels"
imgcodecs,"Functions int hal_ni_merge16u (const ushort **src_data, ushort *dst_data, int len, int cn)  int hal_ni_merge32s (const int **src_data, int *dst_data, int len, int cn)  int hal_ni_merge64s (const int64 **src_data, int64 *dst_data, int len, int cn)  int hal_ni_merge8u (const uchar **src_data, uchar *dst_data, int len, int cn) "
imgcodecs,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
imgcodecs,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
imgcodecs,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
imgcodecs,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
imgcodecs,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
imgcodecs,Custom array allocator.
imgcodecs,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
imgcodecs,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
imgcodecs,The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response.
imgcodecs,For more information see [68] .
imgcodecs,A norm based cost extraction. :
imgcodecs,"Template class for 3D points specified by its coordinates x, y and z."
imgcodecs,"An instance of the class is interchangeable with the C structure CvPoint2D32f . Similarly to Point_ , the coordinates of 3D points can be converted to another type. The vector arithmetic and comparison operations are also supported."
imgcodecs,The following Point3_<> aliases are available:
imgcodecs,Wrapping class for feature detection using the FAST method. :
imgcodecs,"""Universal intrinsics"" is a types and functions set intended to simplify vectorization of code on different platforms. Currently a few different SIMD extensions on different architectures are supported. 128 bit registers of various types support is implemented for a wide range of architectures including x86(SSE/SSE2/SSE4.2), ARM(NEON), PowerPC(VSX), MIPS(MSA). 256 bit long registers are supported on x86(AVX2) and 512 bit long registers are supported on x86(AVX512). In case when there is no SIMD extension available during compilation, fallback C++ implementation of intrinsics will be chosen and code will work as expected although it could be slower."
imgcodecs,Abstract base class for histogram cost algorithms.
imgcodecs,Useful links:
imgcodecs,https://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp
imgcodecs,"Enumerations enum { cv::NORMAL_CLONE = 1 , cv::MIXED_CLONE = 2 , cv::MONOCHROME_TRANSFER = 3 }  seamlessClone algorithm flags More... "
imgcodecs,GMat class represents image or tensor data in the graph.
imgcodecs,"GMat doesn't store any data itself, instead it describes a functional relationship between operations consuming and producing GMat objects."
imgcodecs,"GMat is a virtual counterpart of Mat and UMat, but it doesn't mean G-API use Mat or UMat objects internally to represent GMat objects the internal data representation may be backend-specific or optimized out at all."
imgcodecs,Useful links:
imgcodecs,http://www.inf.ufrgs.br/~eslgastal/DomainTransform
imgcodecs,https://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/
imgcodecs,"Enumerations enum { cv::RECURS_FILTER = 1 , cv::NORMCONV_FILTER = 2 }  Edge preserving filters. More... "
imgcodecs,Base class for dense optical flow algorithms
imgcodecs,Read-Only Sparse Matrix Iterator.
imgcodecs,Here is how to use the iterator to compute the sum of floating-point sparse matrix elements:
imgcodecs,"src_data array of interleaved values (len x cn items) [ B, G, R, B, G, R, ...] dst_data array of pointers to destination arrays (cn items x len items) [ [B, B, ...], [G, G, ...], [R, R, ...] ] len number of elements cn number of channels"
imgcodecs,"Functions int hal_ni_split16u (const ushort *src_data, ushort **dst_data, int len, int cn)  int hal_ni_split32s (const int *src_data, int **dst_data, int len, int cn)  int hal_ni_split64s (const int64 *src_data, int64 **dst_data, int len, int cn)  int hal_ni_split8u (const uchar *src_data, uchar **dst_data, int len, int cn) "
imgcodecs,Singular Value Decomposition.
imgcodecs,"Class for computing Singular Value Decomposition of a floating-point matrix. The Singular Value Decomposition is used to solve least-square problems, under-determined linear systems, invert matrices, compute condition numbers, and so on."
imgcodecs,"If you want to compute a condition number of a matrix or an absolute value of its determinant, you do not need u and vt. You can pass flags=SVD::NO_UV|... . Another flag SVD::FULL_UV indicates that full-size u and vt must be computed, which is not necessary most of the time."
imgcodecs,"Enumerations enum cv::TemplateMatchModes { cv::TM_SQDIFF = 0 , cv::TM_SQDIFF_NORMED = 1 , cv::TM_CCORR = 2 , cv::TM_CCORR_NORMED = 3 , cv::TM_CCOEFF = 4 , cv::TM_CCOEFF_NORMED = 5 }  type of the template matching operation More... "
imgcodecs,Template matrix class derived from Mat.
imgcodecs,"The class Mat_<_Tp> is a thin template wrapper on top of the Mat class. It does not have any extra data fields. Nor this class nor Mat has any virtual methods. Thus, references or pointers to these two classes can be freely but carefully converted one to another. For example:"
imgcodecs,"While Mat is sufficient in most cases, Mat_ can be more convenient if you use a lot of element access operations and if you know matrix type at the compilation time. Note that Mat::at(int y,int x) and Mat_::operator()(int y,int x) do absolutely the same and run at the same speed, but the latter is certainly shorter:"
imgcodecs,"To use Mat_ for multi-channel images/matrices, pass Vec as a Mat_ parameter:"
imgcodecs,Mat_ is fully compatible with C++11 range-based for loop. For example such loop can be used to safely apply look-up table:
imgcodecs,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
imgcodecs,Abstract base class for shape transformation algorithms.
imgcodecs,"Namespace for all functions is cvv, i.e. cvv::showImage()."
imgcodecs,Compilation:
imgcodecs,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
imgcodecs,See cvv tutorial for a commented example application using cvv.
imgcodecs,Namespaces namespace cvv::impl 
imgproc,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
imgproc,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
imgproc,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
imgproc,This module includes photo processing algorithms
imgproc,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
imgproc,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
imgproc,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
imgproc,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
imgproc,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
imgproc,The implemented stitching pipeline is very similar to the one proposed in [41] .
imgproc,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
imgproc,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
imgproc,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
imgproc,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
imgproc,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
imgproc,Template class for a 4-element vector derived from Vec.
imgproc,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
imgproc,Random Number Generator.
imgproc,"Random number generator. It encapsulates the state (currently, a 64-bit integer) and has methods to return scalar random values and to fill arrays with random values. Currently it supports uniform and Gaussian (normal) distributions. The generator uses Multiply-With-Carry algorithm, introduced by G. Marsaglia ( http://en.wikipedia.org/wiki/Multiply-with-carry ). Gaussian-distribution random numbers are generated using the Ziggurat algorithm ( http://en.wikipedia.org/wiki/Ziggurat_algorithm ), introduced by G. Marsaglia and W. W. Tsang."
imgproc,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
imgproc,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
imgproc,This module includes signal processing algorithms.
imgproc,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
imgproc,ICP point-to-plane odometry algorithm
imgproc,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
imgproc,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
imgproc,YOUR ATTENTION PLEASE!
imgproc,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
imgproc,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
imgproc,Note for developers: please don't put videoio dependency in G-API because of this file.
imgproc,Dense optical flow algorithms compute motion for each point:
imgproc,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
imgproc,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
imgproc,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
imgproc,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
imgproc,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
imgproc,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
imgproc,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
imgproc,finds arbitrary template in the grayscale image using Generalized Hough Transform
imgproc,"Detects position, translation and rotation [114] ."
imgproc,Classes class cv::segmentation::IntelligentScissorsMB  Intelligent Scissors image segmentation. More... 
imgproc,"The human perception isn't built for observing fine changes in grayscale images. Human eyes are more sensitive to observing changes between colors, so you often need to recolor your grayscale images to get a clue about them. OpenCV now comes with various colormaps to enhance the visualization in your computer vision application."
imgproc,"In OpenCV you only need applyColorMap to apply a colormap on a given image. The following sample code reads the path to an image from command line, applies a Jet colormap on it and shows the result:"
imgproc,"Enumerations enum cv::ColormapTypes { cv::COLORMAP_AUTUMN = 0 , cv::COLORMAP_BONE = 1 , cv::COLORMAP_JET = 2 , cv::COLORMAP_WINTER = 3 , cv::COLORMAP_RAINBOW = 4 , cv::COLORMAP_OCEAN = 5 , cv::COLORMAP_SUMMER = 6 , cv::COLORMAP_SPRING = 7 , cv::COLORMAP_COOL = 8 , cv::COLORMAP_HSV = 9 , cv::COLORMAP_PINK = 10 , cv::COLORMAP_HOT = 11 , cv::COLORMAP_PARULA = 12 , cv::COLORMAP_MAGMA = 13 , cv::COLORMAP_INFERNO = 14 , cv::COLORMAP_PLASMA = 15 , cv::COLORMAP_VIRIDIS = 16 , cv::COLORMAP_CIVIDIS = 17 , cv::COLORMAP_TWILIGHT = 18 , cv::COLORMAP_TWILIGHT_SHIFTED = 19 , cv::COLORMAP_TURBO = 20 , cv::COLORMAP_DEEPGREEN = 21 }  GNU Octave/MATLAB equivalent colormaps. More... "
imgproc,n-dimensional dense array class
imgproc,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
imgproc,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
imgproc,"In case of a 2-dimensional array, the above formula is reduced to:"
imgproc,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
imgproc,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
imgproc,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
imgproc,There are many different ways to create a Mat object. The most popular options are listed below:
imgproc,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
imgproc,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
imgproc,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
imgproc,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
imgproc,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
imgproc,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
imgproc,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
imgproc,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
imgproc,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
imgproc,Namespaces namespace cv::traits 
imgproc,Namespaces namespace cv::omnidir::internal 
imgproc,Information Flow algorithm implementaton for alphamatting
imgproc,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
imgproc,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
imgproc,The implementation is based on [7].
imgproc,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
imgproc,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
imgproc,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
imgproc,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
imgproc,"Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as Mat's). It means that for each pixel location \((x,y)\) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location \((x,y)\). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one."
imgproc,"Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian \(3 \times 3\) filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (""replicated border"" extrapolation method), or assume that all the non-existing pixels are zeros (""constant border"" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see BorderTypes"
imgproc,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
imgproc,Namespace for all functions is cv::img_hash.
imgproc,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
imgproc,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
imgproc,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
imgproc,This modules is to draw UTF-8 strings with freetype/harfbuzz.
imgproc,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
imgproc,Classes class cv::freetype::FreeType2 
imgproc,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
imgproc,Namespaces namespace NcvCTprep 
imgproc,"Functions void cv::hal::cvtBGR5x5toBGR (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int dcn, bool swapBlue, int greenBits)  void cv::hal::cvtBGR5x5toGray (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int greenBits)  void cv::hal::cvtBGRtoBGR (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int scn, int dcn, bool swapBlue)  void cv::hal::cvtBGRtoBGR5x5 (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int scn, bool swapBlue, int greenBits)  void cv::hal::cvtBGRtoGray (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int scn, bool swapBlue)  void cv::hal::cvtBGRtoHSV (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int scn, bool swapBlue, bool isFullRange, bool isHSV)  void cv::hal::cvtBGRtoLab (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int scn, bool swapBlue, bool isLab, bool srgb)  void cv::hal::cvtBGRtoThreePlaneYUV (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int scn, bool swapBlue, int uIdx)  void cv::hal::cvtBGRtoTwoPlaneYUV (const uchar *src_data, size_t src_step, uchar *y_data, uchar *uv_data, size_t dst_step, int width, int height, int scn, bool swapBlue, int uIdx)  Separate Y and UV planes.  void cv::hal::cvtBGRtoXYZ (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int scn, bool swapBlue)  void cv::hal::cvtBGRtoYUV (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int scn, bool swapBlue, bool isCbCr)  void cv::hal::cvtGraytoBGR (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int dcn)  void cv::hal::cvtGraytoBGR5x5 (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int greenBits)  void cv::hal::cvtHSVtoBGR (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int dcn, bool swapBlue, bool isFullRange, bool isHSV)  void cv::hal::cvtLabtoBGR (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int dcn, bool swapBlue, bool isLab, bool srgb)  void cv::hal::cvtMultipliedRGBAtoRGBA (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height)  void cv::hal::cvtOnePlaneBGRtoYUV (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int scn, bool swapBlue, int uIdx, int ycn)  void cv::hal::cvtOnePlaneYUVtoBGR (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int dcn, bool swapBlue, int uIdx, int ycn)  void cv::hal::cvtRGBAtoMultipliedRGBA (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height)  void cv::hal::cvtThreePlaneYUVtoBGR (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int dst_width, int dst_height, int dcn, bool swapBlue, int uIdx)  void cv::hal::cvtTwoPlaneYUVtoBGR (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int dst_width, int dst_height, int dcn, bool swapBlue, int uIdx)  void cv::hal::cvtTwoPlaneYUVtoBGR (const uchar *y_data, const uchar *uv_data, size_t src_step, uchar *dst_data, size_t dst_step, int dst_width, int dst_height, int dcn, bool swapBlue, int uIdx)  Separate Y and UV planes.  void cv::hal::cvtTwoPlaneYUVtoBGR (const uchar *y_data, size_t y_step, const uchar *uv_data, size_t uv_step, uchar *dst_data, size_t dst_step, int dst_width, int dst_height, int dcn, bool swapBlue, int uIdx)  void cv::hal::cvtXYZtoBGR (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int dcn, bool swapBlue)  void cv::hal::cvtYUVtoBGR (const uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int depth, int dcn, bool swapBlue, bool isCbCr)  void cv::hal::filter2D (int stype, int dtype, int kernel_type, uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int full_width, int full_height, int offset_x, int offset_y, uchar *kernel_data, size_t kernel_step, int kernel_width, int kernel_height, int anchor_x, int anchor_y, double delta, int borderType, bool isSubmatrix)  void cv::hal::integral (int depth, int sdepth, int sqdepth, const uchar *src, size_t srcstep, uchar *sum, size_t sumstep, uchar *sqsum, size_t sqsumstep, uchar *tilted, size_t tstep, int width, int height, int cn)  void cv::hal::morph (int op, int src_type, int dst_type, uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int roi_width, int roi_height, int roi_x, int roi_y, int roi_width2, int roi_height2, int roi_x2, int roi_y2, int kernel_type, uchar *kernel_data, size_t kernel_step, int kernel_width, int kernel_height, int anchor_x, int anchor_y, int borderType, const double borderValue[4], int iterations, bool isSubmatrix)  void cv::hal::resize (int src_type, const uchar *src_data, size_t src_step, int src_width, int src_height, uchar *dst_data, size_t dst_step, int dst_width, int dst_height, double inv_scale_x, double inv_scale_y, int interpolation)  void cv::hal::sepFilter2D (int stype, int dtype, int ktype, uchar *src_data, size_t src_step, uchar *dst_data, size_t dst_step, int width, int height, int full_width, int full_height, int offset_x, int offset_y, uchar *kernelx_data, int kernelx_len, uchar *kernely_data, int kernely_len, int anchor_x, int anchor_y, double delta, int borderType)  void cv::hal::warpAffine (int src_type, const uchar *src_data, size_t src_step, int src_width, int src_height, uchar *dst_data, size_t dst_step, int dst_width, int dst_height, const double M[6], int interpolation, int borderType, const double borderValue[4])  void cv::hal::warpAffineBlockline (int *adelta, int *bdelta, short *xy, short *alpha, int X0, int Y0, int bw)  void cv::hal::warpAffineBlocklineNN (int *adelta, int *bdelta, short *xy, int X0, int Y0, int bw)  void cv::hal::warpPerspective (int src_type, const uchar *src_data, size_t src_step, int src_width, int src_height, uchar *dst_data, size_t dst_step, int dst_width, int dst_height, const double M[9], int interpolation, int borderType, const double borderValue[4])  void cv::hal::warpPerspectiveBlockline (const double *M, short *xy, short *alpha, double X0, double Y0, double W0, int bw)  void cv::hal::warpPerspectiveBlocklineNN (const double *M, short *xy, double X0, double Y0, double W0, int bw) "
imgproc,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
imgproc,This module contains:
imgproc,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
imgproc,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
imgproc,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
imgproc,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
imgproc,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
imgproc,Template class for specifying the size of an image or rectangle.
imgproc,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
imgproc,OpenCV defines the following Size_<> aliases:
imgproc,"Drawing functions work with matrices/images of arbitrary depth. The boundaries of the shapes can be rendered with antialiasing (implemented only for 8-bit images for now). All the functions include the parameter color that uses an RGB value (that may be constructed with the Scalar constructor ) for color images and brightness for grayscale images. For color images, the channel ordering is normally Blue, Green, Red. This is what imshow, imread, and imwrite expect. So, if you form a color using the Scalar constructor, it should look like:"
imgproc,"\[\texttt{Scalar} (blue \_ component, green \_ component, red \_ component[, alpha \_ component])\]"
imgproc,"If you are using your own image rendering and I/O functions, you can use any channel ordering. The drawing functions process each channel independently and do not depend on the channel order or even on the used color space. The whole image can be converted from BGR to RGB or to a different color space using cvtColor ."
imgproc,"If a drawn figure is partially or completely outside the image, the drawing functions clip it. Also, many drawing functions can handle pixel coordinates specified with sub-pixel accuracy. This means that the coordinates can be passed as fixed-point numbers encoded as integers. The number of fractional bits is specified by the shift parameter and the real point coordinates are calculated as \(\texttt{Point}(x,y)\rightarrow\texttt{Point2f}(x*2^{-shift},y*2^{-shift})\) . This feature is especially effective when rendering antialiased shapes."
imgproc,Classes class cv::LineIterator  Class for iterating over all pixels on a raster line segment. More... 
imgproc,Base class for Contrast Limited Adaptive Histogram Equalization.
imgproc,Classes class cv::CLAHE  Base class for Contrast Limited Adaptive Histogram Equalization. More... 
imgproc,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
imgproc,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
imgproc,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
imgproc,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
imgproc,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
imgproc,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
imgproc,"Functions void cv::julia::initJulia (int argc, char **argv) "
imgproc,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
imgproc,finds arbitrary template in the grayscale image using Generalized Hough Transform
imgproc,"Functions void cv::accumulate (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds an image to the accumulator image.  void cv::accumulateProduct (InputArray src1, InputArray src2, InputOutputArray dst, InputArray mask=noArray())  Adds the per-element product of two input images to the accumulator image.  void cv::accumulateSquare (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds the square of a source image to the accumulator image.  void cv::accumulateWeighted (InputArray src, InputOutputArray dst, double alpha, InputArray mask=noArray())  Updates a running average.  void cv::createHanningWindow (OutputArray dst, Size winSize, int type)  This function computes a Hanning window coefficients in two dimensions.  void cv::divSpectrums (InputArray a, InputArray b, OutputArray c, int flags, bool conjB=false)  Performs the per-element division of the first Fourier spectrum by the second Fourier spectrum.  Point2d cv::phaseCorrelate (InputArray src1, InputArray src2, InputArray window=noArray(), double *response=0)  The function is used to detect translational shifts that occur between two images. "
imgproc,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
imgproc,It provides easy interface to:
imgproc,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
imgproc,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
imgproc,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
imgproc,It is planned to have:
imgproc,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
imgproc,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
imgproc,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
imgproc,Classes class cv::DualQuat< _Tp >  class cv::Quat< _Tp >  class cv::QuatEnum 
imgproc,struct returned by cv::moments
imgproc,The spatial moments \(\texttt{Moments::m}_{ji}\) are computed as:
imgproc,"\[\texttt{m} _{ji}= \sum _{x,y} \left ( \texttt{array} (x,y) \cdot x^j \cdot y^i \right )\]"
imgproc,The central moments \(\texttt{Moments::mu}_{ji}\) are computed as:
imgproc,"\[\texttt{mu} _{ji}= \sum _{x,y} \left ( \texttt{array} (x,y) \cdot (x - \bar{x} )^j \cdot (y - \bar{y} )^i \right )\]"
imgproc,"where \((\bar{x}, \bar{y})\) is the mass center:"
imgproc,"\[\bar{x} = \frac{\texttt{m}_{10}}{\texttt{m}_{00}} , \; \bar{y} = \frac{\texttt{m}_{01}}{\texttt{m}_{00}}\]"
imgproc,The normalized central moments \(\texttt{Moments::nu}_{ij}\) are computed as:
imgproc,\[\texttt{nu} _{ji}= \frac{\texttt{mu}_{ji}}{\texttt{m}_{00}^{(i+j)/2+1}} .\]
imgproc,"The moments of a contour are defined in the same way but computed using the Green's formula (see http://en.wikipedia.org/wiki/Green_theorem). So, due to a limited raster resolution, the moments computed for a contour are slightly different from the moments computed for the same rasterized contour."
imgproc,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
imgproc,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
imgproc,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
imgproc,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
imgproc,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
imgproc,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
imgproc,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
imgproc,"Enumerations enum cv::ImreadModes { cv::IMREAD_UNCHANGED = -1 , cv::IMREAD_GRAYSCALE = 0 , cv::IMREAD_COLOR_BGR = 1 , cv::IMREAD_COLOR = 1 , cv::IMREAD_ANYDEPTH = 2 , cv::IMREAD_ANYCOLOR = 4 , cv::IMREAD_LOAD_GDAL = 8 , cv::IMREAD_REDUCED_GRAYSCALE_2 = 16 , cv::IMREAD_REDUCED_COLOR_2 = 17 , cv::IMREAD_REDUCED_GRAYSCALE_4 = 32 , cv::IMREAD_REDUCED_COLOR_4 = 33 , cv::IMREAD_REDUCED_GRAYSCALE_8 = 64 , cv::IMREAD_REDUCED_COLOR_8 = 65 , cv::IMREAD_IGNORE_ORIENTATION = 128 , cv::IMREAD_COLOR_RGB = 256 }  Imread flags. More...  enum cv::ImwriteEXRCompressionFlags { cv::IMWRITE_EXR_COMPRESSION_NO = 0 , cv::IMWRITE_EXR_COMPRESSION_RLE = 1 , cv::IMWRITE_EXR_COMPRESSION_ZIPS = 2 , cv::IMWRITE_EXR_COMPRESSION_ZIP = 3 , cv::IMWRITE_EXR_COMPRESSION_PIZ = 4 , cv::IMWRITE_EXR_COMPRESSION_PXR24 = 5 , cv::IMWRITE_EXR_COMPRESSION_B44 = 6 , cv::IMWRITE_EXR_COMPRESSION_B44A = 7 , cv::IMWRITE_EXR_COMPRESSION_DWAA = 8 , cv::IMWRITE_EXR_COMPRESSION_DWAB = 9 }  enum cv::ImwriteEXRTypeFlags { cv::IMWRITE_EXR_TYPE_HALF = 1 , cv::IMWRITE_EXR_TYPE_FLOAT = 2 }  enum cv::ImwriteFlags { cv::IMWRITE_JPEG_QUALITY = 1 , cv::IMWRITE_JPEG_PROGRESSIVE = 2 , cv::IMWRITE_JPEG_OPTIMIZE = 3 , cv::IMWRITE_JPEG_RST_INTERVAL = 4 , cv::IMWRITE_JPEG_LUMA_QUALITY = 5 , cv::IMWRITE_JPEG_CHROMA_QUALITY = 6 , cv::IMWRITE_JPEG_SAMPLING_FACTOR = 7 , cv::IMWRITE_PNG_COMPRESSION = 16 , cv::IMWRITE_PNG_STRATEGY = 17 , cv::IMWRITE_PNG_BILEVEL = 18 , cv::IMWRITE_PXM_BINARY = 32 , cv::IMWRITE_EXR_TYPE = (3 << 4) + 0 , cv::IMWRITE_EXR_COMPRESSION = (3 << 4) + 1 , cv::IMWRITE_EXR_DWA_COMPRESSION_LEVEL = (3 << 4) + 2 , cv::IMWRITE_WEBP_QUALITY = 64 , cv::IMWRITE_HDR_COMPRESSION = (5 << 4) + 0 , cv::IMWRITE_PAM_TUPLETYPE = 128 , cv::IMWRITE_TIFF_RESUNIT = 256 , cv::IMWRITE_TIFF_XDPI = 257 , cv::IMWRITE_TIFF_YDPI = 258 , cv::IMWRITE_TIFF_COMPRESSION = 259 , cv::IMWRITE_TIFF_ROWSPERSTRIP = 278 , cv::IMWRITE_TIFF_PREDICTOR = 317 , cv::IMWRITE_JPEG2000_COMPRESSION_X1000 = 272 , cv::IMWRITE_AVIF_QUALITY = 512 , cv::IMWRITE_AVIF_DEPTH = 513 , cv::IMWRITE_AVIF_SPEED = 514 }  Imwrite flags. More...  enum cv::ImwriteHDRCompressionFlags { cv::IMWRITE_HDR_COMPRESSION_NONE = 0 , cv::IMWRITE_HDR_COMPRESSION_RLE = 1 }  Imwrite HDR specific values for IMWRITE_HDR_COMPRESSION parameter key. More...  enum cv::ImwriteJPEGSamplingFactorParams { cv::IMWRITE_JPEG_SAMPLING_FACTOR_411 = 0x411111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_420 = 0x221111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_422 = 0x211111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_440 = 0x121111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_444 = 0x111111 }  enum cv::ImwritePAMFlags { cv::IMWRITE_PAM_FORMAT_NULL = 0 , cv::IMWRITE_PAM_FORMAT_BLACKANDWHITE = 1 , cv::IMWRITE_PAM_FORMAT_GRAYSCALE = 2 , cv::IMWRITE_PAM_FORMAT_GRAYSCALE_ALPHA = 3 , cv::IMWRITE_PAM_FORMAT_RGB = 4 , cv::IMWRITE_PAM_FORMAT_RGB_ALPHA = 5 }  Imwrite PAM specific tupletype flags used to define the 'TUPLETYPE' field of a PAM file. More...  enum cv::ImwritePNGFlags { cv::IMWRITE_PNG_STRATEGY_DEFAULT = 0 , cv::IMWRITE_PNG_STRATEGY_FILTERED = 1 , cv::IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY = 2 , cv::IMWRITE_PNG_STRATEGY_RLE = 3 , cv::IMWRITE_PNG_STRATEGY_FIXED = 4 }  Imwrite PNG specific flags used to tune the compression algorithm. More...  enum cv::ImwriteTiffCompressionFlags { cv::IMWRITE_TIFF_COMPRESSION_NONE = 1 , cv::IMWRITE_TIFF_COMPRESSION_CCITTRLE = 2 , cv::IMWRITE_TIFF_COMPRESSION_CCITTFAX3 = 3 , cv::IMWRITE_TIFF_COMPRESSION_CCITT_T4 = 3 , cv::IMWRITE_TIFF_COMPRESSION_CCITTFAX4 = 4 , cv::IMWRITE_TIFF_COMPRESSION_CCITT_T6 = 4 , cv::IMWRITE_TIFF_COMPRESSION_LZW = 5 , cv::IMWRITE_TIFF_COMPRESSION_OJPEG = 6 , cv::IMWRITE_TIFF_COMPRESSION_JPEG = 7 , cv::IMWRITE_TIFF_COMPRESSION_T85 = 9 , cv::IMWRITE_TIFF_COMPRESSION_T43 = 10 , cv::IMWRITE_TIFF_COMPRESSION_NEXT = 32766 , cv::IMWRITE_TIFF_COMPRESSION_CCITTRLEW = 32771 , cv::IMWRITE_TIFF_COMPRESSION_PACKBITS = 32773 , cv::IMWRITE_TIFF_COMPRESSION_THUNDERSCAN = 32809 , cv::IMWRITE_TIFF_COMPRESSION_IT8CTPAD = 32895 , cv::IMWRITE_TIFF_COMPRESSION_IT8LW = 32896 , cv::IMWRITE_TIFF_COMPRESSION_IT8MP = 32897 , cv::IMWRITE_TIFF_COMPRESSION_IT8BL = 32898 , cv::IMWRITE_TIFF_COMPRESSION_PIXARFILM = 32908 , cv::IMWRITE_TIFF_COMPRESSION_PIXARLOG = 32909 , cv::IMWRITE_TIFF_COMPRESSION_DEFLATE = 32946 , cv::IMWRITE_TIFF_COMPRESSION_ADOBE_DEFLATE = 8 , cv::IMWRITE_TIFF_COMPRESSION_DCS = 32947 , cv::IMWRITE_TIFF_COMPRESSION_JBIG = 34661 , cv::IMWRITE_TIFF_COMPRESSION_SGILOG = 34676 , cv::IMWRITE_TIFF_COMPRESSION_SGILOG24 = 34677 , cv::IMWRITE_TIFF_COMPRESSION_JP2000 = 34712 , cv::IMWRITE_TIFF_COMPRESSION_LERC = 34887 , cv::IMWRITE_TIFF_COMPRESSION_LZMA = 34925 , cv::IMWRITE_TIFF_COMPRESSION_ZSTD = 50000 , cv::IMWRITE_TIFF_COMPRESSION_WEBP = 50001 , cv::IMWRITE_TIFF_COMPRESSION_JXL = 50002 }  enum cv::ImwriteTiffPredictorFlags { cv::IMWRITE_TIFF_PREDICTOR_NONE = 1 , cv::IMWRITE_TIFF_PREDICTOR_HORIZONTAL = 2 , cv::IMWRITE_TIFF_PREDICTOR_FLOATINGPOINT = 3 } "
imgproc,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
imgproc,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
imgproc,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
imgproc,This module has been originally developed as a project for Google Summer of Code 2012-2015.
imgproc,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
imgproc,STL namespace.
imgproc,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
imgproc,The distortion-free projective transformation given by a pinhole camera model is shown below.
imgproc,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
imgproc,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
imgproc,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
imgproc,\[p = A P_c.\]
imgproc,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
imgproc,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
imgproc,and thus
imgproc,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
imgproc,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
imgproc,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
imgproc,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
imgproc,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
imgproc,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
imgproc,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
imgproc,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
imgproc,and therefore
imgproc,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
imgproc,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
imgproc,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
imgproc,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
imgproc,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
imgproc,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
imgproc,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
imgproc,with
imgproc,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
imgproc,The following figure illustrates the pinhole camera model.
imgproc,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
imgproc,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
imgproc,where
imgproc,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
imgproc,with
imgproc,\[r^2 = x'^2 + y'^2\]
imgproc,and
imgproc,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
imgproc,if \(Z_c \ne 0\).
imgproc,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
imgproc,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
imgproc,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
imgproc,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
imgproc,where
imgproc,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
imgproc,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
imgproc,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
imgproc,In the functions below the coefficients are passed or returned as
imgproc,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
imgproc,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
imgproc,The functions below use the above model to do the following:
imgproc,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
imgproc,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
imgproc,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
imgproc,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
imgproc,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
imgproc,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
imgproc,if \(W \ne 0\).
imgproc,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
imgproc,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
imgproc,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
imgproc,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
imgproc,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
imgproc,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
imgproc,The class defining termination criteria for iterative algorithms.
imgproc,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
imgproc,"ArUco Marker Detection, module functionality was moved to objdetect module"
imgproc,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
imgproc,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
imgproc,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
imgproc,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
imgproc,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
imgproc,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
imgproc,Template class specifying a continuous subsequence (slice) of a sequence.
imgproc,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
imgproc,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
imgproc,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
imgproc,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
imgproc,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
imgproc,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
imgproc,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
imgproc,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
imgproc,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
imgproc,Template class for 2D points specified by its coordinates x and y.
imgproc,"An instance of the class is interchangeable with C structures, CvPoint and CvPoint2D32f . There is also a cast operator to convert point coordinates to the specified type. The conversion from floating-point coordinates to integer coordinates is done by rounding. Commonly, the conversion uses this operation for each of the coordinates. Besides the class members listed in the declaration above, the following operations on points are implemented:"
imgproc,"For your convenience, the following type aliases are defined:"
imgproc,Example:
imgproc,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
imgproc,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
imgproc,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
imgproc,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
imgproc,"Each class derived from Map implements a motion model, as follows:"
imgproc,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
imgproc,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
imgproc,The classes derived from Mapper are
imgproc,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
imgproc,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
imgproc,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
imgproc,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
imgproc,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
imgproc,Line segment detector class.
imgproc,following the algorithm described at [290] .
imgproc,This module provides storage routines for Hierarchical Data Format objects.
imgproc,Face module changelog Face Recognition with OpenCV
imgproc,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
imgproc,The class represents rotated (i.e. not up-right) rectangles on a plane.
imgproc,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
imgproc,The sample below demonstrates how to use RotatedRect:
imgproc,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
imgproc,Classes class cv::plot::Plot2d 
imgproc,Classes class cv::quality::QualityBase 
imgproc,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
imgproc,finds arbitrary template in the grayscale image using Generalized Hough Transform
imgproc,Detects position only without translation and rotation [15] .
imgproc,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
imgproc,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
imgproc,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
imgproc,Classes class cv::DenseOpticalFlow  class cv::DISOpticalFlow  DIS optical flow algorithm. More...  class cv::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::KalmanFilter  Kalman filter class. More...  class cv::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More...  class cv::Tracker  Base abstract class for the long-term tracker. More...  class cv::TrackerDaSiamRPN  class cv::TrackerGOTURN  the GOTURN (Generic Object Tracking Using Regression Networks) tracker More...  class cv::TrackerMIL  The MIL algorithm trains a classifier in an online manner to separate the object from the background. More...  class cv::TrackerNano  the Nano tracker is a super lightweight dnn-based general object tracking. More...  class cv::TrackerVit  the VIT tracker is a super lightweight dnn-based general object tracking. More...  class cv::VariationalRefinement  Variational optical flow refinement. More... 
imgproc,Namespaces namespace cv::traits 
imgproc,Class for iterating over all pixels on a raster line segment.
imgproc,"The class LineIterator is used to get each pixel of a raster line connecting two specified points. It can be treated as a versatile implementation of the Bresenham algorithm where you can stop at each pixel and do some extra processing, for example, grab pixel values along the line or draw a line with an effect (for example, with XOR operation)."
imgproc,The number of pixels along the line is stored in LineIterator::count. The method LineIterator::pos returns the current position in the image:
imgproc,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
imgproc,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
imgproc,Namespace for all functions is cv::intensity_transform.
imgproc,Classes class cv::LineSegmentDetector  Line segment detector class. More... 
imgproc,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
imgproc,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
imgproc,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
imgproc,Classes struct cvhalFilter2D  Dummy structure storing filtering context. More... 
imgproc,The class SparseMat represents multi-dimensional sparse numerical arrays.
imgproc,"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:"
imgproc,"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5; int size[5] = {10, 10, 10, 10, 10}; SparseMat sparse_mat(dims, size, CV_32F); for(int i = 0; i < 1000; i++) { int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f; } cout << ""nnz = "" << sparse_mat.nzcount() << endl; cv::SparseMatThe class SparseMat represents multi-dimensional sparse numerical arrays.Definition mat.hpp:2751 cv::SparseMat::sizeconst int * size() constreturns the array of sizes, or NULL if the matrix is not allocated cv::SparseMat::dimsint dims() constreturns the matrix dimensionality CV_32F#define CV_32FDefinition interface.h:78 Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix // and the sum of elements. SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>(); double s = 0; int dims = sparse_mat.dims(); for(; it != it_end; ++it) { // print element indices and the element value const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("": %g\n"", it.value<float>()); s += *it; } printf(""Element sum is %g\n"", s); cv::SparseMatConstIterator_Template Read-Only Sparse Matrix Iterator Class.Definition mat.hpp:3354 cv::SparseMatConstIterator::valueconst _Tp & value() consttemplate method returning the current matrix element cv::SparseMatConstIterator::nodeconst SparseMat::Node * node() constreturns the current node of the sparse matrix. it.node->idx is the current element index cv::SparseMat::Nodesparse matrix node - element of a hash tableDefinition mat.hpp:2776 cv::SparseMat::Node::idxint idx[MAX_DIM]index of the matrix elementDefinition mat.hpp:2782 If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation. Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b) { const SparseMat *_a = &a, *_b = &b; // if b contains less elements than a, // it is faster to iterate through b if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // take the next element from the first matrix float avalue = *it; const Node* anode = it.node(); // and try to find an element with the same index in the second matrix. // since the hash value depends only on the element index, // reuse the hash value stored in the node float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr; } cv::SparseMat::endSparseMatIterator end()returns the sparse matrix iterator at the matrix end cv::SparseMat::nzcountsize_t nzcount() constreturns the number of non-zero elements (=the number of hash table nodes) cv::SparseMat::beginSparseMatIterator begin()returns the sparse matrix iterator at the matrix beginning cv::SparseMat::Node::hashvalsize_t hashvalhash valueDefinition mat.hpp:2778"
imgproc,Read and write video or images sequence with OpenCV.
imgproc,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
imgproc,Bioinspired Module Retina Introduction
imgproc,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
imgproc,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
imgproc,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
imgproc,See detailed overview here: Machine Learning Overview.
imgproc,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
imgproc,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
imgproc,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
imgproc,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
imgproc,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
imgproc,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
imgproc,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
imgproc,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
imgproc,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
imgproc,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
imgproc,"The Subdiv2D class described in this section is used to perform various planar subdivision on a set of 2D points (represented as vector of Point2f). OpenCV subdivides a plane into triangles using the Delaunay's algorithm, which corresponds to the dual graph of the Voronoi diagram. In the figure below, the Delaunay's triangulation is marked with black lines and the Voronoi diagram with red lines."
imgproc,"The subdivisions can be used for the 3D piece-wise transformation of a plane, morphing, fast location of points on the plane, building special graphs (such as NNG,RNG), and so forth."
imgproc,Classes class cv::Subdiv2D 
imgproc,Intelligent Scissors image segmentation.
imgproc,This class is used to find the path (contour) between two points which can be used for image segmentation.
imgproc,Usage example:
imgproc,"Reference: ""Intelligent Scissors for Image Composition"" algorithm designed by Eric N. Mortensen and William A. Barrett, Brigham Young University [200]"
imgproc,"Enumerations enum cv::TemplateMatchModes { cv::TM_SQDIFF = 0 , cv::TM_SQDIFF_NORMED = 1 , cv::TM_CCORR = 2 , cv::TM_CCORR_NORMED = 3 , cv::TM_CCOEFF = 4 , cv::TM_CCOEFF_NORMED = 5 }  type of the template matching operation More... "
imgproc,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
imgproc,"Namespace for all functions is cvv, i.e. cvv::showImage()."
imgproc,Compilation:
imgproc,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
imgproc,See cvv tutorial for a commented example application using cvv.
imgproc,Namespaces namespace cvv::impl 
ml,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
ml,"Template ""trait"" class for OpenCV primitive data types."
ml,"A primitive OpenCV data type is one of unsigned char, bool, signed char, unsigned short, signed short, int, float, double, or a tuple of values of one of these types, where all the values in the tuple have the same type. Any primitive type from the list can be defined by an identifier in the form CV_<bit-depth>{U|S|F}C(<number_of_channels>), for example: uchar CV_8UC1, 3-element floating-point tuple CV_32FC3, and so on. A universal OpenCV structure that is able to store a single instance of such a primitive data type is Vec. Multiple instances of such a type can be stored in a std::vector, Mat, Mat_, SparseMat, SparseMat_, or any other container that is able to store Vec instances."
ml,"The DataType class is basically used to provide a description of such primitive data types without adding any fields or methods to the corresponding classes (and it is actually impossible to add anything to primitive C/C++ data types). This technique is known in C++ as class traits. It is not DataType itself that is used but its specialized versions, such as:"
ml,"The main purpose of this class is to convert compilation-time type information to an OpenCV-compatible data type identifier, for example:"
ml,"So, such traits are used to tell OpenCV which data type you are working with, even if such a type is not native to OpenCV. For example, the matrix B initialization above is compiled because OpenCV defines the proper specialized template class DataType<complex<_Tp> > . This mechanism is also useful (and used in OpenCV this way) for generic algorithms implementations."
ml,opencv2/core/traits.hpp
ml,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
ml,The class implements the random forest predictor.
ml,Artificial Neural Networks - Multi-Layer Perceptrons.
ml,"Unlike many other models in ML that are constructed and trained at once, in the MLP model these steps are separated. First, a network with the specified topology is created using the non-default constructor or the method ANN_MLP::create. All the weights are set to zeros. Then, the network is trained using a set of input and output vectors. The training procedure can be repeated more than once, that is, the weights can be adjusted based on the new training data."
ml,Additional flags for StatModel::train are available: ANN_MLP::TrainFlags.
ml,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
ml,This module includes photo processing algorithms
ml,"Matrix expression representation This is a list of implemented matrix operations that can be combined in arbitrary complex expressions (here A, B stand for matrices ( Mat ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double )):"
ml,"Addition, subtraction, negation: A+B, A-B, A+s, A-s, s+A, s-A, -A Scaling: A*alpha Per-element multiplication and division: A.mul(B), A/B, alpha/A Matrix multiplication: A*B Transposition: A.t() (means AT) Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems: A.inv([method]) (~ A<sup>-1</sup>), A.inv([method])*B (~ X: AX=B) Comparison: A cmpop B, A cmpop alpha, alpha cmpop A, where cmpop is one of >, >=, ==, !=, <=, <. The result of comparison is an 8-bit single channel mask whose elements are set to 255 (if the particular element or pair of elements satisfy the condition) or 0. Bitwise logical operations: A logicop B, A logicop s, s logicop A, ~A, where logicop is one of &, |, ^. Element-wise minimum and maximum: min(A, B), min(A, alpha), max(A, B), max(A, alpha) Element-wise absolute value: abs(A) Cross-product, dot-product: A.cross(B), A.dot(B) Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm, mean, sum, countNonZero, trace, determinant, repeat, and others. Matrix initializers ( Mat::eye(), Mat::zeros(), Mat::ones() ), matrix comma-separated initializers, matrix constructors and operators that extract sub-matrices (see Mat description). Mat_<destination_type>() constructors to cast the result to the proper type. NoteComma-separated initializers and probably some other operations may require additional explicit Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity. Here are examples of matrix expressions: // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD) SVD svd(A); Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t(); // compute the new vector of parameters in the Levenberg-Marquardt algorithm x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err); // sharpen image using ""unsharp mask"" algorithm Mat blurred; double sigma = 1, threshold = 5, amount = 1; GaussianBlur(img, blurred, Size(), sigma, sigma); Mat lowContrastMask = abs(img - blurred) < threshold; Mat sharpened = img*(1+amount) + blurred*(-amount); img.copyTo(sharpened, lowContrastMask); cv::MatExpr::absMatExpr abs(const Mat &m)Calculates an absolute value of each matrix element. cv::MatExpr::invMatExpr inv(int method=DECOMP_LU) const cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::Mat::diagMat diag(int d=0) constExtracts a diagonal from a matrix. cv::Mat::uUMatData * uinteraction with UMatDefinition mat.hpp:2174 cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Mat::tMatExpr t() constTransposes a matrix. cv::SVDSingular Value Decomposition.Definition core.hpp:2739 cv::DECOMP_CHOLESKY@ DECOMP_CHOLESKYDefinition base.hpp:143 cv::SizeSize2i SizeDefinition types.hpp:370 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. cv::thresholddouble threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)Applies a fixed-level threshold to each array element."
ml,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
ml,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
ml,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
ml,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
ml,Support Vector Machines.
ml,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
ml,The implemented stitching pipeline is very similar to the one proposed in [41] .
ml,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
ml,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
ml,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
ml,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
ml,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
ml,Template class for a 4-element vector derived from Vec.
ml,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
ml,Random Number Generator.
ml,"Random number generator. It encapsulates the state (currently, a 64-bit integer) and has methods to return scalar random values and to fill arrays with random values. Currently it supports uniform and Gaussian (normal) distributions. The generator uses Multiply-With-Carry algorithm, introduced by G. Marsaglia ( http://en.wikipedia.org/wiki/Multiply-with-carry ). Gaussian-distribution random numbers are generated using the Ziggurat algorithm ( http://en.wikipedia.org/wiki/Ziggurat_algorithm ), introduced by G. Marsaglia and W. W. Tsang."
ml,Matrix read-only iterator.
ml,The class implements the Expectation Maximization algorithm.
ml,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
ml,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
ml,This module includes signal processing algorithms.
ml,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
ml,Data structure for salient point detectors.
ml,"The class instance stores a keypoint, i.e. a point feature found by one of many available keypoint detectors, such as Harris corner detector, FAST, StarDetector, SURF, SIFT etc."
ml,"The keypoint is characterized by the 2D position, scale (proportional to the diameter of the neighborhood that needs to be taken into account), orientation and some other parameters. The keypoint neighborhood is then analyzed by another algorithm that builds a descriptor (usually represented as a feature vector). The keypoints representing the same object in different images can then be matched using KDTree or another method."
ml,ICP point-to-plane odometry algorithm
ml,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
ml,Template class for 2D rectangles.
ml,described by the following parameters:
ml,"Coordinates of the top-left corner. This is a default interpretation of Rect_::x and Rect_::y in OpenCV. Though, in your algorithms you may count x and y from the bottom-left corner. Rectangle width and height."
ml,"OpenCV typically assumes that the top and left boundary of the rectangle are inclusive, while the right and bottom boundaries are not. For example, the method Rect_::contains returns true if"
ml,"\[x \leq pt.x < x+width, y \leq pt.y < y+height\]"
ml,Virtually every loop over an image ROI in OpenCV (where ROI is specified by Rect_<int> ) is implemented as:
ml,"In addition to the class members, the following operations on rectangles are implemented:"
ml,"\(\texttt{rect} = \texttt{rect} \pm \texttt{point}\) (shifting a rectangle by a certain offset) \(\texttt{rect} = \texttt{rect} \pm \texttt{size}\) (expanding or shrinking a rectangle by a certain amount) rect += point, rect -= point, rect += size, rect -= size (augmenting operations) rect = rect1 & rect2 (rectangle intersection) rect = rect1 | rect2 (minimum area rectangle containing rect1 and rect2 ) rect &= rect1, rect |= rect1 (and the corresponding augmenting operations) rect == rect1, rect != rect1 (rectangle comparison)"
ml,This is an example how the partial ordering on rectangles can be established (rect1 \(\subseteq\) rect2):
ml,"For your convenience, the Rect_<> alias is available: cv::Rect"
ml,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
ml,YOUR ATTENTION PLEASE!
ml,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
ml,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
ml,Note for developers: please don't put videoio dependency in G-API because of this file.
ml,Dense optical flow algorithms compute motion for each point:
ml,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
ml,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
ml,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
ml,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
ml,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
ml,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
ml,This type is very similar to InputArray except that it is used for input/output and output function parameters.
ml,"Just like with InputArray, OpenCV users should not care about OutputArray, they just pass Mat, vector<T> etc. to the functions. The same limitation as for InputArray: Do not explicitly create OutputArray instances applies here too."
ml,"If you want to make your function polymorphic (i.e. accept different arrays as output parameters), it is also not very difficult. Take the sample above as the reference. Note that _OutputArray::create() needs to be called before _OutputArray::getMat(). This way you guarantee that the output array is properly allocated."
ml,"Optional output parameters. If you do not need certain output array to be computed and returned to you, pass cv::noArray(), just like you would in the case of optional input array. At the implementation level, use _OutputArray::needed() to check if certain output array needs to be computed or not."
ml,There are several synonyms for OutputArray that are used to assist automatic Python/Java/... wrapper generators:
ml,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
ml,Template Read-Write Sparse Matrix Iterator Class.
ml,This is the derived from cv::SparseMatConstIterator_ class that introduces more convenient operator *() for accessing the current element.
ml,This is a base class for all more or less complex algorithms in OpenCV.
ml,"especially for classes of algorithms, for which there can be multiple implementations. The examples are stereo correspondence (for which there are algorithms like block matching, semi-global block matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck etc.)."
ml,Here is example of SimpleBlobDetector use in your application via Algorithm interface:
ml,n-dimensional dense array class
ml,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
ml,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
ml,"In case of a 2-dimensional array, the above formula is reduced to:"
ml,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
ml,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
ml,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
ml,There are many different ways to create a Mat object. The most popular options are listed below:
ml,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
ml,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
ml,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
ml,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
ml,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
ml,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
ml,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
ml,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
ml,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
ml,Principal Component Analysis.
ml,"The class is used to calculate a special basis for a set of vectors. The basis will consist of eigenvectors of the covariance matrix calculated from the input set of vectors. The class PCA can also transform vectors to/from the new coordinate space defined by the basis. Usually, in this new coordinate system, each vector from the original set (and any linear combination of such vectors) can be quite accurately approximated by taking its first few components, corresponding to the eigenvectors of the largest eigenvalues of the covariance matrix. Geometrically it means that you calculate a projection of the vector to a subspace formed by a few eigenvectors corresponding to the dominant eigenvalues of the covariance matrix. And usually such a projection is very close to the original vector. So, you can represent the original vector from a high-dimensional space with a much shorter vector consisting of the projected vector's coordinates in the subspace. Such a transformation is also known as Karhunen-Loeve Transform, or KLT. See http://en.wikipedia.org/wiki/Principal_component_analysis"
ml,"The sample below is the function that takes two matrices. The first function stores a set of vectors (a row per vector) that is used to calculate PCA. The second function stores another ""test"" set of vectors (a row per vector). First, these vectors are compressed with PCA, then reconstructed back, and then the reconstruction error norm is computed and printed for each vector. :"
ml,Namespaces namespace cv::omnidir::internal 
ml,This is the proxy class for passing read-only input arrays into OpenCV functions.
ml,It is defined as:
ml,"where _InputArray is a class that can be constructed from Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat_<T> >, UMat, std::vector<UMat> or double. It can also be constructed from a matrix expression."
ml,"Since this is mostly implementation-level class, and its interface may change in future versions, we do not describe it in details. There are a few key things, though, that should be kept in mind:"
ml,"When you see in the reference manual or in OpenCV source code a function that takes InputArray, it means that you can actually pass Mat, Matx, vector<T> etc. (see above the complete list). Optional input arguments: If some of the input arrays may be empty, pass cv::noArray() (or simply cv::Mat() as you probably did before). The class is designed solely for passing parameters. That is, normally you should not declare class members, local and global variables of this type. If you want to design your own function or a class method that can operate of arrays of multiple types, you can use InputArray (or OutputArray) for the respective parameters. Inside a function you should use _InputArray::getMat() method to construct a matrix header for the array (without copying data). _InputArray::kind() can be used to distinguish Mat from vector<> etc., but normally it is not needed."
ml,Here is how you can use a function that takes InputArray :
ml,"That is, we form an STL vector containing points, and apply in-place affine transformation to the vector using the 2x3 matrix created inline as Matx<float, 2, 3> instance."
ml,"Here is how such a function can be implemented (for simplicity, we implement a very specific case of it, according to the assertion statement inside) :"
ml,"There is another related type, InputArrayOfArrays, which is currently defined as a synonym for InputArray:"
ml,"It denotes function arguments that are either vectors of vectors or vectors of matrices. A separate synonym is needed to generate Python/Java etc. wrappers properly. At the function implementation level their use is similar, but _InputArray::getMat(idx) should be used to get header for the idx-th component of the outer vector and _InputArray::size().area() should be used to find the number of components (vectors/matrices) of the outer vector."
ml,"In general, type support is limited to cv::Mat types. Other types are forbidden. But in some cases we need to support passing of custom non-general Mat types, like arrays of cv::KeyPoint, cv::DMatch, etc. This data is not intended to be interpreted as an image data, or processed somehow like regular cv::Mat. To pass such custom type use rawIn() / rawOut() / rawInOut() wrappers. Custom type is wrapped as Mat-compatible CV_8UC<N> values (N = sizeof(T), N <= CV_CN_MAX)."
ml,Information Flow algorithm implementaton for alphamatting
ml,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
ml,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
ml,The implementation is based on [7].
ml,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
ml,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
ml,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
ml,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
ml,The class represents a decision tree node.
ml,"Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as Mat's). It means that for each pixel location \((x,y)\) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location \((x,y)\). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one."
ml,"Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian \(3 \times 3\) filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (""replicated border"" extrapolation method), or assume that all the non-existing pixels are zeros (""constant border"" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see BorderTypes"
ml,Bayes classifier for normally distributed data.
ml,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
ml,Namespace for all functions is cv::img_hash.
ml,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
ml,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
ml,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
ml,Class for matching keypoint descriptors.
ml,"query descriptor index, train descriptor index, train image index, and distance between descriptors."
ml,This modules is to draw UTF-8 strings with freetype/harfbuzz.
ml,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
ml,Classes class cv::freetype::FreeType2 
ml,Matrix read-write iterator.
ml,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
ml,Read-write Sparse Matrix Iterator.
ml,"The class is similar to cv::SparseMatConstIterator, but can be used for in-place modification of the matrix elements."
ml,Namespaces namespace NcvCTprep 
ml,n-ary multi-dimensional array iterator.
ml,"Use the class to implement unary, binary, and, generally, n-ary element-wise operations on multi-dimensional arrays. Some of the arguments of an n-ary function may be continuous arrays, some may be not. It is possible to use conventional MatIterator 's for each array but incrementing all of the iterators after each small operations may be a big overhead. In this case consider using NAryMatIterator to iterate through several matrices simultaneously as long as they have the same geometry (dimensionality and all the dimension sizes are the same). On each iteration it.planes[0], it.planes[1],... will be the slices of the corresponding matrices."
ml,The example below illustrates how you can compute a normalized and threshold 3D color histogram:
ml,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
ml,This module contains:
ml,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
ml,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
ml,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
ml,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
ml,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
ml,Template class for specifying the size of an image or rectangle.
ml,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
ml,OpenCV defines the following Size_<> aliases:
ml,Boosted tree classifier derived from DTrees.
ml,Comma-separated Matrix Initializer.
ml,"The class instances are usually not created explicitly. Instead, they are created on ""matrix << firstValue"" operator."
ml,The sample below initializes 2x2 rotation matrix:
ml,The structure represents the logarithmic grid range of statmodel parameters.
ml,"It is used for optimizing statmodel accuracy by varying model parameters, the accuracy estimate being computed by cross-validation."
ml,"Template class for short numerical vectors, a partial case of Matx."
ml,"This template class represents short numerical vectors (of 1, 2, 3, 4 ... elements) on which you can perform basic arithmetical operations, access individual elements using [] operator etc. The vectors are allocated on stack, as opposite to std::valarray, std::vector, cv::Mat etc., which elements are dynamically allocated in the heap."
ml,The template takes 2 parameters:
ml,_Tp element type cn the number of elements
ml,"In addition to the universal notation like Vec<float, 3>, you can use shorter aliases for the most popular specialized variants of Vec, e.g. Vec3f ~ Vec<float, 3>."
ml,"It is possible to convert Vec<T,2> to/from Point_, Vec<T,3> to/from Point3_ , and Vec<T,4> to CvScalar or Scalar_. Use operator[] to access the elements of Vec."
ml,All the expected vector operations are also implemented:
ml,"v1 = v2 + v3 v1 = v2 - v3 v1 = v2 * scale v1 = scale * v2 v1 = -v2 v1 += v2 and other augmenting operations v1 == v2, v1 != v2 norm(v1) (euclidean norm) The Vec class is commonly used to describe pixel types of multi-channel arrays. See Mat for details."
ml,Implements Logistic Regression classifier.
ml,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
ml,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
ml,A helper class for cv::DataType.
ml,The class is specialized for each fundamental numerical data type supported by OpenCV. It provides DataDepth<T>::value constant.
ml,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
ml,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
ml,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
ml,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
ml,"Functions void cv::julia::initJulia (int argc, char **argv) "
ml,Template sparse n-dimensional array class derived from SparseMat.
ml,SparseMat_ is a thin wrapper on top of SparseMat created in the same way as Mat_ . It simplifies notation of some operations:
ml,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
ml,"SoftFloat is a software implementation of floating-point calculations according to IEEE 754 standard. All calculations are done in integers, that's why they are machine-independent and bit-exact. This library can be useful in accuracy-critical parts like look-up tables generation, tests, etc. OpenCV contains a subset of SoftFloat partially rewritten to C++."
ml,"Functions void cv::accumulate (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds an image to the accumulator image.  void cv::accumulateProduct (InputArray src1, InputArray src2, InputOutputArray dst, InputArray mask=noArray())  Adds the per-element product of two input images to the accumulator image.  void cv::accumulateSquare (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds the square of a source image to the accumulator image.  void cv::accumulateWeighted (InputArray src, InputOutputArray dst, double alpha, InputArray mask=noArray())  Updates a running average.  void cv::createHanningWindow (OutputArray dst, Size winSize, int type)  This function computes a Hanning window coefficients in two dimensions.  void cv::divSpectrums (InputArray a, InputArray b, OutputArray c, int flags, bool conjB=false)  Performs the per-element division of the first Fourier spectrum by the second Fourier spectrum.  Point2d cv::phaseCorrelate (InputArray src1, InputArray src2, InputArray window=noArray(), double *response=0)  The function is used to detect translational shifts that occur between two images. "
ml,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
ml,It provides easy interface to:
ml,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
ml,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
ml,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
ml,It is planned to have:
ml,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
ml,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
ml,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
ml,Classes class cv::DualQuat< _Tp >  class cv::Quat< _Tp >  class cv::QuatEnum 
ml,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
ml,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
ml,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
ml,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
ml,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
ml,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
ml,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
ml,The class represents a single decision tree or a collection of decision trees.
ml,"The current public interface of the class allows user to train only a single decision tree, however the class is capable of storing multiple decision trees and using them for prediction (by summing responses or using a voting schemes), and the derived from DTrees classes (such as RTrees and Boost) use this capability to implement decision tree ensembles."
ml,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
ml,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
ml,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
ml,This module has been originally developed as a project for Google Summer of Code 2012-2015.
ml,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
ml,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
ml,The distortion-free projective transformation given by a pinhole camera model is shown below.
ml,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
ml,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
ml,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
ml,\[p = A P_c.\]
ml,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
ml,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
ml,and thus
ml,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
ml,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
ml,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
ml,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
ml,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
ml,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
ml,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
ml,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
ml,and therefore
ml,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
ml,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
ml,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
ml,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
ml,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
ml,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
ml,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
ml,with
ml,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
ml,The following figure illustrates the pinhole camera model.
ml,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
ml,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
ml,where
ml,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
ml,with
ml,\[r^2 = x'^2 + y'^2\]
ml,and
ml,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
ml,if \(Z_c \ne 0\).
ml,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
ml,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
ml,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
ml,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
ml,where
ml,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
ml,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
ml,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
ml,In the functions below the coefficients are passed or returned as
ml,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
ml,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
ml,The functions below use the above model to do the following:
ml,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
ml,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
ml,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
ml,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
ml,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
ml,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
ml,if \(W \ne 0\).
ml,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
ml,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
ml,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
ml,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
ml,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
ml,Template Read-Only Sparse Matrix Iterator Class.
ml,This is the derived from SparseMatConstIterator class that introduces more convenient operator *() for accessing the current element.
ml,The class represents split in a decision tree.
ml,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
ml,The class defining termination criteria for iterative algorithms.
ml,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
ml,"ArUco Marker Detection, module functionality was moved to objdetect module"
ml,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
ml,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
ml,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
ml,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
ml,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
ml,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
ml,Template class specifying a continuous subsequence (slice) of a sequence.
ml,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
ml,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
ml,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
ml,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
ml,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
ml,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
ml,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
ml,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
ml,XML/YAML/JSON file storage class that encapsulates all the information necessary for writing or reading data to/from a file.
ml,A complex number class.
ml,"The template class is similar and compatible with std::complex, however it provides slightly more convenient access to the real and imaginary parts using through the simple field access, as opposite to std::complex::real() and std::complex::imag()."
ml,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
ml,Mersenne Twister random number generator.
ml,Inspired by http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/CODES/mt19937ar.c
ml,Template class for 2D points specified by its coordinates x and y.
ml,"An instance of the class is interchangeable with C structures, CvPoint and CvPoint2D32f . There is also a cast operator to convert point coordinates to the specified type. The conversion from floating-point coordinates to integer coordinates is done by rounding. Commonly, the conversion uses this operation for each of the coordinates. Besides the class members listed in the declaration above, the following operations on points are implemented:"
ml,"For your convenience, the following type aliases are defined:"
ml,Example:
ml,Linear Discriminant Analysis.
ml,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
ml,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
ml,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
ml,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
ml,"Each class derived from Map implements a motion model, as follows:"
ml,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
ml,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
ml,The classes derived from Mapper are
ml,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
ml,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
ml,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
ml,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
ml,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
ml,This module provides storage routines for Hierarchical Data Format objects.
ml,Face module changelog Face Recognition with OpenCV
ml,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
ml,Base class for statistical models in OpenCV ML.
ml,The class represents rotated (i.e. not up-right) rectangles on a plane.
ml,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
ml,The sample below demonstrates how to use RotatedRect:
ml,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
ml,Classes class cv::plot::Plot2d 
ml,Classes class cv::quality::QualityBase 
ml,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
ml,Class encapsulating training data.
ml,"Please note that the class only specifies the interface of training data, but not implementation. All the statistical model classes in ml module accepts Ptr<TrainData> as parameter. In other words, you can create your own class derived from TrainData and pass smart pointer to the instance of this class into StatModel::train."
ml,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
ml,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
ml,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
ml,Namespaces namespace cv::traits 
ml,This class declares example interface for system state used in simulated annealing optimization algorithm.
ml,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
ml,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
ml,Namespace for all functions is cv::intensity_transform.
ml,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
ml,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
ml,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
ml,The class SparseMat represents multi-dimensional sparse numerical arrays.
ml,"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:"
ml,"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5; int size[5] = {10, 10, 10, 10, 10}; SparseMat sparse_mat(dims, size, CV_32F); for(int i = 0; i < 1000; i++) { int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f; } cout << ""nnz = "" << sparse_mat.nzcount() << endl; cv::SparseMatThe class SparseMat represents multi-dimensional sparse numerical arrays.Definition mat.hpp:2751 cv::SparseMat::sizeconst int * size() constreturns the array of sizes, or NULL if the matrix is not allocated cv::SparseMat::dimsint dims() constreturns the matrix dimensionality CV_32F#define CV_32FDefinition interface.h:78 Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix // and the sum of elements. SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>(); double s = 0; int dims = sparse_mat.dims(); for(; it != it_end; ++it) { // print element indices and the element value const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("": %g\n"", it.value<float>()); s += *it; } printf(""Element sum is %g\n"", s); cv::SparseMatConstIterator_Template Read-Only Sparse Matrix Iterator Class.Definition mat.hpp:3354 cv::SparseMatConstIterator::valueconst _Tp & value() consttemplate method returning the current matrix element cv::SparseMatConstIterator::nodeconst SparseMat::Node * node() constreturns the current node of the sparse matrix. it.node->idx is the current element index cv::SparseMat::Nodesparse matrix node - element of a hash tableDefinition mat.hpp:2776 cv::SparseMat::Node::idxint idx[MAX_DIM]index of the matrix elementDefinition mat.hpp:2782 If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation. Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b) { const SparseMat *_a = &a, *_b = &b; // if b contains less elements than a, // it is faster to iterate through b if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // take the next element from the first matrix float avalue = *it; const Node* anode = it.node(); // and try to find an element with the same index in the second matrix. // since the hash value depends only on the element index, // reuse the hash value stored in the node float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr; } cv::SparseMat::endSparseMatIterator end()returns the sparse matrix iterator at the matrix end cv::SparseMat::nzcountsize_t nzcount() constreturns the number of non-zero elements (=the number of hash table nodes) cv::SparseMat::beginSparseMatIterator begin()returns the sparse matrix iterator at the matrix beginning cv::SparseMat::Node::hashvalsize_t hashvalhash valueDefinition mat.hpp:2778"
ml,The class implements K-Nearest Neighbors model.
ml,Read and write video or images sequence with OpenCV.
ml,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
ml,Bioinspired Module Retina Introduction
ml,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
ml,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
ml,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
ml,See detailed overview here: Machine Learning Overview.
ml,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
ml,Stochastic Gradient Descent SVM classifier.
ml,"SVMSGD provides a fast and easy-to-use implementation of the SVM classifier using the Stochastic Gradient Descent approach, as presented in [35]."
ml,The classifier has following parameters:
ml,"model type, margin type, margin regularization ( \(\lambda\)), initial step size ( \(\gamma_0\)), step decreasing power ( \(c\)), and termination criteria."
ml,The model type may have one of the following values: SGD and ASGD.
ml,"SGD is the classic version of SVMSGD classifier: every next step is calculated by the formula \[w_{t+1} = w_t - \gamma(t) \frac{dQ_i}{dw} |_{w = w_t}\] where \(w_t\) is the weights vector for decision function at step \(t\), \(\gamma(t)\) is the step size of model parameters at the iteration \(t\), it is decreased on each step by the formula \(\gamma(t) = \gamma_0 (1 + \lambda \gamma_0 t) ^ {-c}\) \(Q_i\) is the target functional from SVM task for sample with number \(i\), this sample is chosen stochastically on each step of the algorithm. \(w_t\) is the weights vector for decision function at step \(t\), \(\gamma(t)\) is the step size of model parameters at the iteration \(t\), it is decreased on each step by the formula \(\gamma(t) = \gamma_0 (1 + \lambda \gamma_0 t) ^ {-c}\) \(Q_i\) is the target functional from SVM task for sample with number \(i\), this sample is chosen stochastically on each step of the algorithm. ASGD is Average Stochastic Gradient Descent SVM Classifier. ASGD classifier averages weights vector on each step of algorithm by the formula \(\widehat{w}_{t+1} = \frac{t}{1+t}\widehat{w}_{t} + \frac{1}{1+t}w_{t+1}\)"
ml,\[w_{t+1} = w_t - \gamma(t) \frac{dQ_i}{dw} |_{w = w_t}\]
ml,"\(w_t\) is the weights vector for decision function at step \(t\), \(\gamma(t)\) is the step size of model parameters at the iteration \(t\), it is decreased on each step by the formula \(\gamma(t) = \gamma_0 (1 + \lambda \gamma_0 t) ^ {-c}\) \(Q_i\) is the target functional from SVM task for sample with number \(i\), this sample is chosen stochastically on each step of the algorithm."
ml,The recommended model type is ASGD (following [35]).
ml,The margin type may have one of the following values: SOFT_MARGIN or HARD_MARGIN.
ml,"You should use HARD_MARGIN type, if you have linearly separable sets. You should use SOFT_MARGIN type, if you have non-linearly separable sets or sets with outliers. In the general case (if you know nothing about linear separability of your sets), use SOFT_MARGIN."
ml,The other parameters may be described as follows:
ml,"Margin regularization parameter is responsible for weights decreasing at each step and for the strength of restrictions on outliers (the less the parameter, the less probability that an outlier will be ignored). Recommended value for SGD model is 0.0001, for ASGD model is 0.00001. Initial step size parameter is the initial value for the step size \(\gamma(t)\). You will have to find the best initial step for your problem. Step decreasing power is the power parameter for \(\gamma(t)\) decreasing by the formula, mentioned above. Recommended value for SGD model is 1, for ASGD model is 0.75. Termination criteria can be TermCriteria::COUNT, TermCriteria::EPS or TermCriteria::COUNT + TermCriteria::EPS. You will have to find the best termination criteria for your problem."
ml,"Note that the parameters margin regularization, initial step size, and step decreasing power should be positive."
ml,To use SVMSGD algorithm do as follows:
ml,"first, create the SVMSGD object. The algorithm will set optimal parameters by default, but you can set your own parameters via functions setSvmsgdType(), setMarginType(), setMarginRegularization(), setInitialStepSize(), and setStepDecreasingPower(). then the SVM model can be trained using the train features and the correspondent labels by the method train(). after that, the label of a new feature vector can be predicted using the method predict()."
ml,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
ml,File Storage Node class.
ml,"The node is used to store each and every element of the file storage opened for reading. When XML/YAML file is read, it is first parsed and stored in the memory as a hierarchical collection of nodes. Each node can be a ""leaf"" that is contain a single number or a string, or be a collection of other nodes. There can be named collections (mappings) where each element has a name and it is accessed by a name, and ordered collections (sequences) where elements do not have names but rather accessed by index. Type of the file node can be determined using FileNode::type method."
ml,"Note that file nodes are only used for navigating file storages opened for reading. When a file storage is opened for writing, no data is stored in memory after it is written."
ml,Template class for small matrices whose type and size are known at compilation time.
ml,"If you need a more flexible type, use Mat . The elements of the matrix M are accessible using the M(i,j) notation. Most of the common matrix operations (see also MatrixExpressions ) are available. To do an operation on Matx that is not implemented, you can easily convert the matrix to Mat and backwards:"
ml,"Except of the plain constructor which takes a list of elements, Matx can be initialized from a C-array:"
ml,"In case if C++11 features are available, std::initializer_list can be also used to initialize Matx:"
ml,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
ml,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
ml,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
ml,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
ml,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
ml,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
ml,Custom array allocator.
ml,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
ml,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
ml,"Template class for 3D points specified by its coordinates x, y and z."
ml,"An instance of the class is interchangeable with the C structure CvPoint2D32f . Similarly to Point_ , the coordinates of 3D points can be converted to another type. The vector arithmetic and comparison operations are also supported."
ml,The following Point3_<> aliases are available:
ml,Read-Only Sparse Matrix Iterator.
ml,Here is how to use the iterator to compute the sum of floating-point sparse matrix elements:
ml,Singular Value Decomposition.
ml,"Class for computing Singular Value Decomposition of a floating-point matrix. The Singular Value Decomposition is used to solve least-square problems, under-determined linear systems, invert matrices, compute condition numbers, and so on."
ml,"If you want to compute a condition number of a matrix or an absolute value of its determinant, you do not need u and vt. You can pass flags=SVD::NO_UV|... . Another flag SVD::FULL_UV indicates that full-size u and vt must be computed, which is not necessary most of the time."
ml,"Enumerations enum cv::TemplateMatchModes { cv::TM_SQDIFF = 0 , cv::TM_SQDIFF_NORMED = 1 , cv::TM_CCORR = 2 , cv::TM_CCORR_NORMED = 3 , cv::TM_CCOEFF = 4 , cv::TM_CCOEFF_NORMED = 5 }  type of the template matching operation More... "
ml,Template matrix class derived from Mat.
ml,"The class Mat_<_Tp> is a thin template wrapper on top of the Mat class. It does not have any extra data fields. Nor this class nor Mat has any virtual methods. Thus, references or pointers to these two classes can be freely but carefully converted one to another. For example:"
ml,"While Mat is sufficient in most cases, Mat_ can be more convenient if you use a lot of element access operations and if you know matrix type at the compilation time. Note that Mat::at(int y,int x) and Mat_::operator()(int y,int x) do absolutely the same and run at the same speed, but the latter is certainly shorter:"
ml,"To use Mat_ for multi-channel images/matrices, pass Vec as a Mat_ parameter:"
ml,Mat_ is fully compatible with C++11 range-based for loop. For example such loop can be used to safely apply look-up table:
ml,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
ml,"Namespace for all functions is cvv, i.e. cvv::showImage()."
ml,Compilation:
ml,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
ml,See cvv tutorial for a commented example application using cvv.
ml,Namespaces namespace cvv::impl 
objdetect,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
objdetect,ChArUco board is a planar chessboard where the markers are placed inside the white squares of a chessboard.
objdetect,"The benefits of ChArUco boards is that they provide both, ArUco markers versatility and chessboard corner precision, which is important for calibration and pose estimation. The board image can be drawn using generateImage() method."
objdetect,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
objdetect,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
objdetect,This module includes photo processing algorithms
objdetect,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
objdetect,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
objdetect,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
objdetect,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
objdetect,The implemented stitching pipeline is very similar to the one proposed in [41] .
objdetect,"This class is used for grouping object candidates detected by Cascade Classifier, HOG etc."
objdetect,instance of the class is to be passed to cv::partition
objdetect,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
objdetect,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
objdetect,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
objdetect,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
objdetect,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
objdetect,struct DetectorParameters is used by ArucoDetector
objdetect,Cascade classifier class for object detection.
objdetect,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
objdetect,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
objdetect,The main functionality of ArucoDetector class is detection of markers in an image with detectMarkers() method.
objdetect,"After detecting some markers in the image, you can try to find undetected markers from this dictionary with refineDetectedMarkers() method."
objdetect,This module includes signal processing algorithms.
objdetect,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
objdetect,ICP point-to-plane odometry algorithm
objdetect,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
objdetect,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
objdetect,Dense optical flow algorithms compute motion for each point:
objdetect,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
objdetect,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
objdetect,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
objdetect,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
objdetect,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
objdetect,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
objdetect,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
objdetect,struct for detection region of interest (ROI)
objdetect,Namespaces namespace cv::omnidir::internal 
objdetect,Classes struct cv::DetectionROI  struct for detection region of interest (ROI) More...  struct cv::HOGDescriptor  Implementation of HOG (Histogram of Oriented Gradients) descriptor and object detector. More... 
objdetect,Information Flow algorithm implementaton for alphamatting
objdetect,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
objdetect,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
objdetect,The implementation is based on [7].
objdetect,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
objdetect,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
objdetect,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
objdetect,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
objdetect,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
objdetect,Namespace for all functions is cv::img_hash.
objdetect,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
objdetect,Board of ArUco markers.
objdetect,"A board is a set of markers in the 3D space with a common coordinate system. The common form of a board of marker is a planar (2D) board, however any 3D layout can be used. A Board object is composed by:"
objdetect,"The object points of the marker corners, i.e. their coordinates respect to the board system. The dictionary which indicates the type of markers of the board The identifier of all the markers in the board."
objdetect,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
objdetect,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
objdetect,This modules is to draw UTF-8 strings with freetype/harfbuzz.
objdetect,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
objdetect,Classes class cv::freetype::FreeType2 
objdetect,struct RefineParameters is used by ArucoDetector
objdetect,Dictionary is a set of unique ArUco markers of the same size.
objdetect,bytesList storing as 2-dimensions Mat with 4-th channels (CV_8UC4 type was used) and contains the marker codewords where:
objdetect,"bytesList.rows is the dictionary size each marker is encoded using nbytes = ceil(markerSize*markerSize/8.) bytes each row contains all 4 rotations of the marker, so its length is 4*nbytes the byte order in the bytesList[i] row: //bytes without rotation/bytes with rotation 1/bytes with rotation 2/bytes with rotation 3// So bytesList.ptr(i)[k*nbytes + j] is the j-th byte of i-th marker, in its k-th rotation. NotePython bindings generate matrix with shape of bytesList dictionary_size x nbytes x 4, but it should be indexed like C++ version. Python example for j-th byte of i-th marker, in its k-th rotation: aruco_dict.bytesList[id].ravel()[k*nbytes + j]"
objdetect,Implementation of HOG (Histogram of Oriented Gradients) descriptor and object detector.
objdetect,the HOG descriptor algorithm introduced by Navneet Dalal and Bill Triggs [63] .
objdetect,useful links:
objdetect,https://hal.inria.fr/inria-00548512/document/
objdetect,https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients
objdetect,https://software.intel.com/en-us/ipp-dev-reference-histogram-of-oriented-gradients-hog-descriptor
objdetect,http://www.learnopencv.com/histogram-of-oriented-gradients
objdetect,http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial
objdetect,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
objdetect,Namespaces namespace NcvCTprep 
objdetect,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
objdetect,This module contains:
objdetect,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
objdetect,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
objdetect,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
objdetect,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
objdetect,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
objdetect,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
objdetect,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
objdetect,Classes class cv::barcode::BarcodeDetector 
objdetect,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
objdetect,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
objdetect,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
objdetect,"Functions void cv::julia::initJulia (int argc, char **argv) "
objdetect,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
objdetect,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
objdetect,It provides easy interface to:
objdetect,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
objdetect,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
objdetect,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
objdetect,It is planned to have:
objdetect,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
objdetect,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
objdetect,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
objdetect,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
objdetect,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
objdetect,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
objdetect,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
objdetect,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
objdetect,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
objdetect,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
objdetect,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
objdetect,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
objdetect,This module has been originally developed as a project for Google Summer of Code 2012-2015.
objdetect,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
objdetect,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
objdetect,The distortion-free projective transformation given by a pinhole camera model is shown below.
objdetect,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
objdetect,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
objdetect,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
objdetect,\[p = A P_c.\]
objdetect,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
objdetect,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
objdetect,and thus
objdetect,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
objdetect,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
objdetect,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
objdetect,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
objdetect,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
objdetect,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
objdetect,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
objdetect,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
objdetect,and therefore
objdetect,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
objdetect,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
objdetect,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
objdetect,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
objdetect,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
objdetect,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
objdetect,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
objdetect,with
objdetect,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
objdetect,The following figure illustrates the pinhole camera model.
objdetect,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
objdetect,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
objdetect,where
objdetect,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
objdetect,with
objdetect,\[r^2 = x'^2 + y'^2\]
objdetect,and
objdetect,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
objdetect,if \(Z_c \ne 0\).
objdetect,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
objdetect,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
objdetect,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
objdetect,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
objdetect,where
objdetect,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
objdetect,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
objdetect,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
objdetect,In the functions below the coefficients are passed or returned as
objdetect,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
objdetect,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
objdetect,The functions below use the above model to do the following:
objdetect,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
objdetect,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
objdetect,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
objdetect,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
objdetect,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
objdetect,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
objdetect,if \(W \ne 0\).
objdetect,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
objdetect,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
objdetect,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
objdetect,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
objdetect,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
objdetect,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
objdetect,"ArUco Marker Detection, module functionality was moved to objdetect module"
objdetect,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
objdetect,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
objdetect,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
objdetect,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
objdetect,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
objdetect,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
objdetect,DNN-based face recognizer.
objdetect,model download link: https://github.com/opencv/opencv_zoo/tree/master/models/face_recognition_sface
objdetect,Check the corresponding tutorial for more details.
objdetect,Classes class cv::FaceDetectorYN  DNN-based face detector. More...  class cv::FaceRecognizerSF  DNN-based face recognizer. More... 
objdetect,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
objdetect,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
objdetect,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
objdetect,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
objdetect,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
objdetect,"Each class derived from Map implements a motion model, as follows:"
objdetect,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
objdetect,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
objdetect,The classes derived from Mapper are
objdetect,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
objdetect,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
objdetect,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
objdetect,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
objdetect,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
objdetect,This module provides storage routines for Hierarchical Data Format objects.
objdetect,Face module changelog Face Recognition with OpenCV
objdetect,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
objdetect,Classes class cv::plot::Plot2d 
objdetect,Classes class cv::quality::QualityBase 
objdetect,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
objdetect,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
objdetect,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
objdetect,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
objdetect,Namespaces namespace cv::traits 
objdetect,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
objdetect,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
objdetect,Namespace for all functions is cv::intensity_transform.
objdetect,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
objdetect,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
objdetect,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
objdetect,Read and write video or images sequence with OpenCV.
objdetect,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
objdetect,Bioinspired Module Retina Introduction
objdetect,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
objdetect,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
objdetect,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
objdetect,See detailed overview here: Machine Learning Overview.
objdetect,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
objdetect,Planar board with grid arrangement of markers.
objdetect,More common type of board. All markers are placed in the same plane in a grid arrangement. The board image can be drawn using generateImage() method.
objdetect,The object detector described below has been initially proposed by Paul Viola [285] and improved by Rainer Lienhart [168] .
objdetect,"First, a classifier (namely a cascade of boosted classifiers working with haar-like features) is trained with a few hundred sample views of a particular object (i.e., a face or a car), called positive examples, that are scaled to the same size (say, 20x20), and negative examples - arbitrary images of the same size."
objdetect,"After a classifier is trained, it can be applied to a region of interest (of the same size as used during the training) in an input image. The classifier outputs a ""1"" if the region is likely to show the object (i.e., face/car), and ""0"" otherwise. To search for the object in the whole image one can move the search window across the image and check every location using the classifier. The classifier is designed so that it can be easily ""resized"" in order to be able to find the objects of interest at different sizes, which is more efficient than resizing the image itself. So, to find an object of an unknown size in the image the scan procedure should be done several times at different scales."
objdetect,"The word ""cascade"" in the classifier name means that the resultant classifier consists of several simpler classifiers (stages) that are applied subsequently to a region of interest until at some stage the candidate is rejected or all the stages are passed. The word ""boosted"" means that the classifiers at every stage of the cascade are complex themselves and they are built out of basic classifiers using one of four different boosting techniques (weighted voting). Currently Discrete Adaboost, Real Adaboost, Gentle Adaboost and Logitboost are supported. The basic classifiers are decision-tree classifiers with at least 2 leaves. Haar-like features are the input to the basic classifiers, and are calculated as described below. The current algorithm uses the following Haar-like features:"
objdetect,"The feature used in a particular classifier is specified by its shape (1a, 2b etc.), position within the region of interest and the scale (this scale is not the same as the scale used at the detection stage, though these two scales are multiplied). For example, in the case of the third line feature (2c) the response is calculated as the difference between the sum of image pixels under the rectangle covering the whole feature (including the two white stripes and the black stripe in the middle) and the sum of the image pixels under the black stripe multiplied by 3 in order to compensate for the differences in the size of areas. The sums of pixel values over a rectangular regions are calculated rapidly using integral images (see below and the integral description)."
objdetect,Check the corresponding tutorial for more details.
objdetect,The following reference is for the detection part only. There is a separate application called opencv_traincascade that can train a cascade of boosted classifiers from a set of samples.
objdetect,Classes class cv::BaseCascadeClassifier  class cv::CascadeClassifier  Cascade classifier class for object detection. More...  struct cv::DefaultDeleter< CvHaarClassifierCascade >  class cv::DetectionBasedTracker 
objdetect,"ArUco Marker Detection Square fiducial markers (also known as Augmented Reality Markers) are useful for easy, fast and robust camera pose estimation."
objdetect,"The main functionality of ArucoDetector class is detection of markers in an image. If the markers are grouped as a board, then you can try to recover the missing markers with ArucoDetector::refineDetectedMarkers(). ArUco markers can also be used for advanced chessboard corner finding. To do this, group the markers in the CharucoBoard and find the corners of the chessboard with the CharucoDetector::detectBoard()."
objdetect,The implementation is based on the ArUco Library by R. Muoz-Salinas and S. Garrido-Jurado [99].
objdetect,Markers can also be detected based on the AprilTag 2 [292] fiducial detection method.
objdetect,Classes class cv::aruco::ArucoDetector  The main functionality of ArucoDetector class is detection of markers in an image with detectMarkers() method. More...  class cv::aruco::Board  Board of ArUco markers. More...  class cv::aruco::CharucoBoard  ChArUco board is a planar chessboard where the markers are placed inside the white squares of a chessboard. More...  class cv::aruco::CharucoDetector  struct cv::aruco::CharucoParameters  struct cv::aruco::DetectorParameters  struct DetectorParameters is used by ArucoDetector More...  class cv::aruco::Dictionary  Dictionary is a set of unique ArUco markers of the same size. More...  class cv::aruco::GridBoard  Planar board with grid arrangement of markers. More...  struct cv::aruco::RefineParameters  struct RefineParameters is used by ArucoDetector More... 
objdetect,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
objdetect,"Classes class cv::GraphicalCodeDetector  class cv::SimilarRects  This class is used for grouping object candidates detected by Cascade Classifier, HOG etc. More... "
objdetect,DNN-based face detector.
objdetect,model download link: https://github.com/opencv/opencv_zoo/tree/master/models/face_detection_yunet
objdetect,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
objdetect,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
objdetect,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
objdetect,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
objdetect,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
objdetect,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
objdetect,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
objdetect,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
objdetect,Classes class cv::QRCodeDetector  class cv::QRCodeDetectorAruco  class cv::QRCodeEncoder 
objdetect,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
objdetect,"Namespace for all functions is cvv, i.e. cvv::showImage()."
objdetect,Compilation:
objdetect,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
objdetect,See cvv tutorial for a commented example application using cvv.
objdetect,Namespaces namespace cvv::impl 
photo,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
photo,Base storage class for GPU memory with reference counting.
photo,Its interface matches the Mat interface with the following limitations:
photo,no arbitrary dimensions support (only 2D) no functions that return references to their data (because references on GPU are not valid for CPU) no expression templates technique support
photo,Beware that the latter limitation may lead to overloaded matrix operators that cause memory allocations. The GpuMat class is convertible to cuda::PtrStepSz and cuda::PtrStep so it can be passed directly to the kernel.
photo,"Some member functions are described as a ""Blocking Call"" while some are described as a ""Non-Blocking Call"". Blocking functions are synchronous to host. It is guaranteed that the GPU operation is finished when the function returns. However, non-blocking functions are asynchronous to host. Those functions may return even if the GPU operation is not finished."
photo,"Compared to their blocking counterpart, non-blocking functions accept Stream as an additional argument. If a non-default stream is passed, the GPU operation may overlap with operations in other streams."
photo,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
photo,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
photo,This is a global tonemapping operator that models human visual system.
photo,"Mapping function is controlled by adaptation parameter, that is computed using light adaptation and color adaptation."
photo,For more information see [224] .
photo,This module includes photo processing algorithms
photo,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
photo,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
photo,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
photo,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
photo,The implemented stitching pipeline is very similar to the one proposed in [41] .
photo,"Functions void cv::denoise_TVL1 (const std::vector< Mat > &observations, Mat &result, double lambda=1.0, int niters=30)  Primal-dual algorithm is an algorithm for solving special types of variational problems (that is, finding a function to minimize some functional). As the image denoising, in particular, may be seen as the variational problem, primal-dual algorithm then can be used to perform denoising and this is exactly what is implemented.  void cv::cuda::fastNlMeansDenoising (const GpuMat &src, GpuMat &dst, float h, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  void cv::fastNlMeansDenoising (InputArray src, OutputArray dst, const std::vector< float > &h, int templateWindowSize=7, int searchWindowSize=21, int normType=NORM_L2)  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/ with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::cuda::fastNlMeansDenoising (InputArray src, OutputArray dst, float h, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::fastNlMeansDenoising (InputArray src, OutputArray dst, float h=3, int templateWindowSize=7, int searchWindowSize=21)  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/ with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::cuda::fastNlMeansDenoisingColored (const GpuMat &src, GpuMat &dst, float h_luminance, float photo_render, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  void cv::fastNlMeansDenoisingColored (InputArray src, OutputArray dst, float h=3, float hColor=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoising function for colored images.  void cv::cuda::fastNlMeansDenoisingColored (InputArray src, OutputArray dst, float h_luminance, float photo_render, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  Modification of fastNlMeansDenoising function for colored images.  void cv::fastNlMeansDenoisingColoredMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, float h=3, float hColor=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoisingMulti function for colored images sequences.  void cv::fastNlMeansDenoisingMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, const std::vector< float > &h, int templateWindowSize=7, int searchWindowSize=21, int normType=NORM_L2)  Modification of fastNlMeansDenoising function for images sequence where consecutive images have been captured in small period of time. For example video. This version of the function is for grayscale images or for manual manipulation with colorspaces. See [44] for more details (open access here).  void cv::fastNlMeansDenoisingMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, float h=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoising function for images sequence where consecutive images have been captured in small period of time. For example video. This version of the function is for grayscale images or for manual manipulation with colorspaces. See [44] for more details (open access here).  void cv::cuda::nonLocalMeans (const GpuMat &src, GpuMat &dst, float h, int search_window=21, int block_size=7, int borderMode=BORDER_DEFAULT, Stream &stream=Stream::Null())  void cv::cuda::nonLocalMeans (InputArray src, OutputArray dst, float h, int search_window=21, int block_size=7, int borderMode=BORDER_DEFAULT, Stream &stream=Stream::Null())  Performs pure non local means denoising without any simplification, and thus it is not fast. "
photo,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
photo,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
photo,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
photo,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
photo,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
photo,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
photo,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
photo,This module includes signal processing algorithms.
photo,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
photo,ICP point-to-plane odometry algorithm
photo,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
photo,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
photo,Dense optical flow algorithms compute motion for each point:
photo,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
photo,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
photo,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
photo,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
photo,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
photo,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
photo,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
photo,Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. This algorithm uses all image pixels.
photo,For more information see [227] .
photo,n-dimensional dense array class
photo,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
photo,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
photo,"In case of a 2-dimensional array, the above formula is reduced to:"
photo,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
photo,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
photo,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
photo,There are many different ways to create a Mat object. The most popular options are listed below:
photo,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
photo,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
photo,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
photo,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
photo,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
photo,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
photo,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
photo,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
photo,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
photo,Namespaces namespace cv::omnidir::internal 
photo,Useful links:
photo,http://www.cse.cuhk.edu.hk/leojia/projects/color2gray/index.html
photo,"Functions void cv::decolor (InputArray src, OutputArray grayscale, OutputArray color_boost)  Transforms a color image to a grayscale image. It is a basic tool in digital printing, stylized black-and-white photograph rendering, and in many single channel image processing applications [176] . "
photo,Information Flow algorithm implementaton for alphamatting
photo,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
photo,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
photo,The implementation is based on [7].
photo,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
photo,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
photo,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
photo,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
photo,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
photo,Namespace for all functions is cv::img_hash.
photo,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
photo,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
photo,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
photo,The base class for algorithms that align images of the same scene with different exposures.
photo,This modules is to draw UTF-8 strings with freetype/harfbuzz.
photo,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
photo,Classes class cv::freetype::FreeType2 
photo,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
photo,Namespaces namespace NcvCTprep 
photo,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
photo,This module contains:
photo,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
photo,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
photo,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
photo,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
photo,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
photo,The base class algorithms that can merge exposure sequence to a single image.
photo,"This section describes high dynamic range imaging algorithms namely tonemapping, exposure alignment, camera calibration with multiple exposures and exposure fusion."
photo,"Classes class cv::AlignExposures  The base class for algorithms that align images of the same scene with different exposures. More...  class cv::AlignMTB  This algorithm converts images to median threshold bitmaps (1 for pixels brighter than median luminance and 0 otherwise) and than aligns the resulting bitmaps using bit operations. More...  class cv::CalibrateCRF  The base class for camera response calibration algorithms. More...  class cv::CalibrateDebevec  Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. Objective function is constructed using pixel values on the same position in all images, extra term is added to make the result smoother. More...  class cv::CalibrateRobertson  Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. This algorithm uses all image pixels. More...  class cv::MergeDebevec  The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response. More...  class cv::MergeExposures  The base class algorithms that can merge exposure sequence to a single image. More...  class cv::MergeMertens  Pixels are weighted using contrast, saturation and well-exposedness measures, than images are combined using laplacian pyramids. More...  class cv::MergeRobertson  The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response. More...  class cv::Tonemap  Base class for tonemapping algorithms - tools that are used to map HDR image to 8-bit range. More...  class cv::TonemapDrago  Adaptive logarithmic mapping is a fast global tonemapping algorithm that scales the image in logarithmic domain. More...  class cv::TonemapMantiuk  This algorithm transforms image to contrast using gradients on all levels of gaussian pyramid, transforms contrast values to HVS response and scales the response. After this the image is reconstructed from new contrast values. More...  class cv::TonemapReinhard  This is a global tonemapping operator that models human visual system. More... "
photo,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
photo,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
photo,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
photo,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
photo,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
photo,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
photo,"Functions void cv::julia::initJulia (int argc, char **argv) "
photo,The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response.
photo,For more information see [227] .
photo,the inpainting algorithm
photo,"Enumerations enum { cv::INPAINT_NS = 0 , cv::INPAINT_TELEA = 1 } "
photo,This algorithm converts images to median threshold bitmaps (1 for pixels brighter than median luminance and 0 otherwise) and than aligns the resulting bitmaps using bit operations.
photo,"It is invariant to exposure, so exposure values and camera response are not necessary."
photo,In this implementation new image regions are filled with zeros.
photo,For more information see [295] .
photo,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
photo,"Pixels are weighted using contrast, saturation and well-exposedness measures, than images are combined using laplacian pyramids."
photo,"The resulting image weight is constructed as weighted average of contrast, saturation and well-exposedness measures."
photo,"The resulting image doesn't require tonemapping and can be converted to 8-bit image by multiplying by 255, but it's recommended to apply gamma correction and/or linear tonemapping."
photo,For more information see [189] .
photo,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
photo,It provides easy interface to:
photo,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
photo,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
photo,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
photo,It is planned to have:
photo,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
photo,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
photo,Base class for tonemapping algorithms - tools that are used to map HDR image to 8-bit range.
photo,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
photo,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
photo,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
photo,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
photo,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
photo,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
photo,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
photo,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
photo,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
photo,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
photo,This module has been originally developed as a project for Google Summer of Code 2012-2015.
photo,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
photo,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
photo,The distortion-free projective transformation given by a pinhole camera model is shown below.
photo,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
photo,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
photo,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
photo,\[p = A P_c.\]
photo,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
photo,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
photo,and thus
photo,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
photo,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
photo,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
photo,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
photo,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
photo,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
photo,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
photo,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
photo,and therefore
photo,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
photo,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
photo,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
photo,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
photo,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
photo,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
photo,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
photo,with
photo,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
photo,The following figure illustrates the pinhole camera model.
photo,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
photo,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
photo,where
photo,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
photo,with
photo,\[r^2 = x'^2 + y'^2\]
photo,and
photo,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
photo,if \(Z_c \ne 0\).
photo,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
photo,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
photo,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
photo,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
photo,where
photo,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
photo,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
photo,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
photo,In the functions below the coefficients are passed or returned as
photo,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
photo,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
photo,The functions below use the above model to do the following:
photo,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
photo,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
photo,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
photo,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
photo,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
photo,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
photo,if \(W \ne 0\).
photo,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
photo,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
photo,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
photo,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
photo,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
photo,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
photo,"ArUco Marker Detection, module functionality was moved to objdetect module"
photo,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
photo,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
photo,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
photo,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
photo,This class encapsulates a queue of asynchronous calls.
photo,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
photo,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
photo,"Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. Objective function is constructed using pixel values on the same position in all images, extra term is added to make the result smoother."
photo,For more information see [68] .
photo,Adaptive logarithmic mapping is a fast global tonemapping algorithm that scales the image in logarithmic domain.
photo,"Since it's a global operator the same function is applied to all the pixels, it is controlled by the bias parameter."
photo,Optional saturation enhancement is possible as described in [84] .
photo,For more information see [71] .
photo,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
photo,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
photo,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
photo,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
photo,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
photo,"Each class derived from Map implements a motion model, as follows:"
photo,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
photo,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
photo,The classes derived from Mapper are
photo,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
photo,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
photo,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
photo,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
photo,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
photo,This module provides storage routines for Hierarchical Data Format objects.
photo,Face module changelog Face Recognition with OpenCV
photo,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
photo,Classes class cv::plot::Plot2d 
photo,Classes class cv::quality::QualityBase 
photo,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
photo,Common interface for all CUDA filters :
photo,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
photo,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
photo,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
photo,Namespaces namespace cv::traits 
photo,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
photo,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
photo,Namespace for all functions is cv::intensity_transform.
photo,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
photo,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
photo,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
photo,The base class for camera response calibration algorithms.
photo,Read and write video or images sequence with OpenCV.
photo,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
photo,Bioinspired Module Retina Introduction
photo,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
photo,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
photo,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
photo,See detailed overview here: Machine Learning Overview.
photo,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
photo,"This algorithm transforms image to contrast using gradients on all levels of gaussian pyramid, transforms contrast values to HVS response and scales the response. After this the image is reconstructed from new contrast values."
photo,For more information see [182] .
photo,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
photo,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
photo,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
photo,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
photo,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
photo,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
photo,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
photo,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
photo,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
photo,The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response.
photo,For more information see [68] .
photo,Useful links:
photo,https://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp
photo,"Enumerations enum { cv::NORMAL_CLONE = 1 , cv::MIXED_CLONE = 2 , cv::MONOCHROME_TRANSFER = 3 }  seamlessClone algorithm flags More... "
photo,Useful links:
photo,http://www.inf.ufrgs.br/~eslgastal/DomainTransform
photo,https://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/
photo,"Enumerations enum { cv::RECURS_FILTER = 1 , cv::NORMCONV_FILTER = 2 }  Edge preserving filters. More... "
photo,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
photo,"Namespace for all functions is cvv, i.e. cvv::showImage()."
photo,Compilation:
photo,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
photo,See cvv tutorial for a commented example application using cvv.
photo,Namespaces namespace cvv::impl 
stitching,Base class for all minimum graph-cut-based seam estimators.
stitching,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
stitching,Abstract base class for 2D image feature detectors and descriptor extractors.
stitching,"Template ""trait"" class for OpenCV primitive data types."
stitching,"A primitive OpenCV data type is one of unsigned char, bool, signed char, unsigned short, signed short, int, float, double, or a tuple of values of one of these types, where all the values in the tuple have the same type. Any primitive type from the list can be defined by an identifier in the form CV_<bit-depth>{U|S|F}C(<number_of_channels>), for example: uchar CV_8UC1, 3-element floating-point tuple CV_32FC3, and so on. A universal OpenCV structure that is able to store a single instance of such a primitive data type is Vec. Multiple instances of such a type can be stored in a std::vector, Mat, Mat_, SparseMat, SparseMat_, or any other container that is able to store Vec instances."
stitching,"The DataType class is basically used to provide a description of such primitive data types without adding any fields or methods to the corresponding classes (and it is actually impossible to add anything to primitive C/C++ data types). This technique is known in C++ as class traits. It is not DataType itself that is used but its specialized versions, such as:"
stitching,"The main purpose of this class is to convert compilation-time type information to an OpenCV-compatible data type identifier, for example:"
stitching,"So, such traits are used to tell OpenCV which data type you are working with, even if such a type is not native to OpenCV. For example, the matrix B initialization above is compiled because OpenCV defines the proper specialized template class DataType<complex<_Tp> > . This mechanism is also useful (and used in OpenCV this way) for generic algorithms implementations."
stitching,opencv2/core/traits.hpp
stitching,ChArUco board is a planar chessboard where the markers are placed inside the white squares of a chessboard.
stitching,"The benefits of ChArUco boards is that they provide both, ArUco markers versatility and chessboard corner precision, which is important for calibration and pose estimation. The board image can be drawn using generateImage() method."
stitching,Plane warper factory class.
stitching,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
stitching,"Exposure compensator which tries to remove exposure related artifacts by adjusting image intensities, see [41] and [305] for details."
stitching,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
stitching,Affine transformation based estimator.
stitching,This estimator uses pairwise transformations estimated by matcher to estimate final transformation for each camera.
stitching,Classes class cv::detail::AffineWarper  Affine warper that uses rotations and translations. More...  class cv::AffineWarper  Affine warper factory class. More...  struct cv::detail::CompressedRectilinearPortraitProjector  class cv::CompressedRectilinearPortraitWarper  class cv::detail::CompressedRectilinearPortraitWarper  struct cv::detail::CompressedRectilinearProjector  class cv::CompressedRectilinearWarper  class cv::detail::CompressedRectilinearWarper  struct cv::detail::CylindricalPortraitProjector  class cv::detail::CylindricalPortraitWarper  struct cv::detail::CylindricalProjector  class cv::detail::CylindricalWarper  Warper that maps an image onto the x*x + z*z = 1 cylinder. More...  class cv::CylindricalWarper  Cylindrical warper factory class. More...  class cv::detail::CylindricalWarperGpu  struct cv::detail::FisheyeProjector  class cv::detail::FisheyeWarper  class cv::FisheyeWarper  struct cv::detail::MercatorProjector  class cv::detail::MercatorWarper  class cv::MercatorWarper  struct cv::detail::PaniniPortraitProjector  class cv::PaniniPortraitWarper  class cv::detail::PaniniPortraitWarper  struct cv::detail::PaniniProjector  class cv::PaniniWarper  class cv::detail::PaniniWarper  struct cv::detail::PlanePortraitProjector  class cv::detail::PlanePortraitWarper  struct cv::detail::PlaneProjector  class cv::PlaneWarper  Plane warper factory class. More...  class cv::detail::PlaneWarper  Warper that maps an image onto the z = 1 plane. More...  class cv::detail::PlaneWarperGpu  struct cv::detail::ProjectorBase  Base class for warping logic implementation. More...  class cv::detail::RotationWarper  Rotation-only model image warper interface. More...  class cv::detail::RotationWarperBase< P >  Base class for rotation-based warper using a detail::ProjectorBase_ derived class. More...  struct cv::detail::SphericalPortraitProjector  class cv::detail::SphericalPortraitWarper  struct cv::detail::SphericalProjector  class cv::detail::SphericalWarper  Warper that maps an image onto the unit sphere located at the origin. More...  class cv::SphericalWarper  Spherical warper factory class. More...  class cv::detail::SphericalWarperGpu  struct cv::detail::StereographicProjector  class cv::StereographicWarper  class cv::detail::StereographicWarper  struct cv::detail::TransverseMercatorProjector  class cv::detail::TransverseMercatorWarper  class cv::TransverseMercatorWarper  class cv::WarperCreator  Image warper factories base class. More... 
stitching,This module includes photo processing algorithms
stitching,"Matrix expression representation This is a list of implemented matrix operations that can be combined in arbitrary complex expressions (here A, B stand for matrices ( Mat ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double )):"
stitching,"Addition, subtraction, negation: A+B, A-B, A+s, A-s, s+A, s-A, -A Scaling: A*alpha Per-element multiplication and division: A.mul(B), A/B, alpha/A Matrix multiplication: A*B Transposition: A.t() (means AT) Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems: A.inv([method]) (~ A<sup>-1</sup>), A.inv([method])*B (~ X: AX=B) Comparison: A cmpop B, A cmpop alpha, alpha cmpop A, where cmpop is one of >, >=, ==, !=, <=, <. The result of comparison is an 8-bit single channel mask whose elements are set to 255 (if the particular element or pair of elements satisfy the condition) or 0. Bitwise logical operations: A logicop B, A logicop s, s logicop A, ~A, where logicop is one of &, |, ^. Element-wise minimum and maximum: min(A, B), min(A, alpha), max(A, B), max(A, alpha) Element-wise absolute value: abs(A) Cross-product, dot-product: A.cross(B), A.dot(B) Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm, mean, sum, countNonZero, trace, determinant, repeat, and others. Matrix initializers ( Mat::eye(), Mat::zeros(), Mat::ones() ), matrix comma-separated initializers, matrix constructors and operators that extract sub-matrices (see Mat description). Mat_<destination_type>() constructors to cast the result to the proper type. NoteComma-separated initializers and probably some other operations may require additional explicit Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity. Here are examples of matrix expressions: // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD) SVD svd(A); Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t(); // compute the new vector of parameters in the Levenberg-Marquardt algorithm x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err); // sharpen image using ""unsharp mask"" algorithm Mat blurred; double sigma = 1, threshold = 5, amount = 1; GaussianBlur(img, blurred, Size(), sigma, sigma); Mat lowContrastMask = abs(img - blurred) < threshold; Mat sharpened = img*(1+amount) + blurred*(-amount); img.copyTo(sharpened, lowContrastMask); cv::MatExpr::absMatExpr abs(const Mat &m)Calculates an absolute value of each matrix element. cv::MatExpr::invMatExpr inv(int method=DECOMP_LU) const cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::Mat::diagMat diag(int d=0) constExtracts a diagonal from a matrix. cv::Mat::uUMatData * uinteraction with UMatDefinition mat.hpp:2174 cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Mat::tMatExpr t() constTransposes a matrix. cv::SVDSingular Value Decomposition.Definition core.hpp:2739 cv::DECOMP_CHOLESKY@ DECOMP_CHOLESKYDefinition base.hpp:143 cv::SizeSize2i SizeDefinition types.hpp:370 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. cv::thresholddouble threshold(InputArray src, OutputArray dst, double thresh, double maxval, int type)Applies a fixed-level threshold to each array element."
stitching,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
stitching,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
stitching,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
stitching,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
stitching,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
stitching,The implemented stitching pipeline is very similar to the one proposed in [41] .
stitching,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
stitching,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
stitching,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
stitching,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
stitching,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
stitching,Template class for a 4-element vector derived from Vec.
stitching,"Being derived from Vec<_Tp, 4> , Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar . The type Scalar is widely used in OpenCV to pass pixel values."
stitching,Matrix read-only iterator.
stitching,Class passed to an error.
stitching,This class encapsulates all or almost all necessary information about the error happened in the program. The exception is usually constructed and thrown implicitly via CV_Error and CV_Error_ macros.
stitching,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
stitching,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
stitching,The main functionality of ArucoDetector class is detection of markers in an image with detectMarkers() method.
stitching,"After detecting some markers in the image, you can try to find undetected markers from this dictionary with refineDetectedMarkers() method."
stitching,This module includes signal processing algorithms.
stitching,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
stitching,Data structure for salient point detectors.
stitching,"The class instance stores a keypoint, i.e. a point feature found by one of many available keypoint detectors, such as Harris corner detector, FAST, StarDetector, SURF, SIFT etc."
stitching,"The keypoint is characterized by the 2D position, scale (proportional to the diameter of the neighborhood that needs to be taken into account), orientation and some other parameters. The keypoint neighborhood is then analyzed by another algorithm that builds a descriptor (usually represented as a feature vector). The keypoints representing the same object in different images can then be matched using KDTree or another method."
stitching,Base class for all exposure compensators.
stitching,ICP point-to-plane odometry algorithm
stitching,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
stitching,Template class for 2D rectangles.
stitching,described by the following parameters:
stitching,"Coordinates of the top-left corner. This is a default interpretation of Rect_::x and Rect_::y in OpenCV. Though, in your algorithms you may count x and y from the bottom-left corner. Rectangle width and height."
stitching,"OpenCV typically assumes that the top and left boundary of the rectangle are inclusive, while the right and bottom boundaries are not. For example, the method Rect_::contains returns true if"
stitching,"\[x \leq pt.x < x+width, y \leq pt.y < y+height\]"
stitching,Virtually every loop over an image ROI in OpenCV (where ROI is specified by Rect_<int> ) is implemented as:
stitching,"In addition to the class members, the following operations on rectangles are implemented:"
stitching,"\(\texttt{rect} = \texttt{rect} \pm \texttt{point}\) (shifting a rectangle by a certain offset) \(\texttt{rect} = \texttt{rect} \pm \texttt{size}\) (expanding or shrinking a rectangle by a certain amount) rect += point, rect -= point, rect += size, rect -= size (augmenting operations) rect = rect1 & rect2 (rectangle intersection) rect = rect1 | rect2 (minimum area rectangle containing rect1 and rect2 ) rect &= rect1, rect |= rect1 (and the corresponding augmenting operations) rect == rect1, rect != rect1 (rectangle comparison)"
stitching,This is an example how the partial ordering on rectangles can be established (rect1 \(\subseteq\) rect2):
stitching,"For your convenience, the Rect_<> alias is available: cv::Rect"
stitching,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
stitching,YOUR ATTENTION PLEASE!
stitching,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
stitching,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
stitching,Note for developers: please don't put videoio dependency in G-API because of this file.
stitching,Dense optical flow algorithms compute motion for each point:
stitching,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
stitching,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
stitching,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
stitching,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
stitching,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
stitching,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
stitching,High level image stitcher.
stitching,"It's possible to use this class without being aware of the entire stitching pipeline. However, to be able to achieve higher stitching stability and quality of the final images at least being familiar with the theory is recommended."
stitching,A basic example on image stitching can be found at opencv_source_code/samples/cpp/stitching.cpp A basic example on image stitching in Python can be found at opencv_source_code/samples/python/stitching.py A detailed example on image stitching can be found at opencv_source_code/samples/cpp/stitching_detailed.cpp
stitching,Structure containing information about matches between two images.
stitching,It's assumed that there is a transformation between those images. Transformation may be homography or affine transformation based on selected matcher.
stitching,This type is very similar to InputArray except that it is used for input/output and output function parameters.
stitching,"Just like with InputArray, OpenCV users should not care about OutputArray, they just pass Mat, vector<T> etc. to the functions. The same limitation as for InputArray: Do not explicitly create OutputArray instances applies here too."
stitching,"If you want to make your function polymorphic (i.e. accept different arrays as output parameters), it is also not very difficult. Take the sample above as the reference. Note that _OutputArray::create() needs to be called before _OutputArray::getMat(). This way you guarantee that the output array is properly allocated."
stitching,"Optional output parameters. If you do not need certain output array to be computed and returned to you, pass cv::noArray(), just like you would in the case of optional input array. At the implementation level, use _OutputArray::needed() to check if certain output array needs to be computed or not."
stitching,There are several synonyms for OutputArray that are used to assist automatic Python/Java/... wrapper generators:
stitching,Stub exposure compensator which does nothing.
stitching,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
stitching,Template Read-Write Sparse Matrix Iterator Class.
stitching,This is the derived from cv::SparseMatConstIterator_ class that introduces more convenient operator *() for accessing the current element.
stitching,Spherical warper factory class.
stitching,Warper that maps an image onto the z = 1 plane.
stitching,This is a base class for all more or less complex algorithms in OpenCV.
stitching,"especially for classes of algorithms, for which there can be multiple implementations. The examples are stereo correspondence (for which there are algorithms like block matching, semi-global block matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck etc.)."
stitching,Here is example of SimpleBlobDetector use in your application via Algorithm interface:
stitching,n-dimensional dense array class
stitching,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
stitching,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
stitching,"In case of a 2-dimensional array, the above formula is reduced to:"
stitching,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
stitching,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
stitching,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
stitching,There are many different ways to create a Mat object. The most popular options are listed below:
stitching,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
stitching,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
stitching,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
stitching,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
stitching,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
stitching,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
stitching,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
stitching,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
stitching,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
stitching,This interface class allows to build new Layers - are building blocks of networks.
stitching,"Each class, derived from Layer, must implement forward() method to compute outputs. Also before using the new layer into networks you must register your layer by using one of LayerFactory macros."
stitching,Stub seam estimator which does nothing.
stitching,Exposure compensator which tries to remove exposure related artifacts by adjusting image block on each channel.
stitching,Warper that maps an image onto the x*x + z*z = 1 cylinder.
stitching,Features matcher similar to cv::detail::BestOf2NearestMatcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf.
stitching,Unlike cv::detail::BestOf2NearestMatcher this matcher uses affine transformation (affine transformation estimate will be placed in matches_info).
stitching,Namespaces namespace cv::omnidir::internal 
stitching,Describes camera parameters.
stitching,Features matcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf.
stitching,This is the proxy class for passing read-only input arrays into OpenCV functions.
stitching,It is defined as:
stitching,"where _InputArray is a class that can be constructed from Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat_<T> >, UMat, std::vector<UMat> or double. It can also be constructed from a matrix expression."
stitching,"Since this is mostly implementation-level class, and its interface may change in future versions, we do not describe it in details. There are a few key things, though, that should be kept in mind:"
stitching,"When you see in the reference manual or in OpenCV source code a function that takes InputArray, it means that you can actually pass Mat, Matx, vector<T> etc. (see above the complete list). Optional input arguments: If some of the input arrays may be empty, pass cv::noArray() (or simply cv::Mat() as you probably did before). The class is designed solely for passing parameters. That is, normally you should not declare class members, local and global variables of this type. If you want to design your own function or a class method that can operate of arrays of multiple types, you can use InputArray (or OutputArray) for the respective parameters. Inside a function you should use _InputArray::getMat() method to construct a matrix header for the array (without copying data). _InputArray::kind() can be used to distinguish Mat from vector<> etc., but normally it is not needed."
stitching,Here is how you can use a function that takes InputArray :
stitching,"That is, we form an STL vector containing points, and apply in-place affine transformation to the vector using the 2x3 matrix created inline as Matx<float, 2, 3> instance."
stitching,"Here is how such a function can be implemented (for simplicity, we implement a very specific case of it, according to the assertion statement inside) :"
stitching,"There is another related type, InputArrayOfArrays, which is currently defined as a synonym for InputArray:"
stitching,"It denotes function arguments that are either vectors of vectors or vectors of matrices. A separate synonym is needed to generate Python/Java etc. wrappers properly. At the function implementation level their use is similar, but _InputArray::getMat(idx) should be used to get header for the idx-th component of the outer vector and _InputArray::size().area() should be used to find the number of components (vectors/matrices) of the outer vector."
stitching,"In general, type support is limited to cv::Mat types. Other types are forbidden. But in some cases we need to support passing of custom non-general Mat types, like arrays of cv::KeyPoint, cv::DMatch, etc. This data is not intended to be interpreted as an image data, or processed somehow like regular cv::Mat. To pass such custom type use rawIn() / rawOut() / rawInOut() wrappers. Custom type is wrapped as Mat-compatible CV_8UC<N> values (N = sizeof(T), N <= CV_CN_MAX)."
stitching,Information Flow algorithm implementaton for alphamatting
stitching,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
stitching,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
stitching,The implementation is based on [7].
stitching,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
stitching,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
stitching,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
stitching,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
stitching,Simple blender which mixes images at its borders.
stitching,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
stitching,Namespace for all functions is cv::img_hash.
stitching,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
stitching,Board of ArUco markers.
stitching,"A board is a set of markers in the 3D space with a common coordinate system. The common form of a board of marker is a planar (2D) board, however any 3D layout can be used. A Board object is composed by:"
stitching,"The object points of the marker corners, i.e. their coordinates respect to the board system. The dictionary which indicates the type of markers of the board The identifier of all the markers in the board."
stitching,Structure containing image keypoints and descriptors.
stitching,Bundle adjuster that expects affine transformation with 4 DOF represented in homogeneous coordinates in R for each camera param. Implements camera parameters refinement algorithm which minimizes sum of the reprojection error squares.
stitching,It estimates all transformation parameters. Refinement mask is ignored.
stitching,"Functions bool cv::detail::calibrateRotatingCamera (const std::vector< Mat > &Hs, Mat &K)  void cv::detail::estimateFocal (const std::vector< ImageFeatures > &features, const std::vector< MatchesInfo > &pairwise_matches, std::vector< double > &focals)  Estimates focal lengths for each given camera.  void cv::detail::focalsFromHomography (const Mat &H, double &f0, double &f1, bool &f0_ok, bool &f1_ok)  Tries to estimate focal lengths from the given homography under the assumption that the camera undergoes rotations around its centre only. "
stitching,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
stitching,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
stitching,Affine warper factory class.
stitching,Exposure compensator which tries to remove exposure related artifacts by adjusting image intensities on each channel independently.
stitching,Class for matching keypoint descriptors.
stitching,"query descriptor index, train descriptor index, train image index, and distance between descriptors."
stitching,This modules is to draw UTF-8 strings with freetype/harfbuzz.
stitching,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
stitching,Classes class cv::freetype::FreeType2 
stitching,Matrix read-write iterator.
stitching,Blender which uses multi-band blending algorithm (see [45]).
stitching,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
stitching,Base class for all camera parameters refinement methods.
stitching,Read-write Sparse Matrix Iterator.
stitching,"The class is similar to cv::SparseMatConstIterator, but can be used for in-place modification of the matrix elements."
stitching,Namespaces namespace NcvCTprep 
stitching,n-ary multi-dimensional array iterator.
stitching,"Use the class to implement unary, binary, and, generally, n-ary element-wise operations on multi-dimensional arrays. Some of the arguments of an n-ary function may be continuous arrays, some may be not. It is possible to use conventional MatIterator 's for each array but incrementing all of the iterators after each small operations may be a big overhead. In this case consider using NAryMatIterator to iterate through several matrices simultaneously as long as they have the same geometry (dimensionality and all the dimension sizes are the same). On each iteration it.planes[0], it.planes[1],... will be the slices of the corresponding matrices."
stitching,The example below illustrates how you can compute a normalized and threshold 3D color histogram:
stitching,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
stitching,This module contains:
stitching,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
stitching,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
stitching,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
stitching,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
stitching,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
stitching,Base class for all blenders.
stitching,Simple blender which puts one image over another
stitching,Template class for specifying the size of an image or rectangle.
stitching,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
stitching,OpenCV defines the following Size_<> aliases:
stitching,Base class for rotation-based warper using a detail::ProjectorBase_ derived class.
stitching,Bundle adjuster that expects affine transformation represented in homogeneous coordinates in R for each camera param. Implements camera parameters refinement algorithm which minimizes sum of the reprojection error squares.
stitching,It estimates all transformation parameters. Refinement mask is ignored.
stitching,Comma-separated Matrix Initializer.
stitching,"The class instances are usually not created explicitly. Instead, they are created on ""matrix << firstValue"" operator."
stitching,The sample below initializes 2x2 rotation matrix:
stitching,"Template class for short numerical vectors, a partial case of Matx."
stitching,"This template class represents short numerical vectors (of 1, 2, 3, 4 ... elements) on which you can perform basic arithmetical operations, access individual elements using [] operator etc. The vectors are allocated on stack, as opposite to std::valarray, std::vector, cv::Mat etc., which elements are dynamically allocated in the heap."
stitching,The template takes 2 parameters:
stitching,_Tp element type cn the number of elements
stitching,"In addition to the universal notation like Vec<float, 3>, you can use shorter aliases for the most popular specialized variants of Vec, e.g. Vec3f ~ Vec<float, 3>."
stitching,"It is possible to convert Vec<T,2> to/from Point_, Vec<T,3> to/from Point3_ , and Vec<T,4> to CvScalar or Scalar_. Use operator[] to access the elements of Vec."
stitching,All the expected vector operations are also implemented:
stitching,"v1 = v2 + v3 v1 = v2 - v3 v1 = v2 * scale v1 = scale * v2 v1 = -v2 v1 += v2 and other augmenting operations v1 == v2, v1 != v2 norm(v1) (euclidean norm) The Vec class is commonly used to describe pixel types of multi-channel arrays. See Mat for details."
stitching,Warper that maps an image onto the unit sphere located at the origin.
stitching,"Projects image onto unit sphere with origin at (0, 0, 0) and radius scale, measured in pixels. A 360 panorama would therefore have a resulting width of 2 * scale * PI pixels. Poles are located at (0, -1, 0) and (0, 1, 0) points."
stitching,Base class for a seam estimator.
stitching,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
stitching,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
stitching,A helper class for cv::DataType.
stitching,The class is specialized for each fundamental numerical data type supported by OpenCV. It provides DataDepth<T>::value constant.
stitching,Image warper factories base class.
stitching,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
stitching,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
stitching,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
stitching,"Functions void cv::julia::initJulia (int argc, char **argv) "
stitching,Classes class cv::detail::AffineBasedEstimator  Affine transformation based estimator. More...  class cv::detail::BundleAdjusterAffine  Bundle adjuster that expects affine transformation represented in homogeneous coordinates in R for each camera param. Implements camera parameters refinement algorithm which minimizes sum of the reprojection error squares. More...  class cv::detail::BundleAdjusterAffinePartial  Bundle adjuster that expects affine transformation with 4 DOF represented in homogeneous coordinates in R for each camera param. Implements camera parameters refinement algorithm which minimizes sum of the reprojection error squares. More...  class cv::detail::BundleAdjusterBase  Base class for all camera parameters refinement methods. More...  class cv::detail::BundleAdjusterRay  Implementation of the camera parameters refinement algorithm which minimizes sum of the distances between the rays passing through the camera center and a feature. : More...  class cv::detail::BundleAdjusterReproj  Implementation of the camera parameters refinement algorithm which minimizes sum of the reprojection error squares. More...  class cv::detail::Estimator  Rotation estimator base class. More...  class cv::detail::HomographyBasedEstimator  Homography based rotation estimator. More...  class cv::detail::NoBundleAdjuster  Stub bundle adjuster that does nothing. More... 
stitching,"Exposure compensator which tries to remove exposure related artifacts by adjusting image block intensities, see [279] for details."
stitching,Template sparse n-dimensional array class derived from SparseMat.
stitching,SparseMat_ is a thin wrapper on top of SparseMat created in the same way as Mat_ . It simplifies notation of some operations:
stitching,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
stitching,Cylindrical warper factory class.
stitching,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
stitching,It provides easy interface to:
stitching,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
stitching,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
stitching,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
stitching,It is planned to have:
stitching,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
stitching,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
stitching,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
stitching,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
stitching,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
stitching,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
stitching,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
stitching,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
stitching,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
stitching,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
stitching,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
stitching,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
stitching,This module has been originally developed as a project for Google Summer of Code 2012-2015.
stitching,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
stitching,"Classes class cv::text::BaseOCR  class cv::text::OCRBeamSearchDecoder  OCRBeamSearchDecoder class provides an interface for OCR using Beam Search algorithm. More...  class cv::text::OCRHMMDecoder  OCRHMMDecoder class provides an interface for OCR using Hidden Markov Models. More...  class cv::text::OCRHolisticWordRecognizer  OCRHolisticWordRecognizer class provides the functionallity of segmented wordspotting. Given a predefined vocabulary , a DictNet is employed to select the most probable word given an input image. More...  class cv::text::OCRTesseract  OCRTesseract class provides an interface with the tesseract-ocr API (v3.02.02) in C++. More... "
stitching,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
stitching,The distortion-free projective transformation given by a pinhole camera model is shown below.
stitching,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
stitching,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
stitching,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
stitching,\[p = A P_c.\]
stitching,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
stitching,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
stitching,and thus
stitching,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
stitching,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
stitching,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
stitching,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
stitching,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
stitching,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
stitching,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
stitching,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
stitching,and therefore
stitching,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
stitching,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
stitching,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
stitching,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
stitching,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
stitching,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
stitching,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
stitching,with
stitching,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
stitching,The following figure illustrates the pinhole camera model.
stitching,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
stitching,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
stitching,where
stitching,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
stitching,with
stitching,\[r^2 = x'^2 + y'^2\]
stitching,and
stitching,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
stitching,if \(Z_c \ne 0\).
stitching,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
stitching,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
stitching,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
stitching,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
stitching,where
stitching,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
stitching,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
stitching,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
stitching,In the functions below the coefficients are passed or returned as
stitching,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
stitching,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
stitching,The functions below use the above model to do the following:
stitching,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
stitching,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
stitching,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
stitching,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
stitching,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
stitching,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
stitching,if \(W \ne 0\).
stitching,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
stitching,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
stitching,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
stitching,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
stitching,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
stitching,Classes class cv::detail::DpSeamFinder  class cv::detail::GraphCutSeamFinder  Minimum graph cut-based seam estimator. See details in [153] . More...  class cv::detail::GraphCutSeamFinderBase  Base class for all minimum graph-cut-based seam estimators. More...  class cv::detail::NoSeamFinder  Stub seam estimator which does nothing. More...  class cv::detail::PairwiseSeamFinder  Base class for all pairwise seam estimators. More...  class cv::detail::SeamFinder  Base class for a seam estimator. More...  class cv::detail::VoronoiSeamFinder  Voronoi diagram-based seam estimator. More... 
stitching,Template Read-Only Sparse Matrix Iterator Class.
stitching,This is the derived from SparseMatConstIterator class that introduces more convenient operator *() for accessing the current element.
stitching,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
stitching,Classes class cv::detail::Blender  Base class for all blenders. More...  class cv::detail::FeatherBlender  Simple blender which mixes images at its borders. More...  class cv::detail::MultiBandBlender  Blender which uses multi-band blending algorithm (see [45]). More... 
stitching,The class defining termination criteria for iterative algorithms.
stitching,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
stitching,"ArUco Marker Detection, module functionality was moved to objdetect module"
stitching,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
stitching,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
stitching,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
stitching,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
stitching,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
stitching,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
stitching,Template class specifying a continuous subsequence (slice) of a sequence.
stitching,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
stitching,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
stitching,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
stitching,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
stitching,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
stitching,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
stitching,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
stitching,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
stitching,XML/YAML/JSON file storage class that encapsulates all the information necessary for writing or reading data to/from a file.
stitching,Implementation of the camera parameters refinement algorithm which minimizes sum of the distances between the rays passing through the camera center and a feature. :
stitching,It can estimate focal length. It ignores the refinement mask for now.
stitching,Feature matchers base class.
stitching,Rotation-only model image warper interface.
stitching,Exposure compensator which tries to remove exposure related artifacts by adjusting image blocks.
stitching,A complex number class.
stitching,"The template class is similar and compatible with std::complex, however it provides slightly more convenient access to the real and imaginary parts using through the simple field access, as opposite to std::complex::real() and std::complex::imag()."
stitching,Voronoi diagram-based seam estimator.
stitching,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
stitching,This class allows to create and manipulate comprehensive artificial neural networks.
stitching,"Neural network is presented as directed acyclic graph (DAG), where vertices are Layer instances, and edges specify relationships between layers inputs and outputs."
stitching,Each network layer has unique integer id and unique string name inside its network. LayerId can store either layer name or layer id.
stitching,"This class supports reference counting of its instances, i. e. copies point to the same instance."
stitching,Homography based rotation estimator.
stitching,LSTM recurrent layer.
stitching,Template class for 2D points specified by its coordinates x and y.
stitching,"An instance of the class is interchangeable with C structures, CvPoint and CvPoint2D32f . There is also a cast operator to convert point coordinates to the specified type. The conversion from floating-point coordinates to integer coordinates is done by rounding. Commonly, the conversion uses this operation for each of the coordinates. Besides the class members listed in the declaration above, the following operations on points are implemented:"
stitching,"For your convenience, the following type aliases are defined:"
stitching,Example:
stitching,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
stitching,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
stitching,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
stitching,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
stitching,"Each class derived from Map implements a motion model, as follows:"
stitching,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
stitching,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
stitching,The classes derived from Mapper are
stitching,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
stitching,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
stitching,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
stitching,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
stitching,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
stitching,This module provides storage routines for Hierarchical Data Format objects.
stitching,Face module changelog Face Recognition with OpenCV
stitching,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
stitching,This class provides all data needed to initialize layer.
stitching,"It includes dictionary with scalar params (which can be read by using Dict interface), blob params blobs and optional meta information: name and type of layer instance."
stitching,Stub bundle adjuster that does nothing.
stitching,The class represents rotated (i.e. not up-right) rectangles on a plane.
stitching,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
stitching,The sample below demonstrates how to use RotatedRect:
stitching,Minimum graph cut-based seam estimator. See details in [153] .
stitching,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
stitching,Classes class cv::plot::Plot2d 
stitching,Classes class cv::quality::QualityBase 
stitching,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
stitching,Base class for warping logic implementation.
stitching,Implementation of the camera parameters refinement algorithm which minimizes sum of the reprojection error squares.
stitching,"It can estimate focal length, aspect ratio, principal point. You can affect only on them via the refinement mask."
stitching,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
stitching,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
stitching,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
stitching,Classes class cv::DenseOpticalFlow  class cv::DISOpticalFlow  DIS optical flow algorithm. More...  class cv::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::KalmanFilter  Kalman filter class. More...  class cv::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More...  class cv::Tracker  Base abstract class for the long-term tracker. More...  class cv::TrackerDaSiamRPN  class cv::TrackerGOTURN  the GOTURN (Generic Object Tracking Using Regression Networks) tracker More...  class cv::TrackerMIL  The MIL algorithm trains a classifier in an online manner to separate the object from the background. More...  class cv::TrackerNano  the Nano tracker is a super lightweight dnn-based general object tracking. More...  class cv::TrackerVit  the VIT tracker is a super lightweight dnn-based general object tracking. More...  class cv::VariationalRefinement  Variational optical flow refinement. More... 
stitching,TLS data accumulator with gathering methods.
stitching,Classes class cv::BufferPoolController  class cv::ocl::Context  class cv::ocl::Device  class cv::ocl::Image2D  class cv::ocl::Kernel  class cv::ocl::KernelArg  class cv::ocl::OpenCLExecutionContext  class cv::ocl::OpenCLExecutionContextScope  class cv::ocl::Platform  class cv::ocl::PlatformInfo  class cv::ocl::Program  class cv::ocl::ProgramSource  class cv::ocl::Queue  class cv::ocl::Timer 
stitching,Namespaces namespace cv::traits 
stitching,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
stitching,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
stitching,Namespace for all functions is cv::intensity_transform.
stitching,Base class for all pairwise seam estimators.
stitching,Pose estimation parameters.
stitching,"pattern Defines center this system and axes direction (default PatternPositionType::ARUCO_CCW_CENTER). useExtrinsicGuess Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the provided rvec and tvec values as initial approximations of the rotation and translation vectors, respectively, and further optimizes them (default false). solvePnPMethod Method for solving a PnP problem: see calib3d_solvePnP_flags (default SOLVEPNP_ITERATIVE)."
stitching,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
stitching,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
stitching,Affine warper that uses rotations and translations.
stitching,Uses affine transformation in homogeneous coordinates to represent both rotation and translation in camera rotation matrix.
stitching,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
stitching,The class SparseMat represents multi-dimensional sparse numerical arrays.
stitching,"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:"
stitching,"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5; int size[5] = {10, 10, 10, 10, 10}; SparseMat sparse_mat(dims, size, CV_32F); for(int i = 0; i < 1000; i++) { int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f; } cout << ""nnz = "" << sparse_mat.nzcount() << endl; cv::SparseMatThe class SparseMat represents multi-dimensional sparse numerical arrays.Definition mat.hpp:2751 cv::SparseMat::sizeconst int * size() constreturns the array of sizes, or NULL if the matrix is not allocated cv::SparseMat::dimsint dims() constreturns the matrix dimensionality CV_32F#define CV_32FDefinition interface.h:78 Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix // and the sum of elements. SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>(); double s = 0; int dims = sparse_mat.dims(); for(; it != it_end; ++it) { // print element indices and the element value const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("": %g\n"", it.value<float>()); s += *it; } printf(""Element sum is %g\n"", s); cv::SparseMatConstIterator_Template Read-Only Sparse Matrix Iterator Class.Definition mat.hpp:3354 cv::SparseMatConstIterator::valueconst _Tp & value() consttemplate method returning the current matrix element cv::SparseMatConstIterator::nodeconst SparseMat::Node * node() constreturns the current node of the sparse matrix. it.node->idx is the current element index cv::SparseMat::Nodesparse matrix node - element of a hash tableDefinition mat.hpp:2776 cv::SparseMat::Node::idxint idx[MAX_DIM]index of the matrix elementDefinition mat.hpp:2782 If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation. Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b) { const SparseMat *_a = &a, *_b = &b; // if b contains less elements than a, // it is faster to iterate through b if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // take the next element from the first matrix float avalue = *it; const Node* anode = it.node(); // and try to find an element with the same index in the second matrix. // since the hash value depends only on the element index, // reuse the hash value stored in the node float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr; } cv::SparseMat::endSparseMatIterator end()returns the sparse matrix iterator at the matrix end cv::SparseMat::nzcountsize_t nzcount() constreturns the number of non-zero elements (=the number of hash table nodes) cv::SparseMat::beginSparseMatIterator begin()returns the sparse matrix iterator at the matrix beginning cv::SparseMat::Node::hashvalsize_t hashvalhash valueDefinition mat.hpp:2778"
stitching,Read and write video or images sequence with OpenCV.
stitching,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
stitching,Bioinspired Module Retina Introduction
stitching,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
stitching,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
stitching,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
stitching,See detailed overview here: Machine Learning Overview.
stitching,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
stitching,"Classes class cv::detail::BlocksChannelsCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image block on each channel. More...  class cv::detail::BlocksCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image blocks. More...  class cv::detail::BlocksGainCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image block intensities, see [279] for details. More...  class cv::detail::ChannelsCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image intensities on each channel independently. More...  class cv::detail::ExposureCompensator  Base class for all exposure compensators. More...  class cv::detail::GainCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image intensities, see [41] and [305] for details. More...  class cv::detail::NoExposureCompensator  Stub exposure compensator which does nothing. More... "
stitching,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
stitching,File Storage Node class.
stitching,"The node is used to store each and every element of the file storage opened for reading. When XML/YAML file is read, it is first parsed and stored in the memory as a hierarchical collection of nodes. Each node can be a ""leaf"" that is contain a single number or a string, or be a collection of other nodes. There can be named collections (mappings) where each element has a name and it is accessed by a name, and ordered collections (sequences) where elements do not have names but rather accessed by index. Type of the file node can be determined using FileNode::type method."
stitching,"Note that file nodes are only used for navigating file storages opened for reading. When a file storage is opened for writing, no data is stored in memory after it is written."
stitching,Template class for small matrices whose type and size are known at compilation time.
stitching,"If you need a more flexible type, use Mat . The elements of the matrix M are accessible using the M(i,j) notation. Most of the common matrix operations (see also MatrixExpressions ) are available. To do an operation on Matx that is not implemented, you can easily convert the matrix to Mat and backwards:"
stitching,"Except of the plain constructor which takes a list of elements, Matx can be initialized from a C-array:"
stitching,"In case if C++11 features are available, std::initializer_list can be also used to initialize Matx:"
stitching,Rotation estimator base class.
stitching,"It takes features of all images, pairwise matches between all images and estimates rotations of all cameras."
stitching,TLS container base implementation
stitching,Don't use directly.
stitching,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
stitching,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
stitching,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
stitching,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
stitching,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
stitching,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
stitching,Custom array allocator.
stitching,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
stitching,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
stitching,"Template class for 3D points specified by its coordinates x, y and z."
stitching,"An instance of the class is interchangeable with the C structure CvPoint2D32f . Similarly to Point_ , the coordinates of 3D points can be converted to another type. The vector arithmetic and comparison operations are also supported."
stitching,The following Point3_<> aliases are available:
stitching,Classes class cv::detail::AffineBestOf2NearestMatcher  Features matcher similar to cv::detail::BestOf2NearestMatcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf. More...  class cv::detail::BestOf2NearestMatcher  Features matcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf. More...  class cv::detail::BestOf2NearestRangeMatcher  class cv::detail::FeaturesMatcher  Feature matchers base class. More...  struct cv::detail::ImageFeatures  Structure containing image keypoints and descriptors. More...  struct cv::detail::MatchesInfo  Structure containing information about matches between two images. More... 
stitching,"""Universal intrinsics"" is a types and functions set intended to simplify vectorization of code on different platforms. Currently a few different SIMD extensions on different architectures are supported. 128 bit registers of various types support is implemented for a wide range of architectures including x86(SSE/SSE2/SSE4.2), ARM(NEON), PowerPC(VSX), MIPS(MSA). 256 bit long registers are supported on x86(AVX2) and 512 bit long registers are supported on x86(AVX512). In case when there is no SIMD extension available during compilation, fallback C++ implementation of intrinsics will be chosen and code will work as expected although it could be slower."
stitching,Read-Only Sparse Matrix Iterator.
stitching,Here is how to use the iterator to compute the sum of floating-point sparse matrix elements:
stitching,Template matrix class derived from Mat.
stitching,"The class Mat_<_Tp> is a thin template wrapper on top of the Mat class. It does not have any extra data fields. Nor this class nor Mat has any virtual methods. Thus, references or pointers to these two classes can be freely but carefully converted one to another. For example:"
stitching,"While Mat is sufficient in most cases, Mat_ can be more convenient if you use a lot of element access operations and if you know matrix type at the compilation time. Note that Mat::at(int y,int x) and Mat_::operator()(int y,int x) do absolutely the same and run at the same speed, but the latter is certainly shorter:"
stitching,"To use Mat_ for multi-channel images/matrices, pass Vec as a Mat_ parameter:"
stitching,Mat_ is fully compatible with C++11 range-based for loop. For example such loop can be used to safely apply look-up table:
stitching,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
stitching,"Namespace for all functions is cvv, i.e. cvv::showImage()."
stitching,Compilation:
stitching,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
stitching,See cvv tutorial for a commented example application using cvv.
stitching,Namespaces namespace cvv::impl 
video,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
video,Base abstract class for the long-term tracker.
video,The MIL algorithm trains a classifier in an online manner to separate the object from the background.
video,Multiple Instance Learning avoids the drift problem for a robust tracking. The implementation is based on [14] .
video,Original code can be found here http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml
video,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
video,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
video,This module includes photo processing algorithms
video,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
video,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
video,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
video,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
video,The implemented stitching pipeline is very similar to the one proposed in [41] .
video,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
video,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
video,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
video,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
video,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
video,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
video,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
video,This module includes signal processing algorithms.
video,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
video,ICP point-to-plane odometry algorithm
video,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
video,Variational optical flow refinement.
video,"This class implements variational refinement of the input flow field, i.e. it uses input flow to initialize the minimization of the following functional: \(E(U) = \int_{\Omega} \delta \Psi(E_I) + \gamma \Psi(E_G) + \alpha \Psi(E_S) \), where \(E_I,E_G,E_S\) are color constancy, gradient constancy and smoothness terms respectively. \(\Psi(s^2)=\sqrt{s^2+\epsilon^2}\) is a robust penalizer to limit the influence of outliers. A complete formulation and a description of the minimization procedure can be found in [42]"
video,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
video,Dense optical flow algorithms compute motion for each point:
video,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
video,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
video,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
video,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
video,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
video,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
video,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
video,This is a base class for all more or less complex algorithms in OpenCV.
video,"especially for classes of algorithms, for which there can be multiple implementations. The examples are stereo correspondence (for which there are algorithms like block matching, semi-global block matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck etc.)."
video,Here is example of SimpleBlobDetector use in your application via Algorithm interface:
video,n-dimensional dense array class
video,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
video,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
video,"In case of a 2-dimensional array, the above formula is reduced to:"
video,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
video,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
video,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
video,There are many different ways to create a Mat object. The most popular options are listed below:
video,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
video,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
video,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
video,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
video,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
video,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
video,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
video,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
video,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
video,Namespaces namespace cv::omnidir::internal 
video,Information Flow algorithm implementaton for alphamatting
video,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
video,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
video,The implementation is based on [7].
video,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
video,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
video,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
video,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
video,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
video,Namespace for all functions is cv::img_hash.
video,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
video,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
video,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
video,This modules is to draw UTF-8 strings with freetype/harfbuzz.
video,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
video,Classes class cv::freetype::FreeType2 
video,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
video,Namespaces namespace NcvCTprep 
video,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
video,This module contains:
video,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
video,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
video,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
video,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
video,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
video,Class used for calculating a sparse optical flow.
video,The class can calculate an optical flow for a sparse feature set using the iterative Lucas-Kanade method with pyramids.
video,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
video,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
video,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
video,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
video,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
video,"Functions void cv::julia::initJulia (int argc, char **argv) "
video,the GOTURN (Generic Object Tracking Using Regression Networks) tracker
video,"GOTURN ([122]) is kind of trackers based on Convolutional Neural Networks (CNN). While taking all advantages of CNN trackers, GOTURN is much faster due to offline training without online fine-tuning nature. GOTURN tracker addresses the problem of single target tracking: given a bounding box label of an object in the first frame of the video, we track that object through the rest of the video. NOTE: Current method of GOTURN does not handle occlusions; however, it is fairly robust to viewpoint changes, lighting changes, and deformations. Inputs of GOTURN are two RGB patches representing Target and Search patches resized to 227x227. Outputs of GOTURN are predicted bounding box coordinates, relative to Search patch coordinate system, in format X1,Y1,X2,Y2. Original paper is here: http://davheld.github.io/GOTURN/GOTURN.pdf As long as original authors implementation: https://github.com/davheld/GOTURN#train-the-tracker Implementation of training algorithm is placed in separately here due to 3d-party dependencies: https://github.com/Auron-X/GOTURN_Training_Toolkit GOTURN architecture goturn.prototxt and trained model goturn.caffemodel are accessible on opencv_extra GitHub repository."
video,Gaussian Mixture-based Background/Foreground Segmentation Algorithm.
video,The class implements the Gaussian mixture model background subtraction described in [325] and [324] .
video,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
video,Base class for background/foreground segmentation. :
video,The class is only used to define the common interface for the whole family of background/foreground segmentation algorithms.
video,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
video,It provides easy interface to:
video,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
video,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
video,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
video,It is planned to have:
video,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
video,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
video,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
video,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
video,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
video,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
video,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
video,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
video,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
video,the Nano tracker is a super lightweight dnn-based general object tracking.
video,"Nano tracker is much faster and extremely lightweight due to special model structure, the whole model size is about 1.9 MB. Nano tracker needs two models: one for feature extraction (backbone) and the another for localization (neckhead). Model download link: https://github.com/HonglinChu/SiamTrackers/tree/master/NanoTrack/models/nanotrackv2 Original repo is here: https://github.com/HonglinChu/NanoTrack Author: HongLinChu, 16284.nosp@m.6434.nosp@m.5@qq..nosp@m.com"
video,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
video,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
video,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
video,This module has been originally developed as a project for Google Summer of Code 2012-2015.
video,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
video,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
video,The distortion-free projective transformation given by a pinhole camera model is shown below.
video,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
video,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
video,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
video,\[p = A P_c.\]
video,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
video,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
video,and thus
video,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
video,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
video,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
video,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
video,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
video,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
video,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
video,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
video,and therefore
video,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
video,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
video,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
video,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
video,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
video,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
video,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
video,with
video,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
video,The following figure illustrates the pinhole camera model.
video,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
video,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
video,where
video,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
video,with
video,\[r^2 = x'^2 + y'^2\]
video,and
video,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
video,if \(Z_c \ne 0\).
video,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
video,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
video,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
video,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
video,where
video,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
video,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
video,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
video,In the functions below the coefficients are passed or returned as
video,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
video,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
video,The functions below use the above model to do the following:
video,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
video,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
video,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
video,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
video,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
video,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
video,if \(W \ne 0\).
video,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
video,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
video,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
video,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
video,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
video,the VIT tracker is a super lightweight dnn-based general object tracking.
video,"VIT tracker is much faster and extremely lightweight due to special model structure, the model file is about 767KB. Model download link: https://github.com/opencv/opencv_zoo/tree/main/models/object_tracking_vittrack Author: PengyuLiu, 18729.nosp@m.1850.nosp@m.7@qq..nosp@m.com"
video,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
video,The class defining termination criteria for iterative algorithms.
video,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
video,"ArUco Marker Detection, module functionality was moved to objdetect module"
video,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
video,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
video,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
video,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
video,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
video,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
video,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
video,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
video,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
video,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
video,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
video,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
video,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
video,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
video,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
video,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
video,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
video,"Each class derived from Map implements a motion model, as follows:"
video,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
video,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
video,The classes derived from Mapper are
video,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
video,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
video,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
video,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
video,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
video,This module provides storage routines for Hierarchical Data Format objects.
video,Face module changelog Face Recognition with OpenCV
video,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
video,K-nearest neighbours - based Background/Foreground Segmentation Algorithm.
video,The class implements the K-nearest neighbours background subtraction described in [324] . Very efficient if number of foreground pixels is low.
video,The class represents rotated (i.e. not up-right) rectangles on a plane.
video,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
video,The sample below demonstrates how to use RotatedRect:
video,Classes class cv::plot::Plot2d 
video,Classes class cv::quality::QualityBase 
video,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
video,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
video,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
video,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
video,Classes class cv::DenseOpticalFlow  class cv::DISOpticalFlow  DIS optical flow algorithm. More...  class cv::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::KalmanFilter  Kalman filter class. More...  class cv::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More...  class cv::Tracker  Base abstract class for the long-term tracker. More...  class cv::TrackerDaSiamRPN  class cv::TrackerGOTURN  the GOTURN (Generic Object Tracking Using Regression Networks) tracker More...  class cv::TrackerMIL  The MIL algorithm trains a classifier in an online manner to separate the object from the background. More...  class cv::TrackerNano  the Nano tracker is a super lightweight dnn-based general object tracking. More...  class cv::TrackerVit  the VIT tracker is a super lightweight dnn-based general object tracking. More...  class cv::VariationalRefinement  Variational optical flow refinement. More... 
video,Namespaces namespace cv::traits 
video,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
video,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
video,Namespace for all functions is cv::intensity_transform.
video,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
video,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
video,Kalman filter class.
video,"The class implements a standard Kalman filter http://en.wikipedia.org/wiki/Kalman_filter, [297] . However, you can modify transitionMatrix, controlMatrix, and measurementMatrix to get an extended Kalman filter functionality."
video,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
video,Read and write video or images sequence with OpenCV.
video,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
video,Bioinspired Module Retina Introduction
video,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
video,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
video,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
video,See detailed overview here: Machine Learning Overview.
video,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
video,Base interface for sparse optical flow algorithms.
video,DIS optical flow algorithm.
video,"This class implements the Dense Inverse Search (DIS) optical flow algorithm. More details about the algorithm can be found at [151] . Includes three presets with preselected parameters to provide reasonable trade-off between speed and quality. However, even the slowest preset is still relatively fast, use DeepFlow if you need better quality and don't care about speed."
video,"This implementation includes several additional features compared to the algorithm described in the paper, including spatial propagation of flow vectors (getUseSpatialPropagation), as well as an option to utilize an initial flow approximation passed to calc (which is, essentially, temporal propagation, if the previous frame's flow field is passed)."
video,Class computing a dense optical flow using the Gunnar Farneback's algorithm.
video,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
video,Classes class cv::BackgroundSubtractor  Base class for background/foreground segmentation. : More...  class cv::BackgroundSubtractorKNN  K-nearest neighbours - based Background/Foreground Segmentation Algorithm. More...  class cv::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
video,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
video,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
video,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
video,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
video,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
video,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
video,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
video,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
video,Base class for dense optical flow algorithms
video,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
video,"Namespace for all functions is cvv, i.e. cvv::showImage()."
video,Compilation:
video,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
video,See cvv tutorial for a commented example application using cvv.
video,Namespaces namespace cvv::impl 
videoio,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
videoio,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
videoio,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
videoio,This module includes photo processing algorithms
videoio,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
videoio,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
videoio,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
videoio,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
videoio,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
videoio,The implemented stitching pipeline is very similar to the one proposed in [41] .
videoio,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
videoio,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
videoio,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
videoio,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
videoio,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
videoio,Class passed to an error.
videoio,This class encapsulates all or almost all necessary information about the error happened in the program. The exception is usually constructed and thrown implicitly via CV_Error and CV_Error_ macros.
videoio,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
videoio,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
videoio,This module includes signal processing algorithms.
videoio,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
videoio,ICP point-to-plane odometry algorithm
videoio,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
videoio,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
videoio,YOUR ATTENTION PLEASE!
videoio,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
videoio,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
videoio,Note for developers: please don't put videoio dependency in G-API because of this file.
videoio,Dense optical flow algorithms compute motion for each point:
videoio,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
videoio,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
videoio,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
videoio,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
videoio,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
videoio,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
videoio,n-dimensional dense array class
videoio,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
videoio,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
videoio,"In case of a 2-dimensional array, the above formula is reduced to:"
videoio,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
videoio,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
videoio,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
videoio,There are many different ways to create a Mat object. The most popular options are listed below:
videoio,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
videoio,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
videoio,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
videoio,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
videoio,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
videoio,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
videoio,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
videoio,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
videoio,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
videoio,Namespaces namespace cv::omnidir::internal 
videoio,"Enumerations enum cv::VideoCaptureAPIs { cv::CAP_ANY = 0 , cv::CAP_VFW = 200 , cv::CAP_V4L = 200 , cv::CAP_V4L2 = CAP_V4L , cv::CAP_FIREWIRE = 300 , cv::CAP_FIREWARE = CAP_FIREWIRE , cv::CAP_IEEE1394 = CAP_FIREWIRE , cv::CAP_DC1394 = CAP_FIREWIRE , cv::CAP_CMU1394 = CAP_FIREWIRE , cv::CAP_QT = 500 , cv::CAP_UNICAP = 600 , cv::CAP_DSHOW = 700 , cv::CAP_PVAPI = 800 , cv::CAP_OPENNI = 900 , cv::CAP_OPENNI_ASUS = 910 , cv::CAP_ANDROID = 1000 , cv::CAP_XIAPI = 1100 , cv::CAP_AVFOUNDATION = 1200 , cv::CAP_GIGANETIX = 1300 , cv::CAP_MSMF = 1400 , cv::CAP_WINRT = 1410 , cv::CAP_INTELPERC = 1500 , cv::CAP_REALSENSE = 1500 , cv::CAP_OPENNI2 = 1600 , cv::CAP_OPENNI2_ASUS = 1610 , cv::CAP_OPENNI2_ASTRA = 1620 , cv::CAP_GPHOTO2 = 1700 , cv::CAP_GSTREAMER = 1800 , cv::CAP_FFMPEG = 1900 , cv::CAP_IMAGES = 2000 , cv::CAP_ARAVIS = 2100 , cv::CAP_OPENCV_MJPEG = 2200 , cv::CAP_INTEL_MFX = 2300 , cv::CAP_XINE = 2400 , cv::CAP_UEYE = 2500 , cv::CAP_OBSENSOR = 2600 }  cv::VideoCapture API backends identifier. More...  enum cv::VideoCaptureProperties { cv::CAP_PROP_POS_MSEC =0 , cv::CAP_PROP_POS_FRAMES =1 , cv::CAP_PROP_POS_AVI_RATIO =2 , cv::CAP_PROP_FRAME_WIDTH =3 , cv::CAP_PROP_FRAME_HEIGHT =4 , cv::CAP_PROP_FPS =5 , cv::CAP_PROP_FOURCC =6 , cv::CAP_PROP_FRAME_COUNT =7 , cv::CAP_PROP_FORMAT =8 , cv::CAP_PROP_MODE =9 , cv::CAP_PROP_BRIGHTNESS =10 , cv::CAP_PROP_CONTRAST =11 , cv::CAP_PROP_SATURATION =12 , cv::CAP_PROP_HUE =13 , cv::CAP_PROP_GAIN =14 , cv::CAP_PROP_EXPOSURE =15 , cv::CAP_PROP_CONVERT_RGB =16 , cv::CAP_PROP_WHITE_BALANCE_BLUE_U =17 , cv::CAP_PROP_RECTIFICATION =18 , cv::CAP_PROP_MONOCHROME =19 , cv::CAP_PROP_SHARPNESS =20 , cv::CAP_PROP_AUTO_EXPOSURE =21 , cv::CAP_PROP_GAMMA =22 , cv::CAP_PROP_TEMPERATURE =23 , cv::CAP_PROP_TRIGGER =24 , cv::CAP_PROP_TRIGGER_DELAY =25 , cv::CAP_PROP_WHITE_BALANCE_RED_V =26 , cv::CAP_PROP_ZOOM =27 , cv::CAP_PROP_FOCUS =28 , cv::CAP_PROP_GUID =29 , cv::CAP_PROP_ISO_SPEED =30 , cv::CAP_PROP_BACKLIGHT =32 , cv::CAP_PROP_PAN =33 , cv::CAP_PROP_TILT =34 , cv::CAP_PROP_ROLL =35 , cv::CAP_PROP_IRIS =36 , cv::CAP_PROP_SETTINGS =37 , cv::CAP_PROP_BUFFERSIZE =38 , cv::CAP_PROP_AUTOFOCUS =39 , cv::CAP_PROP_SAR_NUM =40 , cv::CAP_PROP_SAR_DEN =41 , cv::CAP_PROP_BACKEND =42 , cv::CAP_PROP_CHANNEL =43 , cv::CAP_PROP_AUTO_WB =44 , cv::CAP_PROP_WB_TEMPERATURE =45 , cv::CAP_PROP_CODEC_PIXEL_FORMAT =46 , cv::CAP_PROP_BITRATE =47 , cv::CAP_PROP_ORIENTATION_META =48 , cv::CAP_PROP_ORIENTATION_AUTO =49 , cv::CAP_PROP_HW_ACCELERATION =50 , cv::CAP_PROP_HW_DEVICE =51 , cv::CAP_PROP_HW_ACCELERATION_USE_OPENCL =52 , cv::CAP_PROP_OPEN_TIMEOUT_MSEC =53 , cv::CAP_PROP_READ_TIMEOUT_MSEC =54 , cv::CAP_PROP_STREAM_OPEN_TIME_USEC =55 , cv::CAP_PROP_VIDEO_TOTAL_CHANNELS = 56 , cv::CAP_PROP_VIDEO_STREAM = 57 , cv::CAP_PROP_AUDIO_STREAM = 58 , cv::CAP_PROP_AUDIO_POS = 59 , cv::CAP_PROP_AUDIO_SHIFT_NSEC = 60 , cv::CAP_PROP_AUDIO_DATA_DEPTH = 61 , cv::CAP_PROP_AUDIO_SAMPLES_PER_SECOND = 62 , cv::CAP_PROP_AUDIO_BASE_INDEX = 63 , cv::CAP_PROP_AUDIO_TOTAL_CHANNELS = 64 , cv::CAP_PROP_AUDIO_TOTAL_STREAMS = 65 , cv::CAP_PROP_AUDIO_SYNCHRONIZE = 66 , cv::CAP_PROP_LRF_HAS_KEY_FRAME = 67 , cv::CAP_PROP_CODEC_EXTRADATA_INDEX = 68 , cv::CAP_PROP_FRAME_TYPE = 69 , cv::CAP_PROP_N_THREADS = 70 , cv::CAP_PROP_PTS = 71 , cv::CAP_PROP_DTS_DELAY = 72 }  cv::VideoCapture generic properties identifier. More...  enum cv::VideoWriterProperties { cv::VIDEOWRITER_PROP_QUALITY = 1 , cv::VIDEOWRITER_PROP_FRAMEBYTES = 2 , cv::VIDEOWRITER_PROP_NSTRIPES = 3 , cv::VIDEOWRITER_PROP_IS_COLOR = 4 , cv::VIDEOWRITER_PROP_DEPTH = 5 , cv::VIDEOWRITER_PROP_HW_ACCELERATION = 6 , cv::VIDEOWRITER_PROP_HW_DEVICE = 7 , cv::VIDEOWRITER_PROP_HW_ACCELERATION_USE_OPENCL = 8 , cv::VIDEOWRITER_PROP_RAW_VIDEO = 9 , cv::VIDEOWRITER_PROP_KEY_INTERVAL = 10 , cv::VIDEOWRITER_PROP_KEY_FLAG = 11 , cv::VIDEOWRITER_PROP_PTS = 12 , cv::VIDEOWRITER_PROP_DTS_DELAY = 13 }  cv::VideoWriter generic properties identifier. More... "
videoio,Information Flow algorithm implementaton for alphamatting
videoio,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
videoio,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
videoio,The implementation is based on [7].
videoio,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
videoio,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
videoio,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
videoio,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
videoio,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
videoio,Namespace for all functions is cv::img_hash.
videoio,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
videoio,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
videoio,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
videoio,This modules is to draw UTF-8 strings with freetype/harfbuzz.
videoio,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
videoio,Classes class cv::freetype::FreeType2 
videoio,Classes class CvAbstractCamera  class CvPhotoCamera  protocol <CvPhotoCameraDelegate>  class CvVideoCamera  protocol <CvVideoCameraDelegate> 
videoio,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
videoio,Namespaces namespace NcvCTprep 
videoio,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
videoio,This module contains:
videoio,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
videoio,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
videoio,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
videoio,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
videoio,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
videoio,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
videoio,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
videoio,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
videoio,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
videoio,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
videoio,"Functions void cv::julia::initJulia (int argc, char **argv) "
videoio,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
videoio,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
videoio,It provides easy interface to:
videoio,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
videoio,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
videoio,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
videoio,It is planned to have:
videoio,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
videoio,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
videoio,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
videoio,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
videoio,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
videoio,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
videoio,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
videoio,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
videoio,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
videoio,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
videoio,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
videoio,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
videoio,This module has been originally developed as a project for Google Summer of Code 2012-2015.
videoio,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
videoio,STL namespace.
videoio,"Class for video capturing from video files, image sequences or cameras."
videoio,The class provides C++ API for capturing video from cameras or for reading video files and image sequences.
videoio,Here is how the class can be used:
videoio,(C++) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/cpp/videocapture_starter.cpp (Python) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/python/video.py (Python) A multi threaded video processing sample can be found at OPENCV_SOURCE_CODE/samples/python/video_threaded.py (Python) VideoCapture sample showcasing some features of the Video4Linux2 backend OPENCV_SOURCE_CODE/samples/python/video_v4l2.py
videoio,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
videoio,The distortion-free projective transformation given by a pinhole camera model is shown below.
videoio,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
videoio,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
videoio,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
videoio,\[p = A P_c.\]
videoio,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
videoio,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
videoio,and thus
videoio,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
videoio,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
videoio,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
videoio,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
videoio,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
videoio,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
videoio,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
videoio,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
videoio,and therefore
videoio,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
videoio,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
videoio,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
videoio,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
videoio,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
videoio,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
videoio,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
videoio,with
videoio,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
videoio,The following figure illustrates the pinhole camera model.
videoio,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
videoio,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
videoio,where
videoio,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
videoio,with
videoio,\[r^2 = x'^2 + y'^2\]
videoio,and
videoio,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
videoio,if \(Z_c \ne 0\).
videoio,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
videoio,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
videoio,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
videoio,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
videoio,where
videoio,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
videoio,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
videoio,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
videoio,In the functions below the coefficients are passed or returned as
videoio,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
videoio,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
videoio,The functions below use the above model to do the following:
videoio,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
videoio,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
videoio,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
videoio,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
videoio,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
videoio,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
videoio,if \(W \ne 0\).
videoio,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
videoio,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
videoio,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
videoio,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
videoio,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
videoio,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
videoio,"ArUco Marker Detection, module functionality was moved to objdetect module"
videoio,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
videoio,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
videoio,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
videoio,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
videoio,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
videoio,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
videoio,Template class specifying a continuous subsequence (slice) of a sequence.
videoio,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
videoio,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
videoio,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
videoio,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
videoio,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
videoio,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
videoio,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
videoio,"Each class derived from Map implements a motion model, as follows:"
videoio,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
videoio,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
videoio,The classes derived from Mapper are
videoio,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
videoio,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
videoio,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
videoio,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
videoio,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
videoio,This module provides storage routines for Hierarchical Data Format objects.
videoio,Face module changelog Face Recognition with OpenCV
videoio,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
videoio,Classes class cv::plot::Plot2d 
videoio,Classes class cv::quality::QualityBase 
videoio,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
videoio,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
videoio,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
videoio,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
videoio,Namespaces namespace cv::traits 
videoio,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
videoio,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
videoio,"Hardware acceleration support enum cv::VideoAccelerationType { cv::VIDEO_ACCELERATION_NONE = 0 , cv::VIDEO_ACCELERATION_ANY = 1 , cv::VIDEO_ACCELERATION_D3D11 = 2 , cv::VIDEO_ACCELERATION_VAAPI = 3 , cv::VIDEO_ACCELERATION_MFX = 4 }  Video Acceleration type. More... "
videoio,Namespace for all functions is cv::intensity_transform.
videoio,"Enumerations enum { cv::OPEN_CAMERA = 300 , cv::CLOSE_CAMERA , cv::UPDATE_IMAGE_ELEMENT , cv::SHOW_TRACKBAR } "
videoio,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
videoio,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
videoio,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
videoio,Video writer class.
videoio,The class provides C++ API for writing video files or image sequences.
videoio,Read and write video or images sequence with OpenCV.
videoio,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
videoio,Bioinspired Module Retina Introduction
videoio,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
videoio,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
videoio,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
videoio,See detailed overview here: Machine Learning Overview.
videoio,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
videoio,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
videoio,This section contains API description how to query/configure available Video I/O backends.
videoio,Runtime configuration options:
videoio,"enable debug mode: OPENCV_VIDEOIO_DEBUG=1 change backend priority: OPENCV_VIDEOIO_PRIORITY_<backend>=9999 disable backend: OPENCV_VIDEOIO_PRIORITY_<backend>=0 specify list of backends with high priority (>100000): OPENCV_VIDEOIO_PRIORITY_LIST=FFMPEG,GSTREAMER"
videoio,"Functions cv::String cv::videoio_registry::getBackendName (VideoCaptureAPIs api)  Returns backend API name or ""UnknownVideoAPI(xxx)"".  std::vector< VideoCaptureAPIs > cv::videoio_registry::getBackends ()  Returns list of all available backends.  std::string cv::videoio_registry::getCameraBackendPluginVersion (VideoCaptureAPIs api, int &version_ABI, int &version_API)  Returns description and ABI/API version of videoio plugin's camera interface.  std::vector< VideoCaptureAPIs > cv::videoio_registry::getCameraBackends ()  Returns list of available backends which works via cv::VideoCapture(int index)  std::string cv::videoio_registry::getStreamBackendPluginVersion (VideoCaptureAPIs api, int &version_ABI, int &version_API)  Returns description and ABI/API version of videoio plugin's stream capture interface.  std::vector< VideoCaptureAPIs > cv::videoio_registry::getStreamBackends ()  Returns list of available backends which works via cv::VideoCapture(filename)  std::string cv::videoio_registry::getWriterBackendPluginVersion (VideoCaptureAPIs api, int &version_ABI, int &version_API)  Returns description and ABI/API version of videoio plugin's writer interface.  std::vector< VideoCaptureAPIs > cv::videoio_registry::getWriterBackends ()  Returns list of available backends which works via cv::VideoWriter()  bool cv::videoio_registry::hasBackend (VideoCaptureAPIs api)  Returns true if backend is available.  bool cv::videoio_registry::isBackendBuiltIn (VideoCaptureAPIs api)  Returns true if backend is built in (false if backend is used as plugin) "
videoio,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
videoio,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
videoio,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
videoio,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
videoio,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
videoio,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
videoio,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
videoio,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
videoio,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
videoio,"Namespace for all functions is cvv, i.e. cvv::showImage()."
videoio,Compilation:
videoio,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
videoio,See cvv tutorial for a commented example application using cvv.
videoio,Namespaces namespace cvv::impl 
