Module,Text
calib3d,Class for extracting blobs from an image. :
calib3d,The class implements a simple algorithm for extracting blobs from an image:
calib3d,This class performs several filtrations of returned blobs. You should set filterBy* to true/false to turn on/off corresponding filtration. Available filtrations:
calib3d,"By color. This filter compares the intensity of a binary image at the center of a blob to blobColor. If they differ, the blob is filtered out. Use blobColor = 0 to extract dark blobs and blobColor = 255 to extract light blobs. By area. Extracted blobs have an area between minArea (inclusive) and maxArea (exclusive). By circularity. Extracted blobs have circularity ( \(\frac{4*\pi*Area}{perimeter * perimeter}\)) between minCircularity (inclusive) and maxCircularity (exclusive). By ratio of the minimum inertia to maximum inertia. Extracted blobs have this ratio between minInertiaRatio (inclusive) and maxInertiaRatio (exclusive). By convexity. Extracted blobs have convexity (area / area of blob convex hull) between minConvexity (inclusive) and maxConvexity (exclusive)."
calib3d,Default values of parameters are tuned to extract dark circular blobs.
calib3d,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
calib3d,The base class for stereo correspondence algorithms.
calib3d,The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows:
calib3d,"By default, the algorithm is single-pass, which means that you consider only 5 directions instead of 8. Set mode=StereoSGBM::MODE_HH in createStereoSGBM to run the full variant of the algorithm but beware that it may consume a lot of memory. The algorithm matches blocks, not individual pixels. Though, setting blockSize=1 reduces the blocks to single pixels. Mutual information cost function is not implemented. Instead, a simpler Birchfield-Tomasi sub-pixel metric from [28] is used. Though, the color images are supported as well. Some pre- and post- processing steps from K. Konolige algorithm StereoBM are included, for example: pre-filtering (StereoBM::PREFILTER_XSOBEL type) and post-filtering (uniqueness check, quadratic interpolation and speckle filtering)."
calib3d,(Python) An example illustrating the use of the StereoSGBM matching algorithm can be found at opencv_source_code/samples/python/stereo_match.py
calib3d,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
calib3d,n-dimensional dense array class
calib3d,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
calib3d,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
calib3d,"In case of a 2-dimensional array, the above formula is reduced to:"
calib3d,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
calib3d,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
calib3d,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
calib3d,There are many different ways to create a Mat object. The most popular options are listed below:
calib3d,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
calib3d,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
calib3d,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
calib3d,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
calib3d,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
calib3d,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
calib3d,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
calib3d,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
calib3d,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
calib3d,"Levenberg-Marquardt solver. Starting with the specified vector of parameters it optimizes the target vector criteria ""err"" (finds local minima of each target vector component absolute value)."
calib3d,"When needed, it calls user-provided callback."
calib3d,"Classes struct cv::Accumulator< T >  struct cv::Accumulator< char >  struct cv::Accumulator< short >  struct cv::Accumulator< unsigned char >  struct cv::Accumulator< unsigned short >  class cv::AffineFeature  Class for implementing the wrapper which makes detectors and extractors to be affine invariant, described as ASIFT in [312] . More...  class cv::AgastFeatureDetector  Wrapping class for feature detection using the AGAST method. : More...  class cv::AKAZE  Class implementing the AKAZE keypoint detector and descriptor extractor, described in [10]. More...  class cv::BRISK  Class implementing the BRISK keypoint detector and descriptor extractor, described in [159] . More...  class cv::FastFeatureDetector  Wrapping class for feature detection using the FAST method. : More...  class cv::Feature2D  Abstract base class for 2D image feature detectors and descriptor extractors. More...  class cv::GFTTDetector  Wrapping class for feature detection using the goodFeaturesToTrack function. : More...  class cv::KAZE  Class implementing the KAZE keypoint detector and descriptor extractor, described in [9] . More...  class cv::KeyPointsFilter  A class filters a vector of keypoints. More...  struct cv::L1< T >  struct cv::L2< T >  class cv::MSER  Maximally stable extremal region extractor. More...  class cv::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More...  class cv::SIFT  Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe [174] . More...  class cv::SimpleBlobDetector  Class for extracting blobs from an image. : More...  struct cv::SL2< T > "
calib3d,Template class for specifying the size of an image or rectangle.
calib3d,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
calib3d,OpenCV defines the following Size_<> aliases:
calib3d,Classes class cv::CLAHE  Base class for Contrast Limited Adaptive Histogram Equalization. More... 
calib3d,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
calib3d,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
calib3d,The distortion-free projective transformation given by a pinhole camera model is shown below.
calib3d,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
calib3d,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
calib3d,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
calib3d,\[p = A P_c.\]
calib3d,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
calib3d,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
calib3d,and thus
calib3d,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
calib3d,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
calib3d,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
calib3d,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
calib3d,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
calib3d,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
calib3d,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
calib3d,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
calib3d,and therefore
calib3d,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
calib3d,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
calib3d,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
calib3d,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
calib3d,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
calib3d,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
calib3d,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
calib3d,with
calib3d,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
calib3d,The following figure illustrates the pinhole camera model.
calib3d,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
calib3d,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
calib3d,where
calib3d,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
calib3d,with
calib3d,\[r^2 = x'^2 + y'^2\]
calib3d,and
calib3d,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
calib3d,if \(Z_c \ne 0\).
calib3d,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
calib3d,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
calib3d,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
calib3d,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
calib3d,where
calib3d,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
calib3d,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
calib3d,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
calib3d,In the functions below the coefficients are passed or returned as
calib3d,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
calib3d,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
calib3d,The functions below use the above model to do the following:
calib3d,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
calib3d,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
calib3d,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
calib3d,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
calib3d,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
calib3d,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
calib3d,if \(W \ne 0\).
calib3d,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
calib3d,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
calib3d,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
calib3d,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
calib3d,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
calib3d,The class defining termination criteria for iterative algorithms.
calib3d,"You can initialize it by default constructor and then override any parameters, or the structure may be fully initialized using the advanced variant of the constructor."
calib3d,"Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige."
calib3d,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
calib3d,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
calib3d,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
calib3d,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
calib3d,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
calib3d,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
calib3d,Definitions: Let P be a point in 3D of coordinates X in the world reference frame (stored in the matrix X) The coordinate vector of P in the camera reference frame is:
calib3d,\[Xc = R X + T\]
calib3d,"where R is the rotation matrix corresponding to the rotation vector om: R = rodrigues(om); call x, y and z the 3 coordinates of Xc:"
calib3d,\[\begin{array}{l} x = Xc_1 \\ y = Xc_2 \\ z = Xc_3 \end{array} \]
calib3d,The pinhole projection coordinates of P is [a; b] where
calib3d,\[\begin{array}{l} a = x / z \ and \ b = y / z \\ r^2 = a^2 + b^2 \\ \theta = atan(r) \end{array} \]
calib3d,Fisheye distortion:
calib3d,\[\theta_d = \theta (1 + k_1 \theta^2 + k_2 \theta^4 + k_3 \theta^6 + k_4 \theta^8)\]
calib3d,The distorted point coordinates are [x'; y'] where
calib3d,\[\begin{array}{l} x' = (\theta_d / r) a \\ y' = (\theta_d / r) b \end{array} \]
calib3d,"Finally, conversion into pixel coordinates: The final pixel coordinates vector [u; v] where:"
calib3d,\[\begin{array}{l} u = f_x (x' + \alpha y') + c_x \\ v = f_y y' + c_y \end{array} \]
calib3d,Summary: Generic camera model [143] with perspective projection and without distortion correction
calib3d,Namespaces namespace cv::fisheye  The methods in this namespace use a so-called fisheye camera model. 
calib3d,Namespaces namespace cv::traits 
calib3d,Classes class cv::LineSegmentDetector  Line segment detector class. More... 
calib3d,Singular Value Decomposition.
calib3d,"Class for computing Singular Value Decomposition of a floating-point matrix. The Singular Value Decomposition is used to solve least-square problems, under-determined linear systems, invert matrices, compute condition numbers, and so on."
calib3d,"If you want to compute a condition number of a matrix or an absolute value of its determinant, you do not need u and vt. You can pass flags=SVD::NO_UV|... . Another flag SVD::FULL_UV indicates that full-size u and vt must be computed, which is not necessary most of the time."
core,These functions are provided for OpenCV-Eigen interoperability. They convert Mat objects to corresponding Eigen::Matrix objects and vice-versa. Consult the Eigen documentation for information about the Matrix template type.
core,Namespaces namespace cv::traits 
core,"The Core module is the backbone of OpenCV, offering fundamental data structures, matrix operations, and utility functions that other modules depend on. Its essential for handling image data, performing mathematical computations, and managing memory efficiently within the OpenCV ecosystem."
core,This section describes Intel VA-API/OpenCL (CL-VA) interoperability.
core,To enable basic VA interoperability build OpenCV with libva library integration enabled: -DWITH_VA=ON (corresponding dev package should be installed).
core,"To enable advanced CL-VA interoperability support on Intel HW, enable option: -DWITH_VA_INTEL=ON (OpenCL integration should be enabled which is the default setting). Special runtime environment should be set up in order to use this feature: correct combination of libva, OpenCL runtime and media driver should be installed."
core,Check usage example for details: samples/va_intel/va_intel_interop.cpp
core,Namespaces namespace cv::va_intel::ocl 
core,This section describes OpenGL interoperability.
core,"To enable OpenGL support, configure OpenCV using CMake with WITH_OPENGL=ON . Currently OpenGL is supported only with WIN32, GTK and Qt backends on Windows and Linux (MacOS and Android are not supported). For GTK-2.0 backend gtkglext-1.0 library is required."
core,"To use OpenGL functionality you should first create OpenGL context (window or frame buffer). You can do this with namedWindow function or with other OpenGL toolkit (GLUT, for example)."
core,Namespaces namespace cv::ogl::ocl 
core,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
core,Classes class cv::ParallelLoopBody  Base class for parallel data processors. More...  class cv::ParallelLoopBodyLambdaWrapper 
core,"Enumerations enum cv::KmeansFlags { cv::KMEANS_RANDOM_CENTERS = 0 , cv::KMEANS_PP_CENTERS = 2 , cv::KMEANS_USE_INITIAL_LABELS = 1 }  k-means flags More... "
core,Classes class cv::DualQuat< _Tp >  class cv::Quat< _Tp >  class cv::QuatEnum 
core,The algorithms in this section minimize or maximize function value within specified constraints or without any constraints.
core,"Classes class cv::ConjGradSolver  This class is used to perform the non-linear non-constrained minimization of a function with known gradient,. More...  class cv::DownhillSolver  This class is used to perform the non-linear non-constrained minimization of a function,. More...  class cv::MinProblemSolver  Basic interface for all solvers. More... "
core,Classes class cv::AsyncArray  Returns result of asynchronous operations. More...  class cv::AsyncPromise  Provides result of asynchronous operations. More... 
core,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
core,Classes class cv::BufferPoolController  class cv::ocl::Context  class cv::ocl::Device  class cv::ocl::Image2D  class cv::ocl::Kernel  class cv::ocl::KernelArg  class cv::ocl::OpenCLExecutionContext  class cv::ocl::OpenCLExecutionContextScope  class cv::ocl::Platform  class cv::ocl::PlatformInfo  class cv::ocl::Program  class cv::ocl::ProgramSource  class cv::ocl::Queue  class cv::ocl::Timer 
core,Namespaces namespace cv::traits 
core,"Functions void cv::directx::convertFromD3D10Texture2D (ID3D10Texture2D *pD3D10Texture2D, OutputArray dst)  Converts ID3D10Texture2D to OutputArray.  void cv::directx::convertFromD3D11Texture2D (ID3D11Texture2D *pD3D11Texture2D, OutputArray dst)  Converts ID3D11Texture2D to OutputArray. If input texture format is DXGI_FORMAT_NV12 then data will be upsampled and color-converted to BGR format.  void cv::directx::convertFromDirect3DSurface9 (IDirect3DSurface9 *pDirect3DSurface9, OutputArray dst, void *surfaceSharedHandle=NULL)  Converts IDirect3DSurface9 to OutputArray.  void cv::directx::convertToD3D10Texture2D (InputArray src, ID3D10Texture2D *pD3D10Texture2D)  Converts InputArray to ID3D10Texture2D.  void cv::directx::convertToD3D11Texture2D (InputArray src, ID3D11Texture2D *pD3D11Texture2D)  Converts InputArray to ID3D11Texture2D. If destination texture format is DXGI_FORMAT_NV12 then input UMat expected to be in BGR format and data will be downsampled and color-converted to NV12.  void cv::directx::convertToDirect3DSurface9 (InputArray src, IDirect3DSurface9 *pDirect3DSurface9, void *surfaceSharedHandle=NULL)  Converts InputArray to IDirect3DSurface9.  int cv::directx::getTypeFromD3DFORMAT (const int iD3DFORMAT)  Get OpenCV type from DirectX type.  int cv::directx::getTypeFromDXGI_FORMAT (const int iDXGI_FORMAT)  Get OpenCV type from DirectX type.  Context & cv::directx::ocl::initializeContextFromD3D10Device (ID3D10Device *pD3D10Device)  Creates OpenCL context from D3D10 device.  Context & cv::directx::ocl::initializeContextFromD3D11Device (ID3D11Device *pD3D11Device)  Creates OpenCL context from D3D11 device.  Context & cv::directx::ocl::initializeContextFromDirect3DDevice9 (IDirect3DDevice9 *pDirect3DDevice9)  Creates OpenCL context from Direct3DDevice9 device.  Context & cv::directx::ocl::initializeContextFromDirect3DDevice9Ex (IDirect3DDevice9Ex *pDirect3DDevice9Ex)  Creates OpenCL context from Direct3DDevice9Ex device. "
dnn,This class represents high-level API for classification models.
dnn,"ClassificationModel allows to set params for preprocessing input image. ClassificationModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and return top-1 prediction."
dnn,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
dnn,Derivatives of this class encapsulates functions of certain backends.
dnn,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
dnn,n-dimensional dense array class
dnn,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
dnn,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
dnn,"In case of a 2-dimensional array, the above formula is reduced to:"
dnn,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
dnn,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
dnn,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
dnn,There are many different ways to create a Mat object. The most popular options are listed below:
dnn,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
dnn,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
dnn,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
dnn,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
dnn,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
dnn,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
dnn,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
dnn,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
dnn,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
dnn,This interface class allows to build new Layers - are building blocks of networks.
dnn,"Each class, derived from Layer, must implement forward() method to compute outputs. Also before using the new layer into networks you must register your layer by using one of LayerFactory macros."
dnn,This class is presented high-level API for neural networks.
dnn,"Model allows to set params for preprocessing input image. Model creates net from file with trained weights and config, sets preprocessing input and runs forward pass."
dnn,This class represents high-level API for object detection networks.
dnn,"DetectionModel allows to set params for preprocessing input image. DetectionModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and return result detections. For DetectionModel SSD, Faster R-CNN, YOLO topologies are supported."
dnn,Layer factory allows to create instances of registered layers.
dnn,"This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64."
dnn,Base class for text detection networks.
dnn,This module contains:
dnn,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
dnn,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
dnn,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
dnn,This subsection of dnn module contains information about built-in layers and their descriptions.
dnn,"Classes listed here, in fact, provides C++ API for creating instances of built-in layers. In addition to this way of layers instantiation, there is a more common factory API (see Utilities for New Layers Registration), it allows to create layers dynamically (by name) and register new ones. You can use both API, but factory API is less convenient for native C++ programming and basically designed for use inside importers (see readNetFromCaffe(), readNetFromTorch(), readNetFromTensorflow())."
dnn,"Built-in layers partially reproduce functionality of corresponding Caffe and Torch7 layers. In particular, the following layers and Caffe importer were tested to reproduce Caffe functionality:"
dnn,"Convolution Deconvolution Pooling InnerProduct TanH, ReLU, Sigmoid, BNLL, Power, AbsVal Softmax Reshape, Flatten, Slice, Split LRN MVN Dropout (since it does nothing on forward pass -))"
dnn,Classes class cv::dnn::AbsLayer  class cv::dnn::AccumLayer  class cv::dnn::AcoshLayer  class cv::dnn::AcosLayer  class cv::dnn::ActivationLayer  class cv::dnn::ActivationLayerInt8  class cv::dnn::ArgLayer  ArgMax/ArgMin layer. More...  class cv::dnn::AsinhLayer  class cv::dnn::AsinLayer  class cv::dnn::AtanhLayer  class cv::dnn::AtanLayer  class cv::dnn::AttentionLayer  class cv::dnn::BaseConvolutionLayer  class cv::dnn::BatchNormLayer  class cv::dnn::BatchNormLayerInt8  class cv::dnn::BlankLayer  class cv::dnn::BNLLLayer  class cv::dnn::CeilLayer  class cv::dnn::CeluLayer  class cv::dnn::ChannelsPReLULayer  class cv::dnn::CompareLayer  class cv::dnn::ConcatLayer  class cv::dnn::ConstLayer  class cv::dnn::ConvolutionLayer  class cv::dnn::ConvolutionLayerInt8  class cv::dnn::CorrelationLayer  class cv::dnn::CoshLayer  class cv::dnn::CosLayer  class cv::dnn::CropAndResizeLayer  class cv::dnn::CropLayer  class cv::dnn::CumSumLayer  class cv::dnn::DataAugmentationLayer  class cv::dnn::DeconvolutionLayer  class cv::dnn::DepthToSpaceLayer  class cv::dnn::DequantizeLayer  class cv::dnn::DetectionOutputLayer  Detection output layer. More...  class cv::dnn::EinsumLayer  This function performs array summation based on the Einstein summation convention. The function allows for concise expressions of various mathematical operations using subscripts. More...  class cv::dnn::EltwiseLayer  Element wise operation on inputs. More...  class cv::dnn::EltwiseLayerInt8  class cv::dnn::ELULayer  class cv::dnn::ErfLayer  class cv::dnn::ExpandLayer  class cv::dnn::ExpLayer  class cv::dnn::FlattenLayer  class cv::dnn::FloorLayer  class cv::dnn::FlowWarpLayer  class cv::dnn::GatherElementsLayer  GatherElements layer GatherElements takes two inputs data and indices of the same rank r >= 1 and an optional attribute axis and works such that: output[i][j][k] = data[index[i][j][k]][j][k] if axis = 0 and r = 3 output[i][j][k] = data[i][index[i][j][k]][k] if axis = 1 and r = 3 output[i][j][k] = data[i][j][index[i][j][k]] if axis = 2 and r = 3. More...  class cv::dnn::GatherLayer  Gather layer. More...  class cv::dnn::GeluApproximationLayer  class cv::dnn::GeluLayer  class cv::dnn::GemmLayer  class cv::dnn::GroupNormLayer  class cv::dnn::GRULayer  GRU recurrent one-layer. More...  class cv::dnn::HardSigmoidLayer  class cv::dnn::HardSwishLayer  class cv::dnn::InnerProductLayer  class cv::dnn::InnerProductLayerInt8  class cv::dnn::InstanceNormLayer  class cv::dnn::InterpLayer  Bilinear resize layer from https://github.com/cdmh/deeplab-public-ver2. More...  class cv::dnn::LayerNormLayer  class cv::dnn::LogLayer  class cv::dnn::LRNLayer  class cv::dnn::LSTMLayer  LSTM recurrent layer. More...  class cv::dnn::MatMulLayer  class cv::dnn::MaxUnpoolLayer  class cv::dnn::MishLayer  class cv::dnn::MVNLayer  class cv::dnn::NaryEltwiseLayer  class cv::dnn::NormalizeBBoxLayer  \( L_p \) - normalization layer. More...  class cv::dnn::NotLayer  class cv::dnn::PaddingLayer  Adds extra values for specific axes. More...  class cv::dnn::PermuteLayer  class cv::dnn::PoolingLayer  class cv::dnn::PoolingLayerInt8  class cv::dnn::PowerLayer  class cv::dnn::PriorBoxLayer  class cv::dnn::ProposalLayer  class cv::dnn::QuantizeLayer  class cv::dnn::ReciprocalLayer  class cv::dnn::ReduceLayer  class cv::dnn::RegionLayer  class cv::dnn::ReLU6Layer  class cv::dnn::ReLULayer  class cv::dnn::ReorgLayer  class cv::dnn::RequantizeLayer  class cv::dnn::ReshapeLayer  class cv::dnn::ResizeLayer  Resize input 4-dimensional blob by nearest neighbor or bilinear strategy. More...  class cv::dnn::RNNLayer  Classical recurrent layer. More...  class cv::dnn::RoundLayer  class cv::dnn::ScaleLayer  class cv::dnn::ScaleLayerInt8  class cv::dnn::ScatterLayer  class cv::dnn::ScatterNDLayer  class cv::dnn::SeluLayer  class cv::dnn::ShiftLayer  class cv::dnn::ShiftLayerInt8  class cv::dnn::ShrinkLayer  class cv::dnn::ShuffleChannelLayer  class cv::dnn::SigmoidLayer  class cv::dnn::SignLayer  class cv::dnn::SinhLayer  class cv::dnn::SinLayer  class cv::dnn::SliceLayer  class cv::dnn::SoftmaxLayer  class cv::dnn::SoftmaxLayerInt8  class cv::dnn::SoftplusLayer  class cv::dnn::SoftsignLayer  class cv::dnn::SpaceToDepthLayer  class cv::dnn::SplitLayer  class cv::dnn::SqrtLayer  class cv::dnn::SwishLayer  class cv::dnn::TanHLayer  class cv::dnn::TanLayer  class cv::dnn::ThresholdedReluLayer  class cv::dnn::TileLayer  class cv::dnn::TopKLayer 
dnn,This class represents high-level API for text detection DL networks compatible with EAST model.
dnn,Configurable parameters:
dnn,"(float) confThreshold - used to filter boxes by confidences, default: 0.5f (float) nmsThreshold - used in non maximum suppression, default: 0.0f"
dnn,"This class implements name-value dictionary, values are instances of DictValue."
dnn,Processing params of image to blob.
dnn,It includes all possible image processing operations and corresponding parameters.
dnn,This class represents high-level API for segmentation models.
dnn,"SegmentationModel allows to set params for preprocessing input image. SegmentationModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and returns the class prediction for each pixel."
dnn,This class represents high-level API for text detection DL networks compatible with DB model.
dnn,"Related publications: [167] Paper: https://arxiv.org/abs/1911.08947 For more information about the hyper-parameters setting, please refer to https://github.com/MhLiao/DB"
dnn,Configurable parameters:
dnn,"(float) binaryThreshold - The threshold of the binary map. It is usually set to 0.3. (float) polygonThreshold - The threshold of text polygons. It is usually set to 0.5, 0.6, and 0.7. Default is 0.5f (double) unclipRatio - The unclip ratio of the detected text region, which determines the output size. It is usually set to 2.0. (int) maxCandidates - The max number of the output results."
dnn,This class allows to create and manipulate comprehensive artificial neural networks.
dnn,"Neural network is presented as directed acyclic graph (DAG), where vertices are Layer instances, and edges specify relationships between layers inputs and outputs."
dnn,Each network layer has unique integer id and unique string name inside its network. LayerId can store either layer name or layer id.
dnn,"This class supports reference counting of its instances, i. e. copies point to the same instance."
dnn,This class provides all data needed to initialize layer.
dnn,"It includes dictionary with scalar params (which can be read by using Dict interface), blob params blobs and optional meta information: name and type of layer instance."
dnn,The class represents rotated (i.e. not up-right) rectangles on a plane.
dnn,"Each rectangle is specified by the center point (mass center), length of each side (represented by Size2f structure) and the rotation angle in degrees."
dnn,The sample below demonstrates how to use RotatedRect:
dnn,This class represents high-level API for keypoints models.
dnn,"KeypointsModel allows to set params for preprocessing input image. KeypointsModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and returns the x and y coordinates of each detected keypoint"
dnn,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
dnn,Namespaces namespace cv::traits 
dnn,This class represents high-level API for text recognition networks.
dnn,"TextRecognitionModel allows to set params for preprocessing input image. TextRecognitionModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and return recognition result. For TextRecognitionModel, CRNN-CTC is supported."
dnn,Derivatives of this class wraps cv::Mat for different backends and targets.
dnn,Classes class cv::dnn::LayerFactory  Layer factory allows to create instances of registered layers. More... 
features2d,"Enumerations enum struct cv::DrawMatchesFlags { cv::DrawMatchesFlags::DEFAULT = 0 , cv::DrawMatchesFlags::DRAW_OVER_OUTIMG = 1 , cv::DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS = 2 , cv::DrawMatchesFlags::DRAW_RICH_KEYPOINTS = 4 } "
features2d,"Classes struct cv::Accumulator< T >  struct cv::Accumulator< char >  struct cv::Accumulator< short >  struct cv::Accumulator< unsigned char >  struct cv::Accumulator< unsigned short >  class cv::AffineFeature  Class for implementing the wrapper which makes detectors and extractors to be affine invariant, described as ASIFT in [312] . More...  class cv::AgastFeatureDetector  Wrapping class for feature detection using the AGAST method. : More...  class cv::AKAZE  Class implementing the AKAZE keypoint detector and descriptor extractor, described in [10]. More...  class cv::BRISK  Class implementing the BRISK keypoint detector and descriptor extractor, described in [159] . More...  class cv::FastFeatureDetector  Wrapping class for feature detection using the FAST method. : More...  class cv::Feature2D  Abstract base class for 2D image feature detectors and descriptor extractors. More...  class cv::GFTTDetector  Wrapping class for feature detection using the goodFeaturesToTrack function. : More...  class cv::KAZE  Class implementing the KAZE keypoint detector and descriptor extractor, described in [9] . More...  class cv::KeyPointsFilter  A class filters a vector of keypoints. More...  struct cv::L1< T >  struct cv::L2< T >  class cv::MSER  Maximally stable extremal region extractor. More...  class cv::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More...  class cv::SIFT  Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe [174] . More...  class cv::SimpleBlobDetector  Class for extracting blobs from an image. : More...  struct cv::SL2< T > "
features2d,Matchers of keypoint descriptors in OpenCV have wrappers with a common interface that enables you to easily switch between different algorithms solving the same problem. This section is devoted to matching descriptors that are represented as vectors in a multidimensional space. All objects that implement vector descriptor matchers inherit the DescriptorMatcher interface.
features2d,Classes class cv::BFMatcher  Brute-force descriptor matcher. More...  class cv::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::FlannBasedMatcher  Flann-based descriptor matcher. More... 
features2d,This section describes approaches based on local 2D features and used to categorize objects.
features2d,Classes class cv::BOWImgDescriptorExtractor  Class to compute an image descriptor using the bag of visual words. More...  class cv::BOWKMeansTrainer  kmeans -based class to train visual vocabulary using the bag of visual words approach. : More...  class cv::BOWTrainer  Abstract base class for training the bag of visual words vocabulary from a set of descriptors. More... 
flann,n-dimensional dense array class
flann,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
flann,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
flann,"In case of a 2-dimensional array, the above formula is reduced to:"
flann,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
flann,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
flann,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
flann,There are many different ways to create a Mat object. The most popular options are listed below:
flann,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
flann,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
flann,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
flann,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
flann,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
flann,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
flann,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
flann,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
flann,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
flann,The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built.
flann,Distance functor specifies the metric to be used to calculate the distance between two points. There are several Distance functors that are readily available:
flann,"cv::cvflann::L2_Simple - Squared Euclidean distance functor. This is the simpler, unrolled version. This is preferable for very low dimensionality data (eg 3D points)"
flann,"cv::flann::L2 - Squared Euclidean distance functor, optimized version."
flann,"cv::flann::L1 - Manhattan distance functor, optimized version."
flann,cv::flann::MinkowskiDistance - The Minkowski distance functor. This is highly optimised with loop unrolling. The computation of squared root at the end is omitted for efficiency.
flann,"cv::flann::MaxDistance - The max distance functor. It computes the maximum distance between two vectors. This distance is not a valid kdtree distance, it's not dimensionwise additive."
flann,cv::flann::HammingLUT - Hamming distance functor. It counts the bit differences between two strings using a lookup table implementation.
flann,"cv::flann::Hamming - Hamming distance functor. Population count is performed using library calls, if available. Lookup table implementation is used as a fallback."
flann,cv::flann::Hamming2 - Hamming distance functor. Population count is implemented in 12 arithmetic operations (one of which is multiplication).
flann,"cv::flann::DNAmmingLUT - Adaptation of the Hamming distance functor to DNA comparison. As the four bases A, C, G, T of the DNA (or A, G, C, U for RNA) can be coded on 2 bits, it counts the bits pairs differences between two sequences using a lookup table implementation."
flann,cv::flann::DNAmming2 - Adaptation of the Hamming distance functor to DNA comparison. Bases differences count are vectorised thanks to arithmetic operations using standard registers (AVX2 and AVX-512 should come in a near future).
flann,cv::flann::HistIntersectionDistance - The histogram intersection distance functor.
flann,cv::flann::HellingerDistance - The Hellinger distance functor.
flann,cv::flann::ChiSquareDistance - The chi-square distance functor.
flann,cv::flann::KL_Divergence - The Kullback-Leibler divergence functor.
flann,"Although the provided implementations cover a vast range of cases, it is also possible to use a custom implementation. The distance functor is a class whose operator() computes the distance between two features. If the distance is also a kd-tree compatible distance, it should also provide an accum_dist() method that computes the distance between individual feature dimensions."
flann,"In addition to operator() and accum_dist(), a distance functor should also define the ElementType and the ResultType as the types of the elements it operates on and the type of the result it computes. If a distance functor can be used as a kd-tree distance (meaning that the full distance between a pair of features can be accumulated from the partial distances between the individual dimensions) a typedef is_kdtree_distance should be present inside the distance functor. If the distance is not a kd-tree distance, but it's a distance in a vector space (the individual dimensions of the elements it operates on can be accessed independently) a typedef is_vector_space_distance should be defined inside the functor. If neither typedef is defined, the distance is assumed to be a metric distance and will only be used with indexes operating on generic metric distances."
flann,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
flann,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
gapi,n-dimensional dense array class
gapi,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
gapi,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
gapi,"In case of a 2-dimensional array, the above formula is reduced to:"
gapi,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
gapi,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
gapi,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
gapi,There are many different ways to create a Mat object. The most popular options are listed below:
gapi,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
gapi,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
gapi,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
gapi,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
gapi,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
gapi,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
gapi,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
gapi,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
gapi,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
gapi,"Functions GMat cv::gapi::BackgroundSubtractor (const GMat &src, const cv::gapi::video::BackgroundSubtractorParams &bsParams)  Gaussian Mixture-based or K-nearest neighbours-based Background/Foreground Segmentation Algorithm. The operation generates a foreground mask.  std::tuple< GArray< GMat >, GScalar > cv::gapi::buildOpticalFlowPyramid (const GMat &img, const Size &winSize, const GScalar &maxLevel, bool withDerivatives=true, int pyrBorder=BORDER_REFLECT_101, int derivBorder=BORDER_CONSTANT, bool tryReuseInputImage=true)  Constructs the image pyramid which can be passed to calcOpticalFlowPyrLK.  std::tuple< GArray< Point2f >, GArray< uchar >, GArray< float > > cv::gapi::calcOpticalFlowPyrLK (const GArray< GMat > &prevPyr, const GArray< GMat > &nextPyr, const GArray< Point2f > &prevPts, const GArray< Point2f > &predPts, const Size &winSize=Size(21, 21), const GScalar &maxLevel=3, const TermCriteria &criteria=TermCriteria(TermCriteria::COUNT|TermCriteria::EPS, 30, 0.01), int flags=0, double minEigThresh=1e-4)  std::tuple< GArray< Point2f >, GArray< uchar >, GArray< float > > cv::gapi::calcOpticalFlowPyrLK (const GMat &prevImg, const GMat &nextImg, const GArray< Point2f > &prevPts, const GArray< Point2f > &predPts, const Size &winSize=Size(21, 21), const GScalar &maxLevel=3, const TermCriteria &criteria=TermCriteria(TermCriteria::COUNT|TermCriteria::EPS, 30, 0.01), int flags=0, double minEigThresh=1e-4)  Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with pyramids.  GMat cv::gapi::KalmanFilter (const GMat &measurement, const GOpaque< bool > &haveMeasurement, const cv::gapi::KalmanParams &kfParams)  GMat cv::gapi::KalmanFilter (const GMat &measurement, const GOpaque< bool > &haveMeasurement, const GMat &control, const cv::gapi::KalmanParams &kfParams)  Standard Kalman filter algorithm http://en.wikipedia.org/wiki/Kalman_filter. "
gapi,"Functions GMat cv::gapi::Canny (const GMat &image, double threshold1, double threshold2, int apertureSize=3, bool L2gradient=false)  Finds edges in an image using the Canny algorithm.  GArray< Point2f > cv::gapi::goodFeaturesToTrack (const GMat &image, int maxCorners, double qualityLevel, double minDistance, const Mat &mask=Mat(), int blockSize=3, bool useHarrisDetector=false, double k=0.04)  Determines strong corners on an image. "
gapi,Template class for specifying the size of an image or rectangle.
gapi,The class includes two members called width and height. The structure can be converted to and from the old OpenCV structures CvSize and CvSize2D32f . The same set of arithmetic and comparison operations as for Point_ is available.
gapi,OpenCV defines the following Size_<> aliases:
gapi,gapi_colorconvert
gapi,"Functions GMat cv::gapi::concatHor (const GMat &src1, const GMat &src2)  Applies horizontal concatenation to given matrices.  GMat cv::gapi::concatHor (const std::vector< GMat > &v)  GMat cv::gapi::concatVert (const GMat &src1, const GMat &src2)  Applies vertical concatenation to given matrices.  GMat cv::gapi::concatVert (const std::vector< GMat > &v)  GMat cv::gapi::convertTo (const GMat &src, int rdepth, double alpha=1, double beta=0)  Converts a matrix to another data depth with optional scaling.  GFrame cv::gapi::copy (const GFrame &in)  Makes a copy of the input frame. Note that this copy may be not real (no actual data copied). Use this function to maintain graph contracts, e.g when graph's input needs to be passed directly to output, like in Streaming mode.  GMat cv::gapi::copy (const GMat &in)  Makes a copy of the input image. Note that this copy may be not real (no actual data copied). Use this function to maintain graph contracts, e.g when graph's input needs to be passed directly to output, like in Streaming mode.  GMat cv::gapi::crop (const GMat &src, const Rect &rect)  Crops a 2D matrix.  GMat cv::gapi::flip (const GMat &src, int flipCode)  Flips a 2D matrix around vertical, horizontal, or both axes.  GMat cv::gapi::LUT (const GMat &src, const Mat &lut)  Performs a look-up table transform of a matrix.  GMat cv::gapi::merge3 (const GMat &src1, const GMat &src2, const GMat &src3)  Creates one 3-channel matrix out of 3 single-channel ones.  GMat cv::gapi::merge4 (const GMat &src1, const GMat &src2, const GMat &src3, const GMat &src4)  Creates one 4-channel matrix out of 4 single-channel ones.  GMat cv::gapi::normalize (const GMat &src, double alpha, double beta, int norm_type, int ddepth=-1)  Normalizes the norm or value range of an array.  GMat cv::gapi::remap (const GMat &src, const Mat &map1, const Mat &map2, int interpolation, int borderMode=BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies a generic geometrical transformation to an image.  GMat cv::gapi::resize (const GMat &src, const Size &dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR)  Resizes an image.  GMatP cv::gapi::resizeP (const GMatP &src, const Size &dsize, int interpolation=cv::INTER_LINEAR)  Resizes a planar image.  std::tuple< GMat, GMat, GMat > cv::gapi::split3 (const GMat &src)  Divides a 3-channel matrix into 3 single-channel matrices.  std::tuple< GMat, GMat, GMat, GMat > cv::gapi::split4 (const GMat &src)  Divides a 4-channel matrix into 4 single-channel matrices.  GMat cv::gapi::warpAffine (const GMat &src, const Mat &M, const Size &dsize, int flags=cv::INTER_LINEAR, int borderMode=cv::BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies an affine transformation to an image.  GMat cv::gapi::warpPerspective (const GMat &src, const Mat &M, const Size &dsize, int flags=cv::INTER_LINEAR, int borderMode=cv::BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies a perspective transformation to an image. "
gapi,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
gapi,It provides easy interface to:
gapi,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
gapi,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
gapi,"Class for video capturing from video files, image sequences or cameras."
gapi,The class provides C++ API for capturing video from cameras or for reading video files and image sequences.
gapi,Here is how the class can be used:
gapi,(C++) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/cpp/videocapture_starter.cpp (Python) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/python/video.py (Python) A multi threaded video processing sample can be found at OPENCV_SOURCE_CODE/samples/python/video_threaded.py (Python) VideoCapture sample showcasing some features of the Video4Linux2 backend OPENCV_SOURCE_CODE/samples/python/video_v4l2.py
gapi,"GComputation class represents a captured computation graph. GComputation objects form boundaries for expression code user writes with G-API, allowing to compile and execute it."
gapi,"G-API computations are defined with input/output data objects. G-API will track automatically which operations connect specified outputs to the inputs, forming up a call graph to be executed. The below example expresses calculation of Sobel operator for edge detection ( \(G = \sqrt{G_x^2 + G_y^2}\)):"
gapi,Full pipeline can be now captured with this object declaration:
gapi,Input/output data objects on which a call graph should be reconstructed are passed using special wrappers cv::GIn and cv::GOut. G-API will track automatically which operations form a path from inputs to outputs and build the execution graph appropriately.
gapi,"Note that cv::GComputation doesn't take ownership on data objects it is defined. Moreover, multiple GComputation objects may be defined on the same expressions, e.g. a smaller pipeline which expects that image gradients are already pre-calculated may be defined like this:"
gapi,"The resulting graph would expect two inputs and produce one output. In this case, it doesn't matter if gx/gy data objects are results of cv::gapi::Sobel operators G-API will stop unrolling expressions and building the underlying graph one reaching this data objects."
gapi,"The way how GComputation is defined is important as its definition specifies graph protocol the way how the graph should be used. Protocol is defined by number of inputs, number of outputs, and shapes of inputs and outputs."
gapi,"In the above example, sobelEdge expects one Mat on input and produces one Mat; while sobelEdgeSub expects two Mats on input and produces one Mat. GComputation's protocol defines how other computation methods should be used cv::GComputation::compile() and cv::GComputation::apply(). For example, if a graph is defined on two GMat inputs, two cv::Mat objects have to be passed to apply() for execution. GComputation checks protocol correctness in runtime so passing a different number of objects in apply() or passing cv::Scalar instead of cv::Mat there would compile well as a C++ source but raise an exception in run-time. G-API also comes with a typed wrapper cv::GComputationT<> which introduces this type-checking in compile-time."
gapi,"cv::GComputation itself is a thin object which just captures what the graph is. The compiled graph (which actually process data) is represented by class GCompiled. Use compile() method to generate a compiled graph with given compile options. cv::GComputation can also be used to process data with implicit graph compilation on-the-fly, see apply() for details."
gapi,"GComputation is a reference-counted object once defined, all its copies will refer to the same instance."
gapi,"Functions GMat cv::gapi::bilateralFilter (const GMat &src, int d, double sigmaColor, double sigmaSpace, int borderType=BORDER_DEFAULT)  Applies the bilateral filter to an image.  GMat cv::gapi::blur (const GMat &src, const Size &ksize, const Point &anchor=Point(-1,-1), int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Blurs an image using the normalized box filter.  GMat cv::gapi::boxFilter (const GMat &src, int dtype, const Size &ksize, const Point &anchor=Point(-1,-1), bool normalize=true, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Blurs an image using the box filter.  GMat cv::gapi::dilate (const GMat &src, const Mat &kernel, const Point &anchor=Point(-1,-1), int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Dilates an image by using a specific structuring element.  GMat cv::gapi::dilate3x3 (const GMat &src, int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Dilates an image by using 3 by 3 rectangular structuring element.  GMat cv::gapi::erode (const GMat &src, const Mat &kernel, const Point &anchor=Point(-1,-1), int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Erodes an image by using a specific structuring element.  GMat cv::gapi::erode3x3 (const GMat &src, int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Erodes an image by using 3 by 3 rectangular structuring element.  GMat cv::gapi::filter2D (const GMat &src, int ddepth, const Mat &kernel, const Point &anchor=Point(-1,-1), const Scalar &delta=Scalar(0), int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Convolves an image with the kernel.  GMat cv::gapi::gaussianBlur (const GMat &src, const Size &ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Blurs an image using a Gaussian filter.  GMat cv::gapi::Laplacian (const GMat &src, int ddepth, int ksize=1, double scale=1, double delta=0, int borderType=BORDER_DEFAULT)  Calculates the Laplacian of an image.  GMat cv::gapi::medianBlur (const GMat &src, int ksize)  Blurs an image using the median filter.  GMat cv::gapi::morphologyEx (const GMat &src, const MorphTypes op, const Mat &kernel, const Point &anchor=Point(-1,-1), const int iterations=1, const BorderTypes borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Performs advanced morphological transformations.  GMat cv::gapi::sepFilter (const GMat &src, int ddepth, const Mat &kernelX, const Mat &kernelY, const Point &anchor, const Scalar &delta, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Applies a separable linear filter to a matrix(image).  GMat cv::gapi::Sobel (const GMat &src, int ddepth, int dx, int dy, int ksize=3, double scale=1, double delta=0, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.  std::tuple< GMat, GMat > cv::gapi::SobelXY (const GMat &src, int ddepth, int order, int ksize=3, double scale=1, double delta=0, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator. "
gapi,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
gapi,"Functions GMat cv::gapi::BayerGR2RGB (const GMat &src_gr)  Converts an image from BayerGR color space to RGB. The function converts an input image from BayerGR color space to RGB. The conventional ranges for G, R, and B channel values are 0 to 255.  GMat cv::gapi::BGR2Gray (const GMat &src)  Converts an image from BGR color space to gray-scaled.  GMat cv::gapi::BGR2I420 (const GMat &src)  Converts an image from BGR color space to I420 color space.  GMat cv::gapi::BGR2LUV (const GMat &src)  Converts an image from BGR color space to LUV color space.  GMat cv::gapi::BGR2RGB (const GMat &src)  Converts an image from BGR color space to RGB color space.  GMat cv::gapi::BGR2YUV (const GMat &src)  Converts an image from BGR color space to YUV color space.  GMat cv::gapi::I4202BGR (const GMat &src)  Converts an image from I420 color space to BGR color space.  GMat cv::gapi::I4202RGB (const GMat &src)  Converts an image from I420 color space to BGR color space.  GMat cv::gapi::LUV2BGR (const GMat &src)  Converts an image from LUV color space to BGR color space.  GMat cv::gapi::NV12toBGR (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to BGR. The function converts an input image from NV12 color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMatP cv::gapi::NV12toBGRp (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to BGR. The function converts an input image from NV12 color space to BGR. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMat cv::gapi::NV12toGray (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to gray-scaled. The function converts an input image from NV12 color space to gray-scaled. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMat cv::gapi::NV12toRGB (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to RGB. The function converts an input image from NV12 color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMatP cv::gapi::NV12toRGBp (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to RGB. The function converts an input image from NV12 color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMat cv::gapi::RGB2Gray (const GMat &src)  Converts an image from RGB color space to gray-scaled.  GMat cv::gapi::RGB2Gray (const GMat &src, float rY, float gY, float bY)  GMat cv::gapi::RGB2HSV (const GMat &src)  Converts an image from RGB color space to HSV. The function converts an input image from RGB color space to HSV. The conventional ranges for R, G, and B channel values are 0 to 255.  GMat cv::gapi::RGB2I420 (const GMat &src)  Converts an image from RGB color space to I420 color space.  GMat cv::gapi::RGB2Lab (const GMat &src)  Converts an image from RGB color space to Lab color space.  GMat cv::gapi::RGB2YUV (const GMat &src)  Converts an image from RGB color space to YUV color space.  GMat cv::gapi::RGB2YUV422 (const GMat &src)  Converts an image from RGB color space to YUV422. The function converts an input image from RGB color space to YUV422. The conventional ranges for R, G, and B channel values are 0 to 255.  GMat cv::gapi::YUV2BGR (const GMat &src)  Converts an image from YUV color space to BGR color space.  GMat cv::gapi::YUV2RGB (const GMat &src)  Converts an image from YUV color space to RGB. The function converts an input image from YUV color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255. "
gapi,GMat class represents image or tensor data in the graph.
gapi,"GMat doesn't store any data itself, instead it describes a functional relationship between operations consuming and producing GMat objects."
gapi,"GMat is a virtual counterpart of Mat and UMat, but it doesn't mean G-API use Mat or UMat objects internally to represent GMat objects the internal data representation may be backend-specific or optimized out at all."
gapi,Functions for in-graph drawing.
gapi,"G-API can do some in-graph drawing with a generic operations and a set of rendering primitives. In contrast with traditional OpenCV, in G-API user need to form a rendering list of primitives to draw. This list can be built manually or generated within a graph. This list is passed to special operations or functions where all primitives are interpreted and applied to the image."
gapi,"For example, in a complex pipeline a list of detected objects can be translated in-graph to a list of cv::gapi::wip::draw::Rect primitives to highlight those with bounding boxes, or a list of detected faces can be translated in-graph to a list of cv::gapi::wip::draw::Mosaic primitives to hide sensitive content or protect privacy."
gapi,"Like any other operations, rendering in G-API can be reimplemented by different backends. Currently only an OpenCV-based backend is available."
gapi,"In addition to the graph-level operations, there are also regular (immediate) OpenCV-like functions are available see cv::gapi::wip::draw::render(). These functions are just wrappers over regular G-API and build the rendering graphs on the fly, so take compilation arguments as parameters."
gapi,"Currently this API is more machine-oriented than human-oriented. The main purpose is to translate a set of domain-specific objects to a list of primitives to draw. For example, in order to generate a picture like this:"
gapi,Rendering list needs to be generated as follows:
highgui,Base storage class for GPU memory with reference counting.
highgui,Its interface matches the Mat interface with the following limitations:
highgui,no arbitrary dimensions support (only 2D) no functions that return references to their data (because references on GPU are not valid for CPU) no expression templates technique support
highgui,Beware that the latter limitation may lead to overloaded matrix operators that cause memory allocations. The GpuMat class is convertible to cuda::PtrStepSz and cuda::PtrStep so it can be passed directly to the kernel.
highgui,"Some member functions are described as a ""Blocking Call"" while some are described as a ""Non-Blocking Call"". Blocking functions are synchronous to host. It is guaranteed that the GPU operation is finished when the function returns. However, non-blocking functions are asynchronous to host. Those functions may return even if the GPU operation is not finished."
highgui,"Compared to their blocking counterpart, non-blocking functions accept Stream as an additional argument. If a non-default stream is passed, the GPU operation may overlap with operations in other streams."
highgui,"Enumerations enum cv::MouseEventFlags { cv::EVENT_FLAG_LBUTTON = 1 , cv::EVENT_FLAG_RBUTTON = 2 , cv::EVENT_FLAG_MBUTTON = 4 , cv::EVENT_FLAG_CTRLKEY = 8 , cv::EVENT_FLAG_SHIFTKEY = 16 , cv::EVENT_FLAG_ALTKEY = 32 }  Mouse Event Flags see cv::MouseCallback. More...  enum cv::MouseEventTypes { cv::EVENT_MOUSEMOVE = 0 , cv::EVENT_LBUTTONDOWN = 1 , cv::EVENT_RBUTTONDOWN = 2 , cv::EVENT_MBUTTONDOWN = 3 , cv::EVENT_LBUTTONUP = 4 , cv::EVENT_RBUTTONUP = 5 , cv::EVENT_MBUTTONUP = 6 , cv::EVENT_LBUTTONDBLCLK = 7 , cv::EVENT_RBUTTONDBLCLK = 8 , cv::EVENT_MBUTTONDBLCLK = 9 , cv::EVENT_MOUSEWHEEL = 10 , cv::EVENT_MOUSEHWHEEL = 11 }  Mouse Events see cv::MouseCallback. More...  enum cv::WindowFlags { cv::WINDOW_NORMAL = 0x00000000 , cv::WINDOW_AUTOSIZE = 0x00000001 , cv::WINDOW_OPENGL = 0x00001000 , cv::WINDOW_FULLSCREEN = 1 , cv::WINDOW_FREERATIO = 0x00000100 , cv::WINDOW_KEEPRATIO = 0x00000000 , cv::WINDOW_GUI_EXPANDED =0x00000000 , cv::WINDOW_GUI_NORMAL = 0x00000010 }  Flags for cv::namedWindow. More...  enum cv::WindowPropertyFlags { cv::WND_PROP_FULLSCREEN = 0 , cv::WND_PROP_AUTOSIZE = 1 , cv::WND_PROP_ASPECT_RATIO = 2 , cv::WND_PROP_OPENGL = 3 , cv::WND_PROP_VISIBLE = 4 , cv::WND_PROP_TOPMOST = 5 , cv::WND_PROP_VSYNC = 6 }  Flags for cv::setWindowProperty / cv::getWindowProperty. More... "
highgui,Smart pointer for OpenGL buffer object with reference counting.
highgui,"Buffer Objects are OpenGL objects that store an array of unformatted memory allocated by the OpenGL context. These can be used to store vertex data, pixel data retrieved from images or the framebuffer, and a variety of other things."
highgui,ogl::Buffer has interface similar with Mat interface and represents 2D array memory.
highgui,ogl::Buffer supports memory transfers between host and device and also can be mapped to CUDA memory.
highgui,Smart pointer for OpenGL 2D texture memory with reference counting.
highgui,"This figure explains new functionality implemented with WinRT GUI. The new GUI provides an Image control, and a slider panel. Slider panel holds trackbars attached to it."
highgui,Sliders are attached below the image control. Every new slider is added below the previous one.
highgui,See below the example used to generate the figure:
highgui,Functions void cv::winrt_initContainer (::Windows::UI::Xaml::Controls::Panel^ container)  Initializes container component that will be used to hold generated window content. 
highgui,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
highgui,It provides easy interface to:
highgui,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
highgui,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
highgui,"This figure explains new functionality implemented with Qt* GUI. The new GUI provides a statusbar, a toolbar, and a control panel. The control panel can have trackbars and buttonbars attached to it. If you cannot see the control panel, press Ctrl+P or right-click any Qt window and select Display properties window."
highgui,"To attach a trackbar, the window name parameter must be NULL. To attach a buttonbar, a button must be created. If the last bar attached to the control panel is a buttonbar, the new button is added to the right of the last button. If the last bar attached to the control panel is a trackbar, or the control panel is empty, a new buttonbar is created. Then, a new button is attached to it."
highgui,See below the example used to generate the figure:
highgui,Classes struct cv::QtFont  QtFont available only for Qt. See cv::fontQt. More... 
highgui,Namespaces namespace cv::traits 
highgui,"Functions void cv::imshow (const String &winname, const ogl::Texture2D &tex)  Displays OpenGL 2D texture in the specified window.  void cv::setOpenGlContext (const String &winname)  Sets the specified window as current OpenGL context.  void cv::setOpenGlDrawCallback (const String &winname, OpenGlDrawCallback onOpenGlDraw, void *userdata=0)  Sets a callback function to be called to draw on top of displayed image.  void cv::updateWindow (const String &winname)  Force window to redraw its context and call draw callback ( See cv::setOpenGlDrawCallback ). "
imgcodecs,Classes struct cvhalDFT  Dummy structure storing DFT/DCT context. More... 
imgcodecs,Class passed to an error.
imgcodecs,This class encapsulates all or almost all necessary information about the error happened in the program. The exception is usually constructed and thrown implicitly via CV_Error and CV_Error_ macros.
imgcodecs,YOUR ATTENTION PLEASE!
imgcodecs,This is a header-only implementation of cv::VideoCapture-based Stream source. It is not built by default with G-API as G-API doesn't depend on videoio module.
imgcodecs,"If you want to use it in your application, please make sure videioio is available in your OpenCV package and is linked to your application."
imgcodecs,Note for developers: please don't put videoio dependency in G-API because of this file.
imgcodecs,"Functions void CGImageToMat (const CGImageRef image, cv::Mat &m, bool alphaExist=false)  CGImageRef MatToCGImage (const cv::Mat &image) CF_RETURNS_RETAINED  UIImage * MatToUIImage (const cv::Mat &image)  void UIImageToMat (const UIImage *image, cv::Mat &m, bool alphaExist=false) "
imgcodecs,n-dimensional dense array class
imgcodecs,"The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as:"
imgcodecs,"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]"
imgcodecs,"In case of a 2-dimensional array, the above formula is reduced to:"
imgcodecs,"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]"
imgcodecs,"Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() ."
imgcodecs,"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions."
imgcodecs,There are many different ways to create a Mat object. The most popular options are listed below:
imgcodecs,"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j. Mat M(7,7,CV_32FC2,Scalar(1,3)); // and now turn M to a 100x60 15-channel 8-bit matrix. // The old content will be deallocated M.create(100,60,CV_8UC(15)); cv::Matn-dimensional dense array classDefinition mat.hpp:828 cv::ScalarScalar_< double > ScalarDefinition types.hpp:709 CV_32FC2#define CV_32FC2Definition interface.h:119 CV_8UC#define CV_8UC(n)Definition interface.h:92 As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones. Create a multi-dimensional array: // create a 100x100x100 8-bit array int sz[] = {100, 100, 100}; Mat bigCube(3, sz, CV_8U, Scalar::all(0)); cv::Scalar_< double >::allstatic Scalar_< double > all(double v0)returns a scalar with all elements set to v0 CV_8U#define CV_8UDefinition interface.h:73 It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty). Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it. Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd row M.row(3) = M.row(3) + M.row(5)*3; // now copy the 7-th column to the 1-st column // M.col(1) = M.col(7); // this will not work Mat M1 = M.col(1); M.col(7).copyTo(M1); // create a new 320x240 image Mat img(Size(320,240),CV_8UC3); // select a ROI Mat roi(img, Rect(10,10,100,100)); // fill the ROI with (0,255,0) (which is green in RGB space); // the original 320x240 image will be modified roi = Scalar(0,255,0); cv::Mat::colMat col(int x) constCreates a matrix header for the specified matrix column. cv::Mat::copyTovoid copyTo(OutputArray m) constCopies the matrix to another one. cv::RectRect2i RectDefinition types.hpp:496 cv::SizeSize2i SizeDefinition types.hpp:370 CV_8UC3#define CV_8UC3Definition interface.h:90 Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S); // extracts A columns, 1 (inclusive) to 3 (exclusive). Mat B = A(Range::all(), Range(1, 3)); // extracts B rows, 5 (inclusive) to 9 (exclusive). // that is, C \~ A(Range(5, 9), Range(1, 3)) Mat C = B(Range(5, 9), Range::all()); Size size; Point ofs; C.locateROI(size, ofs); // size will be (width=10,height=10) and the ofs will be (x=1, y=5) cv::Mat::sizeMatSize sizeDefinition mat.hpp:2176 cv::Mat::locateROIvoid locateROI(Size &wholeSize, Point &ofs) constLocates the matrix header within a parent matrix. cv::Mat::eyestatic CV_NODISCARD_STD MatExpr eye(int rows, int cols, int type)Returns an identity matrix of the specified size and type. cv::Point_< int > cv::RangeTemplate class specifying a continuous subsequence (slice) of a sequence.Definition types.hpp:630 cv::Range::allstatic Range all() cv::Size_Template class for specifying the size of an image or rectangle.Definition types.hpp:335 CV_32S#define CV_32SDefinition interface.h:77 As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices. Make a header for user-allocated data. It can be useful to do the following: Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels, int width, int height, int step) { // wrap input buffer Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result; } cv::Mat::stepMatStep stepDefinition mat.hpp:2177 cv::GaussianBlurvoid GaussianBlur(InputArray src, OutputArray dst, Size ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, AlgorithmHint hint=cv::ALGO_HINT_DEFAULT)Blurs an image using a Gaussian filter. Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}}; Mat M = Mat(3, 3, CV_64F, m).inv(); cv::Mat::MatMat() CV_NOEXCEPT CV_64F#define CV_64FDefinition interface.h:79 Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M. M += Mat::eye(M.rows, M.cols, CV_64F); Use a comma-separated initializer: // create a 3x3 double-precision identity matrix Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); cv::Mat_Template matrix class derived from Mat.Definition mat.hpp:2246 With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors."
imgcodecs,"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release()."
imgcodecs,"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as:"
imgcodecs,assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.
imgcodecs,"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] :"
imgcodecs,"Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row:"
imgcodecs,"In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices."
imgcodecs,"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:"
imgcodecs,"The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort()."
imgcodecs,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
imgcodecs,"Template class for short numerical vectors, a partial case of Matx."
imgcodecs,"This template class represents short numerical vectors (of 1, 2, 3, 4 ... elements) on which you can perform basic arithmetical operations, access individual elements using [] operator etc. The vectors are allocated on stack, as opposite to std::valarray, std::vector, cv::Mat etc., which elements are dynamically allocated in the heap."
imgcodecs,The template takes 2 parameters:
imgcodecs,_Tp element type cn the number of elements
imgcodecs,"In addition to the universal notation like Vec<float, 3>, you can use shorter aliases for the most popular specialized variants of Vec, e.g. Vec3f ~ Vec<float, 3>."
imgcodecs,"It is possible to convert Vec<T,2> to/from Point_, Vec<T,3> to/from Point3_ , and Vec<T,4> to CvScalar or Scalar_. Use operator[] to access the elements of Vec."
imgcodecs,All the expected vector operations are also implemented:
imgcodecs,"v1 = v2 + v3 v1 = v2 - v3 v1 = v2 * scale v1 = scale * v2 v1 = -v2 v1 += v2 and other augmenting operations v1 == v2, v1 != v2 norm(v1) (euclidean norm) The Vec class is commonly used to describe pixel types of multi-channel arrays. See Mat for details."
imgcodecs,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
imgcodecs,"Enumerations enum cv::ImreadModes { cv::IMREAD_UNCHANGED = -1 , cv::IMREAD_GRAYSCALE = 0 , cv::IMREAD_COLOR_BGR = 1 , cv::IMREAD_COLOR = 1 , cv::IMREAD_ANYDEPTH = 2 , cv::IMREAD_ANYCOLOR = 4 , cv::IMREAD_LOAD_GDAL = 8 , cv::IMREAD_REDUCED_GRAYSCALE_2 = 16 , cv::IMREAD_REDUCED_COLOR_2 = 17 , cv::IMREAD_REDUCED_GRAYSCALE_4 = 32 , cv::IMREAD_REDUCED_COLOR_4 = 33 , cv::IMREAD_REDUCED_GRAYSCALE_8 = 64 , cv::IMREAD_REDUCED_COLOR_8 = 65 , cv::IMREAD_IGNORE_ORIENTATION = 128 , cv::IMREAD_COLOR_RGB = 256 }  Imread flags. More...  enum cv::ImwriteEXRCompressionFlags { cv::IMWRITE_EXR_COMPRESSION_NO = 0 , cv::IMWRITE_EXR_COMPRESSION_RLE = 1 , cv::IMWRITE_EXR_COMPRESSION_ZIPS = 2 , cv::IMWRITE_EXR_COMPRESSION_ZIP = 3 , cv::IMWRITE_EXR_COMPRESSION_PIZ = 4 , cv::IMWRITE_EXR_COMPRESSION_PXR24 = 5 , cv::IMWRITE_EXR_COMPRESSION_B44 = 6 , cv::IMWRITE_EXR_COMPRESSION_B44A = 7 , cv::IMWRITE_EXR_COMPRESSION_DWAA = 8 , cv::IMWRITE_EXR_COMPRESSION_DWAB = 9 }  enum cv::ImwriteEXRTypeFlags { cv::IMWRITE_EXR_TYPE_HALF = 1 , cv::IMWRITE_EXR_TYPE_FLOAT = 2 }  enum cv::ImwriteFlags { cv::IMWRITE_JPEG_QUALITY = 1 , cv::IMWRITE_JPEG_PROGRESSIVE = 2 , cv::IMWRITE_JPEG_OPTIMIZE = 3 , cv::IMWRITE_JPEG_RST_INTERVAL = 4 , cv::IMWRITE_JPEG_LUMA_QUALITY = 5 , cv::IMWRITE_JPEG_CHROMA_QUALITY = 6 , cv::IMWRITE_JPEG_SAMPLING_FACTOR = 7 , cv::IMWRITE_PNG_COMPRESSION = 16 , cv::IMWRITE_PNG_STRATEGY = 17 , cv::IMWRITE_PNG_BILEVEL = 18 , cv::IMWRITE_PXM_BINARY = 32 , cv::IMWRITE_EXR_TYPE = (3 << 4) + 0 , cv::IMWRITE_EXR_COMPRESSION = (3 << 4) + 1 , cv::IMWRITE_EXR_DWA_COMPRESSION_LEVEL = (3 << 4) + 2 , cv::IMWRITE_WEBP_QUALITY = 64 , cv::IMWRITE_HDR_COMPRESSION = (5 << 4) + 0 , cv::IMWRITE_PAM_TUPLETYPE = 128 , cv::IMWRITE_TIFF_RESUNIT = 256 , cv::IMWRITE_TIFF_XDPI = 257 , cv::IMWRITE_TIFF_YDPI = 258 , cv::IMWRITE_TIFF_COMPRESSION = 259 , cv::IMWRITE_TIFF_ROWSPERSTRIP = 278 , cv::IMWRITE_TIFF_PREDICTOR = 317 , cv::IMWRITE_JPEG2000_COMPRESSION_X1000 = 272 , cv::IMWRITE_AVIF_QUALITY = 512 , cv::IMWRITE_AVIF_DEPTH = 513 , cv::IMWRITE_AVIF_SPEED = 514 }  Imwrite flags. More...  enum cv::ImwriteHDRCompressionFlags { cv::IMWRITE_HDR_COMPRESSION_NONE = 0 , cv::IMWRITE_HDR_COMPRESSION_RLE = 1 }  Imwrite HDR specific values for IMWRITE_HDR_COMPRESSION parameter key. More...  enum cv::ImwriteJPEGSamplingFactorParams { cv::IMWRITE_JPEG_SAMPLING_FACTOR_411 = 0x411111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_420 = 0x221111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_422 = 0x211111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_440 = 0x121111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_444 = 0x111111 }  enum cv::ImwritePAMFlags { cv::IMWRITE_PAM_FORMAT_NULL = 0 , cv::IMWRITE_PAM_FORMAT_BLACKANDWHITE = 1 , cv::IMWRITE_PAM_FORMAT_GRAYSCALE = 2 , cv::IMWRITE_PAM_FORMAT_GRAYSCALE_ALPHA = 3 , cv::IMWRITE_PAM_FORMAT_RGB = 4 , cv::IMWRITE_PAM_FORMAT_RGB_ALPHA = 5 }  Imwrite PAM specific tupletype flags used to define the 'TUPLETYPE' field of a PAM file. More...  enum cv::ImwritePNGFlags { cv::IMWRITE_PNG_STRATEGY_DEFAULT = 0 , cv::IMWRITE_PNG_STRATEGY_FILTERED = 1 , cv::IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY = 2 , cv::IMWRITE_PNG_STRATEGY_RLE = 3 , cv::IMWRITE_PNG_STRATEGY_FIXED = 4 }  Imwrite PNG specific flags used to tune the compression algorithm. More...  enum cv::ImwriteTiffCompressionFlags { cv::IMWRITE_TIFF_COMPRESSION_NONE = 1 , cv::IMWRITE_TIFF_COMPRESSION_CCITTRLE = 2 , cv::IMWRITE_TIFF_COMPRESSION_CCITTFAX3 = 3 , cv::IMWRITE_TIFF_COMPRESSION_CCITT_T4 = 3 , cv::IMWRITE_TIFF_COMPRESSION_CCITTFAX4 = 4 , cv::IMWRITE_TIFF_COMPRESSION_CCITT_T6 = 4 , cv::IMWRITE_TIFF_COMPRESSION_LZW = 5 , cv::IMWRITE_TIFF_COMPRESSION_OJPEG = 6 , cv::IMWRITE_TIFF_COMPRESSION_JPEG = 7 , cv::IMWRITE_TIFF_COMPRESSION_T85 = 9 , cv::IMWRITE_TIFF_COMPRESSION_T43 = 10 , cv::IMWRITE_TIFF_COMPRESSION_NEXT = 32766 , cv::IMWRITE_TIFF_COMPRESSION_CCITTRLEW = 32771 , cv::IMWRITE_TIFF_COMPRESSION_PACKBITS = 32773 , cv::IMWRITE_TIFF_COMPRESSION_THUNDERSCAN = 32809 , cv::IMWRITE_TIFF_COMPRESSION_IT8CTPAD = 32895 , cv::IMWRITE_TIFF_COMPRESSION_IT8LW = 32896 , cv::IMWRITE_TIFF_COMPRESSION_IT8MP = 32897 , cv::IMWRITE_TIFF_COMPRESSION_IT8BL = 32898 , cv::IMWRITE_TIFF_COMPRESSION_PIXARFILM = 32908 , cv::IMWRITE_TIFF_COMPRESSION_PIXARLOG = 32909 , cv::IMWRITE_TIFF_COMPRESSION_DEFLATE = 32946 , cv::IMWRITE_TIFF_COMPRESSION_ADOBE_DEFLATE = 8 , cv::IMWRITE_TIFF_COMPRESSION_DCS = 32947 , cv::IMWRITE_TIFF_COMPRESSION_JBIG = 34661 , cv::IMWRITE_TIFF_COMPRESSION_SGILOG = 34676 , cv::IMWRITE_TIFF_COMPRESSION_SGILOG24 = 34677 , cv::IMWRITE_TIFF_COMPRESSION_JP2000 = 34712 , cv::IMWRITE_TIFF_COMPRESSION_LERC = 34887 , cv::IMWRITE_TIFF_COMPRESSION_LZMA = 34925 , cv::IMWRITE_TIFF_COMPRESSION_ZSTD = 50000 , cv::IMWRITE_TIFF_COMPRESSION_WEBP = 50001 , cv::IMWRITE_TIFF_COMPRESSION_JXL = 50002 }  enum cv::ImwriteTiffPredictorFlags { cv::IMWRITE_TIFF_PREDICTOR_NONE = 1 , cv::IMWRITE_TIFF_PREDICTOR_HORIZONTAL = 2 , cv::IMWRITE_TIFF_PREDICTOR_FLOATINGPOINT = 3 } "
imgcodecs,STL namespace.
imgcodecs,Template class specifying a continuous subsequence (slice) of a sequence.
imgcodecs,"The class is used to specify a row or a column span in a matrix ( Mat ) and for many other purposes. Range(a,b) is basically the same as a:b in Matlab or a..b in Python. As in Python, start is an inclusive left boundary of the range and end is an exclusive right boundary of the range. Such a half-opened interval is usually denoted as \([start,end)\) ."
imgcodecs,"The static method Range::all() returns a special variable that means ""the whole sequence"" or ""the whole range"", just like "" : "" in Matlab or "" ... "" in Python. All the methods and functions in OpenCV that take Range support this special Range::all() value. But, of course, in case of your own custom processing, you will probably have to check and handle it explicitly:"
imgcodecs,XML/YAML/JSON file storage class that encapsulates all the information necessary for writing or reading data to/from a file.
imgcodecs,To read multi-page images on demand.
imgcodecs,The ImageCollection class provides iterator API to read multi-page images on demand. Create iterator to the collection of the images and iterate over the collection. Decode the necessary page with operator*.
imgcodecs,"The performance of page decoding is O(1) if collection is increment sequentially. If the user wants to access random page, then the time Complexity is O(n) because the collection has to be reinitialized every time in order to go to the correct page. However, the intermediate pages are not decoded during the process, so typically it's quite fast. This is required because multi-page codecs does not support going backwards. After decoding the one page, it is stored inside the collection cache. Hence, trying to get Mat object from already decoded page is O(1). If you need memory, you can use .releaseCache() method to release cached index. The space complexity is O(n) if all pages are decoded into memory. The user is able to decode and release images on demand."
imgcodecs,Namespaces namespace cv  namespace cv::details  namespace cv::Error  namespace cv::instr  namespace cv::utils::fs 
imgcodecs,"Functions void CGImageToMat (const CGImageRef image, cv::Mat &m, bool alphaExist=false)  CGImageRef MatToCGImage (const cv::Mat &image) CF_RETURNS_RETAINED  NSImage * MatToNSImage (const cv::Mat &image)  void NSImageToMat (const NSImage *image, cv::Mat &m, bool alphaExist=false) "
imgcodecs,Namespaces namespace cv::traits 
imgproc,Classes class cv::segmentation::IntelligentScissorsMB  Intelligent Scissors image segmentation. More... 
imgproc,"The human perception isn't built for observing fine changes in grayscale images. Human eyes are more sensitive to observing changes between colors, so you often need to recolor your grayscale images to get a clue about them. OpenCV now comes with various colormaps to enhance the visualization in your computer vision application."
imgproc,"In OpenCV you only need applyColorMap to apply a colormap on a given image. The following sample code reads the path to an image from command line, applies a Jet colormap on it and shows the result:"
imgproc,"Enumerations enum cv::ColormapTypes { cv::COLORMAP_AUTUMN = 0 , cv::COLORMAP_BONE = 1 , cv::COLORMAP_JET = 2 , cv::COLORMAP_WINTER = 3 , cv::COLORMAP_RAINBOW = 4 , cv::COLORMAP_OCEAN = 5 , cv::COLORMAP_SUMMER = 6 , cv::COLORMAP_SPRING = 7 , cv::COLORMAP_COOL = 8 , cv::COLORMAP_HSV = 9 , cv::COLORMAP_PINK = 10 , cv::COLORMAP_HOT = 11 , cv::COLORMAP_PARULA = 12 , cv::COLORMAP_MAGMA = 13 , cv::COLORMAP_INFERNO = 14 , cv::COLORMAP_PLASMA = 15 , cv::COLORMAP_VIRIDIS = 16 , cv::COLORMAP_CIVIDIS = 17 , cv::COLORMAP_TWILIGHT = 18 , cv::COLORMAP_TWILIGHT_SHIFTED = 19 , cv::COLORMAP_TURBO = 20 , cv::COLORMAP_DEEPGREEN = 21 }  GNU Octave/MATLAB equivalent colormaps. More... "
imgproc,Namespaces namespace cv::traits 
imgproc,"Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as Mat's). It means that for each pixel location \((x,y)\) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location \((x,y)\). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one."
imgproc,"Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian \(3 \times 3\) filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (""replicated border"" extrapolation method), or assume that all the non-existing pixels are zeros (""constant border"" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see BorderTypes"
imgproc,"Drawing functions work with matrices/images of arbitrary depth. The boundaries of the shapes can be rendered with antialiasing (implemented only for 8-bit images for now). All the functions include the parameter color that uses an RGB value (that may be constructed with the Scalar constructor ) for color images and brightness for grayscale images. For color images, the channel ordering is normally Blue, Green, Red. This is what imshow, imread, and imwrite expect. So, if you form a color using the Scalar constructor, it should look like:"
imgproc,"\[\texttt{Scalar} (blue \_ component, green \_ component, red \_ component[, alpha \_ component])\]"
imgproc,"If you are using your own image rendering and I/O functions, you can use any channel ordering. The drawing functions process each channel independently and do not depend on the channel order or even on the used color space. The whole image can be converted from BGR to RGB or to a different color space using cvtColor ."
imgproc,"If a drawn figure is partially or completely outside the image, the drawing functions clip it. Also, many drawing functions can handle pixel coordinates specified with sub-pixel accuracy. This means that the coordinates can be passed as fixed-point numbers encoded as integers. The number of fractional bits is specified by the shift parameter and the real point coordinates are calculated as \(\texttt{Point}(x,y)\rightarrow\texttt{Point2f}(x*2^{-shift},y*2^{-shift})\) . This feature is especially effective when rendering antialiased shapes."
imgproc,Classes class cv::LineIterator  Class for iterating over all pixels on a raster line segment. More... 
imgproc,Classes class cv::CLAHE  Base class for Contrast Limited Adaptive Histogram Equalization. More... 
imgproc,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
imgproc,"This module offers a comprehensive suite of image processing functions, enabling tasks such as those listed above."
imgproc,"Functions void cv::accumulate (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds an image to the accumulator image.  void cv::accumulateProduct (InputArray src1, InputArray src2, InputOutputArray dst, InputArray mask=noArray())  Adds the per-element product of two input images to the accumulator image.  void cv::accumulateSquare (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds the square of a source image to the accumulator image.  void cv::accumulateWeighted (InputArray src, InputOutputArray dst, double alpha, InputArray mask=noArray())  Updates a running average.  void cv::createHanningWindow (OutputArray dst, Size winSize, int type)  This function computes a Hanning window coefficients in two dimensions.  void cv::divSpectrums (InputArray a, InputArray b, OutputArray c, int flags, bool conjB=false)  Performs the per-element division of the first Fourier spectrum by the second Fourier spectrum.  Point2d cv::phaseCorrelate (InputArray src1, InputArray src2, InputArray window=noArray(), double *response=0)  The function is used to detect translational shifts that occur between two images. "
imgproc,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
imgproc,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
imgproc,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
imgproc,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
imgproc,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
imgproc,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
imgproc,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
imgproc,Classes class cv::LineSegmentDetector  Line segment detector class. More... 
imgproc,"The Subdiv2D class described in this section is used to perform various planar subdivision on a set of 2D points (represented as vector of Point2f). OpenCV subdivides a plane into triangles using the Delaunay's algorithm, which corresponds to the dual graph of the Voronoi diagram. In the figure below, the Delaunay's triangulation is marked with black lines and the Voronoi diagram with red lines."
imgproc,"The subdivisions can be used for the 3D piece-wise transformation of a plane, morphing, fast location of points on the plane, building special graphs (such as NNG,RNG), and so forth."
imgproc,Classes class cv::Subdiv2D 
imgproc,"Enumerations enum cv::TemplateMatchModes { cv::TM_SQDIFF = 0 , cv::TM_SQDIFF_NORMED = 1 , cv::TM_CCORR = 2 , cv::TM_CCORR_NORMED = 3 , cv::TM_CCOEFF = 4 , cv::TM_CCOEFF_NORMED = 5 }  type of the template matching operation More... "
ml,The class implements the random forest predictor.
ml,Artificial Neural Networks - Multi-Layer Perceptrons.
ml,"Unlike many other models in ML that are constructed and trained at once, in the MLP model these steps are separated. First, a network with the specified topology is created using the non-default constructor or the method ANN_MLP::create. All the weights are set to zeros. Then, the network is trained using a set of input and output vectors. The training procedure can be repeated more than once, that is, the weights can be adjusted based on the new training data."
ml,Additional flags for StatModel::train are available: ANN_MLP::TrainFlags.
ml,Support Vector Machines.
ml,Random Number Generator.
ml,"Random number generator. It encapsulates the state (currently, a 64-bit integer) and has methods to return scalar random values and to fill arrays with random values. Currently it supports uniform and Gaussian (normal) distributions. The generator uses Multiply-With-Carry algorithm, introduced by G. Marsaglia ( http://en.wikipedia.org/wiki/Multiply-with-carry ). Gaussian-distribution random numbers are generated using the Ziggurat algorithm ( http://en.wikipedia.org/wiki/Ziggurat_algorithm ), introduced by G. Marsaglia and W. W. Tsang."
ml,The class implements the Expectation Maximization algorithm.
ml,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
ml,Bayes classifier for normally distributed data.
ml,Boosted tree classifier derived from DTrees.
ml,The structure represents the logarithmic grid range of statmodel parameters.
ml,"It is used for optimizing statmodel accuracy by varying model parameters, the accuracy estimate being computed by cross-validation."
ml,Implements Logistic Regression classifier.
ml,The class represents a single decision tree or a collection of decision trees.
ml,"The current public interface of the class allows user to train only a single decision tree, however the class is capable of storing multiple decision trees and using them for prediction (by summing responses or using a voting schemes), and the derived from DTrees classes (such as RTrees and Boost) use this capability to implement decision tree ensembles."
ml,Base class for statistical models in OpenCV ML.
ml,Class encapsulating training data.
ml,"Please note that the class only specifies the interface of training data, but not implementation. All the statistical model classes in ml module accepts Ptr<TrainData> as parameter. In other words, you can create your own class derived from TrainData and pass smart pointer to the instance of this class into StatModel::train."
ml,Namespaces namespace cv::traits 
ml,This class declares example interface for system state used in simulated annealing optimization algorithm.
ml,The class implements K-Nearest Neighbors model.
ml,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
ml,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
ml,See detailed overview here: Machine Learning Overview.
ml,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
ml,Stochastic Gradient Descent SVM classifier.
ml,"SVMSGD provides a fast and easy-to-use implementation of the SVM classifier using the Stochastic Gradient Descent approach, as presented in [35]."
ml,The classifier has following parameters:
ml,"model type, margin type, margin regularization ( \(\lambda\)), initial step size ( \(\gamma_0\)), step decreasing power ( \(c\)), and termination criteria."
ml,The model type may have one of the following values: SGD and ASGD.
ml,"SGD is the classic version of SVMSGD classifier: every next step is calculated by the formula \[w_{t+1} = w_t - \gamma(t) \frac{dQ_i}{dw} |_{w = w_t}\] where \(w_t\) is the weights vector for decision function at step \(t\), \(\gamma(t)\) is the step size of model parameters at the iteration \(t\), it is decreased on each step by the formula \(\gamma(t) = \gamma_0 (1 + \lambda \gamma_0 t) ^ {-c}\) \(Q_i\) is the target functional from SVM task for sample with number \(i\), this sample is chosen stochastically on each step of the algorithm. \(w_t\) is the weights vector for decision function at step \(t\), \(\gamma(t)\) is the step size of model parameters at the iteration \(t\), it is decreased on each step by the formula \(\gamma(t) = \gamma_0 (1 + \lambda \gamma_0 t) ^ {-c}\) \(Q_i\) is the target functional from SVM task for sample with number \(i\), this sample is chosen stochastically on each step of the algorithm. ASGD is Average Stochastic Gradient Descent SVM Classifier. ASGD classifier averages weights vector on each step of algorithm by the formula \(\widehat{w}_{t+1} = \frac{t}{1+t}\widehat{w}_{t} + \frac{1}{1+t}w_{t+1}\)"
ml,\[w_{t+1} = w_t - \gamma(t) \frac{dQ_i}{dw} |_{w = w_t}\]
ml,"\(w_t\) is the weights vector for decision function at step \(t\), \(\gamma(t)\) is the step size of model parameters at the iteration \(t\), it is decreased on each step by the formula \(\gamma(t) = \gamma_0 (1 + \lambda \gamma_0 t) ^ {-c}\) \(Q_i\) is the target functional from SVM task for sample with number \(i\), this sample is chosen stochastically on each step of the algorithm."
ml,The recommended model type is ASGD (following [35]).
ml,The margin type may have one of the following values: SOFT_MARGIN or HARD_MARGIN.
ml,"You should use HARD_MARGIN type, if you have linearly separable sets. You should use SOFT_MARGIN type, if you have non-linearly separable sets or sets with outliers. In the general case (if you know nothing about linear separability of your sets), use SOFT_MARGIN."
ml,The other parameters may be described as follows:
ml,"Margin regularization parameter is responsible for weights decreasing at each step and for the strength of restrictions on outliers (the less the parameter, the less probability that an outlier will be ignored). Recommended value for SGD model is 0.0001, for ASGD model is 0.00001. Initial step size parameter is the initial value for the step size \(\gamma(t)\). You will have to find the best initial step for your problem. Step decreasing power is the power parameter for \(\gamma(t)\) decreasing by the formula, mentioned above. Recommended value for SGD model is 1, for ASGD model is 0.75. Termination criteria can be TermCriteria::COUNT, TermCriteria::EPS or TermCriteria::COUNT + TermCriteria::EPS. You will have to find the best termination criteria for your problem."
ml,"Note that the parameters margin regularization, initial step size, and step decreasing power should be positive."
ml,To use SVMSGD algorithm do as follows:
ml,"first, create the SVMSGD object. The algorithm will set optimal parameters by default, but you can set your own parameters via functions setSvmsgdType(), setMarginType(), setMarginRegularization(), setInitialStepSize(), and setStepDecreasingPower(). then the SVM model can be trained using the train features and the correspondent labels by the method train(). after that, the label of a new feature vector can be predicted using the method predict()."
objdetect,Classes struct cv::DetectionROI  struct for detection region of interest (ROI) More...  struct cv::HOGDescriptor  Implementation of HOG (Histogram of Oriented Gradients) descriptor and object detector. More... 
objdetect,Classes class cv::barcode::BarcodeDetector 
objdetect,Check the corresponding tutorial for more details.
objdetect,Classes class cv::FaceDetectorYN  DNN-based face detector. More...  class cv::FaceRecognizerSF  DNN-based face recognizer. More... 
objdetect,The object detector described below has been initially proposed by Paul Viola [285] and improved by Rainer Lienhart [168] .
objdetect,"First, a classifier (namely a cascade of boosted classifiers working with haar-like features) is trained with a few hundred sample views of a particular object (i.e., a face or a car), called positive examples, that are scaled to the same size (say, 20x20), and negative examples - arbitrary images of the same size."
objdetect,"After a classifier is trained, it can be applied to a region of interest (of the same size as used during the training) in an input image. The classifier outputs a ""1"" if the region is likely to show the object (i.e., face/car), and ""0"" otherwise. To search for the object in the whole image one can move the search window across the image and check every location using the classifier. The classifier is designed so that it can be easily ""resized"" in order to be able to find the objects of interest at different sizes, which is more efficient than resizing the image itself. So, to find an object of an unknown size in the image the scan procedure should be done several times at different scales."
objdetect,"The word ""cascade"" in the classifier name means that the resultant classifier consists of several simpler classifiers (stages) that are applied subsequently to a region of interest until at some stage the candidate is rejected or all the stages are passed. The word ""boosted"" means that the classifiers at every stage of the cascade are complex themselves and they are built out of basic classifiers using one of four different boosting techniques (weighted voting). Currently Discrete Adaboost, Real Adaboost, Gentle Adaboost and Logitboost are supported. The basic classifiers are decision-tree classifiers with at least 2 leaves. Haar-like features are the input to the basic classifiers, and are calculated as described below. The current algorithm uses the following Haar-like features:"
objdetect,"The feature used in a particular classifier is specified by its shape (1a, 2b etc.), position within the region of interest and the scale (this scale is not the same as the scale used at the detection stage, though these two scales are multiplied). For example, in the case of the third line feature (2c) the response is calculated as the difference between the sum of image pixels under the rectangle covering the whole feature (including the two white stripes and the black stripe in the middle) and the sum of the image pixels under the black stripe multiplied by 3 in order to compensate for the differences in the size of areas. The sums of pixel values over a rectangular regions are calculated rapidly using integral images (see below and the integral description)."
objdetect,Check the corresponding tutorial for more details.
objdetect,The following reference is for the detection part only. There is a separate application called opencv_traincascade that can train a cascade of boosted classifiers from a set of samples.
objdetect,Classes class cv::BaseCascadeClassifier  class cv::CascadeClassifier  Cascade classifier class for object detection. More...  struct cv::DefaultDeleter< CvHaarClassifierCascade >  class cv::DetectionBasedTracker 
objdetect,"ArUco Marker Detection Square fiducial markers (also known as Augmented Reality Markers) are useful for easy, fast and robust camera pose estimation."
objdetect,"The main functionality of ArucoDetector class is detection of markers in an image. If the markers are grouped as a board, then you can try to recover the missing markers with ArucoDetector::refineDetectedMarkers(). ArUco markers can also be used for advanced chessboard corner finding. To do this, group the markers in the CharucoBoard and find the corners of the chessboard with the CharucoDetector::detectBoard()."
objdetect,The implementation is based on the ArUco Library by R. Muoz-Salinas and S. Garrido-Jurado [99].
objdetect,Markers can also be detected based on the AprilTag 2 [292] fiducial detection method.
objdetect,Classes class cv::aruco::ArucoDetector  The main functionality of ArucoDetector class is detection of markers in an image with detectMarkers() method. More...  class cv::aruco::Board  Board of ArUco markers. More...  class cv::aruco::CharucoBoard  ChArUco board is a planar chessboard where the markers are placed inside the white squares of a chessboard. More...  class cv::aruco::CharucoDetector  struct cv::aruco::CharucoParameters  struct cv::aruco::DetectorParameters  struct DetectorParameters is used by ArucoDetector More...  class cv::aruco::Dictionary  Dictionary is a set of unique ArUco markers of the same size. More...  class cv::aruco::GridBoard  Planar board with grid arrangement of markers. More...  struct cv::aruco::RefineParameters  struct RefineParameters is used by ArucoDetector More... 
objdetect,"Classes class cv::GraphicalCodeDetector  class cv::SimilarRects  This class is used for grouping object candidates detected by Cascade Classifier, HOG etc. More... "
objdetect,Classes class cv::QRCodeDetector  class cv::QRCodeDetectorAruco  class cv::QRCodeEncoder 
photo,This module includes photo processing algorithms
photo,"Functions void cv::denoise_TVL1 (const std::vector< Mat > &observations, Mat &result, double lambda=1.0, int niters=30)  Primal-dual algorithm is an algorithm for solving special types of variational problems (that is, finding a function to minimize some functional). As the image denoising, in particular, may be seen as the variational problem, primal-dual algorithm then can be used to perform denoising and this is exactly what is implemented.  void cv::cuda::fastNlMeansDenoising (const GpuMat &src, GpuMat &dst, float h, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  void cv::fastNlMeansDenoising (InputArray src, OutputArray dst, const std::vector< float > &h, int templateWindowSize=7, int searchWindowSize=21, int normType=NORM_L2)  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/ with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::cuda::fastNlMeansDenoising (InputArray src, OutputArray dst, float h, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::fastNlMeansDenoising (InputArray src, OutputArray dst, float h=3, int templateWindowSize=7, int searchWindowSize=21)  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/ with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::cuda::fastNlMeansDenoisingColored (const GpuMat &src, GpuMat &dst, float h_luminance, float photo_render, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  void cv::fastNlMeansDenoisingColored (InputArray src, OutputArray dst, float h=3, float hColor=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoising function for colored images.  void cv::cuda::fastNlMeansDenoisingColored (InputArray src, OutputArray dst, float h_luminance, float photo_render, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  Modification of fastNlMeansDenoising function for colored images.  void cv::fastNlMeansDenoisingColoredMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, float h=3, float hColor=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoisingMulti function for colored images sequences.  void cv::fastNlMeansDenoisingMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, const std::vector< float > &h, int templateWindowSize=7, int searchWindowSize=21, int normType=NORM_L2)  Modification of fastNlMeansDenoising function for images sequence where consecutive images have been captured in small period of time. For example video. This version of the function is for grayscale images or for manual manipulation with colorspaces. See [44] for more details (open access here).  void cv::fastNlMeansDenoisingMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, float h=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoising function for images sequence where consecutive images have been captured in small period of time. For example video. This version of the function is for grayscale images or for manual manipulation with colorspaces. See [44] for more details (open access here).  void cv::cuda::nonLocalMeans (const GpuMat &src, GpuMat &dst, float h, int search_window=21, int block_size=7, int borderMode=BORDER_DEFAULT, Stream &stream=Stream::Null())  void cv::cuda::nonLocalMeans (InputArray src, OutputArray dst, float h, int search_window=21, int block_size=7, int borderMode=BORDER_DEFAULT, Stream &stream=Stream::Null())  Performs pure non local means denoising without any simplification, and thus it is not fast. "
photo,Useful links:
photo,http://www.cse.cuhk.edu.hk/leojia/projects/color2gray/index.html
photo,"Functions void cv::decolor (InputArray src, OutputArray grayscale, OutputArray color_boost)  Transforms a color image to a grayscale image. It is a basic tool in digital printing, stylized black-and-white photograph rendering, and in many single channel image processing applications [176] . "
photo,"This section describes high dynamic range imaging algorithms namely tonemapping, exposure alignment, camera calibration with multiple exposures and exposure fusion."
photo,"Classes class cv::AlignExposures  The base class for algorithms that align images of the same scene with different exposures. More...  class cv::AlignMTB  This algorithm converts images to median threshold bitmaps (1 for pixels brighter than median luminance and 0 otherwise) and than aligns the resulting bitmaps using bit operations. More...  class cv::CalibrateCRF  The base class for camera response calibration algorithms. More...  class cv::CalibrateDebevec  Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. Objective function is constructed using pixel values on the same position in all images, extra term is added to make the result smoother. More...  class cv::CalibrateRobertson  Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. This algorithm uses all image pixels. More...  class cv::MergeDebevec  The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response. More...  class cv::MergeExposures  The base class algorithms that can merge exposure sequence to a single image. More...  class cv::MergeMertens  Pixels are weighted using contrast, saturation and well-exposedness measures, than images are combined using laplacian pyramids. More...  class cv::MergeRobertson  The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response. More...  class cv::Tonemap  Base class for tonemapping algorithms - tools that are used to map HDR image to 8-bit range. More...  class cv::TonemapDrago  Adaptive logarithmic mapping is a fast global tonemapping algorithm that scales the image in logarithmic domain. More...  class cv::TonemapMantiuk  This algorithm transforms image to contrast using gradients on all levels of gaussian pyramid, transforms contrast values to HVS response and scales the response. After this the image is reconstructed from new contrast values. More...  class cv::TonemapReinhard  This is a global tonemapping operator that models human visual system. More... "
photo,the inpainting algorithm
photo,"Enumerations enum { cv::INPAINT_NS = 0 , cv::INPAINT_TELEA = 1 } "
photo,Useful links:
photo,https://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp
photo,"Enumerations enum { cv::NORMAL_CLONE = 1 , cv::MIXED_CLONE = 2 , cv::MONOCHROME_TRANSFER = 3 }  seamlessClone algorithm flags More... "
photo,Useful links:
photo,http://www.inf.ufrgs.br/~eslgastal/DomainTransform
photo,https://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/
photo,"Enumerations enum { cv::RECURS_FILTER = 1 , cv::NORMCONV_FILTER = 2 }  Edge preserving filters. More... "
stitching,Affine transformation based estimator.
stitching,This estimator uses pairwise transformations estimated by matcher to estimate final transformation for each camera.
stitching,Classes class cv::detail::AffineWarper  Affine warper that uses rotations and translations. More...  class cv::AffineWarper  Affine warper factory class. More...  struct cv::detail::CompressedRectilinearPortraitProjector  class cv::CompressedRectilinearPortraitWarper  class cv::detail::CompressedRectilinearPortraitWarper  struct cv::detail::CompressedRectilinearProjector  class cv::CompressedRectilinearWarper  class cv::detail::CompressedRectilinearWarper  struct cv::detail::CylindricalPortraitProjector  class cv::detail::CylindricalPortraitWarper  struct cv::detail::CylindricalProjector  class cv::detail::CylindricalWarper  Warper that maps an image onto the x*x + z*z = 1 cylinder. More...  class cv::CylindricalWarper  Cylindrical warper factory class. More...  class cv::detail::CylindricalWarperGpu  struct cv::detail::FisheyeProjector  class cv::detail::FisheyeWarper  class cv::FisheyeWarper  struct cv::detail::MercatorProjector  class cv::detail::MercatorWarper  class cv::MercatorWarper  struct cv::detail::PaniniPortraitProjector  class cv::PaniniPortraitWarper  class cv::detail::PaniniPortraitWarper  struct cv::detail::PaniniProjector  class cv::PaniniWarper  class cv::detail::PaniniWarper  struct cv::detail::PlanePortraitProjector  class cv::detail::PlanePortraitWarper  struct cv::detail::PlaneProjector  class cv::PlaneWarper  Plane warper factory class. More...  class cv::detail::PlaneWarper  Warper that maps an image onto the z = 1 plane. More...  class cv::detail::PlaneWarperGpu  struct cv::detail::ProjectorBase  Base class for warping logic implementation. More...  class cv::detail::RotationWarper  Rotation-only model image warper interface. More...  class cv::detail::RotationWarperBase< P >  Base class for rotation-based warper using a detail::ProjectorBase_ derived class. More...  struct cv::detail::SphericalPortraitProjector  class cv::detail::SphericalPortraitWarper  struct cv::detail::SphericalProjector  class cv::detail::SphericalWarper  Warper that maps an image onto the unit sphere located at the origin. More...  class cv::SphericalWarper  Spherical warper factory class. More...  class cv::detail::SphericalWarperGpu  struct cv::detail::StereographicProjector  class cv::StereographicWarper  class cv::detail::StereographicWarper  struct cv::detail::TransverseMercatorProjector  class cv::detail::TransverseMercatorWarper  class cv::TransverseMercatorWarper  class cv::WarperCreator  Image warper factories base class. More... 
stitching,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
stitching,The implemented stitching pipeline is very similar to the one proposed in [41] .
stitching,High level image stitcher.
stitching,"It's possible to use this class without being aware of the entire stitching pipeline. However, to be able to achieve higher stitching stability and quality of the final images at least being familiar with the theory is recommended."
stitching,A basic example on image stitching can be found at opencv_source_code/samples/cpp/stitching.cpp A basic example on image stitching in Python can be found at opencv_source_code/samples/python/stitching.py A detailed example on image stitching can be found at opencv_source_code/samples/cpp/stitching_detailed.cpp
stitching,Features matcher similar to cv::detail::BestOf2NearestMatcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf.
stitching,Unlike cv::detail::BestOf2NearestMatcher this matcher uses affine transformation (affine transformation estimate will be placed in matches_info).
stitching,Describes camera parameters.
stitching,Features matcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf.
stitching,Bundle adjuster that expects affine transformation with 4 DOF represented in homogeneous coordinates in R for each camera param. Implements camera parameters refinement algorithm which minimizes sum of the reprojection error squares.
stitching,It estimates all transformation parameters. Refinement mask is ignored.
stitching,"Functions bool cv::detail::calibrateRotatingCamera (const std::vector< Mat > &Hs, Mat &K)  void cv::detail::estimateFocal (const std::vector< ImageFeatures > &features, const std::vector< MatchesInfo > &pairwise_matches, std::vector< double > &focals)  Estimates focal lengths for each given camera.  void cv::detail::focalsFromHomography (const Mat &H, double &f0, double &f1, bool &f0_ok, bool &f1_ok)  Tries to estimate focal lengths from the given homography under the assumption that the camera undergoes rotations around its centre only. "
stitching,Affine warper factory class.
stitching,Bundle adjuster that expects affine transformation represented in homogeneous coordinates in R for each camera param. Implements camera parameters refinement algorithm which minimizes sum of the reprojection error squares.
stitching,It estimates all transformation parameters. Refinement mask is ignored.
stitching,Classes class cv::detail::AffineBasedEstimator  Affine transformation based estimator. More...  class cv::detail::BundleAdjusterAffine  Bundle adjuster that expects affine transformation represented in homogeneous coordinates in R for each camera param. Implements camera parameters refinement algorithm which minimizes sum of the reprojection error squares. More...  class cv::detail::BundleAdjusterAffinePartial  Bundle adjuster that expects affine transformation with 4 DOF represented in homogeneous coordinates in R for each camera param. Implements camera parameters refinement algorithm which minimizes sum of the reprojection error squares. More...  class cv::detail::BundleAdjusterBase  Base class for all camera parameters refinement methods. More...  class cv::detail::BundleAdjusterRay  Implementation of the camera parameters refinement algorithm which minimizes sum of the distances between the rays passing through the camera center and a feature. : More...  class cv::detail::BundleAdjusterReproj  Implementation of the camera parameters refinement algorithm which minimizes sum of the reprojection error squares. More...  class cv::detail::Estimator  Rotation estimator base class. More...  class cv::detail::HomographyBasedEstimator  Homography based rotation estimator. More...  class cv::detail::NoBundleAdjuster  Stub bundle adjuster that does nothing. More... 
stitching,Classes class cv::detail::DpSeamFinder  class cv::detail::GraphCutSeamFinder  Minimum graph cut-based seam estimator. See details in [153] . More...  class cv::detail::GraphCutSeamFinderBase  Base class for all minimum graph-cut-based seam estimators. More...  class cv::detail::NoSeamFinder  Stub seam estimator which does nothing. More...  class cv::detail::PairwiseSeamFinder  Base class for all pairwise seam estimators. More...  class cv::detail::SeamFinder  Base class for a seam estimator. More...  class cv::detail::VoronoiSeamFinder  Voronoi diagram-based seam estimator. More... 
stitching,Classes class cv::detail::Blender  Base class for all blenders. More...  class cv::detail::FeatherBlender  Simple blender which mixes images at its borders. More...  class cv::detail::MultiBandBlender  Blender which uses multi-band blending algorithm (see [45]). More... 
stitching,Implementation of the camera parameters refinement algorithm which minimizes sum of the distances between the rays passing through the camera center and a feature. :
stitching,It can estimate focal length. It ignores the refinement mask for now.
stitching,Homography based rotation estimator.
stitching,Implementation of the camera parameters refinement algorithm which minimizes sum of the reprojection error squares.
stitching,"It can estimate focal length, aspect ratio, principal point. You can affect only on them via the refinement mask."
stitching,Namespaces namespace cv::traits 
stitching,"Classes class cv::detail::BlocksChannelsCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image block on each channel. More...  class cv::detail::BlocksCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image blocks. More...  class cv::detail::BlocksGainCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image block intensities, see [279] for details. More...  class cv::detail::ChannelsCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image intensities on each channel independently. More...  class cv::detail::ExposureCompensator  Base class for all exposure compensators. More...  class cv::detail::GainCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image intensities, see [41] and [305] for details. More...  class cv::detail::NoExposureCompensator  Stub exposure compensator which does nothing. More... "
stitching,Classes class cv::detail::AffineBestOf2NearestMatcher  Features matcher similar to cv::detail::BestOf2NearestMatcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf. More...  class cv::detail::BestOf2NearestMatcher  Features matcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf. More...  class cv::detail::BestOf2NearestRangeMatcher  class cv::detail::FeaturesMatcher  Feature matchers base class. More...  struct cv::detail::ImageFeatures  Structure containing image keypoints and descriptors. More...  struct cv::detail::MatchesInfo  Structure containing information about matches between two images. More... 
video,Classes class cv::DenseOpticalFlow  class cv::DISOpticalFlow  DIS optical flow algorithm. More...  class cv::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::KalmanFilter  Kalman filter class. More...  class cv::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More...  class cv::Tracker  Base abstract class for the long-term tracker. More...  class cv::TrackerDaSiamRPN  class cv::TrackerGOTURN  the GOTURN (Generic Object Tracking Using Regression Networks) tracker More...  class cv::TrackerMIL  The MIL algorithm trains a classifier in an online manner to separate the object from the background. More...  class cv::TrackerNano  the Nano tracker is a super lightweight dnn-based general object tracking. More...  class cv::TrackerVit  the VIT tracker is a super lightweight dnn-based general object tracking. More...  class cv::VariationalRefinement  Variational optical flow refinement. More... 
video,Classes class cv::BackgroundSubtractor  Base class for background/foreground segmentation. : More...  class cv::BackgroundSubtractorKNN  K-nearest neighbours - based Background/Foreground Segmentation Algorithm. More...  class cv::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
videoio,"Enumerations enum cv::VideoCaptureAPIs { cv::CAP_ANY = 0 , cv::CAP_VFW = 200 , cv::CAP_V4L = 200 , cv::CAP_V4L2 = CAP_V4L , cv::CAP_FIREWIRE = 300 , cv::CAP_FIREWARE = CAP_FIREWIRE , cv::CAP_IEEE1394 = CAP_FIREWIRE , cv::CAP_DC1394 = CAP_FIREWIRE , cv::CAP_CMU1394 = CAP_FIREWIRE , cv::CAP_QT = 500 , cv::CAP_UNICAP = 600 , cv::CAP_DSHOW = 700 , cv::CAP_PVAPI = 800 , cv::CAP_OPENNI = 900 , cv::CAP_OPENNI_ASUS = 910 , cv::CAP_ANDROID = 1000 , cv::CAP_XIAPI = 1100 , cv::CAP_AVFOUNDATION = 1200 , cv::CAP_GIGANETIX = 1300 , cv::CAP_MSMF = 1400 , cv::CAP_WINRT = 1410 , cv::CAP_INTELPERC = 1500 , cv::CAP_REALSENSE = 1500 , cv::CAP_OPENNI2 = 1600 , cv::CAP_OPENNI2_ASUS = 1610 , cv::CAP_OPENNI2_ASTRA = 1620 , cv::CAP_GPHOTO2 = 1700 , cv::CAP_GSTREAMER = 1800 , cv::CAP_FFMPEG = 1900 , cv::CAP_IMAGES = 2000 , cv::CAP_ARAVIS = 2100 , cv::CAP_OPENCV_MJPEG = 2200 , cv::CAP_INTEL_MFX = 2300 , cv::CAP_XINE = 2400 , cv::CAP_UEYE = 2500 , cv::CAP_OBSENSOR = 2600 }  cv::VideoCapture API backends identifier. More...  enum cv::VideoCaptureProperties { cv::CAP_PROP_POS_MSEC =0 , cv::CAP_PROP_POS_FRAMES =1 , cv::CAP_PROP_POS_AVI_RATIO =2 , cv::CAP_PROP_FRAME_WIDTH =3 , cv::CAP_PROP_FRAME_HEIGHT =4 , cv::CAP_PROP_FPS =5 , cv::CAP_PROP_FOURCC =6 , cv::CAP_PROP_FRAME_COUNT =7 , cv::CAP_PROP_FORMAT =8 , cv::CAP_PROP_MODE =9 , cv::CAP_PROP_BRIGHTNESS =10 , cv::CAP_PROP_CONTRAST =11 , cv::CAP_PROP_SATURATION =12 , cv::CAP_PROP_HUE =13 , cv::CAP_PROP_GAIN =14 , cv::CAP_PROP_EXPOSURE =15 , cv::CAP_PROP_CONVERT_RGB =16 , cv::CAP_PROP_WHITE_BALANCE_BLUE_U =17 , cv::CAP_PROP_RECTIFICATION =18 , cv::CAP_PROP_MONOCHROME =19 , cv::CAP_PROP_SHARPNESS =20 , cv::CAP_PROP_AUTO_EXPOSURE =21 , cv::CAP_PROP_GAMMA =22 , cv::CAP_PROP_TEMPERATURE =23 , cv::CAP_PROP_TRIGGER =24 , cv::CAP_PROP_TRIGGER_DELAY =25 , cv::CAP_PROP_WHITE_BALANCE_RED_V =26 , cv::CAP_PROP_ZOOM =27 , cv::CAP_PROP_FOCUS =28 , cv::CAP_PROP_GUID =29 , cv::CAP_PROP_ISO_SPEED =30 , cv::CAP_PROP_BACKLIGHT =32 , cv::CAP_PROP_PAN =33 , cv::CAP_PROP_TILT =34 , cv::CAP_PROP_ROLL =35 , cv::CAP_PROP_IRIS =36 , cv::CAP_PROP_SETTINGS =37 , cv::CAP_PROP_BUFFERSIZE =38 , cv::CAP_PROP_AUTOFOCUS =39 , cv::CAP_PROP_SAR_NUM =40 , cv::CAP_PROP_SAR_DEN =41 , cv::CAP_PROP_BACKEND =42 , cv::CAP_PROP_CHANNEL =43 , cv::CAP_PROP_AUTO_WB =44 , cv::CAP_PROP_WB_TEMPERATURE =45 , cv::CAP_PROP_CODEC_PIXEL_FORMAT =46 , cv::CAP_PROP_BITRATE =47 , cv::CAP_PROP_ORIENTATION_META =48 , cv::CAP_PROP_ORIENTATION_AUTO =49 , cv::CAP_PROP_HW_ACCELERATION =50 , cv::CAP_PROP_HW_DEVICE =51 , cv::CAP_PROP_HW_ACCELERATION_USE_OPENCL =52 , cv::CAP_PROP_OPEN_TIMEOUT_MSEC =53 , cv::CAP_PROP_READ_TIMEOUT_MSEC =54 , cv::CAP_PROP_STREAM_OPEN_TIME_USEC =55 , cv::CAP_PROP_VIDEO_TOTAL_CHANNELS = 56 , cv::CAP_PROP_VIDEO_STREAM = 57 , cv::CAP_PROP_AUDIO_STREAM = 58 , cv::CAP_PROP_AUDIO_POS = 59 , cv::CAP_PROP_AUDIO_SHIFT_NSEC = 60 , cv::CAP_PROP_AUDIO_DATA_DEPTH = 61 , cv::CAP_PROP_AUDIO_SAMPLES_PER_SECOND = 62 , cv::CAP_PROP_AUDIO_BASE_INDEX = 63 , cv::CAP_PROP_AUDIO_TOTAL_CHANNELS = 64 , cv::CAP_PROP_AUDIO_TOTAL_STREAMS = 65 , cv::CAP_PROP_AUDIO_SYNCHRONIZE = 66 , cv::CAP_PROP_LRF_HAS_KEY_FRAME = 67 , cv::CAP_PROP_CODEC_EXTRADATA_INDEX = 68 , cv::CAP_PROP_FRAME_TYPE = 69 , cv::CAP_PROP_N_THREADS = 70 , cv::CAP_PROP_PTS = 71 , cv::CAP_PROP_DTS_DELAY = 72 }  cv::VideoCapture generic properties identifier. More...  enum cv::VideoWriterProperties { cv::VIDEOWRITER_PROP_QUALITY = 1 , cv::VIDEOWRITER_PROP_FRAMEBYTES = 2 , cv::VIDEOWRITER_PROP_NSTRIPES = 3 , cv::VIDEOWRITER_PROP_IS_COLOR = 4 , cv::VIDEOWRITER_PROP_DEPTH = 5 , cv::VIDEOWRITER_PROP_HW_ACCELERATION = 6 , cv::VIDEOWRITER_PROP_HW_DEVICE = 7 , cv::VIDEOWRITER_PROP_HW_ACCELERATION_USE_OPENCL = 8 , cv::VIDEOWRITER_PROP_RAW_VIDEO = 9 , cv::VIDEOWRITER_PROP_KEY_INTERVAL = 10 , cv::VIDEOWRITER_PROP_KEY_FLAG = 11 , cv::VIDEOWRITER_PROP_PTS = 12 , cv::VIDEOWRITER_PROP_DTS_DELAY = 13 }  cv::VideoWriter generic properties identifier. More... "
videoio,Classes class CvAbstractCamera  class CvPhotoCamera  protocol <CvPhotoCameraDelegate>  class CvVideoCamera  protocol <CvVideoCameraDelegate> 
videoio,"Class for video capturing from video files, image sequences or cameras."
videoio,The class provides C++ API for capturing video from cameras or for reading video files and image sequences.
videoio,Here is how the class can be used:
videoio,(C++) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/cpp/videocapture_starter.cpp (Python) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/python/video.py (Python) A multi threaded video processing sample can be found at OPENCV_SOURCE_CODE/samples/python/video_threaded.py (Python) VideoCapture sample showcasing some features of the Video4Linux2 backend OPENCV_SOURCE_CODE/samples/python/video_v4l2.py
videoio,"Hardware acceleration support enum cv::VideoAccelerationType { cv::VIDEO_ACCELERATION_NONE = 0 , cv::VIDEO_ACCELERATION_ANY = 1 , cv::VIDEO_ACCELERATION_D3D11 = 2 , cv::VIDEO_ACCELERATION_VAAPI = 3 , cv::VIDEO_ACCELERATION_MFX = 4 }  Video Acceleration type. More... "
videoio,"Enumerations enum { cv::OPEN_CAMERA = 300 , cv::CLOSE_CAMERA , cv::UPDATE_IMAGE_ELEMENT , cv::SHOW_TRACKBAR } "
videoio,Video writer class.
videoio,The class provides C++ API for writing video files or image sequences.
videoio,Read and write video or images sequence with OpenCV.
videoio,This section contains API description how to query/configure available Video I/O backends.
videoio,Runtime configuration options:
videoio,"enable debug mode: OPENCV_VIDEOIO_DEBUG=1 change backend priority: OPENCV_VIDEOIO_PRIORITY_<backend>=9999 disable backend: OPENCV_VIDEOIO_PRIORITY_<backend>=0 specify list of backends with high priority (>100000): OPENCV_VIDEOIO_PRIORITY_LIST=FFMPEG,GSTREAMER"
videoio,"Functions cv::String cv::videoio_registry::getBackendName (VideoCaptureAPIs api)  Returns backend API name or ""UnknownVideoAPI(xxx)"".  std::vector< VideoCaptureAPIs > cv::videoio_registry::getBackends ()  Returns list of all available backends.  std::string cv::videoio_registry::getCameraBackendPluginVersion (VideoCaptureAPIs api, int &version_ABI, int &version_API)  Returns description and ABI/API version of videoio plugin's camera interface.  std::vector< VideoCaptureAPIs > cv::videoio_registry::getCameraBackends ()  Returns list of available backends which works via cv::VideoCapture(int index)  std::string cv::videoio_registry::getStreamBackendPluginVersion (VideoCaptureAPIs api, int &version_ABI, int &version_API)  Returns description and ABI/API version of videoio plugin's stream capture interface.  std::vector< VideoCaptureAPIs > cv::videoio_registry::getStreamBackends ()  Returns list of available backends which works via cv::VideoCapture(filename)  std::string cv::videoio_registry::getWriterBackendPluginVersion (VideoCaptureAPIs api, int &version_ABI, int &version_API)  Returns description and ABI/API version of videoio plugin's writer interface.  std::vector< VideoCaptureAPIs > cv::videoio_registry::getWriterBackends ()  Returns list of available backends which works via cv::VideoWriter()  bool cv::videoio_registry::hasBackend (VideoCaptureAPIs api)  Returns true if backend is available.  bool cv::videoio_registry::isBackendBuiltIn (VideoCaptureAPIs api)  Returns true if backend is built in (false if backend is used as plugin) "
