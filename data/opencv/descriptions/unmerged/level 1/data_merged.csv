Module,Text
calib3d,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
calib3d,The distortion-free projective transformation given by a pinhole camera model is shown below.
calib3d,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
calib3d,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
calib3d,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
calib3d,\[p = A P_c.\]
calib3d,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
calib3d,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
calib3d,and thus
calib3d,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
calib3d,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
calib3d,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
calib3d,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
calib3d,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
calib3d,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
calib3d,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
calib3d,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
calib3d,and therefore
calib3d,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
calib3d,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
calib3d,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
calib3d,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
calib3d,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
calib3d,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
calib3d,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
calib3d,with
calib3d,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
calib3d,The following figure illustrates the pinhole camera model.
calib3d,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
calib3d,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
calib3d,where
calib3d,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
calib3d,with
calib3d,\[r^2 = x'^2 + y'^2\]
calib3d,and
calib3d,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
calib3d,if \(Z_c \ne 0\).
calib3d,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
calib3d,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
calib3d,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
calib3d,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
calib3d,where
calib3d,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
calib3d,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
calib3d,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
calib3d,In the functions below the coefficients are passed or returned as
calib3d,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
calib3d,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
calib3d,The functions below use the above model to do the following:
calib3d,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
calib3d,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
calib3d,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
calib3d,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
calib3d,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
calib3d,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
calib3d,if \(W \ne 0\).
calib3d,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
calib3d,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
calib3d,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
calib3d,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
calib3d,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
calib3d,Definitions: Let P be a point in 3D of coordinates X in the world reference frame (stored in the matrix X) The coordinate vector of P in the camera reference frame is:
calib3d,\[Xc = R X + T\]
calib3d,"where R is the rotation matrix corresponding to the rotation vector om: R = rodrigues(om); call x, y and z the 3 coordinates of Xc:"
calib3d,\[\begin{array}{l} x = Xc_1 \\ y = Xc_2 \\ z = Xc_3 \end{array} \]
calib3d,The pinhole projection coordinates of P is [a; b] where
calib3d,\[\begin{array}{l} a = x / z \ and \ b = y / z \\ r^2 = a^2 + b^2 \\ \theta = atan(r) \end{array} \]
calib3d,Fisheye distortion:
calib3d,\[\theta_d = \theta (1 + k_1 \theta^2 + k_2 \theta^4 + k_3 \theta^6 + k_4 \theta^8)\]
calib3d,The distorted point coordinates are [x'; y'] where
calib3d,\[\begin{array}{l} x' = (\theta_d / r) a \\ y' = (\theta_d / r) b \end{array} \]
calib3d,"Finally, conversion into pixel coordinates: The final pixel coordinates vector [u; v] where:"
calib3d,\[\begin{array}{l} u = f_x (x' + \alpha y') + c_x \\ v = f_y y' + c_y \end{array} \]
calib3d,Summary: Generic camera model [143] with perspective projection and without distortion correction
calib3d,Namespaces namespace cv::fisheye  The methods in this namespace use a so-called fisheye camera model. 
core,Classes class cv::cuda::CannyEdgeDetector  Base class for Canny Edge Detector. : More...  class cv::cuda::TemplateMatching  Base class for Template Matching. : More... 
core,"Classes struct cv::cudacodec::EncodeQp  class cv::cudacodec::EncoderCallback  Interface for encoder callbacks. More...  struct cv::cudacodec::EncoderParams  Different parameters for CUDA video encoder. More...  struct cv::cudacodec::FormatInfo  Struct providing information about video file format. : More...  class cv::cudacodec::RawVideoSource  Interface for video demultiplexing. : More...  class cv::cudacodec::VideoReader  Video reader interface, see createVideoReader(). More...  struct cv::cudacodec::VideoReaderInitParams  VideoReader initialization parameters. More...  class cv::cudacodec::VideoWriter  Video writer interface, see createVideoWriter(). More... "
core,Namespaces namespace cv::traits 
core,This module includes photo processing algorithms
core,This section describes 3D visualization window as well as classes and methods that are used to interact with it.
core,"3D visualization window (see Viz3d) is used to display widgets (see Widget), and it provides several methods to interact with scene and widgets."
core,"Classes class cv::viz::Camera  This class wraps intrinsic parameters of a camera. More...  class cv::viz::Color  This class represents color in BGR order. More...  class cv::viz::KeyboardEvent  This class represents a keyboard event. More...  class cv::viz::Mesh  This class wraps mesh attributes, and it can load a mesh from a ply file. : More...  class cv::viz::MouseEvent  This class represents a mouse event. More...  class cv::viz::Viz3d  The Viz3d class represents a 3D visualizer window. This class is implicitly shared. More... "
core,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
core,The implemented stitching pipeline is very similar to the one proposed in [41] .
core,Classes class cv::AffineTransformer  Wrapper class for the OpenCV Affine Transformation algorithm. : More...  class cv::ChiHistogramCostExtractor  An Chi based cost extraction. : More...  class cv::EMDHistogramCostExtractor  An EMD based cost extraction. : More...  class cv::EMDL1HistogramCostExtractor  An EMD-L1 based cost extraction. : More...  class cv::HausdorffDistanceExtractor  A simple Hausdorff distance measure between shapes defined by contours. More...  class cv::HistogramCostExtractor  Abstract base class for histogram cost algorithms. More...  class cv::NormHistogramCostExtractor  A norm based cost extraction. : More...  class cv::ShapeContextDistanceExtractor  Implementation of the Shape Context descriptor and matching algorithm. More...  class cv::ShapeDistanceExtractor  Abstract base class for shape distance algorithms. More...  class cv::ShapeTransformer  Abstract base class for shape transformation algorithms. More...  class cv::ThinPlateSplineShapeTransformer  Definition of the transformation. More... 
core,"Structured light is considered one of the most effective techniques to acquire 3D models. This technique is based on projecting a light pattern and capturing the illuminated scene from one or more points of view. Since the pattern is coded, correspondences between image points and points of the projected pattern can be quickly found and 3D information easily retrieved."
core,"One of the most commonly exploited coding strategies is based on trmatime-multiplexing. In this case, a set of patterns are successively projected onto the measuring surface. The codeword for a given pixel is usually formed by the sequence of illuminance values for that pixel across the projected patterns. Thus, the codification is called temporal because the bits of the codewords are multiplexed in time [233] ."
core,"In this module a time-multiplexing coding strategy based on Gray encoding is implemented following the (stereo) approach described in 3DUNDERWORLD algorithm [124] . For more details, see Structured Light tutorials."
core,"Classes class cv::structured_light::GrayCodePattern  Class implementing the Gray-code pattern, based on [124]. More...  class cv::structured_light::SinusoidalPattern  Class implementing Fourier transform profilometry (FTP) , phase-shifting profilometry (PSP) and Fourier-assisted phase-shifting profilometry (FAPS) based on [62]. More...  class cv::structured_light::StructuredLightPattern  Abstract base class for generating and decoding structured light patterns. More... "
core,ovis is a simplified rendering wrapper around ogre3d. The Ogre terminology is used in the API and Ogre Script is assumed to be used for advanced customization.
core,"Besides the API you see here, there are several environment variables that control the behavior of ovis. They are documented in createWindow."
core,This module includes signal processing algorithms.
core,"Functions void cv::signal::resampleSignal (InputArray inputSignal, OutputArray outSignal, const int inFreq, const int outFreq)  Signal resampling. "
core,ICP point-to-plane odometry algorithm
core,Classes class cv::linemod::ColorGradient  Modality that computes quantized gradient orientations from a color image. More...  class cv::rgbd::DepthCleaner  class cv::linemod::DepthNormal  Modality that computes quantized surface normals from a dense depth map. More...  class cv::linemod::Detector  Object detector using the LINE template matching algorithm with any set of modalities. More...  class cv::rgbd::FastICPOdometry  struct cv::linemod::Feature  Discriminant feature described by its location and label. More...  class cv::rgbd::ICPOdometry  struct cv::linemod::Match  Represents a successful template match. More...  class cv::linemod::Modality  Interface for modalities that plug into the LINE template matching representation. More...  class cv::rgbd::Odometry  struct cv::rgbd::OdometryFrame  class cv::linemod::QuantizedPyramid  Represents a modality operating over an image pyramid. More...  struct cv::rgbd::RgbdFrame  class cv::rgbd::RgbdICPOdometry  class cv::rgbd::RgbdNormals  class cv::rgbd::RgbdOdometry  class cv::rgbd::RgbdPlane  struct cv::linemod::Template 
core,"Classes class cv::bgsegm::BackgroundSubtractorCNT  Background subtraction based on counting. More...  class cv::bgsegm::BackgroundSubtractorGMG  Background Subtractor module based on the algorithm given in [106] . More...  class cv::bgsegm::BackgroundSubtractorGSOC  Implementation of the different yet better algorithm which is called GSOC, as it was implemented during GSOC and was not originated from any paper. More...  class cv::bgsegm::BackgroundSubtractorLSBP  Background Subtraction using Local SVD Binary Pattern. More details about the algorithm can be found at [115]. More...  class cv::bgsegm::BackgroundSubtractorLSBPDesc  This is for calculation of the LSBP descriptors. More...  class cv::bgsegm::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::bgsegm::SyntheticSequenceGenerator  Synthetic frame sequence generator for testing background subtraction algorithms. More... "
core,Dense optical flow algorithms compute motion for each point:
core,cv::optflow::calcOpticalFlowSF cv::optflow::createOptFlow_DeepFlow
core,Motion templates is alternative technique for detecting motion and computing its direction. See samples/motempl.py.
core,cv::motempl::updateMotionHistory cv::motempl::calcMotionGradient cv::motempl::calcGlobalOrientation cv::motempl::segmentMotion
core,"Functions reading and writing .flo files in ""Middlebury"" format, see: http://vision.middlebury.edu/flow/code/flow-code/README.txt"
core,cv::optflow::readOpticalFlow cv::optflow::writeOpticalFlow
core,"Classes class cv::optflow::DenseRLOFOpticalFlow  Fast dense optical flow computation based on robust local optical flow (RLOF) algorithms and sparse-to-dense interpolation scheme. More...  class cv::optflow::DualTVL1OpticalFlow  ""Dual TV L1"" Optical Flow Algorithm. More...  class cv::optflow::GPCDetails  class cv::optflow::GPCForest< T >  struct cv::optflow::GPCMatchingParams  Class encapsulating matching parameters. More...  struct cv::optflow::GPCPatchDescriptor  struct cv::optflow::GPCPatchSample  struct cv::optflow::GPCTrainingParams  Class encapsulating training parameters. More...  class cv::optflow::GPCTrainingSamples  Class encapsulating training samples. More...  class cv::optflow::GPCTree  Class for individual tree. More...  class cv::optflow::OpticalFlowPCAFlow  PCAFlow algorithm. More...  class cv::optflow::PCAPrior  This class can be used for imposing a learned prior on the resulting optical flow. Solution will be regularized according to this prior. You need to generate appropriate prior file with ""learn_prior.py"" script beforehand. More...  class cv::optflow::RLOFOpticalFlowParameter  This is used store and set up the parameters of the robust local optical flow (RLOF) algoritm. More...  class cv::optflow::SparseRLOFOpticalFlow  Class used for calculation sparse optical flow and feature tracking with robust local optical flow (RLOF) algorithms. More... "
core,Namespaces namespace cv::omnidir::internal 
core,Information Flow algorithm implementaton for alphamatting
core,Alpha matting is used to extract a foreground object with soft boundaries from a background image.
core,"This module is dedicated to computing alpha matte of objects in images from a given input image and a greyscale trimap image that contains information about the foreground, background and unknown pixels. The unknown pixels are assumed to be a combination of foreground and background pixels. The algorithm uses a combination of multiple carefully defined pixels affinities to estimate the opacity of the foreground pixels in the unkown region."
core,The implementation is based on [7].
core,This module was developed by Muskaan Kularia and Sunita Nayak as a project for Google Summer of Code 2019 (GSoC 19).
core,"Functions void cv::alphamat::infoFlow (InputArray image, InputArray tmap, OutputArray result)  Compute alpha matte of an object in an image. "
core,Classes class cv::xobjdetect::WBDetector  WaldBoost detector. More... 
core,The opencv_text module provides different algorithms for text detection and recognition in natural scene images.
core,Provide algorithms to extract the hash of images and fast way to figure out most similar images in huge data set.
core,Namespace for all functions is cv::img_hash.
core,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
core,"implements ""RAPID-a video rate object tracker"" [116] with the dynamic control point extraction of [73]"
core,Classes class cv::rapid::GOSTracker  class cv::rapid::OLSTracker  class cv::rapid::Rapid  wrapper around silhouette based 3D object tracking function for uniform access More...  class cv::rapid::Tracker  Abstract base class for stateful silhouette trackers. More... 
core,This modules is to draw UTF-8 strings with freetype/harfbuzz.
core,"If thickness parameter is negative, drawing glyph is filled. If thickness parameter is positive, drawing glyph is outlined with thickness. If line_type parameter is 16(or CV_AA), drawing glyph is smooth."
core,Classes class cv::freetype::FreeType2 
core,"The video stabilization module contains a set of functions and classes that can be used to solve the problem of video stabilization. There are a few methods implemented, most of them are described in the papers [188] and [113] . However, there are some extensions and deviations from the original paper methods."
core,Namespaces namespace NcvCTprep 
core,Classes class cv::dnn_objdetect::InferBbox  A class to post process model predictions. More...  struct cv::dnn_objdetect::object  Structure to hold the details pertaining to a single bounding box. More... 
core,This module contains:
core,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
core,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
core,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
core,Classes class cv::cuda::BackgroundSubtractorMOG  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More...  class cv::cuda::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
core,Classes class cv::cuda::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::cuda::FastFeatureDetector  Wrapping class for feature detection using the FAST method. More...  class cv::cuda::Feature2DAsync  Abstract base class for CUDA asynchronous 2D image feature detectors and descriptor extractors. More...  class cv::cuda::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More... 
core,"The Super Resolution module contains a set of functions and classes that can be used to solve the problem of resolution enhancement. There are a few methods implemented, most of them are described in the papers [83] and [197] ."
core,Classes class cv::superres::BroxOpticalFlow  class cv::superres::DenseOpticalFlowExt  class cv::superres::DualTVL1OpticalFlow  class cv::superres::FarnebackOpticalFlow  class cv::superres::FrameSource  class cv::superres::PyrLKOpticalFlow  class cv::superres::SuperResolution  Base class for Super Resolution algorithms. More... 
core,"Classes class cv::cuda::BroxOpticalFlow  Class computing the optical flow for two images using Brox et al Optical Flow algorithm ([42]). More...  class cv::cuda::DenseOpticalFlow  Base interface for dense optical flow algorithms. More...  class cv::cuda::DensePyrLKOpticalFlow  Class used for calculating a dense optical flow. More...  class cv::cuda::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::cuda::NvidiaHWOpticalFlow  Base Interface for optical flow algorithms using NVIDIA Optical Flow SDK. More...  class cv::cuda::NvidiaOpticalFlow_1_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 1.0. More...  class cv::cuda::NvidiaOpticalFlow_2_0  Class for computing the optical flow vectors between two images using NVIDIA Optical Flow hardware and Optical Flow SDK 2.0. More...  class cv::cuda::OpticalFlowDual_TVL1  Implementation of the Zach, Pock and Bischof Dual TV-L1 Optical Flow method. More...  class cv::cuda::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::cuda::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More... "
core,"Julia (https://julialang.org) is a programming language for scientific community with growing popularity. These are bindings for a subset of OpenCV functionality, based on libcxxwrap-julia and CxxWrap packages."
core,"For installation instructions, see README.md in this module or OpenCV wiki (https://github.com/opencv/opencv/wiki)"
core,"Functions void cv::julia::initJulia (int argc, char **argv) "
core,This module includes image-processing functions.
core,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
core,It provides easy interface to:
core,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
core,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
core,"The datasets module includes classes for working with different datasets: load data, evaluate different algorithms on them, contains benchmarks, etc."
core,It is planned to have:
core,"basic: loading code for all datasets to help start work with them. next stage: quick benchmarks for all datasets to show how to solve them using OpenCV and implement evaluation code. finally: implement on OpenCV state-of-the-art algorithms, which solve these tasks."
core,Classes class cv::datasets::Dataset  struct cv::datasets::Object 
core,"Many computer vision applications may benefit from understanding where humans focus given a scene. Other than cognitively understanding the way human perceive images and scenes, finding salient regions and objects in the images helps various tasks such as speeding up object detection, object recognition, object tracking and content-aware image editing."
core,"About the saliency, there is a rich literature but the development is very fragmented. The principal purpose of this API is to give a unique interface, a unique framework for use and plug sever saliency algorithms, also with very different nature and methodology, but they share the same purpose, organizing algorithms into three main categories:"
core,"Static Saliency**: algorithms belonging to this category, exploit different image features that allow to detect salient objects in a non dynamic scenarios."
core,"Motion Saliency**: algorithms belonging to this category, are particularly focused to detect salient objects over time (hence also over frame), then there is a temporal component sealing cosider that allows to detect ""moving"" objects as salient, meaning therefore also the more general sense of detection the changes in the scene."
core,"Objectness**: Objectness is usually represented as a value which reflects how likely an image window covers an object of any category. Algorithms belonging to this category, avoid making decisions early on, by proposing a small number of category-independent proposals, that are expected to cover all objects in an image. Being able to perceive objects before identifying them is closely related to bottom up visual attention (saliency)."
core,"To see how API works, try tracker demo: https://github.com/fpuja/opencv_contrib/blob/saliencyModuleDevelop/modules/saliency/samples/computeSaliency.cpp"
core,"Classes class cv::saliency::MotionSaliency  class cv::saliency::MotionSaliencyBinWangApr2014  the Fast Self-tuning Background Subtraction Algorithm from [291] More...  class cv::saliency::Objectness  class cv::saliency::ObjectnessBING  Objectness algorithms based on [3] [3] Cheng, Ming-Ming, et al. ""BING: Binarized normed gradients for objectness estimation at 300fps."" IEEE CVPR. 2014. More...  class cv::saliency::Saliency  class cv::saliency::StaticSaliency  class cv::saliency::StaticSaliencyFineGrained  the Fine Grained Saliency approach from [198] More...  class cv::saliency::StaticSaliencySpectralResidual  the Spectral Residual approach from [128] More... "
core,The opencv_sfm module contains algorithms to perform 3d reconstruction from 2d images. The core of the module is based on a light version of Libmv originally developed by Sameer Agarwal and Keir Mierle.
core,"Whats is libmv? libmv, also known as the Library for Multiview Reconstruction (or LMV), is the computer vision backend for Blender's motion tracking abilities. Unlike other vision libraries with general ambitions, libmv is focused on algorithms for match moving, specifically targeting Blender as the primary customer. Dense reconstruction, reconstruction from unorganized photo collections, image recognition, and other tasks are not a focus of libmv."
core,"Development libmv is officially under the Blender umbrella, and so is developed on developer.blender.org. The source repository can get checked out independently from Blender."
core,This module has been originally developed as a project for Google Summer of Code 2012-2015.
core,"Notice that it is compiled only when Eigen, GLog and GFlags are correctly installed. Check installation instructions in the following tutorial: SFM module installation"
core,"The functions in this section use a so-called pinhole camera model. The view of a scene is obtained by projecting a scene's 3D point \(P_w\) into the image plane using a perspective transformation which forms the corresponding pixel \(p\). Both \(P_w\) and \(p\) are represented in homogeneous coordinates, i.e. as 3D and 2D homogeneous vector respectively. You will find a brief introduction to projective geometry, homogeneous vectors and homogeneous transformations at the end of this section's introduction. For more succinct notation, we often drop the 'homogeneous' and say vector instead of homogeneous vector."
core,The distortion-free projective transformation given by a pinhole camera model is shown below.
core,"\[s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w,\]"
core,"where \(P_w\) is a 3D point expressed with respect to the world coordinate system, \(p\) is a 2D pixel in the image plane, \(A\) is the camera intrinsic matrix, \(R\) and \(t\) are the rotation and translation that describe the change of coordinates from world to camera coordinate systems (or camera frame) and \(s\) is the projective transformation's arbitrary scaling and not part of the camera model."
core,"The camera intrinsic matrix \(A\) (notation used as in [319] and also generally notated as \(K\)) projects 3D points given in the camera coordinate system to 2D pixel coordinates, i.e."
core,\[p = A P_c.\]
core,"The camera intrinsic matrix \(A\) is composed of the focal lengths \(f_x\) and \(f_y\), which are expressed in pixel units, and the principal point \((c_x, c_y)\), that is usually close to the image center:"
core,"\[A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1},\]"
core,and thus
core,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \vecthree{X_c}{Y_c}{Z_c}.\]
core,"The matrix of intrinsic parameters does not depend on the scene viewed. So, once estimated, it can be re-used as long as the focal length is fixed (in case of a zoom lens). Thus, if an image from the camera is scaled by a factor, all of these parameters need to be scaled (multiplied/divided, respectively) by the same factor."
core,The joint rotation-translation matrix \([R|t]\) is the matrix product of a projective transformation and a homogeneous transformation. The 3-by-4 projective transformation maps 3D points represented in camera coordinates to 2D points in the image plane and represented in normalized camera coordinates \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\):
core,\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix}.\]
core,"The homogeneous transformation is encoded by the extrinsic parameters \(R\) and \(t\) and represents the change of basis from world coordinate system \(w\) to the camera coordinate sytem \(c\). Thus, given the representation of the point \(P\) in world coordinates, \(P_w\), we obtain \(P\)'s representation in the camera coordinate system, \(P_c\), by"
core,"\[P_c = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_w,\]"
core,"This homogeneous transformation is composed out of \(R\), a 3-by-3 rotation matrix, and \(t\), a 3-by-1 translation vector:"
core,"\[\begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \]"
core,and therefore
core,\[\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
core,"Combining the projective transformation and the homogeneous transformation, we obtain the projective transformation that maps 3D points in world coordinates into 2D points in the image plane and in normalized camera coordinates:"
core,"\[Z_c \begin{bmatrix} x' \\ y' \\ 1 \end{bmatrix} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix} = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix},\]"
core,"with \(x' = X_c / Z_c\) and \(y' = Y_c / Z_c\). Putting the equations for instrincs and extrinsics together, we can write out \(s \; p = A \begin{bmatrix} R|t \end{bmatrix} P_w\) as"
core,\[s \vecthree{u}{v}{1} = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
core,"If \(Z_c \ne 0\), the transformation above is equivalent to the following,"
core,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x X_c/Z_c + c_x \\ f_y Y_c/Z_c + c_y \end{bmatrix}\]
core,with
core,\[\vecthree{X_c}{Y_c}{Z_c} = \begin{bmatrix} R|t \end{bmatrix} \begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \end{bmatrix}.\]
core,The following figure illustrates the pinhole camera model.
core,"Real lenses usually have some distortion, mostly radial distortion, and slight tangential distortion. So, the above model is extended as:"
core,\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x'' + c_x \\ f_y y'' + c_y \end{bmatrix}\]
core,where
core,\[\begin{bmatrix} x'' \\ y'' \end{bmatrix} = \begin{bmatrix} x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2 p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4 \\ y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ \end{bmatrix}\]
core,with
core,\[r^2 = x'^2 + y'^2\]
core,and
core,"\[\begin{bmatrix} x'\\ y' \end{bmatrix} = \begin{bmatrix} X_c/Z_c \\ Y_c/Z_c \end{bmatrix},\]"
core,if \(Z_c \ne 0\).
core,"The distortion parameters are the radial coefficients \(k_1\), \(k_2\), \(k_3\), \(k_4\), \(k_5\), and \(k_6\) , \(p_1\) and \(p_2\) are the tangential distortion coefficients, and \(s_1\), \(s_2\), \(s_3\), and \(s_4\), are the thin prism distortion coefficients. Higher-order coefficients are not considered in OpenCV."
core,"The next figures show two common types of radial distortion: barrel distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically decreasing) and pincushion distortion ( \( 1 + k_1 r^2 + k_2 r^4 + k_3 r^6 \) monotonically increasing). Radial distortion is always monotonic for real lenses, and if the estimator produces a non-monotonic result, this should be considered a calibration failure. More generally, radial distortion must be monotonic and the distortion function must be bijective. A failed estimation result may look deceptively good near the image center but will work poorly in e.g. AR/SFM applications. The optimization method used in OpenCV camera calibration does not include these constraints as the framework does not support the required integer programming and polynomial inequalities. See issue #15992 for additional information."
core,"In some cases, the image sensor may be tilted in order to focus an oblique plane in front of the camera (Scheimpflug principle). This can be useful for particle image velocimetry (PIV) or triangulation with a laser fan. The tilt causes a perspective distortion of \(x''\) and \(y''\). This distortion can be modeled in the following way, see e.g. [172]."
core,"\[\begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} f_x x''' + c_x \\ f_y y''' + c_y \end{bmatrix},\]"
core,where
core,"\[s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}(\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\]"
core,"and the matrix \(R(\tau_x, \tau_y)\) is defined by two rotations with angular parameter \(\tau_x\) and \(\tau_y\), respectively,"
core,"\[ R(\tau_x, \tau_y) = \vecthreethree{\cos(\tau_y)}{0}{-\sin(\tau_y)}{0}{1}{0}{\sin(\tau_y)}{0}{\cos(\tau_y)} \vecthreethree{1}{0}{0}{0}{\cos(\tau_x)}{\sin(\tau_x)}{0}{-\sin(\tau_x)}{\cos(\tau_x)} = \vecthreethree{\cos(\tau_y)}{\sin(\tau_y)\sin(\tau_x)}{-\sin(\tau_y)\cos(\tau_x)} {0}{\cos(\tau_x)}{\sin(\tau_x)} {\sin(\tau_y)}{-\cos(\tau_y)\sin(\tau_x)}{\cos(\tau_y)\cos(\tau_x)}. \]"
core,In the functions below the coefficients are passed or returned as
core,"\[(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\]"
core,"vector. That is, if the vector contains four elements, it means that \(k_3=0\) . The distortion coefficients do not depend on the scene viewed. Thus, they also belong to the intrinsic camera parameters. And they remain the same regardless of the captured image resolution. If, for example, a camera has been calibrated on images of 320 x 240 resolution, absolutely the same distortion coefficients can be used for 640 x 480 images from the same camera while \(f_x\), \(f_y\), \(c_x\), and \(c_y\) need to be scaled appropriately."
core,The functions below use the above model to do the following:
core,"Project 3D points to the image plane given intrinsic and extrinsic parameters. Compute extrinsic parameters given intrinsic parameters, a few 3D points, and their projections. Estimate intrinsic and extrinsic camera parameters from several views of a known calibration pattern (every view is described by several 3D-2D point correspondences). Estimate the relative position and orientation of the stereo camera ""heads"" and compute the rectification* transformation that makes the camera optical axes parallel."
core,"Homogeneous Coordinates Homogeneous Coordinates are a system of coordinates that are used in projective geometry. Their use allows to represent points at infinity by finite coordinates and simplifies formulas when compared to the cartesian counterparts, e.g. they have the advantage that affine transformations can be expressed as linear homogeneous transformation."
core,One obtains the homogeneous vector \(P_h\) by appending a 1 along an n-dimensional cartesian vector \(P\) e.g. for a 3D cartesian vector the mapping \(P \rightarrow P_h\) is:
core,\[\begin{bmatrix} X \\ Y \\ Z \end{bmatrix} \rightarrow \begin{bmatrix} X \\ Y \\ Z \\ 1 \end{bmatrix}.\]
core,"For the inverse mapping \(P_h \rightarrow P\), one divides all elements of the homogeneous vector by its last element, e.g. for a 3D homogeneous vector one gets its 2D cartesian counterpart by:"
core,"\[\begin{bmatrix} X \\ Y \\ W \end{bmatrix} \rightarrow \begin{bmatrix} X / W \\ Y / W \end{bmatrix},\]"
core,if \(W \ne 0\).
core,"Due to this mapping, all multiples \(k P_h\), for \(k \ne 0\), of a homogeneous point represent the same point \(P_h\). An intuitive understanding of this property is that under a projective transformation, all multiples of \(P_h\) are mapped to the same point. This is the physical observation one does for pinhole cameras, as all points along a ray through the camera's pinhole are projected to the same image point, e.g. all points along the red ray in the image of the pinhole camera model above would be mapped to the same image coordinate. This property is also the source for the scale ambiguity s in the equation of the pinhole camera model."
core,"As mentioned, by using homogeneous coordinates we can express any change of basis parameterized by \(R\) and \(t\) as a linear transformation, e.g. for the change of basis from coordinate system 0 to coordinate system 1 becomes:"
core,\[P_1 = R P_0 + t \rightarrow P_{h_1} = \begin{bmatrix} R & t \\ 0 & 1 \end{bmatrix} P_{h_0}.\]
core,"Many functions in this module take a camera intrinsic matrix as an input parameter. Although all functions assume the same structure of this parameter, they may name it differently. The parameter's description, however, will be clear in that a camera intrinsic matrix with the structure shown above is required. A calibration sample for 3 cameras in a horizontal position can be found at opencv_source_code/samples/cpp/3calibration.cpp A calibration sample based on a sequence of images can be found at opencv_source_code/samples/cpp/calibration.cpp A calibration sample in order to do 3D reconstruction can be found at opencv_source_code/samples/cpp/build3dmodel.cpp A calibration example on stereo calibration can be found at opencv_source_code/samples/cpp/stereo_calib.cpp A calibration example on stereo matching can be found at opencv_source_code/samples/cpp/stereo_match.cpp (Python) A camera calibration sample can be found at opencv_source_code/samples/python/calibrate.py"
core,"Classes struct cv::CirclesGridFinderParameters  class cv::LMSolver  class cv::StereoBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::StereoMatcher  The base class for stereo correspondence algorithms. More...  class cv::StereoSGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  struct cv::UsacParams "
core,Classes class cv::cuda::CascadeClassifier  Cascade classifier class used for object detection. Supports HAAR and LBP cascades. : More...  class cv::cuda::HOG  The class implements Histogram of Oriented Gradients ([63]) object detector. More... 
core,"ArUco Marker Detection, module functionality was moved to objdetect module"
core,Classes struct cv::aruco::EstimateParameters  Pose estimation parameters. More... 
core,This module contains functionality for upscaling an image via convolutional neural networks. The following four models are implemented:
core,EDSR https://arxiv.org/abs/1707.02921 ESPCN https://arxiv.org/abs/1609.05158 FSRCNN https://arxiv.org/abs/1608.00367 LapSRN https://arxiv.org/abs/1710.01992
core,Classes class cv::dnn_superres::DnnSuperResImpl  A class to upscale images via convolutional neural networks. The following four models are implemented: More... 
core,Tracking is an important issue for many computer vision applications in real world scenario. The development in this area is very fragmented and this API is an interface useful for plug several algorithms and compare them.
core,Classes class cv::TrackerCSRT  the CSRT tracker More...  class cv::TrackerKCF  the KCF (Kernelized Correlation Filter) tracker More... 
core,"Functions void cv::cuda::buildWarpAffineMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for affine transformation.  void cv::cuda::buildWarpAffineMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpAffineMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (InputArray M, bool inverse, Size dsize, OutputArray xmap, OutputArray ymap, Stream &stream=Stream::Null())  Builds transformation maps for perspective transformation.  void cv::cuda::buildWarpPerspectiveMaps (Mat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::buildWarpPerspectiveMaps (UMat M, bool inverse, Size dsize, GpuMat &xmap, GpuMat &ymap, Stream &stream=Stream::Null())  void cv::cuda::pyrDown (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Smoothes an image and downsamples it.  void cv::cuda::pyrUp (InputArray src, OutputArray dst, Stream &stream=Stream::Null())  Upsamples an image and then smoothes it.  void cv::cuda::remap (InputArray src, OutputArray dst, InputArray xmap, InputArray ymap, int interpolation, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a generic geometrical transformation to an image.  void cv::cuda::resize (InputArray src, OutputArray dst, Size dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Resizes an image.  void cv::cuda::rotate (InputArray src, OutputArray dst, Size dsize, double angle, double xShift=0, double yShift=0, int interpolation=INTER_LINEAR, Stream &stream=Stream::Null())  Rotates an image around the origin (0,0) and then shifts it.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies an affine transformation to an image.  void cv::cuda::warpAffine (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpAffine (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, InputArray M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  Applies a perspective transformation to an image.  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, Mat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null())  void cv::cuda::warpPerspective (InputArray src, OutputArray dst, UMat M, Size dsize, int flags=INTER_LINEAR, int borderMode=BORDER_CONSTANT, Scalar borderValue=Scalar(), Stream &stream=Stream::Null()) "
core,"The Registration module implements parametric image registration. The implemented method is direct alignment, that is, it uses directly the pixel values for calculating the registration between a pair of images, as opposed to feature-based registration. The implementation follows essentially the corresponding part of [259] ."
core,"Feature based methods have some advantages over pixel based methods when we are trying to register pictures that have been shoot under different lighting conditions or exposition times, or when the images overlap only partially. On the other hand, the main advantage of pixel-based methods when compared to feature based methods is their better precision for some pictures (those shoot under similar lighting conditions and that have a significative overlap), due to the fact that we are using all the information available in the image, which allows us to achieve subpixel accuracy. This is particularly important for certain applications like multi-frame denoising or super-resolution."
core,"In fact, pixel and feature registration methods can complement each other: an application could first obtain a coarse registration using features and then refine the registration using a pixel based method on the overlapping area of the images. The code developed allows this use case."
core,"The module implements classes derived from the abstract classes cv::reg::Map or cv::reg::Mapper. The former models a coordinate transformation between two reference frames, while the later encapsulates a way of invoking a method that calculates a Map between two images. Although the objective has been to implement pixel based methods, the module can be extended to support other methods that can calculate transformations between images (feature methods, optical flow, etc.)."
core,"Each class derived from Map implements a motion model, as follows:"
core,MapShift: Models a simple translation MapAffine: Models an affine transformation MapProjec: Models a projective transformation
core,"MapProject can also be used to model affine motion or translations, but some operations on it are more costly, and that is the reason for defining the other two classes."
core,The classes derived from Mapper are
core,"MapperGradShift: Gradient based alignment for calculating translations. It produces a MapShift (two parameters that correspond to the shift vector). MapperGradEuclid: Gradient based alignment for euclidean motions, that is, rotations and translations. It calculates three parameters (angle and shift vector), although the result is stored in a MapAffine object for convenience. MapperGradSimilar: Gradient based alignment for calculating similarities, which adds scaling to the euclidean motion. It calculates four parameters (two for the anti-symmetric matrix and two for the shift vector), although the result is stored in a MapAffine object for better convenience. MapperGradAffine: Gradient based alignment for an affine motion model. The number of parameters is six and the result is stored in a MapAffine object. MapperGradProj: Gradient based alignment for calculating projective transformations. The number of parameters is eight and the result is stored in a MapProject object. MapperPyramid: It implements hyerarchical motion estimation using a Gaussian pyramid. Its constructor accepts as argument any other object that implements the Mapper interface, and it is that mapper the one called by MapperPyramid for each scale of the pyramid."
core,"If the motion between the images is not very small, the normal way of using these classes is to create a MapperGrad* object and use it as input to create a MapperPyramid, which in turn is called to perform the calculation. However, if the motion between the images is small enough, we can use directly the MapperGrad* classes. Another possibility is to use first a feature based method to perform a coarse registration and then do a refinement through MapperPyramid or directly a MapperGrad* object. The ""calculate"" method of the mappers accepts an initial estimation of the motion as input."
core,"When deciding which MapperGrad to use we must take into account that mappers with more parameters can handle more complex motions, but involve more calculations and are therefore slower. Also, if we are confident on the motion model that is followed by the sequence, increasing the number of parameters beyond what we need will decrease the accuracy: it is better to use the least number of degrees of freedom that we can."
core,In the module tests there are examples that show how to register a pair of images using any of the implemented mappers.
core,Classes class cv::reg::Map  Base class for modelling a Map between two images. More...  class cv::reg::MapAffine  class cv::reg::Mapper  Base class for modelling an algorithm for calculating a map. More...  class cv::reg::MapperGradAffine  class cv::reg::MapperGradEuclid  class cv::reg::MapperGradProj  class cv::reg::MapperGradShift  class cv::reg::MapperGradSimilar  class cv::reg::MapperPyramid  class cv::reg::MapProjec  class cv::reg::MapShift  class cv::reg::MapTypeCaster 
core,This module provides storage routines for Hierarchical Data Format objects.
core,Face module changelog Face Recognition with OpenCV
core,"Classes class cv::face::BasicFaceRecognizer  struct cv::face::CParams  class cv::face::EigenFaceRecognizer  class cv::face::Facemark  Abstract base class for all facemark models. More...  class cv::face::FacemarkAAM  class cv::face::FacemarkLBF  class cv::face::FacemarkTrain  Abstract base class for trainable facemark models. More...  class cv::face::FaceRecognizer  Abstract base class for all face recognition models. More...  class cv::face::FisherFaceRecognizer  class cv::face::LBPHFaceRecognizer  class cv::face::MACE  Minimum Average Correlation Energy Filter useful for authentication with (cancellable) biometrical features. (does not need many positives to train (10-50), and no negatives at all, also robust to noise/salting) More...  class cv::face::PredictCollector  Abstract base class for all strategies of prediction result handling. More...  class cv::face::StandardCollector  Default predict collector. More... "
core,Classes class cv::plot::Plot2d 
core,Classes class cv::quality::QualityBase 
core,"The opencv hfs module contains an efficient algorithm to segment an image. This module is implemented based on the paper Hierarchical Feature Selection for Efficient Image Segmentation, ECCV 2016. The original project was developed by Yun Liu(https://github.com/yun-liu/hfs)."
core,Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images.
core,An example containing all basic morphology operators like erode and dilate can be found at opencv_source_code/samples/gpu/morphology.cpp
core,Classes class cv::cuda::Filter  Common interface for all CUDA filters : More... 
core,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
core,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
core,Namespace for all functions is cv::intensity_transform.
core,Classes class cv::cuda::DisparityBilateralFilter  Class refining a disparity map using joint bilateral filtering. : More...  class cv::cuda::StereoBeliefPropagation  Class computing stereo correspondence using the belief propagation algorithm. : More...  class cv::cuda::StereoBM  Class computing stereo correspondence (disparity map) using the block matching algorithm. : More...  class cv::cuda::StereoConstantSpaceBP  Class computing stereo correspondence using the constant space belief propagation algorithm. : More...  class cv::cuda::StereoSGM  The class implements the modified H. Hirschmuller algorithm [126]. Limitation and difference are as follows: More... 
core,Classes class cv::wechat_qrcode::WeChatQRCode  WeChat QRCode includes two CNN-based models: A object detection model and a super resolution model. Object detection model is applied to detect QRCode with the bounding box. super resolution model is applied to zoom in QRCode when it is small. More... 
core,"Classes struct cv::stereo::MatchQuasiDense  struct cv::stereo::PropagationParameters  class cv::stereo::QuasiDenseStereo  Class containing the methods needed for Quasi Dense Stereo computation. More...  class cv::stereo::StereoBinaryBM  Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. More...  class cv::stereo::StereoBinarySGBM  The class implements the modified H. Hirschmuller algorithm [126] that differs from the original one as follows: More...  class cv::stereo::StereoMatcher  Filters off small noise blobs (speckles) in the disparity map. More... "
core,Read and write video or images sequence with OpenCV.
core,The module provides biological visual systems models (human visual system and others). It also provides derivated objects that take advantage of those bio-inspired models.
core,Bioinspired Module Retina Introduction
core,Classes class cv::bioinspired::Retina  class which allows the Gipsa/Listic Labs model to be used with OpenCV. More...  class cv::bioinspired::RetinaFastToneMapping  a wrapper class which allows the tone mapping algorithm of Meylan&al(2007) to be used with OpenCV. More...  struct cv::bioinspired::RetinaParameters  retina model parameters structure More...  struct cv::bioinspired::SegmentationParameters  parameter structure that stores the transient events detector setup parameters More...  class cv::bioinspired::TransientAreasSegmentationModule  class which provides a transient/moving areas segmentation module More... 
core,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
core,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
core,See detailed overview here: Machine Learning Overview.
core,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
core,Classes class cv::xphoto::GrayworldWB  Gray-world white balance algorithm. More...  class cv::xphoto::LearningBasedWB  More sophisticated learning-based automatic white balance algorithm. More...  class cv::xphoto::SimpleWB  A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. More...  class cv::xphoto::TonemapDurand  This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. More...  class cv::xphoto::WhiteBalancer  The base class for auto white balance algorithms. More... 
core,"Enumerations enum cv::ximgproc::LocalBinarizationMethods { cv::ximgproc::BINARIZATION_NIBLACK = 0 , cv::ximgproc::BINARIZATION_SAUVOLA = 1 , cv::ximgproc::BINARIZATION_WOLF = 2 , cv::ximgproc::BINARIZATION_NICK = 3 }  Specifies the binarization method to use in cv::ximgproc::niBlackThreshold. More...  enum cv::ximgproc::ThinningTypes { cv::ximgproc::THINNING_ZHANGSUEN = 0 , cv::ximgproc::THINNING_GUOHALL = 1 } "
core,"Two-dimensional phase unwrapping is found in different applications like terrain elevation estimation in synthetic aperture radar (SAR), field mapping in magnetic resonance imaging or as a way of finding corresponding pixels in structured light reconstruction with sinusoidal patterns."
core,"Given a phase map, wrapped between [-pi; pi], phase unwrapping aims at finding the ""true"" phase map by adding the right number of 2*pi to each pixel."
core,"The problem is straightforward for perfect wrapped phase map, but real data are usually not noise-free. Among the different algorithms that were developed, quality-guided phase unwrapping methods are fast and efficient. They follow a path that unwraps high quality pixels first, avoiding error propagation from the start."
core,"In this module, a quality-guided phase unwrapping is implemented following the approach described in [157] ."
core,"Classes class cv::phase_unwrapping::HistogramPhaseUnwrapping  Class implementing two-dimensional phase unwrapping based on [157] This algorithm belongs to the quality-guided phase unwrapping methods. First, it computes a reliability map from second differences between a pixel and its eight neighbours. Reliability values lie between 0 and 16*pi*pi. Then, this reliability map is used to compute the reliabilities of ""edges"". An edge is an entity defined by two pixels that are connected horizontally or vertically. Its reliability is found by adding the the reliabilities of the two pixels connected through it. Edges are sorted in a histogram based on their reliability values. This histogram is then used to unwrap pixels, starting from the highest quality pixel. More...  class cv::phase_unwrapping::PhaseUnwrapping  Abstract base class for phase unwrapping. More... "
core,Namespace for all functions is ft. The module brings implementation of the last image processing algorithms based on fuzzy mathematics. Method are named based on the pattern FT_degree_dimension_method.
core,"Enumerations enum { cv::ft::LINEAR = 1 , cv::ft::SINUS = 2 }  enum { cv::ft::ONE_STEP = 1 , cv::ft::MULTI_STEP = 2 , cv::ft::ITERATIVE = 3 } "
core,Namespaces namespace cv::cudev::functional_detail  namespace cv::cudev::vec_math_detail 
core,"Namespace for all functions is cvv, i.e. cvv::showImage()."
core,Compilation:
core,"For development, i.e. for cvv GUI to show up, compile your code using cvv with g++ -DCVVISUAL_DEBUGMODE*. For release, i.e. cvv calls doing nothing, compile your code without above flag."
core,See cvv tutorial for a commented example application using cvv.
core,Namespaces namespace cvv::impl 
dnn,This class represents high-level API for classification models.
dnn,"ClassificationModel allows to set params for preprocessing input image. ClassificationModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and return top-1 prediction."
dnn,Derivatives of this class encapsulates functions of certain backends.
dnn,This interface class allows to build new Layers - are building blocks of networks.
dnn,"Each class, derived from Layer, must implement forward() method to compute outputs. Also before using the new layer into networks you must register your layer by using one of LayerFactory macros."
dnn,This class is presented high-level API for neural networks.
dnn,"Model allows to set params for preprocessing input image. Model creates net from file with trained weights and config, sets preprocessing input and runs forward pass."
dnn,This class represents high-level API for object detection networks.
dnn,"DetectionModel allows to set params for preprocessing input image. DetectionModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and return result detections. For DetectionModel SSD, Faster R-CNN, YOLO topologies are supported."
dnn,Layer factory allows to create instances of registered layers.
dnn,"This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64."
dnn,Base class for text detection networks.
dnn,This module contains:
dnn,"API for new layers creation, layers are building bricks of neural networks; set of built-in most-useful Layers; API to construct and modify comprehensive neural networks from layers; functionality for loading serialized networks models from different frameworks."
dnn,Functionality of this module is designed only for forward pass computations (i.e. network testing). A network training is in principle not supported.
dnn,"Classes class cv::dnn::BackendNode  Derivatives of this class encapsulates functions of certain backends. More...  class cv::dnn::BackendWrapper  Derivatives of this class wraps cv::Mat for different backends and targets. More...  class cv::dnn::ClassificationModel  This class represents high-level API for classification models. More...  class cv::dnn::DetectionModel  This class represents high-level API for object detection networks. More...  class cv::dnn::Dict  This class implements name-value dictionary, values are instances of DictValue. More...  struct cv::dnn::DictValue  This struct stores the scalar value (or array) of one of the following type: double, cv::String or int64. More...  struct cv::dnn::Image2BlobParams  Processing params of image to blob. More...  class cv::dnn::KeypointsModel  This class represents high-level API for keypoints models. More...  class cv::dnn::Layer  This interface class allows to build new Layers - are building blocks of networks. More...  class cv::dnn::LayerParams  This class provides all data needed to initialize layer. More...  class cv::dnn::Model  This class is presented high-level API for neural networks. More...  class cv::dnn::Net  This class allows to create and manipulate comprehensive artificial neural networks. More...  class cv::dnn::SegmentationModel  This class represents high-level API for segmentation models. More...  class cv::dnn::TextDetectionModel  Base class for text detection networks. More...  class cv::dnn::TextDetectionModel_DB  This class represents high-level API for text detection DL networks compatible with DB model. More...  class cv::dnn::TextDetectionModel_EAST  This class represents high-level API for text detection DL networks compatible with EAST model. More...  class cv::dnn::TextRecognitionModel  This class represents high-level API for text recognition networks. More... "
dnn,This subsection of dnn module contains information about built-in layers and their descriptions.
dnn,"Classes listed here, in fact, provides C++ API for creating instances of built-in layers. In addition to this way of layers instantiation, there is a more common factory API (see Utilities for New Layers Registration), it allows to create layers dynamically (by name) and register new ones. You can use both API, but factory API is less convenient for native C++ programming and basically designed for use inside importers (see readNetFromCaffe(), readNetFromTorch(), readNetFromTensorflow())."
dnn,"Built-in layers partially reproduce functionality of corresponding Caffe and Torch7 layers. In particular, the following layers and Caffe importer were tested to reproduce Caffe functionality:"
dnn,"Convolution Deconvolution Pooling InnerProduct TanH, ReLU, Sigmoid, BNLL, Power, AbsVal Softmax Reshape, Flatten, Slice, Split LRN MVN Dropout (since it does nothing on forward pass -))"
dnn,Classes class cv::dnn::AbsLayer  class cv::dnn::AccumLayer  class cv::dnn::AcoshLayer  class cv::dnn::AcosLayer  class cv::dnn::ActivationLayer  class cv::dnn::ActivationLayerInt8  class cv::dnn::ArgLayer  ArgMax/ArgMin layer. More...  class cv::dnn::AsinhLayer  class cv::dnn::AsinLayer  class cv::dnn::AtanhLayer  class cv::dnn::AtanLayer  class cv::dnn::AttentionLayer  class cv::dnn::BaseConvolutionLayer  class cv::dnn::BatchNormLayer  class cv::dnn::BatchNormLayerInt8  class cv::dnn::BlankLayer  class cv::dnn::BNLLLayer  class cv::dnn::CeilLayer  class cv::dnn::CeluLayer  class cv::dnn::ChannelsPReLULayer  class cv::dnn::CompareLayer  class cv::dnn::ConcatLayer  class cv::dnn::ConstLayer  class cv::dnn::ConvolutionLayer  class cv::dnn::ConvolutionLayerInt8  class cv::dnn::CorrelationLayer  class cv::dnn::CoshLayer  class cv::dnn::CosLayer  class cv::dnn::CropAndResizeLayer  class cv::dnn::CropLayer  class cv::dnn::CumSumLayer  class cv::dnn::DataAugmentationLayer  class cv::dnn::DeconvolutionLayer  class cv::dnn::DepthToSpaceLayer  class cv::dnn::DequantizeLayer  class cv::dnn::DetectionOutputLayer  Detection output layer. More...  class cv::dnn::EinsumLayer  This function performs array summation based on the Einstein summation convention. The function allows for concise expressions of various mathematical operations using subscripts. More...  class cv::dnn::EltwiseLayer  Element wise operation on inputs. More...  class cv::dnn::EltwiseLayerInt8  class cv::dnn::ELULayer  class cv::dnn::ErfLayer  class cv::dnn::ExpandLayer  class cv::dnn::ExpLayer  class cv::dnn::FlattenLayer  class cv::dnn::FloorLayer  class cv::dnn::FlowWarpLayer  class cv::dnn::GatherElementsLayer  GatherElements layer GatherElements takes two inputs data and indices of the same rank r >= 1 and an optional attribute axis and works such that: output[i][j][k] = data[index[i][j][k]][j][k] if axis = 0 and r = 3 output[i][j][k] = data[i][index[i][j][k]][k] if axis = 1 and r = 3 output[i][j][k] = data[i][j][index[i][j][k]] if axis = 2 and r = 3. More...  class cv::dnn::GatherLayer  Gather layer. More...  class cv::dnn::GeluApproximationLayer  class cv::dnn::GeluLayer  class cv::dnn::GemmLayer  class cv::dnn::GroupNormLayer  class cv::dnn::GRULayer  GRU recurrent one-layer. More...  class cv::dnn::HardSigmoidLayer  class cv::dnn::HardSwishLayer  class cv::dnn::InnerProductLayer  class cv::dnn::InnerProductLayerInt8  class cv::dnn::InstanceNormLayer  class cv::dnn::InterpLayer  Bilinear resize layer from https://github.com/cdmh/deeplab-public-ver2. More...  class cv::dnn::LayerNormLayer  class cv::dnn::LogLayer  class cv::dnn::LRNLayer  class cv::dnn::LSTMLayer  LSTM recurrent layer. More...  class cv::dnn::MatMulLayer  class cv::dnn::MaxUnpoolLayer  class cv::dnn::MishLayer  class cv::dnn::MVNLayer  class cv::dnn::NaryEltwiseLayer  class cv::dnn::NormalizeBBoxLayer  \( L_p \) - normalization layer. More...  class cv::dnn::NotLayer  class cv::dnn::PaddingLayer  Adds extra values for specific axes. More...  class cv::dnn::PermuteLayer  class cv::dnn::PoolingLayer  class cv::dnn::PoolingLayerInt8  class cv::dnn::PowerLayer  class cv::dnn::PriorBoxLayer  class cv::dnn::ProposalLayer  class cv::dnn::QuantizeLayer  class cv::dnn::ReciprocalLayer  class cv::dnn::ReduceLayer  class cv::dnn::RegionLayer  class cv::dnn::ReLU6Layer  class cv::dnn::ReLULayer  class cv::dnn::ReorgLayer  class cv::dnn::RequantizeLayer  class cv::dnn::ReshapeLayer  class cv::dnn::ResizeLayer  Resize input 4-dimensional blob by nearest neighbor or bilinear strategy. More...  class cv::dnn::RNNLayer  Classical recurrent layer. More...  class cv::dnn::RoundLayer  class cv::dnn::ScaleLayer  class cv::dnn::ScaleLayerInt8  class cv::dnn::ScatterLayer  class cv::dnn::ScatterNDLayer  class cv::dnn::SeluLayer  class cv::dnn::ShiftLayer  class cv::dnn::ShiftLayerInt8  class cv::dnn::ShrinkLayer  class cv::dnn::ShuffleChannelLayer  class cv::dnn::SigmoidLayer  class cv::dnn::SignLayer  class cv::dnn::SinhLayer  class cv::dnn::SinLayer  class cv::dnn::SliceLayer  class cv::dnn::SoftmaxLayer  class cv::dnn::SoftmaxLayerInt8  class cv::dnn::SoftplusLayer  class cv::dnn::SoftsignLayer  class cv::dnn::SpaceToDepthLayer  class cv::dnn::SplitLayer  class cv::dnn::SqrtLayer  class cv::dnn::SwishLayer  class cv::dnn::TanHLayer  class cv::dnn::TanLayer  class cv::dnn::ThresholdedReluLayer  class cv::dnn::TileLayer  class cv::dnn::TopKLayer 
dnn,This class represents high-level API for text detection DL networks compatible with EAST model.
dnn,Configurable parameters:
dnn,"(float) confThreshold - used to filter boxes by confidences, default: 0.5f (float) nmsThreshold - used in non maximum suppression, default: 0.0f"
dnn,"This class implements name-value dictionary, values are instances of DictValue."
dnn,Processing params of image to blob.
dnn,It includes all possible image processing operations and corresponding parameters.
dnn,This class represents high-level API for segmentation models.
dnn,"SegmentationModel allows to set params for preprocessing input image. SegmentationModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and returns the class prediction for each pixel."
dnn,This class represents high-level API for text detection DL networks compatible with DB model.
dnn,"Related publications: [167] Paper: https://arxiv.org/abs/1911.08947 For more information about the hyper-parameters setting, please refer to https://github.com/MhLiao/DB"
dnn,Configurable parameters:
dnn,"(float) binaryThreshold - The threshold of the binary map. It is usually set to 0.3. (float) polygonThreshold - The threshold of text polygons. It is usually set to 0.5, 0.6, and 0.7. Default is 0.5f (double) unclipRatio - The unclip ratio of the detected text region, which determines the output size. It is usually set to 2.0. (int) maxCandidates - The max number of the output results."
dnn,This class allows to create and manipulate comprehensive artificial neural networks.
dnn,"Neural network is presented as directed acyclic graph (DAG), where vertices are Layer instances, and edges specify relationships between layers inputs and outputs."
dnn,Each network layer has unique integer id and unique string name inside its network. LayerId can store either layer name or layer id.
dnn,"This class supports reference counting of its instances, i. e. copies point to the same instance."
dnn,This class provides all data needed to initialize layer.
dnn,"It includes dictionary with scalar params (which can be read by using Dict interface), blob params blobs and optional meta information: name and type of layer instance."
dnn,This class represents high-level API for keypoints models.
dnn,"KeypointsModel allows to set params for preprocessing input image. KeypointsModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and returns the x and y coordinates of each detected keypoint"
dnn,This class represents high-level API for text recognition networks.
dnn,"TextRecognitionModel allows to set params for preprocessing input image. TextRecognitionModel creates net from file with trained weights and config, sets preprocessing input, runs forward pass and return recognition result. For TextRecognitionModel, CRNN-CTC is supported."
dnn,Derivatives of this class wraps cv::Mat for different backends and targets.
dnn,Classes class cv::dnn::LayerFactory  Layer factory allows to create instances of registered layers. More... 
features2d,"Enumerations enum struct cv::DrawMatchesFlags { cv::DrawMatchesFlags::DEFAULT = 0 , cv::DrawMatchesFlags::DRAW_OVER_OUTIMG = 1 , cv::DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS = 2 , cv::DrawMatchesFlags::DRAW_RICH_KEYPOINTS = 4 } "
features2d,"Classes struct cv::Accumulator< T >  struct cv::Accumulator< char >  struct cv::Accumulator< short >  struct cv::Accumulator< unsigned char >  struct cv::Accumulator< unsigned short >  class cv::AffineFeature  Class for implementing the wrapper which makes detectors and extractors to be affine invariant, described as ASIFT in [312] . More...  class cv::AgastFeatureDetector  Wrapping class for feature detection using the AGAST method. : More...  class cv::AKAZE  Class implementing the AKAZE keypoint detector and descriptor extractor, described in [10]. More...  class cv::BRISK  Class implementing the BRISK keypoint detector and descriptor extractor, described in [159] . More...  class cv::FastFeatureDetector  Wrapping class for feature detection using the FAST method. : More...  class cv::Feature2D  Abstract base class for 2D image feature detectors and descriptor extractors. More...  class cv::GFTTDetector  Wrapping class for feature detection using the goodFeaturesToTrack function. : More...  class cv::KAZE  Class implementing the KAZE keypoint detector and descriptor extractor, described in [9] . More...  class cv::KeyPointsFilter  A class filters a vector of keypoints. More...  struct cv::L1< T >  struct cv::L2< T >  class cv::MSER  Maximally stable extremal region extractor. More...  class cv::ORB  Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. More...  class cv::SIFT  Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe [174] . More...  class cv::SimpleBlobDetector  Class for extracting blobs from an image. : More...  struct cv::SL2< T > "
features2d,Matchers of keypoint descriptors in OpenCV have wrappers with a common interface that enables you to easily switch between different algorithms solving the same problem. This section is devoted to matching descriptors that are represented as vectors in a multidimensional space. All objects that implement vector descriptor matchers inherit the DescriptorMatcher interface.
features2d,Classes class cv::BFMatcher  Brute-force descriptor matcher. More...  class cv::DescriptorMatcher  Abstract base class for matching keypoint descriptors. More...  class cv::FlannBasedMatcher  Flann-based descriptor matcher. More... 
features2d,This section describes approaches based on local 2D features and used to categorize objects.
features2d,Classes class cv::BOWImgDescriptorExtractor  Class to compute an image descriptor using the bag of visual words. More...  class cv::BOWKMeansTrainer  kmeans -based class to train visual vocabulary using the bag of visual words approach. : More...  class cv::BOWTrainer  Abstract base class for training the bag of visual words vocabulary from a set of descriptors. More... 
flann,The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built.
flann,Distance functor specifies the metric to be used to calculate the distance between two points. There are several Distance functors that are readily available:
flann,"cv::cvflann::L2_Simple - Squared Euclidean distance functor. This is the simpler, unrolled version. This is preferable for very low dimensionality data (eg 3D points)"
flann,"cv::flann::L2 - Squared Euclidean distance functor, optimized version."
flann,"cv::flann::L1 - Manhattan distance functor, optimized version."
flann,cv::flann::MinkowskiDistance - The Minkowski distance functor. This is highly optimised with loop unrolling. The computation of squared root at the end is omitted for efficiency.
flann,"cv::flann::MaxDistance - The max distance functor. It computes the maximum distance between two vectors. This distance is not a valid kdtree distance, it's not dimensionwise additive."
flann,cv::flann::HammingLUT - Hamming distance functor. It counts the bit differences between two strings using a lookup table implementation.
flann,"cv::flann::Hamming - Hamming distance functor. Population count is performed using library calls, if available. Lookup table implementation is used as a fallback."
flann,cv::flann::Hamming2 - Hamming distance functor. Population count is implemented in 12 arithmetic operations (one of which is multiplication).
flann,"cv::flann::DNAmmingLUT - Adaptation of the Hamming distance functor to DNA comparison. As the four bases A, C, G, T of the DNA (or A, G, C, U for RNA) can be coded on 2 bits, it counts the bits pairs differences between two sequences using a lookup table implementation."
flann,cv::flann::DNAmming2 - Adaptation of the Hamming distance functor to DNA comparison. Bases differences count are vectorised thanks to arithmetic operations using standard registers (AVX2 and AVX-512 should come in a near future).
flann,cv::flann::HistIntersectionDistance - The histogram intersection distance functor.
flann,cv::flann::HellingerDistance - The Hellinger distance functor.
flann,cv::flann::ChiSquareDistance - The chi-square distance functor.
flann,cv::flann::KL_Divergence - The Kullback-Leibler divergence functor.
flann,"Although the provided implementations cover a vast range of cases, it is also possible to use a custom implementation. The distance functor is a class whose operator() computes the distance between two features. If the distance is also a kd-tree compatible distance, it should also provide an accum_dist() method that computes the distance between individual feature dimensions."
flann,"In addition to operator() and accum_dist(), a distance functor should also define the ElementType and the ResultType as the types of the elements it operates on and the type of the result it computes. If a distance functor can be used as a kd-tree distance (meaning that the full distance between a pair of features can be accumulated from the partial distances between the individual dimensions) a typedef is_kdtree_distance should be present inside the distance functor. If the distance is not a kd-tree distance, but it's a distance in a vector space (the individual dimensions of the elements it operates on can be accessed independently) a typedef is_vector_space_distance should be defined inside the functor. If neither typedef is defined, the distance is assumed to be a metric distance and will only be used with indexes operating on generic metric distances."
flann,This section documents OpenCV's interface to the FLANN library. FLANN (Fast Library for Approximate Nearest Neighbors) is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional features. More information about FLANN can be found in [201] .
flann,Classes struct cv::flann::CvType< T >  struct cv::flann::CvType< char >  struct cv::flann::CvType< double >  struct cv::flann::CvType< float >  struct cv::flann::CvType< short >  struct cv::flann::CvType< unsigned char >  struct cv::flann::CvType< unsigned short >  class cv::flann::GenericIndex< Distance >  The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built. More... 
gapi,"Functions GMat cv::gapi::BackgroundSubtractor (const GMat &src, const cv::gapi::video::BackgroundSubtractorParams &bsParams)  Gaussian Mixture-based or K-nearest neighbours-based Background/Foreground Segmentation Algorithm. The operation generates a foreground mask.  std::tuple< GArray< GMat >, GScalar > cv::gapi::buildOpticalFlowPyramid (const GMat &img, const Size &winSize, const GScalar &maxLevel, bool withDerivatives=true, int pyrBorder=BORDER_REFLECT_101, int derivBorder=BORDER_CONSTANT, bool tryReuseInputImage=true)  Constructs the image pyramid which can be passed to calcOpticalFlowPyrLK.  std::tuple< GArray< Point2f >, GArray< uchar >, GArray< float > > cv::gapi::calcOpticalFlowPyrLK (const GArray< GMat > &prevPyr, const GArray< GMat > &nextPyr, const GArray< Point2f > &prevPts, const GArray< Point2f > &predPts, const Size &winSize=Size(21, 21), const GScalar &maxLevel=3, const TermCriteria &criteria=TermCriteria(TermCriteria::COUNT|TermCriteria::EPS, 30, 0.01), int flags=0, double minEigThresh=1e-4)  std::tuple< GArray< Point2f >, GArray< uchar >, GArray< float > > cv::gapi::calcOpticalFlowPyrLK (const GMat &prevImg, const GMat &nextImg, const GArray< Point2f > &prevPts, const GArray< Point2f > &predPts, const Size &winSize=Size(21, 21), const GScalar &maxLevel=3, const TermCriteria &criteria=TermCriteria(TermCriteria::COUNT|TermCriteria::EPS, 30, 0.01), int flags=0, double minEigThresh=1e-4)  Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with pyramids.  GMat cv::gapi::KalmanFilter (const GMat &measurement, const GOpaque< bool > &haveMeasurement, const cv::gapi::KalmanParams &kfParams)  GMat cv::gapi::KalmanFilter (const GMat &measurement, const GOpaque< bool > &haveMeasurement, const GMat &control, const cv::gapi::KalmanParams &kfParams)  Standard Kalman filter algorithm http://en.wikipedia.org/wiki/Kalman_filter. "
gapi,"Functions GMat cv::gapi::Canny (const GMat &image, double threshold1, double threshold2, int apertureSize=3, bool L2gradient=false)  Finds edges in an image using the Canny algorithm.  GArray< Point2f > cv::gapi::goodFeaturesToTrack (const GMat &image, int maxCorners, double qualityLevel, double minDistance, const Mat &mask=Mat(), int blockSize=3, bool useHarrisDetector=false, double k=0.04)  Determines strong corners on an image. "
gapi,gapi_colorconvert
gapi,"Functions GMat cv::gapi::concatHor (const GMat &src1, const GMat &src2)  Applies horizontal concatenation to given matrices.  GMat cv::gapi::concatHor (const std::vector< GMat > &v)  GMat cv::gapi::concatVert (const GMat &src1, const GMat &src2)  Applies vertical concatenation to given matrices.  GMat cv::gapi::concatVert (const std::vector< GMat > &v)  GMat cv::gapi::convertTo (const GMat &src, int rdepth, double alpha=1, double beta=0)  Converts a matrix to another data depth with optional scaling.  GFrame cv::gapi::copy (const GFrame &in)  Makes a copy of the input frame. Note that this copy may be not real (no actual data copied). Use this function to maintain graph contracts, e.g when graph's input needs to be passed directly to output, like in Streaming mode.  GMat cv::gapi::copy (const GMat &in)  Makes a copy of the input image. Note that this copy may be not real (no actual data copied). Use this function to maintain graph contracts, e.g when graph's input needs to be passed directly to output, like in Streaming mode.  GMat cv::gapi::crop (const GMat &src, const Rect &rect)  Crops a 2D matrix.  GMat cv::gapi::flip (const GMat &src, int flipCode)  Flips a 2D matrix around vertical, horizontal, or both axes.  GMat cv::gapi::LUT (const GMat &src, const Mat &lut)  Performs a look-up table transform of a matrix.  GMat cv::gapi::merge3 (const GMat &src1, const GMat &src2, const GMat &src3)  Creates one 3-channel matrix out of 3 single-channel ones.  GMat cv::gapi::merge4 (const GMat &src1, const GMat &src2, const GMat &src3, const GMat &src4)  Creates one 4-channel matrix out of 4 single-channel ones.  GMat cv::gapi::normalize (const GMat &src, double alpha, double beta, int norm_type, int ddepth=-1)  Normalizes the norm or value range of an array.  GMat cv::gapi::remap (const GMat &src, const Mat &map1, const Mat &map2, int interpolation, int borderMode=BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies a generic geometrical transformation to an image.  GMat cv::gapi::resize (const GMat &src, const Size &dsize, double fx=0, double fy=0, int interpolation=INTER_LINEAR)  Resizes an image.  GMatP cv::gapi::resizeP (const GMatP &src, const Size &dsize, int interpolation=cv::INTER_LINEAR)  Resizes a planar image.  std::tuple< GMat, GMat, GMat > cv::gapi::split3 (const GMat &src)  Divides a 3-channel matrix into 3 single-channel matrices.  std::tuple< GMat, GMat, GMat, GMat > cv::gapi::split4 (const GMat &src)  Divides a 4-channel matrix into 4 single-channel matrices.  GMat cv::gapi::warpAffine (const GMat &src, const Mat &M, const Size &dsize, int flags=cv::INTER_LINEAR, int borderMode=cv::BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies an affine transformation to an image.  GMat cv::gapi::warpPerspective (const GMat &src, const Mat &M, const Size &dsize, int flags=cv::INTER_LINEAR, int borderMode=cv::BORDER_CONSTANT, const Scalar &borderValue=Scalar())  Applies a perspective transformation to an image. "
gapi,"Functions GMat cv::gapi::bilateralFilter (const GMat &src, int d, double sigmaColor, double sigmaSpace, int borderType=BORDER_DEFAULT)  Applies the bilateral filter to an image.  GMat cv::gapi::blur (const GMat &src, const Size &ksize, const Point &anchor=Point(-1,-1), int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Blurs an image using the normalized box filter.  GMat cv::gapi::boxFilter (const GMat &src, int dtype, const Size &ksize, const Point &anchor=Point(-1,-1), bool normalize=true, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Blurs an image using the box filter.  GMat cv::gapi::dilate (const GMat &src, const Mat &kernel, const Point &anchor=Point(-1,-1), int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Dilates an image by using a specific structuring element.  GMat cv::gapi::dilate3x3 (const GMat &src, int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Dilates an image by using 3 by 3 rectangular structuring element.  GMat cv::gapi::erode (const GMat &src, const Mat &kernel, const Point &anchor=Point(-1,-1), int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Erodes an image by using a specific structuring element.  GMat cv::gapi::erode3x3 (const GMat &src, int iterations=1, int borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Erodes an image by using 3 by 3 rectangular structuring element.  GMat cv::gapi::filter2D (const GMat &src, int ddepth, const Mat &kernel, const Point &anchor=Point(-1,-1), const Scalar &delta=Scalar(0), int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Convolves an image with the kernel.  GMat cv::gapi::gaussianBlur (const GMat &src, const Size &ksize, double sigmaX, double sigmaY=0, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Blurs an image using a Gaussian filter.  GMat cv::gapi::Laplacian (const GMat &src, int ddepth, int ksize=1, double scale=1, double delta=0, int borderType=BORDER_DEFAULT)  Calculates the Laplacian of an image.  GMat cv::gapi::medianBlur (const GMat &src, int ksize)  Blurs an image using the median filter.  GMat cv::gapi::morphologyEx (const GMat &src, const MorphTypes op, const Mat &kernel, const Point &anchor=Point(-1,-1), const int iterations=1, const BorderTypes borderType=BORDER_CONSTANT, const Scalar &borderValue=morphologyDefaultBorderValue())  Performs advanced morphological transformations.  GMat cv::gapi::sepFilter (const GMat &src, int ddepth, const Mat &kernelX, const Mat &kernelY, const Point &anchor, const Scalar &delta, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Applies a separable linear filter to a matrix(image).  GMat cv::gapi::Sobel (const GMat &src, int ddepth, int dx, int dy, int ksize=3, double scale=1, double delta=0, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.  std::tuple< GMat, GMat > cv::gapi::SobelXY (const GMat &src, int ddepth, int order, int ksize=3, double scale=1, double delta=0, int borderType=BORDER_DEFAULT, const Scalar &borderValue=Scalar(0))  Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator. "
gapi,"Functions GMat cv::gapi::BayerGR2RGB (const GMat &src_gr)  Converts an image from BayerGR color space to RGB. The function converts an input image from BayerGR color space to RGB. The conventional ranges for G, R, and B channel values are 0 to 255.  GMat cv::gapi::BGR2Gray (const GMat &src)  Converts an image from BGR color space to gray-scaled.  GMat cv::gapi::BGR2I420 (const GMat &src)  Converts an image from BGR color space to I420 color space.  GMat cv::gapi::BGR2LUV (const GMat &src)  Converts an image from BGR color space to LUV color space.  GMat cv::gapi::BGR2RGB (const GMat &src)  Converts an image from BGR color space to RGB color space.  GMat cv::gapi::BGR2YUV (const GMat &src)  Converts an image from BGR color space to YUV color space.  GMat cv::gapi::I4202BGR (const GMat &src)  Converts an image from I420 color space to BGR color space.  GMat cv::gapi::I4202RGB (const GMat &src)  Converts an image from I420 color space to BGR color space.  GMat cv::gapi::LUV2BGR (const GMat &src)  Converts an image from LUV color space to BGR color space.  GMat cv::gapi::NV12toBGR (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to BGR. The function converts an input image from NV12 color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMatP cv::gapi::NV12toBGRp (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to BGR. The function converts an input image from NV12 color space to BGR. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMat cv::gapi::NV12toGray (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to gray-scaled. The function converts an input image from NV12 color space to gray-scaled. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMat cv::gapi::NV12toRGB (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to RGB. The function converts an input image from NV12 color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMatP cv::gapi::NV12toRGBp (const GMat &src_y, const GMat &src_uv)  Converts an image from NV12 (YUV420p) color space to RGB. The function converts an input image from NV12 color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255.  GMat cv::gapi::RGB2Gray (const GMat &src)  Converts an image from RGB color space to gray-scaled.  GMat cv::gapi::RGB2Gray (const GMat &src, float rY, float gY, float bY)  GMat cv::gapi::RGB2HSV (const GMat &src)  Converts an image from RGB color space to HSV. The function converts an input image from RGB color space to HSV. The conventional ranges for R, G, and B channel values are 0 to 255.  GMat cv::gapi::RGB2I420 (const GMat &src)  Converts an image from RGB color space to I420 color space.  GMat cv::gapi::RGB2Lab (const GMat &src)  Converts an image from RGB color space to Lab color space.  GMat cv::gapi::RGB2YUV (const GMat &src)  Converts an image from RGB color space to YUV color space.  GMat cv::gapi::RGB2YUV422 (const GMat &src)  Converts an image from RGB color space to YUV422. The function converts an input image from RGB color space to YUV422. The conventional ranges for R, G, and B channel values are 0 to 255.  GMat cv::gapi::YUV2BGR (const GMat &src)  Converts an image from YUV color space to BGR color space.  GMat cv::gapi::YUV2RGB (const GMat &src)  Converts an image from YUV color space to RGB. The function converts an input image from YUV color space to RGB. The conventional ranges for Y, U, and V channel values are 0 to 255. "
gapi,Functions for in-graph drawing.
gapi,"G-API can do some in-graph drawing with a generic operations and a set of rendering primitives. In contrast with traditional OpenCV, in G-API user need to form a rendering list of primitives to draw. This list can be built manually or generated within a graph. This list is passed to special operations or functions where all primitives are interpreted and applied to the image."
gapi,"For example, in a complex pipeline a list of detected objects can be translated in-graph to a list of cv::gapi::wip::draw::Rect primitives to highlight those with bounding boxes, or a list of detected faces can be translated in-graph to a list of cv::gapi::wip::draw::Mosaic primitives to hide sensitive content or protect privacy."
gapi,"Like any other operations, rendering in G-API can be reimplemented by different backends. Currently only an OpenCV-based backend is available."
gapi,"In addition to the graph-level operations, there are also regular (immediate) OpenCV-like functions are available see cv::gapi::wip::draw::render(). These functions are just wrappers over regular G-API and build the rendering graphs on the fly, so take compilation arguments as parameters."
gapi,"Currently this API is more machine-oriented than human-oriented. The main purpose is to translate a set of domain-specific objects to a list of primitives to draw. For example, in order to generate a picture like this:"
gapi,Rendering list needs to be generated as follows:
highgui,"Enumerations enum cv::MouseEventFlags { cv::EVENT_FLAG_LBUTTON = 1 , cv::EVENT_FLAG_RBUTTON = 2 , cv::EVENT_FLAG_MBUTTON = 4 , cv::EVENT_FLAG_CTRLKEY = 8 , cv::EVENT_FLAG_SHIFTKEY = 16 , cv::EVENT_FLAG_ALTKEY = 32 }  Mouse Event Flags see cv::MouseCallback. More...  enum cv::MouseEventTypes { cv::EVENT_MOUSEMOVE = 0 , cv::EVENT_LBUTTONDOWN = 1 , cv::EVENT_RBUTTONDOWN = 2 , cv::EVENT_MBUTTONDOWN = 3 , cv::EVENT_LBUTTONUP = 4 , cv::EVENT_RBUTTONUP = 5 , cv::EVENT_MBUTTONUP = 6 , cv::EVENT_LBUTTONDBLCLK = 7 , cv::EVENT_RBUTTONDBLCLK = 8 , cv::EVENT_MBUTTONDBLCLK = 9 , cv::EVENT_MOUSEWHEEL = 10 , cv::EVENT_MOUSEHWHEEL = 11 }  Mouse Events see cv::MouseCallback. More...  enum cv::WindowFlags { cv::WINDOW_NORMAL = 0x00000000 , cv::WINDOW_AUTOSIZE = 0x00000001 , cv::WINDOW_OPENGL = 0x00001000 , cv::WINDOW_FULLSCREEN = 1 , cv::WINDOW_FREERATIO = 0x00000100 , cv::WINDOW_KEEPRATIO = 0x00000000 , cv::WINDOW_GUI_EXPANDED =0x00000000 , cv::WINDOW_GUI_NORMAL = 0x00000010 }  Flags for cv::namedWindow. More...  enum cv::WindowPropertyFlags { cv::WND_PROP_FULLSCREEN = 0 , cv::WND_PROP_AUTOSIZE = 1 , cv::WND_PROP_ASPECT_RATIO = 2 , cv::WND_PROP_OPENGL = 3 , cv::WND_PROP_VISIBLE = 4 , cv::WND_PROP_TOPMOST = 5 , cv::WND_PROP_VSYNC = 6 }  Flags for cv::setWindowProperty / cv::getWindowProperty. More... "
highgui,"This figure explains new functionality implemented with WinRT GUI. The new GUI provides an Image control, and a slider panel. Slider panel holds trackbars attached to it."
highgui,Sliders are attached below the image control. Every new slider is added below the previous one.
highgui,See below the example used to generate the figure:
highgui,Functions void cv::winrt_initContainer (::Windows::UI::Xaml::Controls::Panel^ container)  Initializes container component that will be used to hold generated window content. 
highgui,"While OpenCV was designed for use in full-scale applications and can be used within functionally rich UI frameworks (such as Qt*, WinForms*, or Cocoa*) or without any UI at all, sometimes there it is required to try functionality quickly and visualize the results. This is what the HighGUI module has been designed for."
highgui,It provides easy interface to:
highgui,"Create and manipulate windows that can display images and ""remember"" their content (no need to handle repaint events from OS). Add trackbars to the windows, handle simple mouse events as well as keyboard commands."
highgui,"Typedefs typedef void(* cv::ButtonCallback) (int state, void *userdata)  Callback function for a button created by cv::createButton.  typedef void(* cv::MouseCallback) (int event, int x, int y, int flags, void *userdata)  Callback function for mouse events. see cv::setMouseCallback.  typedef void(* cv::OpenGlDrawCallback) (void *userdata)  Callback function defined to be called every frame. See cv::setOpenGlDrawCallback.  typedef void(* cv::TrackbarCallback) (int pos, void *userdata)  Callback function for Trackbar see cv::createTrackbar. "
highgui,"This figure explains new functionality implemented with Qt* GUI. The new GUI provides a statusbar, a toolbar, and a control panel. The control panel can have trackbars and buttonbars attached to it. If you cannot see the control panel, press Ctrl+P or right-click any Qt window and select Display properties window."
highgui,"To attach a trackbar, the window name parameter must be NULL. To attach a buttonbar, a button must be created. If the last bar attached to the control panel is a buttonbar, the new button is added to the right of the last button. If the last bar attached to the control panel is a trackbar, or the control panel is empty, a new buttonbar is created. Then, a new button is attached to it."
highgui,See below the example used to generate the figure:
highgui,Classes struct cv::QtFont  QtFont available only for Qt. See cv::fontQt. More... 
highgui,"Functions void cv::imshow (const String &winname, const ogl::Texture2D &tex)  Displays OpenGL 2D texture in the specified window.  void cv::setOpenGlContext (const String &winname)  Sets the specified window as current OpenGL context.  void cv::setOpenGlDrawCallback (const String &winname, OpenGlDrawCallback onOpenGlDraw, void *userdata=0)  Sets a callback function to be called to draw on top of displayed image.  void cv::updateWindow (const String &winname)  Force window to redraw its context and call draw callback ( See cv::setOpenGlDrawCallback ). "
imgcodecs,"Functions void CGImageToMat (const CGImageRef image, cv::Mat &m, bool alphaExist=false)  CGImageRef MatToCGImage (const cv::Mat &image) CF_RETURNS_RETAINED  UIImage * MatToUIImage (const cv::Mat &image)  void UIImageToMat (const UIImage *image, cv::Mat &m, bool alphaExist=false) "
imgcodecs,Classes class cv::ImageCollection  To read multi-page images on demand. More... 
imgcodecs,"Enumerations enum cv::ImreadModes { cv::IMREAD_UNCHANGED = -1 , cv::IMREAD_GRAYSCALE = 0 , cv::IMREAD_COLOR_BGR = 1 , cv::IMREAD_COLOR = 1 , cv::IMREAD_ANYDEPTH = 2 , cv::IMREAD_ANYCOLOR = 4 , cv::IMREAD_LOAD_GDAL = 8 , cv::IMREAD_REDUCED_GRAYSCALE_2 = 16 , cv::IMREAD_REDUCED_COLOR_2 = 17 , cv::IMREAD_REDUCED_GRAYSCALE_4 = 32 , cv::IMREAD_REDUCED_COLOR_4 = 33 , cv::IMREAD_REDUCED_GRAYSCALE_8 = 64 , cv::IMREAD_REDUCED_COLOR_8 = 65 , cv::IMREAD_IGNORE_ORIENTATION = 128 , cv::IMREAD_COLOR_RGB = 256 }  Imread flags. More...  enum cv::ImwriteEXRCompressionFlags { cv::IMWRITE_EXR_COMPRESSION_NO = 0 , cv::IMWRITE_EXR_COMPRESSION_RLE = 1 , cv::IMWRITE_EXR_COMPRESSION_ZIPS = 2 , cv::IMWRITE_EXR_COMPRESSION_ZIP = 3 , cv::IMWRITE_EXR_COMPRESSION_PIZ = 4 , cv::IMWRITE_EXR_COMPRESSION_PXR24 = 5 , cv::IMWRITE_EXR_COMPRESSION_B44 = 6 , cv::IMWRITE_EXR_COMPRESSION_B44A = 7 , cv::IMWRITE_EXR_COMPRESSION_DWAA = 8 , cv::IMWRITE_EXR_COMPRESSION_DWAB = 9 }  enum cv::ImwriteEXRTypeFlags { cv::IMWRITE_EXR_TYPE_HALF = 1 , cv::IMWRITE_EXR_TYPE_FLOAT = 2 }  enum cv::ImwriteFlags { cv::IMWRITE_JPEG_QUALITY = 1 , cv::IMWRITE_JPEG_PROGRESSIVE = 2 , cv::IMWRITE_JPEG_OPTIMIZE = 3 , cv::IMWRITE_JPEG_RST_INTERVAL = 4 , cv::IMWRITE_JPEG_LUMA_QUALITY = 5 , cv::IMWRITE_JPEG_CHROMA_QUALITY = 6 , cv::IMWRITE_JPEG_SAMPLING_FACTOR = 7 , cv::IMWRITE_PNG_COMPRESSION = 16 , cv::IMWRITE_PNG_STRATEGY = 17 , cv::IMWRITE_PNG_BILEVEL = 18 , cv::IMWRITE_PXM_BINARY = 32 , cv::IMWRITE_EXR_TYPE = (3 << 4) + 0 , cv::IMWRITE_EXR_COMPRESSION = (3 << 4) + 1 , cv::IMWRITE_EXR_DWA_COMPRESSION_LEVEL = (3 << 4) + 2 , cv::IMWRITE_WEBP_QUALITY = 64 , cv::IMWRITE_HDR_COMPRESSION = (5 << 4) + 0 , cv::IMWRITE_PAM_TUPLETYPE = 128 , cv::IMWRITE_TIFF_RESUNIT = 256 , cv::IMWRITE_TIFF_XDPI = 257 , cv::IMWRITE_TIFF_YDPI = 258 , cv::IMWRITE_TIFF_COMPRESSION = 259 , cv::IMWRITE_TIFF_ROWSPERSTRIP = 278 , cv::IMWRITE_TIFF_PREDICTOR = 317 , cv::IMWRITE_JPEG2000_COMPRESSION_X1000 = 272 , cv::IMWRITE_AVIF_QUALITY = 512 , cv::IMWRITE_AVIF_DEPTH = 513 , cv::IMWRITE_AVIF_SPEED = 514 }  Imwrite flags. More...  enum cv::ImwriteHDRCompressionFlags { cv::IMWRITE_HDR_COMPRESSION_NONE = 0 , cv::IMWRITE_HDR_COMPRESSION_RLE = 1 }  Imwrite HDR specific values for IMWRITE_HDR_COMPRESSION parameter key. More...  enum cv::ImwriteJPEGSamplingFactorParams { cv::IMWRITE_JPEG_SAMPLING_FACTOR_411 = 0x411111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_420 = 0x221111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_422 = 0x211111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_440 = 0x121111 , cv::IMWRITE_JPEG_SAMPLING_FACTOR_444 = 0x111111 }  enum cv::ImwritePAMFlags { cv::IMWRITE_PAM_FORMAT_NULL = 0 , cv::IMWRITE_PAM_FORMAT_BLACKANDWHITE = 1 , cv::IMWRITE_PAM_FORMAT_GRAYSCALE = 2 , cv::IMWRITE_PAM_FORMAT_GRAYSCALE_ALPHA = 3 , cv::IMWRITE_PAM_FORMAT_RGB = 4 , cv::IMWRITE_PAM_FORMAT_RGB_ALPHA = 5 }  Imwrite PAM specific tupletype flags used to define the 'TUPLETYPE' field of a PAM file. More...  enum cv::ImwritePNGFlags { cv::IMWRITE_PNG_STRATEGY_DEFAULT = 0 , cv::IMWRITE_PNG_STRATEGY_FILTERED = 1 , cv::IMWRITE_PNG_STRATEGY_HUFFMAN_ONLY = 2 , cv::IMWRITE_PNG_STRATEGY_RLE = 3 , cv::IMWRITE_PNG_STRATEGY_FIXED = 4 }  Imwrite PNG specific flags used to tune the compression algorithm. More...  enum cv::ImwriteTiffCompressionFlags { cv::IMWRITE_TIFF_COMPRESSION_NONE = 1 , cv::IMWRITE_TIFF_COMPRESSION_CCITTRLE = 2 , cv::IMWRITE_TIFF_COMPRESSION_CCITTFAX3 = 3 , cv::IMWRITE_TIFF_COMPRESSION_CCITT_T4 = 3 , cv::IMWRITE_TIFF_COMPRESSION_CCITTFAX4 = 4 , cv::IMWRITE_TIFF_COMPRESSION_CCITT_T6 = 4 , cv::IMWRITE_TIFF_COMPRESSION_LZW = 5 , cv::IMWRITE_TIFF_COMPRESSION_OJPEG = 6 , cv::IMWRITE_TIFF_COMPRESSION_JPEG = 7 , cv::IMWRITE_TIFF_COMPRESSION_T85 = 9 , cv::IMWRITE_TIFF_COMPRESSION_T43 = 10 , cv::IMWRITE_TIFF_COMPRESSION_NEXT = 32766 , cv::IMWRITE_TIFF_COMPRESSION_CCITTRLEW = 32771 , cv::IMWRITE_TIFF_COMPRESSION_PACKBITS = 32773 , cv::IMWRITE_TIFF_COMPRESSION_THUNDERSCAN = 32809 , cv::IMWRITE_TIFF_COMPRESSION_IT8CTPAD = 32895 , cv::IMWRITE_TIFF_COMPRESSION_IT8LW = 32896 , cv::IMWRITE_TIFF_COMPRESSION_IT8MP = 32897 , cv::IMWRITE_TIFF_COMPRESSION_IT8BL = 32898 , cv::IMWRITE_TIFF_COMPRESSION_PIXARFILM = 32908 , cv::IMWRITE_TIFF_COMPRESSION_PIXARLOG = 32909 , cv::IMWRITE_TIFF_COMPRESSION_DEFLATE = 32946 , cv::IMWRITE_TIFF_COMPRESSION_ADOBE_DEFLATE = 8 , cv::IMWRITE_TIFF_COMPRESSION_DCS = 32947 , cv::IMWRITE_TIFF_COMPRESSION_JBIG = 34661 , cv::IMWRITE_TIFF_COMPRESSION_SGILOG = 34676 , cv::IMWRITE_TIFF_COMPRESSION_SGILOG24 = 34677 , cv::IMWRITE_TIFF_COMPRESSION_JP2000 = 34712 , cv::IMWRITE_TIFF_COMPRESSION_LERC = 34887 , cv::IMWRITE_TIFF_COMPRESSION_LZMA = 34925 , cv::IMWRITE_TIFF_COMPRESSION_ZSTD = 50000 , cv::IMWRITE_TIFF_COMPRESSION_WEBP = 50001 , cv::IMWRITE_TIFF_COMPRESSION_JXL = 50002 }  enum cv::ImwriteTiffPredictorFlags { cv::IMWRITE_TIFF_PREDICTOR_NONE = 1 , cv::IMWRITE_TIFF_PREDICTOR_HORIZONTAL = 2 , cv::IMWRITE_TIFF_PREDICTOR_FLOATINGPOINT = 3 } "
imgcodecs,"Functions void CGImageToMat (const CGImageRef image, cv::Mat &m, bool alphaExist=false)  CGImageRef MatToCGImage (const cv::Mat &image) CF_RETURNS_RETAINED  NSImage * MatToNSImage (const cv::Mat &image)  void NSImageToMat (const NSImage *image, cv::Mat &m, bool alphaExist=false) "
imgproc,Classes class cv::segmentation::IntelligentScissorsMB  Intelligent Scissors image segmentation. More... 
imgproc,"The human perception isn't built for observing fine changes in grayscale images. Human eyes are more sensitive to observing changes between colors, so you often need to recolor your grayscale images to get a clue about them. OpenCV now comes with various colormaps to enhance the visualization in your computer vision application."
imgproc,"In OpenCV you only need applyColorMap to apply a colormap on a given image. The following sample code reads the path to an image from command line, applies a Jet colormap on it and shows the result:"
imgproc,"Enumerations enum cv::ColormapTypes { cv::COLORMAP_AUTUMN = 0 , cv::COLORMAP_BONE = 1 , cv::COLORMAP_JET = 2 , cv::COLORMAP_WINTER = 3 , cv::COLORMAP_RAINBOW = 4 , cv::COLORMAP_OCEAN = 5 , cv::COLORMAP_SUMMER = 6 , cv::COLORMAP_SPRING = 7 , cv::COLORMAP_COOL = 8 , cv::COLORMAP_HSV = 9 , cv::COLORMAP_PINK = 10 , cv::COLORMAP_HOT = 11 , cv::COLORMAP_PARULA = 12 , cv::COLORMAP_MAGMA = 13 , cv::COLORMAP_INFERNO = 14 , cv::COLORMAP_PLASMA = 15 , cv::COLORMAP_VIRIDIS = 16 , cv::COLORMAP_CIVIDIS = 17 , cv::COLORMAP_TWILIGHT = 18 , cv::COLORMAP_TWILIGHT_SHIFTED = 19 , cv::COLORMAP_TURBO = 20 , cv::COLORMAP_DEEPGREEN = 21 }  GNU Octave/MATLAB equivalent colormaps. More... "
imgproc,Namespaces namespace cv::traits 
imgproc,"Functions and classes described in this section are used to perform various linear or non-linear filtering operations on 2D images (represented as Mat's). It means that for each pixel location \((x,y)\) in the source image (normally, rectangular), its neighborhood is considered and used to compute the response. In case of a linear filter, it is a weighted sum of pixel values. In case of morphological operations, it is the minimum or maximum values, and so on. The computed response is stored in the destination image at the same location \((x,y)\). It means that the output image will be of the same size as the input image. Normally, the functions support multi-channel arrays, in which case every channel is processed independently. Therefore, the output image will also have the same number of channels as the input one."
imgproc,"Another common feature of the functions and classes described in this section is that, unlike simple arithmetic functions, they need to extrapolate values of some non-existing pixels. For example, if you want to smooth an image using a Gaussian \(3 \times 3\) filter, then, when processing the left-most pixels in each row, you need pixels to the left of them, that is, outside of the image. You can let these pixels be the same as the left-most image pixels (""replicated border"" extrapolation method), or assume that all the non-existing pixels are zeros (""constant border"" extrapolation method), and so on. OpenCV enables you to specify the extrapolation method. For details, see BorderTypes"
imgproc,"Drawing functions work with matrices/images of arbitrary depth. The boundaries of the shapes can be rendered with antialiasing (implemented only for 8-bit images for now). All the functions include the parameter color that uses an RGB value (that may be constructed with the Scalar constructor ) for color images and brightness for grayscale images. For color images, the channel ordering is normally Blue, Green, Red. This is what imshow, imread, and imwrite expect. So, if you form a color using the Scalar constructor, it should look like:"
imgproc,"\[\texttt{Scalar} (blue \_ component, green \_ component, red \_ component[, alpha \_ component])\]"
imgproc,"If you are using your own image rendering and I/O functions, you can use any channel ordering. The drawing functions process each channel independently and do not depend on the channel order or even on the used color space. The whole image can be converted from BGR to RGB or to a different color space using cvtColor ."
imgproc,"If a drawn figure is partially or completely outside the image, the drawing functions clip it. Also, many drawing functions can handle pixel coordinates specified with sub-pixel accuracy. This means that the coordinates can be passed as fixed-point numbers encoded as integers. The number of fractional bits is specified by the shift parameter and the real point coordinates are calculated as \(\texttt{Point}(x,y)\rightarrow\texttt{Point2f}(x*2^{-shift},y*2^{-shift})\) . This feature is especially effective when rendering antialiased shapes."
imgproc,Classes class cv::LineIterator  Class for iterating over all pixels on a raster line segment. More... 
imgproc,Classes class cv::CLAHE  Base class for Contrast Limited Adaptive Histogram Equalization. More... 
imgproc,"Enumerations enum cv::AdaptiveThresholdTypes { cv::ADAPTIVE_THRESH_MEAN_C = 0 , cv::ADAPTIVE_THRESH_GAUSSIAN_C = 1 }  enum cv::DistanceTransformLabelTypes { cv::DIST_LABEL_CCOMP = 0 , cv::DIST_LABEL_PIXEL = 1 }  distanceTransform algorithm flags More...  enum cv::DistanceTransformMasks { cv::DIST_MASK_3 = 3 , cv::DIST_MASK_5 = 5 , cv::DIST_MASK_PRECISE = 0 }  Mask size for distance transform. More...  enum cv::DistanceTypes { cv::DIST_USER = -1 , cv::DIST_L1 = 1 , cv::DIST_L2 = 2 , cv::DIST_C = 3 , cv::DIST_L12 = 4 , cv::DIST_FAIR = 5 , cv::DIST_WELSCH = 6 , cv::DIST_HUBER = 7 }  enum cv::FloodFillFlags { cv::FLOODFILL_FIXED_RANGE = 1 << 16 , cv::FLOODFILL_MASK_ONLY = 1 << 17 }  floodfill algorithm flags More...  enum cv::GrabCutClasses { cv::GC_BGD = 0 , cv::GC_FGD = 1 , cv::GC_PR_BGD = 2 , cv::GC_PR_FGD = 3 }  class of the pixel in GrabCut algorithm More...  enum cv::GrabCutModes { cv::GC_INIT_WITH_RECT = 0 , cv::GC_INIT_WITH_MASK = 1 , cv::GC_EVAL = 2 , cv::GC_EVAL_FREEZE_MODEL = 3 }  GrabCut algorithm flags. More...  enum cv::ThresholdTypes { cv::THRESH_BINARY = 0 , cv::THRESH_BINARY_INV = 1 , cv::THRESH_TRUNC = 2 , cv::THRESH_TOZERO = 3 , cv::THRESH_TOZERO_INV = 4 , cv::THRESH_MASK = 7 , cv::THRESH_OTSU = 8 , cv::THRESH_TRIANGLE = 16 } "
imgproc,This module includes image-processing functions.
imgproc,"Functions void cv::accumulate (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds an image to the accumulator image.  void cv::accumulateProduct (InputArray src1, InputArray src2, InputOutputArray dst, InputArray mask=noArray())  Adds the per-element product of two input images to the accumulator image.  void cv::accumulateSquare (InputArray src, InputOutputArray dst, InputArray mask=noArray())  Adds the square of a source image to the accumulator image.  void cv::accumulateWeighted (InputArray src, InputOutputArray dst, double alpha, InputArray mask=noArray())  Updates a running average.  void cv::createHanningWindow (OutputArray dst, Size winSize, int type)  This function computes a Hanning window coefficients in two dimensions.  void cv::divSpectrums (InputArray a, InputArray b, OutputArray c, int flags, bool conjB=false)  Performs the per-element division of the first Fourier spectrum by the second Fourier spectrum.  Point2d cv::phaseCorrelate (InputArray src1, InputArray src2, InputArray window=noArray(), double *response=0)  The function is used to detect translational shifts that occur between two images. "
imgproc,"Enumerations enum cv::ColorConversionCodes { cv::COLOR_BGR2BGRA = 0 , cv::COLOR_RGB2RGBA = COLOR_BGR2BGRA , cv::COLOR_BGRA2BGR = 1 , cv::COLOR_RGBA2RGB = COLOR_BGRA2BGR , cv::COLOR_BGR2RGBA = 2 , cv::COLOR_RGB2BGRA = COLOR_BGR2RGBA , cv::COLOR_RGBA2BGR = 3 , cv::COLOR_BGRA2RGB = COLOR_RGBA2BGR , cv::COLOR_BGR2RGB = 4 , cv::COLOR_RGB2BGR = COLOR_BGR2RGB , cv::COLOR_BGRA2RGBA = 5 , cv::COLOR_RGBA2BGRA = COLOR_BGRA2RGBA , cv::COLOR_BGR2GRAY = 6 , cv::COLOR_RGB2GRAY = 7 , cv::COLOR_GRAY2BGR = 8 , cv::COLOR_GRAY2RGB = COLOR_GRAY2BGR , cv::COLOR_GRAY2BGRA = 9 , cv::COLOR_GRAY2RGBA = COLOR_GRAY2BGRA , cv::COLOR_BGRA2GRAY = 10 , cv::COLOR_RGBA2GRAY = 11 , cv::COLOR_BGR2BGR565 = 12 , cv::COLOR_RGB2BGR565 = 13 , cv::COLOR_BGR5652BGR = 14 , cv::COLOR_BGR5652RGB = 15 , cv::COLOR_BGRA2BGR565 = 16 , cv::COLOR_RGBA2BGR565 = 17 , cv::COLOR_BGR5652BGRA = 18 , cv::COLOR_BGR5652RGBA = 19 , cv::COLOR_GRAY2BGR565 = 20 , cv::COLOR_BGR5652GRAY = 21 , cv::COLOR_BGR2BGR555 = 22 , cv::COLOR_RGB2BGR555 = 23 , cv::COLOR_BGR5552BGR = 24 , cv::COLOR_BGR5552RGB = 25 , cv::COLOR_BGRA2BGR555 = 26 , cv::COLOR_RGBA2BGR555 = 27 , cv::COLOR_BGR5552BGRA = 28 , cv::COLOR_BGR5552RGBA = 29 , cv::COLOR_GRAY2BGR555 = 30 , cv::COLOR_BGR5552GRAY = 31 , cv::COLOR_BGR2XYZ = 32 , cv::COLOR_RGB2XYZ = 33 , cv::COLOR_XYZ2BGR = 34 , cv::COLOR_XYZ2RGB = 35 , cv::COLOR_BGR2YCrCb = 36 , cv::COLOR_RGB2YCrCb = 37 , cv::COLOR_YCrCb2BGR = 38 , cv::COLOR_YCrCb2RGB = 39 , cv::COLOR_BGR2HSV = 40 , cv::COLOR_RGB2HSV = 41 , cv::COLOR_BGR2Lab = 44 , cv::COLOR_RGB2Lab = 45 , cv::COLOR_BGR2Luv = 50 , cv::COLOR_RGB2Luv = 51 , cv::COLOR_BGR2HLS = 52 , cv::COLOR_RGB2HLS = 53 , cv::COLOR_HSV2BGR = 54 , cv::COLOR_HSV2RGB = 55 , cv::COLOR_Lab2BGR = 56 , cv::COLOR_Lab2RGB = 57 , cv::COLOR_Luv2BGR = 58 , cv::COLOR_Luv2RGB = 59 , cv::COLOR_HLS2BGR = 60 , cv::COLOR_HLS2RGB = 61 , cv::COLOR_BGR2HSV_FULL = 66 , cv::COLOR_RGB2HSV_FULL = 67 , cv::COLOR_BGR2HLS_FULL = 68 , cv::COLOR_RGB2HLS_FULL = 69 , cv::COLOR_HSV2BGR_FULL = 70 , cv::COLOR_HSV2RGB_FULL = 71 , cv::COLOR_HLS2BGR_FULL = 72 , cv::COLOR_HLS2RGB_FULL = 73 , cv::COLOR_LBGR2Lab = 74 , cv::COLOR_LRGB2Lab = 75 , cv::COLOR_LBGR2Luv = 76 , cv::COLOR_LRGB2Luv = 77 , cv::COLOR_Lab2LBGR = 78 , cv::COLOR_Lab2LRGB = 79 , cv::COLOR_Luv2LBGR = 80 , cv::COLOR_Luv2LRGB = 81 , cv::COLOR_BGR2YUV = 82 , cv::COLOR_RGB2YUV = 83 , cv::COLOR_YUV2BGR = 84 , cv::COLOR_YUV2RGB = 85 , cv::COLOR_YUV2RGB_NV12 = 90 , cv::COLOR_YUV2BGR_NV12 = 91 , cv::COLOR_YUV2RGB_NV21 = 92 , cv::COLOR_YUV2BGR_NV21 = 93 , cv::COLOR_YUV420sp2RGB = COLOR_YUV2RGB_NV21 , cv::COLOR_YUV420sp2BGR = COLOR_YUV2BGR_NV21 , cv::COLOR_YUV2RGBA_NV12 = 94 , cv::COLOR_YUV2BGRA_NV12 = 95 , cv::COLOR_YUV2RGBA_NV21 = 96 , cv::COLOR_YUV2BGRA_NV21 = 97 , cv::COLOR_YUV420sp2RGBA = COLOR_YUV2RGBA_NV21 , cv::COLOR_YUV420sp2BGRA = COLOR_YUV2BGRA_NV21 , cv::COLOR_YUV2RGB_YV12 = 98 , cv::COLOR_YUV2BGR_YV12 = 99 , cv::COLOR_YUV2RGB_IYUV = 100 , cv::COLOR_YUV2BGR_IYUV = 101 , cv::COLOR_YUV2RGB_I420 = COLOR_YUV2RGB_IYUV , cv::COLOR_YUV2BGR_I420 = COLOR_YUV2BGR_IYUV , cv::COLOR_YUV420p2RGB = COLOR_YUV2RGB_YV12 , cv::COLOR_YUV420p2BGR = COLOR_YUV2BGR_YV12 , cv::COLOR_YUV2RGBA_YV12 = 102 , cv::COLOR_YUV2BGRA_YV12 = 103 , cv::COLOR_YUV2RGBA_IYUV = 104 , cv::COLOR_YUV2BGRA_IYUV = 105 , cv::COLOR_YUV2RGBA_I420 = COLOR_YUV2RGBA_IYUV , cv::COLOR_YUV2BGRA_I420 = COLOR_YUV2BGRA_IYUV , cv::COLOR_YUV420p2RGBA = COLOR_YUV2RGBA_YV12 , cv::COLOR_YUV420p2BGRA = COLOR_YUV2BGRA_YV12 , cv::COLOR_YUV2GRAY_420 = 106 , cv::COLOR_YUV2GRAY_NV21 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_NV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_YV12 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_IYUV = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2GRAY_I420 = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420sp2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV420p2GRAY = COLOR_YUV2GRAY_420 , cv::COLOR_YUV2RGB_UYVY = 107 , cv::COLOR_YUV2BGR_UYVY = 108 , cv::COLOR_YUV2RGB_Y422 = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_Y422 = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGB_UYNV = COLOR_YUV2RGB_UYVY , cv::COLOR_YUV2BGR_UYNV = COLOR_YUV2BGR_UYVY , cv::COLOR_YUV2RGBA_UYVY = 111 , cv::COLOR_YUV2BGRA_UYVY = 112 , cv::COLOR_YUV2RGBA_Y422 = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_Y422 = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGBA_UYNV = COLOR_YUV2RGBA_UYVY , cv::COLOR_YUV2BGRA_UYNV = COLOR_YUV2BGRA_UYVY , cv::COLOR_YUV2RGB_YUY2 = 115 , cv::COLOR_YUV2BGR_YUY2 = 116 , cv::COLOR_YUV2RGB_YVYU = 117 , cv::COLOR_YUV2BGR_YVYU = 118 , cv::COLOR_YUV2RGB_YUYV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUYV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGB_YUNV = COLOR_YUV2RGB_YUY2 , cv::COLOR_YUV2BGR_YUNV = COLOR_YUV2BGR_YUY2 , cv::COLOR_YUV2RGBA_YUY2 = 119 , cv::COLOR_YUV2BGRA_YUY2 = 120 , cv::COLOR_YUV2RGBA_YVYU = 121 , cv::COLOR_YUV2BGRA_YVYU = 122 , cv::COLOR_YUV2RGBA_YUYV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUYV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2RGBA_YUNV = COLOR_YUV2RGBA_YUY2 , cv::COLOR_YUV2BGRA_YUNV = COLOR_YUV2BGRA_YUY2 , cv::COLOR_YUV2GRAY_UYVY = 123 , cv::COLOR_YUV2GRAY_YUY2 = 124 , cv::COLOR_YUV2GRAY_Y422 = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_UYNV = COLOR_YUV2GRAY_UYVY , cv::COLOR_YUV2GRAY_YVYU = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUYV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_YUV2GRAY_YUNV = COLOR_YUV2GRAY_YUY2 , cv::COLOR_RGBA2mRGBA = 125 , cv::COLOR_mRGBA2RGBA = 126 , cv::COLOR_RGB2YUV_I420 = 127 , cv::COLOR_BGR2YUV_I420 = 128 , cv::COLOR_RGB2YUV_IYUV = COLOR_RGB2YUV_I420 , cv::COLOR_BGR2YUV_IYUV = COLOR_BGR2YUV_I420 , cv::COLOR_RGBA2YUV_I420 = 129 , cv::COLOR_BGRA2YUV_I420 = 130 , cv::COLOR_RGBA2YUV_IYUV = COLOR_RGBA2YUV_I420 , cv::COLOR_BGRA2YUV_IYUV = COLOR_BGRA2YUV_I420 , cv::COLOR_RGB2YUV_YV12 = 131 , cv::COLOR_BGR2YUV_YV12 = 132 , cv::COLOR_RGBA2YUV_YV12 = 133 , cv::COLOR_BGRA2YUV_YV12 = 134 , cv::COLOR_BayerBG2BGR = 46 , cv::COLOR_BayerGB2BGR = 47 , cv::COLOR_BayerRG2BGR = 48 , cv::COLOR_BayerGR2BGR = 49 , cv::COLOR_BayerRGGB2BGR = COLOR_BayerBG2BGR , cv::COLOR_BayerGRBG2BGR = COLOR_BayerGB2BGR , cv::COLOR_BayerBGGR2BGR = COLOR_BayerRG2BGR , cv::COLOR_BayerGBRG2BGR = COLOR_BayerGR2BGR , cv::COLOR_BayerRGGB2RGB = COLOR_BayerBGGR2BGR , cv::COLOR_BayerGRBG2RGB = COLOR_BayerGBRG2BGR , cv::COLOR_BayerBGGR2RGB = COLOR_BayerRGGB2BGR , cv::COLOR_BayerGBRG2RGB = COLOR_BayerGRBG2BGR , cv::COLOR_BayerBG2RGB = COLOR_BayerRG2BGR , cv::COLOR_BayerGB2RGB = COLOR_BayerGR2BGR , cv::COLOR_BayerRG2RGB = COLOR_BayerBG2BGR , cv::COLOR_BayerGR2RGB = COLOR_BayerGB2BGR , cv::COLOR_BayerBG2GRAY = 86 , cv::COLOR_BayerGB2GRAY = 87 , cv::COLOR_BayerRG2GRAY = 88 , cv::COLOR_BayerGR2GRAY = 89 , cv::COLOR_BayerRGGB2GRAY = COLOR_BayerBG2GRAY , cv::COLOR_BayerGRBG2GRAY = COLOR_BayerGB2GRAY , cv::COLOR_BayerBGGR2GRAY = COLOR_BayerRG2GRAY , cv::COLOR_BayerGBRG2GRAY = COLOR_BayerGR2GRAY , cv::COLOR_BayerBG2BGR_VNG = 62 , cv::COLOR_BayerGB2BGR_VNG = 63 , cv::COLOR_BayerRG2BGR_VNG = 64 , cv::COLOR_BayerGR2BGR_VNG = 65 , cv::COLOR_BayerRGGB2BGR_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGRBG2BGR_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBGGR2BGR_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGBRG2BGR_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRGGB2RGB_VNG = COLOR_BayerBGGR2BGR_VNG , cv::COLOR_BayerGRBG2RGB_VNG = COLOR_BayerGBRG2BGR_VNG , cv::COLOR_BayerBGGR2RGB_VNG = COLOR_BayerRGGB2BGR_VNG , cv::COLOR_BayerGBRG2RGB_VNG = COLOR_BayerGRBG2BGR_VNG , cv::COLOR_BayerBG2RGB_VNG = COLOR_BayerRG2BGR_VNG , cv::COLOR_BayerGB2RGB_VNG = COLOR_BayerGR2BGR_VNG , cv::COLOR_BayerRG2RGB_VNG = COLOR_BayerBG2BGR_VNG , cv::COLOR_BayerGR2RGB_VNG = COLOR_BayerGB2BGR_VNG , cv::COLOR_BayerBG2BGR_EA = 135 , cv::COLOR_BayerGB2BGR_EA = 136 , cv::COLOR_BayerRG2BGR_EA = 137 , cv::COLOR_BayerGR2BGR_EA = 138 , cv::COLOR_BayerRGGB2BGR_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGRBG2BGR_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBGGR2BGR_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGBRG2BGR_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRGGB2RGB_EA = COLOR_BayerBGGR2BGR_EA , cv::COLOR_BayerGRBG2RGB_EA = COLOR_BayerGBRG2BGR_EA , cv::COLOR_BayerBGGR2RGB_EA = COLOR_BayerRGGB2BGR_EA , cv::COLOR_BayerGBRG2RGB_EA = COLOR_BayerGRBG2BGR_EA , cv::COLOR_BayerBG2RGB_EA = COLOR_BayerRG2BGR_EA , cv::COLOR_BayerGB2RGB_EA = COLOR_BayerGR2BGR_EA , cv::COLOR_BayerRG2RGB_EA = COLOR_BayerBG2BGR_EA , cv::COLOR_BayerGR2RGB_EA = COLOR_BayerGB2BGR_EA , cv::COLOR_BayerBG2BGRA = 139 , cv::COLOR_BayerGB2BGRA = 140 , cv::COLOR_BayerRG2BGRA = 141 , cv::COLOR_BayerGR2BGRA = 142 , cv::COLOR_BayerRGGB2BGRA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGRBG2BGRA = COLOR_BayerGB2BGRA , cv::COLOR_BayerBGGR2BGRA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGBRG2BGRA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRGGB2RGBA = COLOR_BayerBGGR2BGRA , cv::COLOR_BayerGRBG2RGBA = COLOR_BayerGBRG2BGRA , cv::COLOR_BayerBGGR2RGBA = COLOR_BayerRGGB2BGRA , cv::COLOR_BayerGBRG2RGBA = COLOR_BayerGRBG2BGRA , cv::COLOR_BayerBG2RGBA = COLOR_BayerRG2BGRA , cv::COLOR_BayerGB2RGBA = COLOR_BayerGR2BGRA , cv::COLOR_BayerRG2RGBA = COLOR_BayerBG2BGRA , cv::COLOR_BayerGR2RGBA = COLOR_BayerGB2BGRA , cv::COLOR_RGB2YUV_UYVY = 143 , cv::COLOR_BGR2YUV_UYVY = 144 , cv::COLOR_RGB2YUV_Y422 = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_Y422 = COLOR_BGR2YUV_UYVY , cv::COLOR_RGB2YUV_UYNV = COLOR_RGB2YUV_UYVY , cv::COLOR_BGR2YUV_UYNV = COLOR_BGR2YUV_UYVY , cv::COLOR_RGBA2YUV_UYVY = 145 , cv::COLOR_BGRA2YUV_UYVY = 146 , cv::COLOR_RGBA2YUV_Y422 = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_Y422 = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGBA2YUV_UYNV = COLOR_RGBA2YUV_UYVY , cv::COLOR_BGRA2YUV_UYNV = COLOR_BGRA2YUV_UYVY , cv::COLOR_RGB2YUV_YUY2 = 147 , cv::COLOR_BGR2YUV_YUY2 = 148 , cv::COLOR_RGB2YUV_YVYU = 149 , cv::COLOR_BGR2YUV_YVYU = 150 , cv::COLOR_RGB2YUV_YUYV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUYV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGB2YUV_YUNV = COLOR_RGB2YUV_YUY2 , cv::COLOR_BGR2YUV_YUNV = COLOR_BGR2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUY2 = 151 , cv::COLOR_BGRA2YUV_YUY2 = 152 , cv::COLOR_RGBA2YUV_YVYU = 153 , cv::COLOR_BGRA2YUV_YVYU = 154 , cv::COLOR_RGBA2YUV_YUYV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUYV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_RGBA2YUV_YUNV = COLOR_RGBA2YUV_YUY2 , cv::COLOR_BGRA2YUV_YUNV = COLOR_BGRA2YUV_YUY2 , cv::COLOR_COLORCVT_MAX = 155 } "
imgproc,"The functions in this section perform various geometrical transformations of 2D images. They do not change the image content but deform the pixel grid and map this deformed grid to the destination image. In fact, to avoid sampling artifacts, the mapping is done in the reverse order, from destination to the source. That is, for each pixel \((x, y)\) of the destination image, the functions compute coordinates of the corresponding ""donor"" pixel in the source image and copy the pixel value:"
imgproc,"\[\texttt{dst} (x,y)= \texttt{src} (f_x(x,y), f_y(x,y))\]"
imgproc,"In case when you specify the forward mapping \(\left<g_x, g_y\right>: \texttt{src} \rightarrow \texttt{dst}\), the OpenCV functions first compute the corresponding inverse mapping \(\left<f_x, f_y\right>: \texttt{dst} \rightarrow \texttt{src}\) and then use the above formula."
imgproc,"The actual implementations of the geometrical transformations, from the most generic remap and to the simplest and the fastest resize, need to solve two main problems with the above formula:"
imgproc,"Extrapolation of non-existing pixels. Similarly to the filtering functions described in the previous section, for some \((x,y)\), either one of \(f_x(x,y)\), or \(f_y(x,y)\), or both of them may fall outside of the image. In this case, an extrapolation method needs to be used. OpenCV provides the same selection of extrapolation methods as in the filtering functions. In addition, it provides the method BORDER_TRANSPARENT. This means that the corresponding pixels in the destination image will not be modified at all. Interpolation of pixel values. Usually \(f_x(x,y)\) and \(f_y(x,y)\) are floating-point numbers. This means that \(\left<f_x, f_y\right>\) can be either an affine or perspective transformation, or radial lens distortion correction, and so on. So, a pixel value at fractional coordinates needs to be retrieved. In the simplest case, the coordinates can be just rounded to the nearest integer coordinates and the corresponding pixel can be used. This is called a nearest-neighbor interpolation. However, a better result can be achieved by using more sophisticated interpolation methods , where a polynomial function is fit into some neighborhood of the computed pixel \((f_x(x,y), f_y(x,y))\), and then the value of the polynomial at \((f_x(x,y), f_y(x,y))\) is taken as the interpolated pixel value. In OpenCV, you can choose between several interpolation methods. See resize for details."
imgproc,"Enumerations enum cv::InterpolationFlags { cv::INTER_NEAREST = 0 , cv::INTER_LINEAR = 1 , cv::INTER_CUBIC = 2 , cv::INTER_AREA = 3 , cv::INTER_LANCZOS4 = 4 , cv::INTER_LINEAR_EXACT = 5 , cv::INTER_NEAREST_EXACT = 6 , cv::INTER_MAX = 7 , cv::WARP_FILL_OUTLIERS = 8 , cv::WARP_INVERSE_MAP = 16 , cv::WARP_RELATIVE_MAP = 32 }  interpolation algorithm More...  enum cv::InterpolationMasks { cv::INTER_BITS = 5 , cv::INTER_BITS2 = INTER_BITS * 2 , cv::INTER_TAB_SIZE = 1 << INTER_BITS , cv::INTER_TAB_SIZE2 = INTER_TAB_SIZE * INTER_TAB_SIZE }  enum cv::WarpPolarMode { cv::WARP_POLAR_LINEAR = 0 , cv::WARP_POLAR_LOG = 256 }  Specify the polar mapping mode. More... "
imgproc,Classes class cv::LineSegmentDetector  Line segment detector class. More... 
imgproc,"The Subdiv2D class described in this section is used to perform various planar subdivision on a set of 2D points (represented as vector of Point2f). OpenCV subdivides a plane into triangles using the Delaunay's algorithm, which corresponds to the dual graph of the Voronoi diagram. In the figure below, the Delaunay's triangulation is marked with black lines and the Voronoi diagram with red lines."
imgproc,"The subdivisions can be used for the 3D piece-wise transformation of a plane, morphing, fast location of points on the plane, building special graphs (such as NNG,RNG), and so forth."
imgproc,Classes class cv::Subdiv2D 
imgproc,"Enumerations enum cv::TemplateMatchModes { cv::TM_SQDIFF = 0 , cv::TM_SQDIFF_NORMED = 1 , cv::TM_CCORR = 2 , cv::TM_CCORR_NORMED = 3 , cv::TM_CCOEFF = 4 , cv::TM_CCOEFF_NORMED = 5 }  type of the template matching operation More... "
ml,The class implements the random forest predictor.
ml,Artificial Neural Networks - Multi-Layer Perceptrons.
ml,"Unlike many other models in ML that are constructed and trained at once, in the MLP model these steps are separated. First, a network with the specified topology is created using the non-default constructor or the method ANN_MLP::create. All the weights are set to zeros. Then, the network is trained using a set of input and output vectors. The training procedure can be repeated more than once, that is, the weights can be adjusted based on the new training data."
ml,Additional flags for StatModel::train are available: ANN_MLP::TrainFlags.
ml,Support Vector Machines.
ml,Random Number Generator.
ml,"Random number generator. It encapsulates the state (currently, a 64-bit integer) and has methods to return scalar random values and to fill arrays with random values. Currently it supports uniform and Gaussian (normal) distributions. The generator uses Multiply-With-Carry algorithm, introduced by G. Marsaglia ( http://en.wikipedia.org/wiki/Multiply-with-carry ). Gaussian-distribution random numbers are generated using the Ziggurat algorithm ( http://en.wikipedia.org/wiki/Ziggurat_algorithm ), introduced by G. Marsaglia and W. W. Tsang."
ml,The class implements the Expectation Maximization algorithm.
ml,Classes class cv::LDA  Linear Discriminant Analysis. More...  class cv::PCA  Principal Component Analysis. More...  class cv::RNG  Random Number Generator. More...  class cv::RNG_MT19937  Mersenne Twister random number generator. More...  class cv::SVD  Singular Value Decomposition. More... 
ml,Bayes classifier for normally distributed data.
ml,Boosted tree classifier derived from DTrees.
ml,The structure represents the logarithmic grid range of statmodel parameters.
ml,"It is used for optimizing statmodel accuracy by varying model parameters, the accuracy estimate being computed by cross-validation."
ml,Implements Logistic Regression classifier.
ml,The class represents a single decision tree or a collection of decision trees.
ml,"The current public interface of the class allows user to train only a single decision tree, however the class is capable of storing multiple decision trees and using them for prediction (by summing responses or using a voting schemes), and the derived from DTrees classes (such as RTrees and Boost) use this capability to implement decision tree ensembles."
ml,Base class for statistical models in OpenCV ML.
ml,Class encapsulating training data.
ml,"Please note that the class only specifies the interface of training data, but not implementation. All the statistical model classes in ml module accepts Ptr<TrainData> as parameter. In other words, you can create your own class derived from TrainData and pass smart pointer to the instance of this class into StatModel::train."
ml,Namespaces namespace cv::traits 
ml,This class declares example interface for system state used in simulated annealing optimization algorithm.
ml,The class implements K-Nearest Neighbors model.
ml,"The Machine Learning Library (MLL) is a set of classes and functions for statistical classification, regression, and clustering of data."
ml,"Most of the classification and regression algorithms are implemented as C++ classes. As the algorithms have different sets of features (like an ability to handle missing measurements or categorical input variables), there is a little common ground between the classes. This common ground is defined by the class cv::ml::StatModel that all the other ML classes are derived from."
ml,See detailed overview here: Machine Learning Overview.
ml,Classes class cv::ml::ANN_MLP  Artificial Neural Networks - Multi-Layer Perceptrons. More...  class cv::ml::Boost  Boosted tree classifier derived from DTrees. More...  class cv::ml::DTrees  The class represents a single decision tree or a collection of decision trees. More...  class cv::ml::EM  The class implements the Expectation Maximization algorithm. More...  class cv::ml::KNearest  The class implements K-Nearest Neighbors model. More...  class cv::ml::LogisticRegression  Implements Logistic Regression classifier. More...  class cv::ml::NormalBayesClassifier  Bayes classifier for normally distributed data. More...  class cv::ml::ParamGrid  The structure represents the logarithmic grid range of statmodel parameters. More...  class cv::ml::RTrees  The class implements the random forest predictor. More...  struct cv::ml::SimulatedAnnealingSolverSystem  This class declares example interface for system state used in simulated annealing optimization algorithm. More...  class cv::ml::StatModel  Base class for statistical models in OpenCV ML. More...  class cv::ml::SVM  Support Vector Machines. More...  class cv::ml::SVMSGD  Stochastic Gradient Descent SVM classifier. More...  class cv::ml::TrainData  Class encapsulating training data. More... 
ml,Stochastic Gradient Descent SVM classifier.
ml,"SVMSGD provides a fast and easy-to-use implementation of the SVM classifier using the Stochastic Gradient Descent approach, as presented in [35]."
ml,The classifier has following parameters:
ml,"model type, margin type, margin regularization ( \(\lambda\)), initial step size ( \(\gamma_0\)), step decreasing power ( \(c\)), and termination criteria."
ml,The model type may have one of the following values: SGD and ASGD.
ml,"SGD is the classic version of SVMSGD classifier: every next step is calculated by the formula \[w_{t+1} = w_t - \gamma(t) \frac{dQ_i}{dw} |_{w = w_t}\] where \(w_t\) is the weights vector for decision function at step \(t\), \(\gamma(t)\) is the step size of model parameters at the iteration \(t\), it is decreased on each step by the formula \(\gamma(t) = \gamma_0 (1 + \lambda \gamma_0 t) ^ {-c}\) \(Q_i\) is the target functional from SVM task for sample with number \(i\), this sample is chosen stochastically on each step of the algorithm. \(w_t\) is the weights vector for decision function at step \(t\), \(\gamma(t)\) is the step size of model parameters at the iteration \(t\), it is decreased on each step by the formula \(\gamma(t) = \gamma_0 (1 + \lambda \gamma_0 t) ^ {-c}\) \(Q_i\) is the target functional from SVM task for sample with number \(i\), this sample is chosen stochastically on each step of the algorithm. ASGD is Average Stochastic Gradient Descent SVM Classifier. ASGD classifier averages weights vector on each step of algorithm by the formula \(\widehat{w}_{t+1} = \frac{t}{1+t}\widehat{w}_{t} + \frac{1}{1+t}w_{t+1}\)"
ml,\[w_{t+1} = w_t - \gamma(t) \frac{dQ_i}{dw} |_{w = w_t}\]
ml,"\(w_t\) is the weights vector for decision function at step \(t\), \(\gamma(t)\) is the step size of model parameters at the iteration \(t\), it is decreased on each step by the formula \(\gamma(t) = \gamma_0 (1 + \lambda \gamma_0 t) ^ {-c}\) \(Q_i\) is the target functional from SVM task for sample with number \(i\), this sample is chosen stochastically on each step of the algorithm."
ml,The recommended model type is ASGD (following [35]).
ml,The margin type may have one of the following values: SOFT_MARGIN or HARD_MARGIN.
ml,"You should use HARD_MARGIN type, if you have linearly separable sets. You should use SOFT_MARGIN type, if you have non-linearly separable sets or sets with outliers. In the general case (if you know nothing about linear separability of your sets), use SOFT_MARGIN."
ml,The other parameters may be described as follows:
ml,"Margin regularization parameter is responsible for weights decreasing at each step and for the strength of restrictions on outliers (the less the parameter, the less probability that an outlier will be ignored). Recommended value for SGD model is 0.0001, for ASGD model is 0.00001. Initial step size parameter is the initial value for the step size \(\gamma(t)\). You will have to find the best initial step for your problem. Step decreasing power is the power parameter for \(\gamma(t)\) decreasing by the formula, mentioned above. Recommended value for SGD model is 1, for ASGD model is 0.75. Termination criteria can be TermCriteria::COUNT, TermCriteria::EPS or TermCriteria::COUNT + TermCriteria::EPS. You will have to find the best termination criteria for your problem."
ml,"Note that the parameters margin regularization, initial step size, and step decreasing power should be positive."
ml,To use SVMSGD algorithm do as follows:
ml,"first, create the SVMSGD object. The algorithm will set optimal parameters by default, but you can set your own parameters via functions setSvmsgdType(), setMarginType(), setMarginRegularization(), setInitialStepSize(), and setStepDecreasingPower(). then the SVM model can be trained using the train features and the correspondent labels by the method train(). after that, the label of a new feature vector can be predicted using the method predict()."
objdetect,Classes struct cv::DetectionROI  struct for detection region of interest (ROI) More...  struct cv::HOGDescriptor  Implementation of HOG (Histogram of Oriented Gradients) descriptor and object detector. More... 
objdetect,Classes class cv::barcode::BarcodeDetector 
objdetect,Check the corresponding tutorial for more details.
objdetect,Classes class cv::FaceDetectorYN  DNN-based face detector. More...  class cv::FaceRecognizerSF  DNN-based face recognizer. More... 
objdetect,The object detector described below has been initially proposed by Paul Viola [285] and improved by Rainer Lienhart [168] .
objdetect,"First, a classifier (namely a cascade of boosted classifiers working with haar-like features) is trained with a few hundred sample views of a particular object (i.e., a face or a car), called positive examples, that are scaled to the same size (say, 20x20), and negative examples - arbitrary images of the same size."
objdetect,"After a classifier is trained, it can be applied to a region of interest (of the same size as used during the training) in an input image. The classifier outputs a ""1"" if the region is likely to show the object (i.e., face/car), and ""0"" otherwise. To search for the object in the whole image one can move the search window across the image and check every location using the classifier. The classifier is designed so that it can be easily ""resized"" in order to be able to find the objects of interest at different sizes, which is more efficient than resizing the image itself. So, to find an object of an unknown size in the image the scan procedure should be done several times at different scales."
objdetect,"The word ""cascade"" in the classifier name means that the resultant classifier consists of several simpler classifiers (stages) that are applied subsequently to a region of interest until at some stage the candidate is rejected or all the stages are passed. The word ""boosted"" means that the classifiers at every stage of the cascade are complex themselves and they are built out of basic classifiers using one of four different boosting techniques (weighted voting). Currently Discrete Adaboost, Real Adaboost, Gentle Adaboost and Logitboost are supported. The basic classifiers are decision-tree classifiers with at least 2 leaves. Haar-like features are the input to the basic classifiers, and are calculated as described below. The current algorithm uses the following Haar-like features:"
objdetect,"The feature used in a particular classifier is specified by its shape (1a, 2b etc.), position within the region of interest and the scale (this scale is not the same as the scale used at the detection stage, though these two scales are multiplied). For example, in the case of the third line feature (2c) the response is calculated as the difference between the sum of image pixels under the rectangle covering the whole feature (including the two white stripes and the black stripe in the middle) and the sum of the image pixels under the black stripe multiplied by 3 in order to compensate for the differences in the size of areas. The sums of pixel values over a rectangular regions are calculated rapidly using integral images (see below and the integral description)."
objdetect,Check the corresponding tutorial for more details.
objdetect,The following reference is for the detection part only. There is a separate application called opencv_traincascade that can train a cascade of boosted classifiers from a set of samples.
objdetect,Classes class cv::BaseCascadeClassifier  class cv::CascadeClassifier  Cascade classifier class for object detection. More...  struct cv::DefaultDeleter< CvHaarClassifierCascade >  class cv::DetectionBasedTracker 
objdetect,"ArUco Marker Detection Square fiducial markers (also known as Augmented Reality Markers) are useful for easy, fast and robust camera pose estimation."
objdetect,"The main functionality of ArucoDetector class is detection of markers in an image. If the markers are grouped as a board, then you can try to recover the missing markers with ArucoDetector::refineDetectedMarkers(). ArUco markers can also be used for advanced chessboard corner finding. To do this, group the markers in the CharucoBoard and find the corners of the chessboard with the CharucoDetector::detectBoard()."
objdetect,The implementation is based on the ArUco Library by R. Muoz-Salinas and S. Garrido-Jurado [99].
objdetect,Markers can also be detected based on the AprilTag 2 [292] fiducial detection method.
objdetect,Classes class cv::aruco::ArucoDetector  The main functionality of ArucoDetector class is detection of markers in an image with detectMarkers() method. More...  class cv::aruco::Board  Board of ArUco markers. More...  class cv::aruco::CharucoBoard  ChArUco board is a planar chessboard where the markers are placed inside the white squares of a chessboard. More...  class cv::aruco::CharucoDetector  struct cv::aruco::CharucoParameters  struct cv::aruco::DetectorParameters  struct DetectorParameters is used by ArucoDetector More...  class cv::aruco::Dictionary  Dictionary is a set of unique ArUco markers of the same size. More...  class cv::aruco::GridBoard  Planar board with grid arrangement of markers. More...  struct cv::aruco::RefineParameters  struct RefineParameters is used by ArucoDetector More... 
objdetect,"Classes class cv::GraphicalCodeDetector  class cv::SimilarRects  This class is used for grouping object candidates detected by Cascade Classifier, HOG etc. More... "
objdetect,Classes class cv::QRCodeDetector  class cv::QRCodeDetectorAruco  class cv::QRCodeEncoder 
photo,This module includes photo processing algorithms
photo,"Functions void cv::denoise_TVL1 (const std::vector< Mat > &observations, Mat &result, double lambda=1.0, int niters=30)  Primal-dual algorithm is an algorithm for solving special types of variational problems (that is, finding a function to minimize some functional). As the image denoising, in particular, may be seen as the variational problem, primal-dual algorithm then can be used to perform denoising and this is exactly what is implemented.  void cv::cuda::fastNlMeansDenoising (const GpuMat &src, GpuMat &dst, float h, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  void cv::fastNlMeansDenoising (InputArray src, OutputArray dst, const std::vector< float > &h, int templateWindowSize=7, int searchWindowSize=21, int normType=NORM_L2)  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/ with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::cuda::fastNlMeansDenoising (InputArray src, OutputArray dst, float h, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::fastNlMeansDenoising (InputArray src, OutputArray dst, float h=3, int templateWindowSize=7, int searchWindowSize=21)  Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/ with several computational optimizations. Noise expected to be a gaussian white noise.  void cv::cuda::fastNlMeansDenoisingColored (const GpuMat &src, GpuMat &dst, float h_luminance, float photo_render, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  void cv::fastNlMeansDenoisingColored (InputArray src, OutputArray dst, float h=3, float hColor=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoising function for colored images.  void cv::cuda::fastNlMeansDenoisingColored (InputArray src, OutputArray dst, float h_luminance, float photo_render, int search_window=21, int block_size=7, Stream &stream=Stream::Null())  Modification of fastNlMeansDenoising function for colored images.  void cv::fastNlMeansDenoisingColoredMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, float h=3, float hColor=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoisingMulti function for colored images sequences.  void cv::fastNlMeansDenoisingMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, const std::vector< float > &h, int templateWindowSize=7, int searchWindowSize=21, int normType=NORM_L2)  Modification of fastNlMeansDenoising function for images sequence where consecutive images have been captured in small period of time. For example video. This version of the function is for grayscale images or for manual manipulation with colorspaces. See [44] for more details (open access here).  void cv::fastNlMeansDenoisingMulti (InputArrayOfArrays srcImgs, OutputArray dst, int imgToDenoiseIndex, int temporalWindowSize, float h=3, int templateWindowSize=7, int searchWindowSize=21)  Modification of fastNlMeansDenoising function for images sequence where consecutive images have been captured in small period of time. For example video. This version of the function is for grayscale images or for manual manipulation with colorspaces. See [44] for more details (open access here).  void cv::cuda::nonLocalMeans (const GpuMat &src, GpuMat &dst, float h, int search_window=21, int block_size=7, int borderMode=BORDER_DEFAULT, Stream &stream=Stream::Null())  void cv::cuda::nonLocalMeans (InputArray src, OutputArray dst, float h, int search_window=21, int block_size=7, int borderMode=BORDER_DEFAULT, Stream &stream=Stream::Null())  Performs pure non local means denoising without any simplification, and thus it is not fast. "
photo,Useful links:
photo,http://www.cse.cuhk.edu.hk/leojia/projects/color2gray/index.html
photo,"Functions void cv::decolor (InputArray src, OutputArray grayscale, OutputArray color_boost)  Transforms a color image to a grayscale image. It is a basic tool in digital printing, stylized black-and-white photograph rendering, and in many single channel image processing applications [176] . "
photo,"This section describes high dynamic range imaging algorithms namely tonemapping, exposure alignment, camera calibration with multiple exposures and exposure fusion."
photo,"Classes class cv::AlignExposures  The base class for algorithms that align images of the same scene with different exposures. More...  class cv::AlignMTB  This algorithm converts images to median threshold bitmaps (1 for pixels brighter than median luminance and 0 otherwise) and than aligns the resulting bitmaps using bit operations. More...  class cv::CalibrateCRF  The base class for camera response calibration algorithms. More...  class cv::CalibrateDebevec  Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. Objective function is constructed using pixel values on the same position in all images, extra term is added to make the result smoother. More...  class cv::CalibrateRobertson  Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. This algorithm uses all image pixels. More...  class cv::MergeDebevec  The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response. More...  class cv::MergeExposures  The base class algorithms that can merge exposure sequence to a single image. More...  class cv::MergeMertens  Pixels are weighted using contrast, saturation and well-exposedness measures, than images are combined using laplacian pyramids. More...  class cv::MergeRobertson  The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response. More...  class cv::Tonemap  Base class for tonemapping algorithms - tools that are used to map HDR image to 8-bit range. More...  class cv::TonemapDrago  Adaptive logarithmic mapping is a fast global tonemapping algorithm that scales the image in logarithmic domain. More...  class cv::TonemapMantiuk  This algorithm transforms image to contrast using gradients on all levels of gaussian pyramid, transforms contrast values to HVS response and scales the response. After this the image is reconstructed from new contrast values. More...  class cv::TonemapReinhard  This is a global tonemapping operator that models human visual system. More... "
photo,the inpainting algorithm
photo,"Enumerations enum { cv::INPAINT_NS = 0 , cv::INPAINT_TELEA = 1 } "
photo,Useful links:
photo,https://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp
photo,"Enumerations enum { cv::NORMAL_CLONE = 1 , cv::MIXED_CLONE = 2 , cv::MONOCHROME_TRANSFER = 3 }  seamlessClone algorithm flags More... "
photo,Useful links:
photo,http://www.inf.ufrgs.br/~eslgastal/DomainTransform
photo,https://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/
photo,"Enumerations enum { cv::RECURS_FILTER = 1 , cv::NORMCONV_FILTER = 2 }  Edge preserving filters. More... "
stitching,Classes class cv::detail::AffineWarper  Affine warper that uses rotations and translations. More...  class cv::AffineWarper  Affine warper factory class. More...  struct cv::detail::CompressedRectilinearPortraitProjector  class cv::CompressedRectilinearPortraitWarper  class cv::detail::CompressedRectilinearPortraitWarper  struct cv::detail::CompressedRectilinearProjector  class cv::CompressedRectilinearWarper  class cv::detail::CompressedRectilinearWarper  struct cv::detail::CylindricalPortraitProjector  class cv::detail::CylindricalPortraitWarper  struct cv::detail::CylindricalProjector  class cv::detail::CylindricalWarper  Warper that maps an image onto the x*x + z*z = 1 cylinder. More...  class cv::CylindricalWarper  Cylindrical warper factory class. More...  class cv::detail::CylindricalWarperGpu  struct cv::detail::FisheyeProjector  class cv::detail::FisheyeWarper  class cv::FisheyeWarper  struct cv::detail::MercatorProjector  class cv::detail::MercatorWarper  class cv::MercatorWarper  struct cv::detail::PaniniPortraitProjector  class cv::PaniniPortraitWarper  class cv::detail::PaniniPortraitWarper  struct cv::detail::PaniniProjector  class cv::PaniniWarper  class cv::detail::PaniniWarper  struct cv::detail::PlanePortraitProjector  class cv::detail::PlanePortraitWarper  struct cv::detail::PlaneProjector  class cv::PlaneWarper  Plane warper factory class. More...  class cv::detail::PlaneWarper  Warper that maps an image onto the z = 1 plane. More...  class cv::detail::PlaneWarperGpu  struct cv::detail::ProjectorBase  Base class for warping logic implementation. More...  class cv::detail::RotationWarper  Rotation-only model image warper interface. More...  class cv::detail::RotationWarperBase< P >  Base class for rotation-based warper using a detail::ProjectorBase_ derived class. More...  struct cv::detail::SphericalPortraitProjector  class cv::detail::SphericalPortraitWarper  struct cv::detail::SphericalProjector  class cv::detail::SphericalWarper  Warper that maps an image onto the unit sphere located at the origin. More...  class cv::SphericalWarper  Spherical warper factory class. More...  class cv::detail::SphericalWarperGpu  struct cv::detail::StereographicProjector  class cv::StereographicWarper  class cv::detail::StereographicWarper  struct cv::detail::TransverseMercatorProjector  class cv::detail::TransverseMercatorWarper  class cv::TransverseMercatorWarper  class cv::WarperCreator  Image warper factories base class. More... 
stitching,"This figure illustrates the stitching module pipeline implemented in the Stitcher class. Using that class it's possible to configure/remove some steps, i.e. adjust the stitching pipeline according to the particular needs. All building blocks from the pipeline are available in the detail namespace, one can combine and use them separately."
stitching,The implemented stitching pipeline is very similar to the one proposed in [41] .
stitching,"Functions bool cv::detail::calibrateRotatingCamera (const std::vector< Mat > &Hs, Mat &K)  void cv::detail::estimateFocal (const std::vector< ImageFeatures > &features, const std::vector< MatchesInfo > &pairwise_matches, std::vector< double > &focals)  Estimates focal lengths for each given camera.  void cv::detail::focalsFromHomography (const Mat &H, double &f0, double &f1, bool &f0_ok, bool &f1_ok)  Tries to estimate focal lengths from the given homography under the assumption that the camera undergoes rotations around its centre only. "
stitching,Classes class cv::detail::AffineBasedEstimator  Affine transformation based estimator. More...  class cv::detail::BundleAdjusterAffine  Bundle adjuster that expects affine transformation represented in homogeneous coordinates in R for each camera param. Implements camera parameters refinement algorithm which minimizes sum of the reprojection error squares. More...  class cv::detail::BundleAdjusterAffinePartial  Bundle adjuster that expects affine transformation with 4 DOF represented in homogeneous coordinates in R for each camera param. Implements camera parameters refinement algorithm which minimizes sum of the reprojection error squares. More...  class cv::detail::BundleAdjusterBase  Base class for all camera parameters refinement methods. More...  class cv::detail::BundleAdjusterRay  Implementation of the camera parameters refinement algorithm which minimizes sum of the distances between the rays passing through the camera center and a feature. : More...  class cv::detail::BundleAdjusterReproj  Implementation of the camera parameters refinement algorithm which minimizes sum of the reprojection error squares. More...  class cv::detail::Estimator  Rotation estimator base class. More...  class cv::detail::HomographyBasedEstimator  Homography based rotation estimator. More...  class cv::detail::NoBundleAdjuster  Stub bundle adjuster that does nothing. More... 
stitching,Classes class cv::detail::DpSeamFinder  class cv::detail::GraphCutSeamFinder  Minimum graph cut-based seam estimator. See details in [153] . More...  class cv::detail::GraphCutSeamFinderBase  Base class for all minimum graph-cut-based seam estimators. More...  class cv::detail::NoSeamFinder  Stub seam estimator which does nothing. More...  class cv::detail::PairwiseSeamFinder  Base class for all pairwise seam estimators. More...  class cv::detail::SeamFinder  Base class for a seam estimator. More...  class cv::detail::VoronoiSeamFinder  Voronoi diagram-based seam estimator. More... 
stitching,Classes class cv::detail::Blender  Base class for all blenders. More...  class cv::detail::FeatherBlender  Simple blender which mixes images at its borders. More...  class cv::detail::MultiBandBlender  Blender which uses multi-band blending algorithm (see [45]). More... 
stitching,"Classes class cv::detail::BlocksChannelsCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image block on each channel. More...  class cv::detail::BlocksCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image blocks. More...  class cv::detail::BlocksGainCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image block intensities, see [279] for details. More...  class cv::detail::ChannelsCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image intensities on each channel independently. More...  class cv::detail::ExposureCompensator  Base class for all exposure compensators. More...  class cv::detail::GainCompensator  Exposure compensator which tries to remove exposure related artifacts by adjusting image intensities, see [41] and [305] for details. More...  class cv::detail::NoExposureCompensator  Stub exposure compensator which does nothing. More... "
stitching,Classes class cv::detail::AffineBestOf2NearestMatcher  Features matcher similar to cv::detail::BestOf2NearestMatcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf. More...  class cv::detail::BestOf2NearestMatcher  Features matcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf. More...  class cv::detail::BestOf2NearestRangeMatcher  class cv::detail::FeaturesMatcher  Feature matchers base class. More...  struct cv::detail::ImageFeatures  Structure containing image keypoints and descriptors. More...  struct cv::detail::MatchesInfo  Structure containing information about matches between two images. More... 
video,Classes class cv::DenseOpticalFlow  class cv::DISOpticalFlow  DIS optical flow algorithm. More...  class cv::FarnebackOpticalFlow  Class computing a dense optical flow using the Gunnar Farneback's algorithm. More...  class cv::KalmanFilter  Kalman filter class. More...  class cv::SparseOpticalFlow  Base interface for sparse optical flow algorithms. More...  class cv::SparsePyrLKOpticalFlow  Class used for calculating a sparse optical flow. More...  class cv::Tracker  Base abstract class for the long-term tracker. More...  class cv::TrackerDaSiamRPN  class cv::TrackerGOTURN  the GOTURN (Generic Object Tracking Using Regression Networks) tracker More...  class cv::TrackerMIL  The MIL algorithm trains a classifier in an online manner to separate the object from the background. More...  class cv::TrackerNano  the Nano tracker is a super lightweight dnn-based general object tracking. More...  class cv::TrackerVit  the VIT tracker is a super lightweight dnn-based general object tracking. More...  class cv::VariationalRefinement  Variational optical flow refinement. More... 
video,Classes class cv::BackgroundSubtractor  Base class for background/foreground segmentation. : More...  class cv::BackgroundSubtractorKNN  K-nearest neighbours - based Background/Foreground Segmentation Algorithm. More...  class cv::BackgroundSubtractorMOG2  Gaussian Mixture-based Background/Foreground Segmentation Algorithm. More... 
videoio,"Enumerations enum cv::VideoCaptureAPIs { cv::CAP_ANY = 0 , cv::CAP_VFW = 200 , cv::CAP_V4L = 200 , cv::CAP_V4L2 = CAP_V4L , cv::CAP_FIREWIRE = 300 , cv::CAP_FIREWARE = CAP_FIREWIRE , cv::CAP_IEEE1394 = CAP_FIREWIRE , cv::CAP_DC1394 = CAP_FIREWIRE , cv::CAP_CMU1394 = CAP_FIREWIRE , cv::CAP_QT = 500 , cv::CAP_UNICAP = 600 , cv::CAP_DSHOW = 700 , cv::CAP_PVAPI = 800 , cv::CAP_OPENNI = 900 , cv::CAP_OPENNI_ASUS = 910 , cv::CAP_ANDROID = 1000 , cv::CAP_XIAPI = 1100 , cv::CAP_AVFOUNDATION = 1200 , cv::CAP_GIGANETIX = 1300 , cv::CAP_MSMF = 1400 , cv::CAP_WINRT = 1410 , cv::CAP_INTELPERC = 1500 , cv::CAP_REALSENSE = 1500 , cv::CAP_OPENNI2 = 1600 , cv::CAP_OPENNI2_ASUS = 1610 , cv::CAP_OPENNI2_ASTRA = 1620 , cv::CAP_GPHOTO2 = 1700 , cv::CAP_GSTREAMER = 1800 , cv::CAP_FFMPEG = 1900 , cv::CAP_IMAGES = 2000 , cv::CAP_ARAVIS = 2100 , cv::CAP_OPENCV_MJPEG = 2200 , cv::CAP_INTEL_MFX = 2300 , cv::CAP_XINE = 2400 , cv::CAP_UEYE = 2500 , cv::CAP_OBSENSOR = 2600 }  cv::VideoCapture API backends identifier. More...  enum cv::VideoCaptureProperties { cv::CAP_PROP_POS_MSEC =0 , cv::CAP_PROP_POS_FRAMES =1 , cv::CAP_PROP_POS_AVI_RATIO =2 , cv::CAP_PROP_FRAME_WIDTH =3 , cv::CAP_PROP_FRAME_HEIGHT =4 , cv::CAP_PROP_FPS =5 , cv::CAP_PROP_FOURCC =6 , cv::CAP_PROP_FRAME_COUNT =7 , cv::CAP_PROP_FORMAT =8 , cv::CAP_PROP_MODE =9 , cv::CAP_PROP_BRIGHTNESS =10 , cv::CAP_PROP_CONTRAST =11 , cv::CAP_PROP_SATURATION =12 , cv::CAP_PROP_HUE =13 , cv::CAP_PROP_GAIN =14 , cv::CAP_PROP_EXPOSURE =15 , cv::CAP_PROP_CONVERT_RGB =16 , cv::CAP_PROP_WHITE_BALANCE_BLUE_U =17 , cv::CAP_PROP_RECTIFICATION =18 , cv::CAP_PROP_MONOCHROME =19 , cv::CAP_PROP_SHARPNESS =20 , cv::CAP_PROP_AUTO_EXPOSURE =21 , cv::CAP_PROP_GAMMA =22 , cv::CAP_PROP_TEMPERATURE =23 , cv::CAP_PROP_TRIGGER =24 , cv::CAP_PROP_TRIGGER_DELAY =25 , cv::CAP_PROP_WHITE_BALANCE_RED_V =26 , cv::CAP_PROP_ZOOM =27 , cv::CAP_PROP_FOCUS =28 , cv::CAP_PROP_GUID =29 , cv::CAP_PROP_ISO_SPEED =30 , cv::CAP_PROP_BACKLIGHT =32 , cv::CAP_PROP_PAN =33 , cv::CAP_PROP_TILT =34 , cv::CAP_PROP_ROLL =35 , cv::CAP_PROP_IRIS =36 , cv::CAP_PROP_SETTINGS =37 , cv::CAP_PROP_BUFFERSIZE =38 , cv::CAP_PROP_AUTOFOCUS =39 , cv::CAP_PROP_SAR_NUM =40 , cv::CAP_PROP_SAR_DEN =41 , cv::CAP_PROP_BACKEND =42 , cv::CAP_PROP_CHANNEL =43 , cv::CAP_PROP_AUTO_WB =44 , cv::CAP_PROP_WB_TEMPERATURE =45 , cv::CAP_PROP_CODEC_PIXEL_FORMAT =46 , cv::CAP_PROP_BITRATE =47 , cv::CAP_PROP_ORIENTATION_META =48 , cv::CAP_PROP_ORIENTATION_AUTO =49 , cv::CAP_PROP_HW_ACCELERATION =50 , cv::CAP_PROP_HW_DEVICE =51 , cv::CAP_PROP_HW_ACCELERATION_USE_OPENCL =52 , cv::CAP_PROP_OPEN_TIMEOUT_MSEC =53 , cv::CAP_PROP_READ_TIMEOUT_MSEC =54 , cv::CAP_PROP_STREAM_OPEN_TIME_USEC =55 , cv::CAP_PROP_VIDEO_TOTAL_CHANNELS = 56 , cv::CAP_PROP_VIDEO_STREAM = 57 , cv::CAP_PROP_AUDIO_STREAM = 58 , cv::CAP_PROP_AUDIO_POS = 59 , cv::CAP_PROP_AUDIO_SHIFT_NSEC = 60 , cv::CAP_PROP_AUDIO_DATA_DEPTH = 61 , cv::CAP_PROP_AUDIO_SAMPLES_PER_SECOND = 62 , cv::CAP_PROP_AUDIO_BASE_INDEX = 63 , cv::CAP_PROP_AUDIO_TOTAL_CHANNELS = 64 , cv::CAP_PROP_AUDIO_TOTAL_STREAMS = 65 , cv::CAP_PROP_AUDIO_SYNCHRONIZE = 66 , cv::CAP_PROP_LRF_HAS_KEY_FRAME = 67 , cv::CAP_PROP_CODEC_EXTRADATA_INDEX = 68 , cv::CAP_PROP_FRAME_TYPE = 69 , cv::CAP_PROP_N_THREADS = 70 , cv::CAP_PROP_PTS = 71 , cv::CAP_PROP_DTS_DELAY = 72 }  cv::VideoCapture generic properties identifier. More...  enum cv::VideoWriterProperties { cv::VIDEOWRITER_PROP_QUALITY = 1 , cv::VIDEOWRITER_PROP_FRAMEBYTES = 2 , cv::VIDEOWRITER_PROP_NSTRIPES = 3 , cv::VIDEOWRITER_PROP_IS_COLOR = 4 , cv::VIDEOWRITER_PROP_DEPTH = 5 , cv::VIDEOWRITER_PROP_HW_ACCELERATION = 6 , cv::VIDEOWRITER_PROP_HW_DEVICE = 7 , cv::VIDEOWRITER_PROP_HW_ACCELERATION_USE_OPENCL = 8 , cv::VIDEOWRITER_PROP_RAW_VIDEO = 9 , cv::VIDEOWRITER_PROP_KEY_INTERVAL = 10 , cv::VIDEOWRITER_PROP_KEY_FLAG = 11 , cv::VIDEOWRITER_PROP_PTS = 12 , cv::VIDEOWRITER_PROP_DTS_DELAY = 13 }  cv::VideoWriter generic properties identifier. More... "
videoio,Classes class CvAbstractCamera  class CvPhotoCamera  protocol <CvPhotoCameraDelegate>  class CvVideoCamera  protocol <CvVideoCameraDelegate> 
videoio,"Hardware acceleration support enum cv::VideoAccelerationType { cv::VIDEO_ACCELERATION_NONE = 0 , cv::VIDEO_ACCELERATION_ANY = 1 , cv::VIDEO_ACCELERATION_D3D11 = 2 , cv::VIDEO_ACCELERATION_VAAPI = 3 , cv::VIDEO_ACCELERATION_MFX = 4 }  Video Acceleration type. More... "
videoio,"Enumerations enum { cv::OPEN_CAMERA = 300 , cv::CLOSE_CAMERA , cv::UPDATE_IMAGE_ELEMENT , cv::SHOW_TRACKBAR } "
videoio,Read and write video or images sequence with OpenCV.
videoio,This section contains API description how to query/configure available Video I/O backends.
videoio,Runtime configuration options:
videoio,"enable debug mode: OPENCV_VIDEOIO_DEBUG=1 change backend priority: OPENCV_VIDEOIO_PRIORITY_<backend>=9999 disable backend: OPENCV_VIDEOIO_PRIORITY_<backend>=0 specify list of backends with high priority (>100000): OPENCV_VIDEOIO_PRIORITY_LIST=FFMPEG,GSTREAMER"
videoio,"Functions cv::String cv::videoio_registry::getBackendName (VideoCaptureAPIs api)  Returns backend API name or ""UnknownVideoAPI(xxx)"".  std::vector< VideoCaptureAPIs > cv::videoio_registry::getBackends ()  Returns list of all available backends.  std::string cv::videoio_registry::getCameraBackendPluginVersion (VideoCaptureAPIs api, int &version_ABI, int &version_API)  Returns description and ABI/API version of videoio plugin's camera interface.  std::vector< VideoCaptureAPIs > cv::videoio_registry::getCameraBackends ()  Returns list of available backends which works via cv::VideoCapture(int index)  std::string cv::videoio_registry::getStreamBackendPluginVersion (VideoCaptureAPIs api, int &version_ABI, int &version_API)  Returns description and ABI/API version of videoio plugin's stream capture interface.  std::vector< VideoCaptureAPIs > cv::videoio_registry::getStreamBackends ()  Returns list of available backends which works via cv::VideoCapture(filename)  std::string cv::videoio_registry::getWriterBackendPluginVersion (VideoCaptureAPIs api, int &version_ABI, int &version_API)  Returns description and ABI/API version of videoio plugin's writer interface.  std::vector< VideoCaptureAPIs > cv::videoio_registry::getWriterBackends ()  Returns list of available backends which works via cv::VideoWriter()  bool cv::videoio_registry::hasBackend (VideoCaptureAPIs api)  Returns true if backend is available.  bool cv::videoio_registry::isBackendBuiltIn (VideoCaptureAPIs api)  Returns true if backend is built in (false if backend is used as plugin) "
