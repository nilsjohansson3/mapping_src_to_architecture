{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master.ipynb\n",
    "\n",
    "#!pip install --user textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt\n",
    "#import textacy\n",
    "#print(textacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\a258142\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from lib.data_loader import DataLoader\n",
    "from lib.augmenter import Augmenter\n",
    "import lib.configurator as configurator\n",
    "from lib.classifier import Classifier\n",
    "from lib.feature_extractor import FeatureExtractor\n",
    "import lib.source_code_processing.methods_identifier_extractor as MethodsIdentifiersExtractor\n",
    "from lib.method_classifier import MethodClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from collections import Counter\n",
    "import math\n",
    "#from threadpoolctl import threadpool_limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(y_train, y_eval_functions, y_eval_files):\n",
    "    # Add \"-1\" class to labels\n",
    "    y_eval_with_null = np.append(y_eval_functions, \"~unknown\")\n",
    "    \n",
    "    # Combine labels from both y_train and y_eval\n",
    "    all_labels = np.concatenate((y_train, y_eval_with_null, y_eval_files))\n",
    "    \n",
    "    # Create a single LabelEncoder instance and fit it on all labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_labels)\n",
    "    \n",
    "    # Encode both y_train and y_eval using the same label encoder\n",
    "    y_train_encoded = label_encoder.transform(y_train)\n",
    "    y_eval_functions_encoded = label_encoder.transform(y_eval_functions)\n",
    "    y_eval_files_encoded = label_encoder.transform(y_eval_files)\n",
    "\n",
    "    return label_encoder, y_train_encoded, y_eval_functions_encoded, y_eval_files_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(label_encoder, encoded_array):\n",
    "    def recursive_decode(value):\n",
    "        # If the value is a scalar, decode it\n",
    "        if np.isscalar(value):\n",
    "            try:\n",
    "                return label_encoder.inverse_transform([value])[0]\n",
    "            except ValueError:\n",
    "                return \"~unknown\"\n",
    "        # If the value is an array/list, decode each element recursively\n",
    "        elif isinstance(value, (list, np.ndarray)):\n",
    "            return [recursive_decode(v) for v in value]\n",
    "        else:\n",
    "            return \"~unknown\"\n",
    "\n",
    "    # Start the decoding process\n",
    "    return recursive_decode(encoded_array)\n",
    "    \"\"\"decoded_array = []\n",
    "    for encoded_value in encoded_array:\n",
    "        if encoded_value == -1:\n",
    "            decoded_array.append(\"~unknown\")\n",
    "        else:\n",
    "            decoded_array.append(label_encoder.inverse_transform([encoded_value])[0])\n",
    "    return decoded_array\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_results(now, n_o_classes, preprocessing_options, resampling_outlier_options,              \n",
    "                 classifier, hyperparameters, metrics, training_directory, evaluation_directory, method_predictions, N, X_train_tokens,\n",
    "                 apply_stemming, apply_lemmitization, do_clean_code,do_remove_stopwords,class_sizes, \n",
    "                 n_o_methods,n_LoC, n_LoC_mean, n_LoC_median, n_LoC_std, X_eval_mean_entropy, X_eval_mean_density, do_map_files,do_merge_training_data):\n",
    "     # Initialize default values\n",
    "    data_creation_method = 'Paragraphs'\n",
    "    vectorization_model = 'None'\n",
    "    do_resampling = 'None'\n",
    "    n_neighbours = 'None'\n",
    "    do_outlier_filtering = 'None'\n",
    "    contamination = 'None'\n",
    "    n_neighbours_lof = 'None'\n",
    "    \n",
    "    if preprocessing_options and preprocessing_options.get('data_creation', {}).get('data_creation_method'):\n",
    "        data_creation_method = preprocessing_options['data_creation']['data_creation_method']\n",
    "        vectorization_model = preprocessing_options.get('vectorization', {}).get('method', 'None')\n",
    "    \n",
    "    if resampling_outlier_options:\n",
    "        do_resampling = resampling_outlier_options.get('resampling', {}).get('do_resample', 'None')\n",
    "        n_neighbours = resampling_outlier_options.get('resampling', {}).get('n_neighbours', 'None')\n",
    "        do_outlier_filtering = resampling_outlier_options.get('outlier_filtering', {}).get('do_filter', 'None')\n",
    "        contamination = resampling_outlier_options.get('outlier_filtering', {}).get('contamination', 'None')\n",
    "        n_neighbours_lof = resampling_outlier_options.get('outlier_filtering', {}).get('n_neighbours', 'None')\n",
    "        \n",
    "    # Check if the results file exists to determine if we need to create a new file or append to an existing one\n",
    "    if not os.path.exists('results.csv'):\n",
    "        max_id = 0\n",
    "    else:\n",
    "        results_df = pd.read_csv('results.csv')\n",
    "        max_id = results_df['ID'].max() if len(results_df) > 0 else 0\n",
    "\n",
    "    # Increment the max ID to use for new entries\n",
    "    max_id += 1\n",
    "\n",
    "    if classifier == 'kNN':\n",
    "        k = hyperparameters['n_neighbors']\n",
    "        classifier = f'{k}-NN'\n",
    "    \n",
    "    # Append new row to results.csv\n",
    "    new_row = pd.DataFrame({\n",
    "        'ID': [max_id],\n",
    "        'Timestamp': [now],\n",
    "        'data_creation_method': [data_creation_method],\n",
    "        'vectorization_model': [vectorization_model],\n",
    "        'do_resample': [do_resampling],\n",
    "        'n_neighbours': [n_neighbours],\n",
    "        'do_outlier_filtering': [do_outlier_filtering],\n",
    "        'contamination': [contamination],\n",
    "        'n_neighbours_lof': [n_neighbours_lof],\n",
    "        'classifier': [classifier],\n",
    "        'classifier_hyperparams': [hyperparameters],\n",
    "        'Training_Directory': [training_directory],\n",
    "        'Evaluation_Directory': [evaluation_directory],\n",
    "        'Fraction of data after outlier filtering': [metrics['data_fraction_after_outlier_filtering']],\n",
    "        'Fraction of data classified after outlier filtering': [metrics['fraction_classified']],\n",
    "        'Accuracy': [metrics['accuracy']],\n",
    "        'Precision': [metrics['precision']],\n",
    "        'Recall': [metrics['recall']],\n",
    "        'F1_Score': [metrics['f1_score']],\n",
    "        'Number of classes': [n_o_classes],\n",
    "        'Top-N': [N],  # Adding the new column for the number of top classes considered (N)\n",
    "        'n_tokens': [X_train_tokens],\n",
    "        'Stemming':[apply_stemming],\n",
    "        'Lemmitize':[apply_lemmitization],\n",
    "        'Clean code':[do_clean_code],\n",
    "        'Remove stop words':[do_remove_stopwords],\n",
    "        'Class sizes:': [class_sizes],\n",
    "        '#codePoints:':[n_o_methods],\n",
    "        'Mean nb density': [X_eval_mean_density],\n",
    "        'Mean nb entropy': [X_eval_mean_entropy],\n",
    "        'n_LoC': [n_LoC],\n",
    "        'n_LoC_mean': [n_LoC_mean],\n",
    "        'n_LoC_median': [n_LoC_median],\n",
    "        'n_LoC_std': [n_LoC_std],\n",
    "        'Mapped files': [do_map_files],\n",
    "        'do_merge_training_data': [do_merge_training_data]\n",
    "    })\n",
    "\n",
    "    # Save the new row to results.csv\n",
    "    new_row.to_csv('results.csv', mode='a', header=not os.path.exists('results.csv'), index=False)\n",
    "\n",
    "    # Save to classifications_report.csv\n",
    "    method_predictions['ID'] = max_id  # Set the ID for the current predictions\n",
    "    # Reorder columns to have 'ID' as the first column\n",
    "    method_predictions = method_predictions[['ID', 'Best Prediction','True Class','Entropy','Density','Lines of code','Non-generic terms','Unique non-generic terms','n_ext_includes','n_comments','n_NL_words']]# + [col for col in method_predictions.columns if col != 'ID']]\n",
    "    method_predictions.to_csv('classifications_report.csv', mode='a', header=not os.path.exists('classifications_report.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used\n",
    "def evaluate_predictions(y_true, y_pred, n_original_methods):\n",
    "    # Filter out \"Unknown\" class from y_true and y_pred\n",
    "    filtered_indices = [i for i, predicted_label in enumerate(y_pred) if predicted_label != \"~unknown\"]\n",
    "    y_true_filtered = [y_true[i] for i in filtered_indices]\n",
    "    y_pred_filtered = [y_pred[i] for i in filtered_indices]\n",
    "\n",
    "    fractionClassified = len(y_true_filtered) / len(y_true)\n",
    "    remainingDataAfterOutlierFiltering = len(y_true) / n_original_methods\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(y_true_filtered, y_pred_filtered, average='macro') # wrt unknown predictions\n",
    "\n",
    "    metrics = {\n",
    "        'data_fraction_after_outlier_filtering': remainingDataAfterOutlierFiltering,\n",
    "        'fraction_classified': fractionClassified,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def evaluate_topN_predictions(method_predictions_df, n_original_methods, N):\n",
    "    \"\"\"\n",
    "    Evaluate the predictions for the top N classes.\n",
    "\n",
    "    :param method_predictions_df: DataFrame containing method predictions and true classes.\n",
    "    :param n_original_methods: The original number of methods.\n",
    "    :param N: The number of top predictions to consider.\n",
    "    :return: Dictionary of evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Identify the correct column for the true class labels\n",
    "    #true_class_column = method_predictions_df.columns.intersection(['True Class 1', 'True Class']).tolist()\n",
    "    #true_class_column = method_predictions_df['True Class'].tolist()\n",
    "\n",
    "    #if not true_class_column:\n",
    "       # raise ValueError(\"Could not find a column containing true class labels.\")\n",
    "    \n",
    "    #true_class_column = true_class_column[0]\n",
    "    \n",
    "    # Filter out \"Unknown\" class from predictions\n",
    "    method_predictions_filtered = method_predictions_df[method_predictions_df['Predicted Class 1'] != \"~unknown\"]\n",
    "    \n",
    "    fractionClassified = len(method_predictions_filtered) / len(method_predictions_df)\n",
    "    remainingDataAfterOutlierFiltering = len(method_predictions_df) / n_original_methods\n",
    "\n",
    "    # Create a new column for the best prediction\n",
    "    def get_best_prediction(row):\n",
    "        true_class = row['True Class']\n",
    "        topN_predictions = row[[f'Predicted Class {i+1}' for i in range(N)]].values\n",
    "        if true_class in topN_predictions:\n",
    "            return true_class\n",
    "        else:\n",
    "            return row['Predicted Class 1']\n",
    "\n",
    "    method_predictions_filtered['Best Prediction'] = method_predictions_filtered.apply(get_best_prediction, axis=1)\n",
    "    method_predictions_df['Best Prediction'] = method_predictions_df.apply(get_best_prediction, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(method_predictions_filtered['True Class'], method_predictions_filtered['Best Prediction'])\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(method_predictions_filtered['True Class'],\n",
    "                                                                     method_predictions_filtered['Best Prediction'],\n",
    "                                                                     average='macro',\n",
    "                                                                     zero_division=0)\n",
    "\n",
    "    metrics = {\n",
    "        'data_fraction_after_outlier_filtering': remainingDataAfterOutlierFiltering,\n",
    "        'fraction_classified': fractionClassified,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "    return metrics, method_predictions_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data into a dictionary\n",
    "def save_data(X_train, y_train, X_train_embeddings,\n",
    "                            X_eval_function_content, y_eval_function_content,\n",
    "                            X_eval_function_content_embeddings, function_ids,\n",
    "                            X_eval_files_content, y_eval_files_content,\n",
    "                            X_eval_files_content_embeddings, file_ids):\n",
    "    saved_data = {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'X_train_embeddings': X_train_embeddings,\n",
    "        'X_eval_function_content': X_eval_function_content,\n",
    "        'y_eval_function_content': y_eval_function_content,\n",
    "        'X_eval_function_content_embeddings': X_eval_function_content_embeddings,\n",
    "        'function_ids': function_ids,\n",
    "        'X_eval_files_content': X_eval_files_content,\n",
    "        'y_eval_files_content': y_eval_files_content,\n",
    "        'X_eval_files_content_embeddings': X_eval_files_content_embeddings,\n",
    "        'file_ids': file_ids\n",
    "    }\n",
    "    return saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data from a dictionary\n",
    "def load_data(saved_data):\n",
    "    return (saved_data['X_train'], saved_data['y_train'], saved_data['X_train_embeddings'],\n",
    "            saved_data['X_eval_function_content'], saved_data['y_eval_function_content'],\n",
    "            saved_data['X_eval_function_content_embeddings'],saved_data['function_ids'],\n",
    "            saved_data['X_eval_files_content'], saved_data['y_eval_files_content'],\n",
    "            saved_data['X_eval_files_content_embeddings'],saved_data['file_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parent_classes_using_list(directory_path, parent_paths):\n",
    "    parent_classes = {}\n",
    "\n",
    "    # Normalize the parent paths to absolute paths\n",
    "    absolute_parent_paths = [os.path.abspath(os.path.join(directory_path, parent_path)) for parent_path in parent_paths]\n",
    "\n",
    "    # Create a mapping from absolute parent paths to their unique parent class names\n",
    "    parent_class_map = {}\n",
    "    for parent_path in absolute_parent_paths:\n",
    "        relative_parent_path = os.path.relpath(parent_path, directory_path)\n",
    "        top_level_dir, sub_dir = os.path.split(relative_parent_path)\n",
    "        parent_class_name = f\"{os.path.basename(top_level_dir)}_{sub_dir}\"\n",
    "        parent_class_map[parent_path] = parent_class_name\n",
    "\n",
    "    # Traverse the directory structure and map files to their parent classes\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        # Normalize root path\n",
    "        normalized_root = os.path.abspath(root)\n",
    "\n",
    "        # Check if the current root is under any of the specified parent paths\n",
    "        for parent_path in absolute_parent_paths:\n",
    "            if normalized_root.startswith(parent_path):\n",
    "                parent_class = parent_class_map[parent_path]\n",
    "                for file_name in files:\n",
    "                    if file_name.endswith((\".cpp\", \".cc\", \".java\")):\n",
    "                        file_key = os.path.splitext(file_name)[0]\n",
    "                        parent_classes[file_key] = parent_class\n",
    "\n",
    "    return parent_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parent_classes(directory_path, parent_folder):\n",
    "    parent_classes = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        if (root.endswith(parent_folder)) and dirs:\n",
    "            for dir_name in dirs:\n",
    "                parent_class = dir_name\n",
    "                for _, _, files in os.walk(os.path.join(root, dir_name)):\n",
    "                    for file in files:\n",
    "                        if file.endswith((\".cpp\", \".cc\", \".java\",\"c\")):\n",
    "                            file_key = os.path.splitext(file)[0]\n",
    "                            parent_classes[file_key] = parent_class\n",
    "    return parent_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parent_classes(directory_path, parent_folder):\n",
    "    parent_classes = {}\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        if root.endswith(parent_folder) and dirs:\n",
    "            for dir_name in dirs:\n",
    "                parent_class = dir_name\n",
    "                for sub_root, _, files in os.walk(os.path.join(root, dir_name)):\n",
    "                    for file in files:\n",
    "                        if file.endswith((\".cpp\", \".cc\", \".java\", \"c\")):\n",
    "                            # Use the full relative file path as the key\n",
    "                            relative_file_path = os.path.relpath(os.path.join(sub_root, file), directory_path)\n",
    "                            parent_classes[relative_file_path] = parent_class\n",
    "    return parent_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_parent_classes(y_train, class_dict):\n",
    "    return [class_dict[label] if label in class_dict else label for label in y_train]\n",
    "\n",
    "def print_unmpaped_classes(y_train, class_dict):\n",
    "    converted_labels = []\n",
    "    unmapped_classes = set()\n",
    "    \n",
    "    for label in y_train:\n",
    "        if label in class_dict:\n",
    "            converted_labels.append(class_dict[label])\n",
    "        else:\n",
    "            #print('Unmapped: ', label)\n",
    "            converted_labels.append(label)\n",
    "            unmapped_classes.add(label)\n",
    "    \n",
    "    return\n",
    "\n",
    "def update_class_names_with_parent_classes(df_src, class_dict):\n",
    "    def map_class_name(class_name):\n",
    "        if class_name not in class_dict:\n",
    "            #print('Did not find class', class_name)\n",
    "            return class_name\n",
    "        else:\n",
    "            #print('Found class', class_name)\n",
    "            return class_dict[class_name]\n",
    "\n",
    "    df_src['Original class'] = df_src['Class']\n",
    "    df_src['Class'] = df_src['Class'].apply(map_class_name)\n",
    "    return df_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_duplicate_data_points(X_train, y_train):\n",
    "    # Initialize a dictionary to store the indices of X_train associated with each unique data point\n",
    "    data_point_indices = {}\n",
    "\n",
    "    # Iterate through the training data to populate the dictionary\n",
    "    for idx, (data_point, label) in enumerate(zip(X_train, y_train)):\n",
    "        data_point_str = str(data_point)  # Convert the data point to a string to use it as a dictionary key\n",
    "        if data_point_str not in data_point_indices:\n",
    "            data_point_indices[data_point_str] = [idx]  # Initialize a new list with the index\n",
    "        else:\n",
    "            data_point_indices[data_point_str].append(idx)  # Append the index to the existing list\n",
    "\n",
    "    # Initialize lists to store filtered X_train and y_train\n",
    "    filtered_X_train = []\n",
    "    filtered_y_train = []\n",
    "\n",
    "    # Iterate through the dictionary to filter out data points associated with multiple labels\n",
    "    for data_point_str, indices in data_point_indices.items():\n",
    "        if len(indices) == 1:\n",
    "            # If the data point is associated with only one label, keep it\n",
    "            filtered_X_train.append(X_train[indices[0]])\n",
    "            filtered_y_train.append(y_train[indices[0]])\n",
    "        #else:\n",
    "            #print('Removed data point: ', data_point_str)\n",
    "\n",
    "    # Convert the filtered lists back to arrays\n",
    "    X_train_filtered = np.array(filtered_X_train)\n",
    "    y_train_filtered = np.array(filtered_y_train)\n",
    "\n",
    "    return X_train_filtered, y_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_generic_data_points(X_train, y_train):\n",
    "    #Removes data points that appear in all classes\n",
    "    # Initialize a dictionary to store the unique labels associated with each data point\n",
    "    data_point_labels = {}\n",
    "    unique_labels = set(y_train)  # Get all unique labels in y_train\n",
    "\n",
    "    # Iterate through the training data to populate the dictionary\n",
    "    for idx, (data_point, label) in enumerate(zip(X_train, y_train)):\n",
    "        data_point_str = str(data_point)  # Convert the data point to a string to use it as a dictionary key\n",
    "        if data_point_str not in data_point_labels:\n",
    "            data_point_labels[data_point_str] = {'labels': {label}, 'indices': [idx]}  # Store labels and indices\n",
    "        else:\n",
    "            data_point_labels[data_point_str]['labels'].add(label)  # Add the label to the set\n",
    "            data_point_labels[data_point_str]['indices'].append(idx)  # Store all indices\n",
    "\n",
    "    # Initialize lists to store filtered X_train and y_train\n",
    "    filtered_X_train = []\n",
    "    filtered_y_train = []\n",
    "\n",
    "    # Iterate through the dictionary to filter out data points associated with all labels\n",
    "    for data_point_str, info in data_point_labels.items():\n",
    "        if info['labels'] != unique_labels:\n",
    "            # If the data point is not associated with all unique labels, keep it\n",
    "            for idx in info['indices']:  # There may be multiple instances with the same label\n",
    "                filtered_X_train.append(X_train[idx])\n",
    "                filtered_y_train.append(y_train[idx])\n",
    "\n",
    "    # Convert the filtered lists back to arrays\n",
    "    X_train_filtered = np.array(filtered_X_train)\n",
    "    y_train_filtered = np.array(filtered_y_train)\n",
    "\n",
    "    return X_train_filtered, y_train_filtered\n",
    "\n",
    "def filter_frequent_entries(X_train, y_train, max_occurrences):\n",
    "    # Count occurrences of each entry in X_train\n",
    "    from collections import Counter\n",
    "    entry_counts = Counter(X_train)\n",
    "    \n",
    "    # Create a boolean mask to filter entries that occur more than max_occurrences\n",
    "    mask = [entry_counts[x] <= max_occurrences for x in X_train]\n",
    "    \n",
    "    # Apply the mask to X_train and y_train\n",
    "    X_train_filtered = [x for x, keep in zip(X_train, mask) if keep]\n",
    "    y_train_filtered = [y for y, keep in zip(y_train, mask) if keep]\n",
    "    \n",
    "    return X_train_filtered, y_train_filtered\n",
    "\n",
    "def remove_duplicates(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Removes duplicate entries in X_train and y_train based on unique (X_train, y_train) pairs.\n",
    "    Keeps only one entry for each unique pair.\n",
    "\n",
    "    Args:\n",
    "        X_train (list): List of text data points.\n",
    "        y_train (list): List of corresponding class labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Cleaned X_train and y_train lists.\n",
    "    \"\"\"\n",
    "    # Use a set to track unique (text, label) pairs\n",
    "    seen = set()\n",
    "    unique_X_train = []\n",
    "    unique_y_train = []\n",
    "\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        # Create a tuple for the current pair\n",
    "        pair = (x, y)\n",
    "        if pair not in seen:\n",
    "            seen.add(pair)\n",
    "            unique_X_train.append(x)\n",
    "            unique_y_train.append(y)\n",
    "\n",
    "    return unique_X_train, unique_y_train\n",
    "\n",
    "def remove_non_strings(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Removes non-string entries from X_train and y_train based on X_train containing strings only.\n",
    "    Removes any pair where X_train entry is not a string.\n",
    "\n",
    "    Args:\n",
    "        X_train (list): List of data points (potentially non-string).\n",
    "        y_train (list): List of corresponding class labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Cleaned X_train and y_train lists with only string data points.\n",
    "    \"\"\"\n",
    "    cleaned_X_train = []\n",
    "    cleaned_y_train = []\n",
    "\n",
    "    for x, y in zip(X_train, y_train):\n",
    "        # Only add the pair if the entry in X_train is a string\n",
    "        if isinstance(x, str):\n",
    "            cleaned_X_train.append(x)\n",
    "            cleaned_y_train.append(y)\n",
    "\n",
    "    return cleaned_X_train, cleaned_y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_classes_not_in_training_data(df_src, X_train, y_train):\n",
    "    # Get unique classes from y_train\n",
    "    unique_classes_in_training = set(y_train)\n",
    "    \n",
    "    # Filter df_src to keep only rows where 'Class' exists in the training data (y_train)\n",
    "    df_src_filtered = df_src[df_src['Class'].isin(unique_classes_in_training)]\n",
    "    \n",
    "    return df_src_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_empty_data_points(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Filters out empty rows where either an entry in X_train or y_train is empty.\n",
    "\n",
    "    Args:\n",
    "        X_train (list): List of input data points.\n",
    "        y_train (list): List of corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Filtered X_train and y_train lists.\n",
    "    \"\"\"\n",
    "    filtered_X_train = []\n",
    "    filtered_y_train = []\n",
    "    \n",
    "    for x, y in zip(X_train, y_train):\n",
    "        if x and y:  # Checks if neither x nor y is empty\n",
    "            filtered_X_train.append(x)\n",
    "            filtered_y_train.append(y)\n",
    "    \n",
    "    return filtered_X_train, filtered_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_with_token_limit(X_train, y_train, max_tokens=1024):\n",
    "    \"\"\"\n",
    "    Merges the text data into one or more data points per unique class, \n",
    "    using cells from X_train, controlled by a max tokens limit.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: List of text data points.\n",
    "    - y_train: List of corresponding class labels.\n",
    "    - max_tokens: Maximum token limit for each data point.\n",
    "    \n",
    "    Returns:\n",
    "    - merged_X: List of merged text data points.\n",
    "    - merged_y: List of corresponding class labels for the merged data points.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    # Initialize a dictionary to hold merged text for each class\n",
    "    merged_X = []\n",
    "    merged_y = []\n",
    "    \n",
    "    current_text = \"\"\n",
    "    current_label = None\n",
    "    current_token_count = 0\n",
    "\n",
    "    for text, label in zip(X_train, y_train):\n",
    "        # Split the current text into words (tokens)\n",
    "        tokens = text.split()\n",
    "        token_count = len(tokens)\n",
    "\n",
    "        # If adding the current text exceeds the token limit, save the previous text as a datapoint\n",
    "        if current_label is not None and label == current_label and current_token_count + token_count > max_tokens:\n",
    "            merged_X.append(current_text.strip())  # Save the accumulated text\n",
    "            merged_y.append(current_label)\n",
    "            current_text = \"\"\n",
    "            current_token_count = 0\n",
    "\n",
    "        # Add the current text to the accumulated data\n",
    "        current_text += text + \" \"\n",
    "        current_token_count += token_count\n",
    "        current_label = label\n",
    "\n",
    "    # Don't forget to add the last accumulated data if it exists\n",
    "    if current_text.strip():\n",
    "        merged_X.append(current_text.strip())\n",
    "        merged_y.append(current_label)\n",
    "\n",
    "    return merged_X, merged_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:49: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:49: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\a258142\\AppData\\Local\\Temp\\ipykernel_31296\\1513111741.py:49: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def camel_to_words(text):\n",
    "    # Convert CamelCase to words (e.g., isDataReady -> is Data Ready)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    # Convert snake_case or snakyCamelCase to words (e.g., current_odometry -> current odometry)\n",
    "    text = re.sub(r'_([a-zA-Z])', r' \\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def replace_stopwords_with_punctuation(text):\n",
    "    # Replace specific stopwords with punctuation\n",
    "    stopwords_to_punctuation = {\n",
    "        ';': '.', \n",
    "        '{': '.', \n",
    "        '}': '.'\n",
    "    }\n",
    "    \n",
    "    for stopword, punctuation in stopwords_to_punctuation.items():\n",
    "        text = text.replace(stopword, punctuation)\n",
    "\n",
    "    return text\n",
    "\n",
    "def replace_operators_with_natural_language(text):\n",
    "    # Replace operators with natural language equivalents\n",
    "    replacements = {\n",
    "        '==': 'is equal to',\n",
    "        '!=': 'is not equal to',\n",
    "        '>=': 'is greater than or equal to',\n",
    "        '<=': 'is less than or equal to',\n",
    "        '>': 'is greater than',\n",
    "        '<': 'is less than',\n",
    "        '=': 'is defined as',\n",
    "        '&&': 'and',\n",
    "        '||': 'or',\n",
    "        '+': 'plus',\n",
    "        '-': 'minus',\n",
    "        #'*': 'times',\n",
    "        '/': 'divided by',\n",
    "    }\n",
    "    \n",
    "    for symbol, replacement in replacements.items():\n",
    "        text = text.replace(symbol, replacement)\n",
    "    \n",
    "    return text\n",
    "\"\"\"\n",
    "def clean_code_snippet(text, apply_stemming=True, apply_lemmatization=True):\n",
    "    # Initialize the lemmatizer and stemmer\n",
    "    lemmatizer = WordNetLemmatizer() if apply_lemmatization else None\n",
    "    stemmer = PorterStemmer() if apply_stemming else None\n",
    "\n",
    "    # Step 1: Convert CamelCase and snake_case to words\n",
    "    text = camel_to_words(text)\n",
    "\n",
    "    # Step 2: Handle cases where words are concatenated with numbers or special symbols\n",
    "    text = re.sub(r'([a-zA-Z])(\\d)', r'\\1 \\2', text)  # Separate words from numbers\n",
    "    text = re.sub(r'(\\d)([a-zA-Z])', r'\\1 \\2', text)  # Separate numbers from words\n",
    "    text = re.sub(r'([a-zA-Z])([^a-zA-Z\\s])', r'\\1 \\2', text)  # Add space before non-letters\n",
    "    text = re.sub(r'([^a-zA-Z\\s])([a-zA-Z])', r'\\1 \\2', text)  # Add space after non-letters\n",
    "\n",
    "    # Step 3: Add punctuation based on specific patterns (e.g., stopwords)\n",
    "    text = replace_stopwords_with_punctuation(text)\n",
    "\n",
    "    # Step 4: Replace operators and non-letter characters with meaningful words\n",
    "    text = replace_operators_with_natural_language(text)\n",
    "\n",
    "    # Step 5: Replace non-letter characters (except specified punctuation) with spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s.]+', ' ', text)\n",
    "\n",
    "       # Step 6: Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Step 7: Apply stemming if enabled\n",
    "    if apply_stemming:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Step 8: Apply lemmatization if enabled\n",
    "    if apply_lemmatization:\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Step 9: Join the words back into a single string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    # Step 10: Convert text to lowercase for uniformity\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 11: Normalize multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\"\"\"\n",
    "\n",
    "def clean_code_snippet(text):\n",
    "    # Step 1: Replace underscores with spaces\n",
    "    text = text.replace('_', ' ')\n",
    "    \n",
    "    # Step 2: Handle CamelCase (only when a lowercase letter is followed by an uppercase letter)\n",
    "    text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n",
    "    \n",
    "    # Step 3: Remove special characters and symbols, except spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # Step 4: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 5: Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\a258142\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "common_keywords = {\n",
    "    # C++ https://en.cppreference.com/w/cpp/keyword\n",
    "    'alignas', 'alignof', 'and', 'and_eq', 'asm', 'atomic_cancel', 'atomic_commit', \n",
    "    'atomic_noexcept', 'auto', 'bitand', 'bitor', 'bool', 'break', 'case', 'catch', \n",
    "    'char', 'char8_t', 'char16_t', 'char32_t', 'class', 'compl', 'concept', 'const', \n",
    "    'consteval', 'constexpr', 'constinit', 'const_cast', 'continue', 'co_await', \n",
    "    'co_return', 'co_yield', 'decltype', 'default', 'delete', 'do', 'double', \n",
    "    'dynamic_cast', 'else', 'enum', 'explicit', 'export', 'extern', 'false', 'float', \n",
    "    'for', 'friend', 'goto', 'if', 'inline', 'int', 'long', 'mutable', 'namespace', \n",
    "    'new', 'noexcept', 'not', 'not_eq', 'nullptr', 'operator', 'or', 'or_eq', 'private', \n",
    "    'protected', 'public', 'reflexpr', 'register', 'reinterpret_cast', 'requires', 'return', \n",
    "    'short', 'signed', 'sizeof', 'static', 'static_assert', 'static_cast', 'struct', 'switch', \n",
    "    'synchronized', 'template', 'this', 'thread_local', 'throw', 'true', 'try', 'typedef', \n",
    "    'typeid', 'typename', 'union', 'unsigned', 'using', 'virtual', 'void', 'volatile', \n",
    "    'wchar_t', 'while', 'xor', 'xor_eq',\n",
    "    # C https://en.cppreference.com/w/c/keyword\n",
    "    'alignas', 'alignof', 'auto', 'bool', 'break', 'case', 'char', 'const', 'constexpr', \n",
    "    'continue', 'default', 'do', 'double', 'else', 'enum', 'extern', 'false', 'float', \n",
    "    'for', 'goto', 'if', 'inline', 'int', 'long', 'nullptr', 'register', 'restrict', 'return', \n",
    "    'short', 'signed', 'sizeof', 'static', 'static_assert', 'struct', 'switch', 'thread_local', \n",
    "    'true', 'typedef', 'typeof', 'typeof_unqual', 'union', 'unsigned', 'void', 'volatile', 'while',\n",
    "    '_Alignas', '_Alignof', '_Atomic', '_BitInt', '_Bool', '_Complex', '_Decimal128', '_Decimal32', \n",
    "    '_Decimal64', '_Generic', '_Imaginary', '_Noreturn', '_Static_assert', '_Thread_local', \n",
    "    'if', 'elif', 'else', 'endif', 'ifdef', 'ifndef', 'elifdef', 'elifndef', 'define', 'undef', \n",
    "    'include', 'embed', 'line', 'error', 'warning', 'pragma', 'defined', '__has_include', \n",
    "    '__has_embed', '__has_c_attribute',\n",
    "\n",
    "    # java https://docs.oracle.com/javase/tutorial/java/nutsandbolts/_keywords.html\n",
    "    'abstract', 'continue', 'for', 'new', 'switch',\n",
    "    'assert', 'default', 'goto', 'package', 'synchronized',\n",
    "    'boolean', 'do', 'if', 'private', 'this',\n",
    "    'break', 'double', 'implements', 'protected', 'throw',\n",
    "    'byte', 'else', 'import', 'public', 'throws',\n",
    "    'case', 'enum', 'instanceof', 'return', 'transient',\n",
    "    'catch', 'extends', 'int', 'short', 'try',\n",
    "    'char', 'final', 'interface', 'static', 'void',\n",
    "    'class', 'finally', 'long', 'strictfp', 'volatile',\n",
    "    'const', 'float', 'native', 'super', 'while','true', 'false','null'\n",
    " }\n",
    "\n",
    "# Load English words from NLTK\n",
    "word_set = set(words.words())\n",
    "\n",
    "def count_natural_language_words(code_snippet):\n",
    "    # Step 1: Tokenize the code snippet\n",
    "    # Split by non-word characters (punctuation, whitespace, etc.)\n",
    "    tokens = re.findall(r'[a-zA-Z]+', code_snippet)\n",
    "    \n",
    "    # Step 2: Check if tokens are valid English words\n",
    "    natural_language_words = [token for token in tokens if token.lower() not in common_keywords]\n",
    "    natural_language_words = [token for token in natural_language_words if token.lower() in word_set]\n",
    "    \n",
    "    # Step 3: Count the natural language words\n",
    "    return len(natural_language_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_system_and_module_name(df_src_functions, df_src_files, X_train, y_train, system_name):\n",
    "    \"\"\"\n",
    "    Clean the system name from text data and replace module names (labels) with 'module'.\n",
    "    \n",
    "    Args:\n",
    "        df_src_functions (DataFrame): DataFrame containing functions with 'Content' column to clean.\n",
    "        df_src_files (DataFrame): DataFrame containing files with 'Content' column to clean.\n",
    "        X_train (list): List of text data points.\n",
    "        y_train (list): List of corresponding class labels (module names).\n",
    "        system_name (str): The system name to be replaced with \"system\".\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Cleaned DataFrames (df_src_functions, df_src_files) and cleaned X_train list.\n",
    "    \"\"\"\n",
    "    # Prepare lowercase versions for matching\n",
    "    system_name_lower = system_name.lower()\n",
    "    labels_lower = set(str(label).lower() for label in set(y_train))\n",
    "\n",
    "    def clean_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "\n",
    "        # Replace ' system_name ' (surrounded by spaces) with ' system '\n",
    "        pattern_system = r'(?i)(?<=\\s)' + re.escape(system_name) + r'(?=\\s)'\n",
    "        cleaned = re.sub(pattern_system, ' ', text) # re.sub(pattern_system, ' system ', text) # does not work of system name is first (no whitespace)\n",
    "\n",
    "        # Replace each label with ' module ' (surrounded by word boundaries or spaces, case-insensitive)\n",
    "        for label in labels_lower:\n",
    "            pattern_label = r'(?i)(?<=\\s)' + re.escape(label) + r'(?=\\s)'\n",
    "            cleaned = re.sub(pattern_label, ' ', cleaned) # re.sub(pattern_label, ' module ', cleaned)\n",
    "\n",
    "        # Optional: clean up multiple spaces after replacements\n",
    "        cleaned = re.sub(r'\\s{2,}', ' ', cleaned)\n",
    "\n",
    "        return cleaned.strip()\n",
    "\n",
    "    # Apply to df_src_functions and df_src_files\n",
    "    #df_src_functions['Content'] = df_src_functions['Content'].apply(clean_text)\n",
    "    #df_src_files['Content'] = df_src_files['Content'].apply(clean_text)\n",
    "\n",
    "    # Clean X_train\n",
    "    X_train = [clean_text(text) for text in X_train]\n",
    "\n",
    "    return df_src_functions, df_src_files, X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_terms(snippet, method_contents, threshold=0.7):\n",
    "    # Split the snippet into words\n",
    "    words = snippet.split()\n",
    "    \n",
    "    # Count the frequency of each word across all methods\n",
    "    all_words = ' '.join(method_contents).split()\n",
    "    word_frequencies = Counter(all_words)\n",
    "    total_methods = len(method_contents)\n",
    "    \n",
    "    # Calculate IDF (Inverse Document Frequency) for each word\n",
    "    word_document_count = Counter([word for content in method_contents for word in set(content.split())])\n",
    "    idf_scores = {word: math.log(total_methods / (1 + word_document_count[word])) for word in word_document_count}\n",
    "    \n",
    "    # Create a cleaned snippet by filtering out high-frequency words\n",
    "    cleaned_words = [word for word in words if idf_scores.get(word, 0) > threshold]\n",
    "    \n",
    "    return ' '.join(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def count_openai_tokens(X_train, model_name=\"gpt-4o\"):\n",
    "    # Initialize the tokenizer for the specified model\n",
    "    tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "    \n",
    "    total_tokens = 0\n",
    "    \n",
    "    for text in X_train:\n",
    "        # Encode the text to count tokens\n",
    "        tokens = tokenizer.encode(text)\n",
    "        total_tokens += len(tokens)\n",
    "    \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(X_train):\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for text in X_train:\n",
    "        # Split the text into tokens using regex to handle various delimiters\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "        total_tokens += len(tokens)\n",
    "    \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(strings_list):\n",
    "    \"\"\"\n",
    "    Counts the total number of words in a list of strings.\n",
    "    \n",
    "    Parameters:\n",
    "        strings_list (list of str): A list of strings to count words from.\n",
    "    \n",
    "    Returns:\n",
    "        int: The total number of words across all strings in the list.\n",
    "    \"\"\"\n",
    "    if not isinstance(strings_list, list) or not all(isinstance(s, str) for s in strings_list):\n",
    "        raise ValueError(\"Input must be a list of strings.\")\n",
    "    \n",
    "    total_words = sum(len(s.split()) for s in strings_list)\n",
    "    return total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def save_training_stats(n_src_functions, n_src_files, n_train_words,n_train_points, training_directory,n_Loc_files,n_Loc_functions, filename=\"training_stats.csv\"):\n",
    "    # Create a DataFrame with the new data\n",
    "    new_data = pd.DataFrame({\n",
    "        \"Training Directory\": [training_directory],\n",
    "        \"Number of Source Functions\": [n_src_functions],\n",
    "        \"Number of Source Files\": [n_src_files],\n",
    "        \"Number of Training Words\": [n_train_words],\n",
    "        \"Number of Training Points\": [n_train_points],\n",
    "        \"Number of LoC files\": [n_Loc_files],\n",
    "        \"Number of LoC functions\": [n_Loc_functions]\n",
    "    })\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(filename):\n",
    "        # Load existing data and append new row\n",
    "        df_existing = pd.read_csv(filename)\n",
    "        df_combined = pd.concat([df_existing, new_data], ignore_index=True)\n",
    "    else:\n",
    "        df_combined = new_data  # If file doesn't exist, use new data as the DataFrame\n",
    "\n",
    "    # Save to CSV\n",
    "    df_combined.to_csv(filename, index=False)\n",
    "\n",
    "    print(f\"Training stats appended to {filename}\")\n",
    "\n",
    "# Example usage:\n",
    "#save_training_stats(n_src_functions, n_src_files, n_train_words, training_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_centroids(X, y):\n",
    "    # Convert to DataFrame and add the class labels as a new column\n",
    "    df = pd.DataFrame(X)\n",
    "    df['Class'] = y\n",
    "    \n",
    "    # Calculate mean embeddings per class\n",
    "    centroids = df.groupby('Class').mean()  # No need to drop the 'Class' column here\n",
    "    centroids.index.name = 'Class'\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import rbf_kernel, pairwise_distances\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import entropy, gaussian_kde\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def compute_jaccard_similarity(X_train, X_eval):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard similarity between the training and evaluation datasets based on their vocabularies.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: List of training content.\n",
    "    - X_eval: List of evaluation content.\n",
    "    \n",
    "    Returns:\n",
    "    - Jaccard similarity (float): Size of intersection divided by size of union of vocabularies.\n",
    "    \"\"\"\n",
    "    # Use CountVectorizer to create vocabularies\n",
    "    vectorizer_train = CountVectorizer()\n",
    "    vectorizer_train.fit(X_train)\n",
    "    vocab_train = set(vectorizer_train.vocabulary_.keys())\n",
    "    \n",
    "    vectorizer_eval = CountVectorizer()\n",
    "    vectorizer_eval.fit(X_eval)\n",
    "    vocab_eval = set(vectorizer_eval.vocabulary_.keys())\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = vocab_train.intersection(vocab_eval)\n",
    "    union = vocab_train.union(vocab_eval)\n",
    "    \n",
    "    jaccard_similarity = len(intersection) / len(union) if len(union) > 0 else 0.0\n",
    "    \n",
    "    return jaccard_similarity\n",
    "\n",
    "def compute_oov_rate(X_train, X_eval):\n",
    "    \"\"\"\n",
    "    Computes vocabulary size and OOV rate for the given datasets using existing libraries.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: List of training content.\n",
    "    - y_train: List of training labels.\n",
    "    - X_eval: List of evaluation content.\n",
    "    - training_directory: Directory to save the results.\n",
    "    - file_name: Name of the output CSV file.\n",
    "    \"\"\"\n",
    "    # Use CountVectorizer to create a vocabulary from the training data\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(X_train)\n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    \n",
    "    # Prepare to calculate OOV rates functions\n",
    "    total_words = 0\n",
    "    oov_count = 0\n",
    "    \n",
    "    for content in X_eval:\n",
    "        words = content.split()\n",
    "        total_words += len(words)\n",
    "        oov_count += sum(1 for word in words if word not in vocabulary)\n",
    "    \n",
    "    oov_rate = oov_count / total_words if total_words > 0 else 0\n",
    "    \n",
    "    return oov_rate\n",
    "\n",
    "def save_oov_results(oov_results, vocabulary_size, training_directory,inputConfig, file_name):\n",
    "    \"\"\"\n",
    "    Saves OOV results and vocabulary size to CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    - oov_results: Dictionary containing OOV results.\n",
    "    - vocabulary_size: Size of the vocabulary.\n",
    "    - training_directory: Directory to save the results.\n",
    "    - file_name: Name of the output CSV file.\n",
    "    \"\"\"\n",
    "    # Prepare the results with the training directory\n",
    "    oov_results['Directory'] = training_directory\n",
    "    oov_results['Vocabulary Size'] = vocabulary_size\n",
    "    oov_results[\"Data creation method\"] = inputConfig\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    df_results = pd.DataFrame([oov_results])  # Convert to DataFrame\n",
    "    \n",
    "    if not os.path.exists(file_name):\n",
    "        df_results.to_csv(file_name, index=False)\n",
    "    else:\n",
    "        df_results.to_csv(file_name, mode='a', header=False, index=False)\n",
    "\n",
    "def compute_silhouette_score(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Computes the silhouette score for the embeddings and labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        score = silhouette_score(embeddings, labels)\n",
    "        return score\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "def compute_fdr(embeddings, labels):\n",
    "    \"\"\"\n",
    "    Computes the Fisher's Discriminant Ratio (FDR) for the embeddings and labels.\n",
    "    \"\"\"\n",
    "    unique_classes = np.unique(labels)\n",
    "    overall_mean = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    between_class_variance = 0\n",
    "    within_class_variance = 0\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        class_embeddings = embeddings[labels == cls]\n",
    "        class_mean = np.mean(class_embeddings, axis=0)\n",
    "        n_cls = class_embeddings.shape[0]\n",
    "        between_class_variance += n_cls * np.sum((class_mean - overall_mean) ** 2)\n",
    "        within_class_variance += np.sum((class_embeddings - class_mean) ** 2)\n",
    "    \n",
    "    fdr = between_class_variance / within_class_variance if within_class_variance > 0 else 1\n",
    "    return fdr\n",
    "\n",
    "\n",
    "def compute_entropy(id, labels,training_directory,inputConfig):\n",
    "    \"\"\"\n",
    "    Computes entropy for class distributions.\n",
    "    \"\"\"\n",
    "    counter = Counter(labels)\n",
    "    total = sum(counter.values())\n",
    "    proportions = [count / total for count in counter.values()]\n",
    "    entropy = -sum(p * np.log2(p) for p in proportions if p > 0)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "\n",
    "def compute_mmd(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Computes the Maximum Mean Discrepancy (MMD) between two sets of embeddings.\n",
    "    Uses RBF kernel.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): shape (n_samples_train, n_features)\n",
    "        X_test (np.ndarray): shape (n_samples_test, n_features)\n",
    "        gamma (float): kernel coefficient for RBF\n",
    "\n",
    "    Returns:\n",
    "        float: MMD score (lower = more similar)\n",
    "    \"\"\"\n",
    "    # Combine data for pairwise distance calculation\n",
    "    X = np.vstack([X_train, X_test])\n",
    "     # Use pairwise Euclidean distances and compute median (excluding self-distances)\n",
    "    dists = pairwise_distances(X, metric='euclidean')\n",
    "    dists = dists[np.triu_indices_from(dists, k=1)]\n",
    "    median_dist = np.median(dists)\n",
    "    gamma = 1.0 / (2 * median_dist**2) if median_dist > 0 else 1.0\n",
    "    \n",
    "\n",
    "    K_XX = rbf_kernel(X_train, X_train, gamma=gamma) # remove gamma to use default\n",
    "    K_YY = rbf_kernel(X_test, X_test, gamma=gamma)\n",
    "    K_XY = rbf_kernel(X_train, X_test, gamma=gamma)\n",
    "\n",
    "    m = X_train.shape[0]\n",
    "    n = X_test.shape[0]\n",
    "\n",
    "    mmd = (np.sum(K_XX) - np.trace(K_XX)) / (m * (m - 1)) + \\\n",
    "          (np.sum(K_YY) - np.trace(K_YY)) / (n * (n - 1)) - \\\n",
    "          2 * np.sum(K_XY) / (m * n)\n",
    "\n",
    "    return mmd\n",
    "\n",
    "\n",
    "\n",
    "def estimate_density_kde(data, sample_grid):\n",
    "    \"\"\"\n",
    "    Estimate a probability density function using KDE and return\n",
    "    the probabilities on a fixed sample grid.\n",
    "    \"\"\"\n",
    "    mask = np.std(data, axis=0) > 1e-10\n",
    "    data = data[:, mask]\n",
    "    sample_grid = sample_grid[:, mask]\n",
    "    kde = gaussian_kde(data.T)\n",
    "    density = kde(sample_grid.T)\n",
    "    return density / density.sum()  # Normalize to make it a probability distribution\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def compute_kl_js_divergence(X_train, X_test, n_grid=500, pca_dim=5):\n",
    "    \n",
    "    #min_samples = min(len(X_train), len(X_test))\n",
    "    #X_train = X_train[np.random.choice(len(X_train), min_samples, replace=False)]\n",
    "    #X_test = X_test[np.random.choice(len(X_test), min_samples, replace=False)]\n",
    "\n",
    "    # Reduce dimensionality\n",
    "    pca = PCA(n_components=pca_dim)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    n_grid = min(100, len(X_train) * 5)\n",
    "    dim = X_train.shape[1]\n",
    "\n",
    "    # Build grid\n",
    "    min_vec = np.minimum(X_train.min(axis=0), X_test.min(axis=0))\n",
    "    max_vec = np.maximum(X_train.max(axis=0), X_test.max(axis=0))\n",
    "    grid = np.random.uniform(min_vec, max_vec, size=(n_grid, dim))\n",
    "\n",
    "    # Estimate KDE\n",
    "    p_train = estimate_density_kde(X_train, grid)\n",
    "    p_test = estimate_density_kde(X_test, grid)\n",
    "\n",
    "    epsilon = 1e-12\n",
    "    p_train += epsilon\n",
    "    p_test += epsilon\n",
    "    p_train /= p_train.sum()\n",
    "    p_test /= p_test.sum()\n",
    "\n",
    "    js_div = jensenshannon(p_test, p_train) ** 2\n",
    "    return js_div\n",
    "\n",
    "    #kl_div = entropy(p_test, p_train)\n",
    "\n",
    "    # JS Divergence (symmetrical)\n",
    "    #js_div = jensenshannon(p_test, p_train) ** 2\n",
    "\n",
    "    #return js_div # kl_div,\n",
    "\n",
    "def compute_and_save_data_properties(id,\n",
    "                                      X_train, \n",
    "                                      X_train_embeddings, \n",
    "                                      y_train, \n",
    "                                      X_eval,\n",
    "                                      X_eval_embeddings,\n",
    "                                      y_eval_encoded,\n",
    "                                      directory,\n",
    "                                      data_creation_method):\n",
    "    \n",
    "    fdr_train = compute_fdr(X_train_embeddings, y_train)\n",
    "    fdr_eval = compute_fdr(X_eval_embeddings, y_eval_encoded)\n",
    "    fdr_total = compute_fdr(np.vstack([X_eval_embeddings, X_train_embeddings]), np.hstack([y_eval_encoded, y_train]))\n",
    "\n",
    "    entropy = compute_entropy(id, y_train, directory, data_creation_method)\n",
    "    jaccard_similarity = compute_jaccard_similarity(X_train, X_eval)\n",
    "\n",
    "    divergence = compute_kl_js_divergence(X_train_embeddings, X_eval_embeddings)\n",
    "\n",
    "    #Covariate shift\n",
    "    mmd = compute_mmd(X_train_embeddings, X_eval_embeddings)\n",
    "    #mmd_functions = compute_mmd(X_train_embeddings, X_eval_function_content_embeddings)\n",
    "\n",
    "    # Create a DataFrame with the entropy result\n",
    "    df_results = pd.DataFrame({\n",
    "        'ID': id,\n",
    "        'Directory': [directory],  # Single value in a list\n",
    "        'Data creation method': [data_creation_method],  # Single value in a list\n",
    "        'Entropy': [entropy],  # Single value in a list\n",
    "        'Jaccard similarity': [jaccard_similarity],\n",
    "        'Divergence': [divergence],\n",
    "        'MMD': [mmd],\n",
    "        'FDR train': [fdr_train],\n",
    "        'FDR eval': [fdr_eval],\n",
    "        'FDR total': [fdr_total],\n",
    "    })\n",
    "    file_name = 'data_properties.csv'\n",
    "    #Save results to CSV\n",
    "    if not os.path.exists(file_name):\n",
    "        df_results.to_csv(file_name, index=False)\n",
    "    else:\n",
    "        df_results.to_csv(file_name, mode='a', header=False, index=False)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ['paragraphs','sentence_shuffling', 'sentences','rake','yake','random_sentence_augment','synonym_augment','random_word_augment','sentence_spelling_augment','abstractive_summarization_augment']\n",
    "\"\"\" 'data_creation_method': [['paragraphs'],\n",
    "                                 ['paragraphs','sentences_sliding_window'],\n",
    "                                 ['paragraphs','decompose_text_with_stopwords'], # not used as sw are removed\n",
    "                                 ['paragraphs','sentence_back_translation_augment'],\n",
    "                                 ['paragraphs','abstractive_summarization_augment'],\n",
    "                                 ['paragraphs','sentences'],\n",
    "                                 ['paragraphs','rake'],\n",
    "                                 ['paragraphs','sentence_shuffling'], # same as random sentence aug\n",
    "                                 ['paragraphs','random_sentence_augment'],\n",
    "                                 ['paragraphs','synonym_augment'],\n",
    "                                 ['paragraphs','sentence_spelling_augment'],\n",
    "                                 ['paragraphs','random_word_augment']],\n",
    "        # does not take a long time\n",
    "        'data_creation_method': [['paragraphs'],\n",
    "                                 ['paragraphs','sentences_sliding_window'],\n",
    "                                 ['paragraphs','sentences'],\n",
    "                                 ['paragraphs','rake'],\n",
    "                                 ['paragraphs','random_sentence_augment'],\n",
    "                                 ['paragraphs','synonym_augment'],\n",
    "                                 ['paragraphs','sentence_spelling_augment'],\n",
    "                                 ['paragraphs','random_word_augment']]\n",
    "         # does take a long time\n",
    "    'data_creation_method': [['paragraphs','abstractive_summarization_augment'],\n",
    "                             ['paragraphs','sentence_back_translation_augment']],\n",
    "'data_creation_method': [['paragraphs'],\n",
    "                                ['paragraphs','sentences_sliding_window'],\n",
    "                                ['paragraphs','sentences'],\n",
    "                                ['paragraphs','rake'],\n",
    "                                ['paragraphs','random_sentence_augment'],\n",
    "                                ['paragraphs','synonym_augment'],\n",
    "                                ['paragraphs','sentence_spelling_augment'],\n",
    "                                ['paragraphs','random_word_augment']]\"\"\"\n",
    "\n",
    "# This is constant\n",
    "preprocessing_options = {\n",
    "    'data_creation':\n",
    "    {\n",
    "        'data_creation_method': [['paragraphs','sentences']],\n",
    "        'sliding_window_size': [[3]],\n",
    "        'window_movement': ['non-overlapping']\n",
    "    },\n",
    "    'vectorization':\n",
    "    {\n",
    "        'method': ['openai-embedding'] # 'unixcoder-base-nine' 'openai-embedding' 'sentencetransformer' 'openai-code-embedding'\n",
    "    },\n",
    "}\n",
    "# Variable\n",
    "resampling_outlier_options = {\n",
    "    'resampling':\n",
    "    {\n",
    "        'do_resample': [False],\n",
    "        'n_neighbours': [5], # ['sqrt_min_class','min_class']\n",
    "    },\n",
    "    'outlier_filtering':\n",
    "    {\n",
    "        'do_filter': [False],\n",
    "        'contamination': [0.1,'auto'], #dependent on density? - set to auto? ['auto', 0.05, 0.1, 0.2]\n",
    "        'n_neighbours': [9], # should be dependent on smallest class n_points (max n_points of smallest class)# set to k * density , where k_0 = 30/density (or k * n_points since range is constant)#dependent on density - compare with all training data points. should we really differentiate? - maybe since id's are classified seperately\n",
    "    }\n",
    "}\n",
    "\n",
    "classifier_options= ['kNN']\n",
    "\n",
    "classifier_hyperparameters = {\n",
    "    'kNN': {\n",
    "        'n_neighbors': ['sqrt_min_class'], #['sqrt_min_class','min_class', 1]\n",
    "        'threshold': [0.01],\n",
    "        'weights': ['uniform'] #'distance','uniform'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 configurations.\n",
      "{'data_creation': {'data_creation_method': ['paragraphs', 'sentences'], 'sliding_window_size': [0], 'window_movement': 'default'}, 'vectorization': {'method': 'openai-embedding'}} {'outlier_filtering': {'contamination': 0, 'do_filter': False, 'n_neighbours': 0}, 'resampling': {'do_resample': False, 'n_neighbours': 0, 'resample_before_outlier_filtering': False}}\n"
     ]
    }
   ],
   "source": [
    "# Generate all possible configurations for machine learning\n",
    "machine_learning_configurations = configurator.generate_machine_learning_configs(classifier_options, classifier_hyperparameters)\n",
    "\n",
    "# Generate all possible configurations for loading and preprocessing data\n",
    "data_loading_augmenting_configurations = configurator.generate_data_configurations(preprocessing_options)\n",
    "resampling_outlier_configurations = configurator.generate_resampling_outlier_configurations(resampling_outlier_options)\n",
    "\n",
    "print('Running ' + str(len(data_loading_augmenting_configurations) * len(resampling_outlier_configurations)) + ' configurations.')\n",
    "for inputconfig in data_loading_augmenting_configurations:\n",
    "    for config in resampling_outlier_configurations:\n",
    "        print(str(inputconfig) + ' ' + str(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text data for training\n",
    "autoware_lv0 = ['./data/autoware/descriptions/level 1', \"./data/autoware/code\", '.cpp', ['txt'], False,False,'autoware', True, True, True]\n",
    "autoware_lv1 = ['./data/autoware/descriptions/level 2', \"./data/autoware/code\", '.cpp', ['txt'], False,False,'autoware', True, True, True]\n",
    "# True, True, False]\n",
    "\n",
    "opencv_lv0 = ['./data/opencv/descriptions/level 1', \"./data/opencv/code\", '.cpp', ['csv'], False, False,'opencv', True, True, True]\n",
    "opencv_lv1 = ['./data/opencv/descriptions/level 2', \"./data/opencv/code\", '.cpp', ['csv'], False,False,'opencv', True, True, True]\n",
    "opencv_lv2 = ['./data/opencv/descriptions/level 3', \"./data/opencv/code\", '.cpp', ['csv'], False,False,'opencv', True, True, True]\n",
    "# False, True, True]\n",
    "\n",
    "rtems_lv0 = ['./data/rtems/descriptions/level 0', \"./data/rtems/code\", '.c', ['csv'], False,False,'rtems',True, True, True]\n",
    "rtems_lv1 = ['./data/rtems/descriptions/level 1', \"./data/rtems/code\", '.c', ['csv'], False,False,'rtems',True, True, True]\n",
    "rtems_lv2 = ['./data/rtems/descriptions/level 2', \"./data/rtems/code\", '.c', ['csv'], False,False,'rtems',True, True, True]\n",
    "rtems_lv3 = ['./data/rtems/descriptions/level 3', \"./data/rtems/code\", '.c', ['csv'], False,False,'rtems',True, True, True]\n",
    "# True, True, False]\n",
    "\n",
    "teammates_lv0 = ['./data/teammates/descriptions/level 1', \"./data/teammates/code\", '.java', ['txt'], False,False,'teammates', True, True, True]\n",
    "teammates_lv1 = ['./data/teammates/descriptions/level 2', \"./data/teammates/code\", '.java', ['csv','txt'], False,False,'teammates', True, True, True]\n",
    "teammates_lv2 = ['./data/teammates/descriptions/level 3', \"./data/teammates/code\", '.java', ['csv','txt'], False,False,'teammates', True, True, True]\n",
    "\n",
    "solr_lv0 = ['./data/solr/descriptions/level 0', \"./data/solr/code\", '.java', ['csv'], False,False,'solr', True, True, True]\n",
    "solr_lv1 = ['./data/solr/descriptions/level 1', \"./data/solr/code\", '.java', ['csv'], False,False,'solr', True, True, True]\n",
    "solr_lv2 = ['./data/solr/descriptions/level 2', \"./data/solr/code\", '.java', ['csv'], False,False,'solr', True, True, True]\n",
    "\n",
    "log4j_lv0 = ['./data/log4j/descriptions/level 0', \"./data/log4j/code\", '.java', ['csv'], False,False,'solr', True, True, True]\n",
    "log4j_lv1 = ['./data/log4j/descriptions/level 1', \"./data/log4j/code\", '.java', ['csv'], False,False,'solr', True, True, True]\n",
    "log4j_lv2 = ['./data/log4j/descriptions/level 2', \"./data/log4j/code\", '.java', ['csv'], False,False,'solr', True, True, True]\n",
    "\n",
    "# True, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = autoware_lv1\n",
    "\n",
    "training_directory = system[0]\n",
    "src_directory = system[1]\n",
    "file_extension = system[2] # '.py'#\n",
    "loading_method = system[3]\n",
    "do_merge_text = system[4]\n",
    "do_merge_training_data = system[5]\n",
    "system_name = system[6]\n",
    "\n",
    "root = src_directory.split('/')[-1]\n",
    "use_super_classes = True\n",
    "use_prob_threshold = False\n",
    "\n",
    "apply_stemming = system[7]\n",
    "apply_lemmitization = system[8]\n",
    "do_remove_stopwords = system[9]\n",
    "\n",
    "apply_stemming = True\n",
    "apply_lemmitization = True\n",
    "do_remove_stopwords = True\n",
    "\n",
    "do_clean_code = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classification(training_directory, src_directory, root, file_extension, use_super_classes, loading_method,\n",
    "    apply_stemming, apply_lemmitization, do_remove_stopwords, do_clean_code, do_merge_text, system_name, do_merge_training_data):\n",
    "\n",
    "    n_iterations_total = len(data_loading_augmenting_configurations) * len(resampling_outlier_configurations)\n",
    "    it = 0\n",
    "\n",
    "    for inputConfig in data_loading_augmenting_configurations:\n",
    "        try:\n",
    "            # Instantiation\n",
    "            data_loader = DataLoader()\n",
    "            augmenter = Augmenter()\n",
    "            \n",
    "            ## Load data\n",
    "            df_src_functions = MethodsIdentifiersExtractor.extract_methods_identifiers_from_directory(src_directory,system_name, output_file_name=True,file_extension=file_extension,extract_whole_file=False)\n",
    "            df_src_files = MethodsIdentifiersExtractor.extract_methods_identifiers_from_directory(src_directory,system_name, output_file_name=True,file_extension=file_extension,extract_whole_file=True)\n",
    "            \n",
    "            if do_clean_code:\n",
    "                df_src_functions['Content'] = df_src_functions['Content'].apply(clean_code_snippet)\n",
    "                df_src_files['Content'] = df_src_files['Content'].apply(clean_code_snippet)\n",
    "            \n",
    "            # Load module descriptions\n",
    "            X_train, y_train = data_loader.load_module_descriptions(training_directory,loading_method,merge_text=do_merge_text)\n",
    "            \n",
    "            if do_merge_training_data:\n",
    "                X_train, y_train = merge_data_with_token_limit(X_train, y_train, max_tokens=256)\n",
    "            \n",
    "\n",
    "            ## Pre-processing\n",
    "            if use_super_classes: # convert labels of subclasses to superclasses\n",
    "                class_dict = find_parent_classes(src_directory, root)\n",
    "                #class_dict = find_parent_classes_using_list(full_src_directory, root)\n",
    "                #print_unmpaped_classes(y_train, class_dict)\n",
    "                df_src_functions = update_class_names_with_parent_classes(df_src_functions, class_dict)\n",
    "                df_src_files = update_class_names_with_parent_classes(df_src_files, class_dict)\n",
    "                y_train = convert_labels_to_parent_classes(y_train, class_dict)\n",
    "\n",
    "            df_src_functions, df_src_files, X_train = clean_system_and_module_name(df_src_functions, df_src_files, X_train, y_train, system_name)\n",
    "            data_loader.identify_frequent_tokens(X_train, y_train)\n",
    "\n",
    "            X_train, y_train = data_loader.clean_text_data(X_train, y_train, remove_stopwords=False, rm_pct=False, remove_frequent=False)\n",
    "            \n",
    "            ## Augment data\n",
    "            X_train, y_train = augmenter.augment_text(\n",
    "                X_train, y_train, augmenting_methods=inputConfig['data_creation']['data_creation_method'],\n",
    "                sliding_window_sizes=inputConfig[\"data_creation\"][\"sliding_window_size\"],\n",
    "                sliding_window_overlap=inputConfig[\"data_creation\"][\"window_movement\"],use_resampling=False, upsample_largest_class=None)\n",
    "            \n",
    "            ## Final processing\n",
    "            X_train, y_train = filter_empty_data_points(X_train, y_train)\n",
    "            X_train, y_train = remove_duplicates(X_train, y_train)\n",
    "            X_train, y_train = remove_non_strings(X_train, y_train)\n",
    "            \n",
    "            df_src_functions = filter_classes_not_in_training_data(df_src_functions, X_train, y_train)\n",
    "            df_src_files = filter_classes_not_in_training_data(df_src_files, X_train, y_train)\n",
    "            \n",
    "            ## Data properties gathering\n",
    "            df_src_functions['n_NL_words'] = df_src_functions['Content'].apply(count_natural_language_words)\n",
    "            df_src_files['n_NL_words'] = df_src_files['Content'].apply(count_natural_language_words)\n",
    "\n",
    "            X_train_tokens = count_openai_tokens(X_train) #tokens_after_augment old\n",
    "            X_train_total_words = count_words(X_train)        \n",
    "        \n",
    "            unique_classes, counts = np.unique(y_train, return_counts=True)\n",
    "            class_sizes = dict(zip(unique_classes, counts))\n",
    "\n",
    "            original_n_o_functions = len(df_src_functions['Id'].to_numpy())\n",
    "            original_n_o_files = len(df_src_files['Id'].to_numpy())\n",
    "\n",
    "            # Convert the column to numeric (in case there are non-numeric values)\n",
    "            lines_of_code_functions = pd.to_numeric(df_src_functions['Lines of code'], errors='coerce')\n",
    "            lines_of_code_files = pd.to_numeric(df_src_files['Lines of code'], errors='coerce')\n",
    "\n",
    "            n_LoC_functions = lines_of_code_functions.sum()\n",
    "            n_LoC_mean_functions = lines_of_code_functions.mean()\n",
    "            n_LoC_median_functions = lines_of_code_functions.median()\n",
    "            n_LoC_std_functions = lines_of_code_functions.std()\n",
    "            \n",
    "            n_LoC_files = lines_of_code_files.sum()\n",
    "            n_LoC_mean_files = lines_of_code_files.mean()\n",
    "            n_LoC_median_files = lines_of_code_files.median()\n",
    "            n_LoC_std_files = lines_of_code_files.std()  \n",
    "\n",
    "\n",
    "            X_eval_function_content = df_src_functions['Content'].to_numpy() # consider preprocessing method content\n",
    "            y_eval_function_content = df_src_functions['Class'].to_numpy()\n",
    "            function_ids = df_src_functions['Id'].to_numpy()\n",
    "\n",
    "            X_eval_files_content = df_src_files['Content'].to_numpy() # consider preprocessing method content\n",
    "            y_eval_files_content = df_src_files['Class'].to_numpy()\n",
    "            file_ids = df_src_files['Id'].to_numpy()\n",
    "\n",
    "            X_eval_files_content, y_eval_files_content = remove_non_strings(X_eval_files_content, y_eval_files_content)\n",
    "            X_eval_function_content, y_eval_function_content = remove_non_strings(X_eval_function_content, y_eval_function_content)\n",
    "            \n",
    "            X_eval_files_content, y_eval_files_content = data_loader.clean_text_data(X_eval_files_content, y_eval_files_content, remove_stopwords=True, rm_pct=True, remove_frequent=True)\n",
    "            X_eval_function_content, y_eval_function_content = data_loader.clean_text_data(X_eval_function_content, y_eval_function_content, remove_stopwords=True,  rm_pct=True, remove_frequent=True)\n",
    "            X_train, y_train = data_loader.clean_text_data(X_train, y_train, remove_stopwords=True, rm_pct=True, remove_frequent=True)\n",
    "\n",
    "            X_eval_files_content = data_loader.filter_frequent_tokens_test_data(X_eval_files_content)\n",
    "            X_eval_function_content = data_loader.filter_frequent_tokens_test_data(X_eval_function_content)\n",
    "\n",
    "            X_train, y_train = filter_empty_data_points(X_train, y_train)\n",
    "\n",
    "            ## Generate embeddings\n",
    "            featureExtractor = FeatureExtractor(inputConfig['vectorization']['method'],useLocal=True)\n",
    "            X_train_embeddings = featureExtractor.get_feature_vectors(X_train)\n",
    "\n",
    "            if len(X_train_embeddings) != len(X_train):\n",
    "                print('Lengths X_train and embeddings not equal:', len(X_train),' and ', len(X_train_embeddings),'. Stopping iteration.')\n",
    "                continue\n",
    "                #X_train_embeddings = X_train_embeddings[:len(X_train)]\n",
    "\n",
    "            X_eval_function_content_embeddings = featureExtractor.get_feature_vectors(X_eval_function_content)\n",
    "            X_eval_files_content_embeddings = featureExtractor.get_feature_vectors(X_eval_files_content)\n",
    "\n",
    "            # Save data\n",
    "            saved_data = save_data(X_train, y_train, X_train_embeddings,\n",
    "                                X_eval_function_content, y_eval_function_content,\n",
    "                                X_eval_function_content_embeddings, function_ids,\n",
    "                                X_eval_files_content, y_eval_files_content,\n",
    "                                X_eval_files_content_embeddings, file_ids)\n",
    "\n",
    "            for resampling_outlier_config in resampling_outlier_configurations:\n",
    "                # Load fresh unprocessed data\n",
    "                (X_train, y_train, X_train_embeddings,\n",
    "                X_eval_function_content, y_eval_function_content, \n",
    "                X_eval_function_content_embeddings, function_ids, \n",
    "                X_eval_files_content, y_eval_files_content,\n",
    "                X_eval_files_content_embeddings, file_ids) = load_data(saved_data)\n",
    "\n",
    "                #original_classes_of_methods = original_classes_of_methods_saved\n",
    "                print('Performing iteration ', it+1, ' of ', n_iterations_total)\n",
    "                it += 1\n",
    "\n",
    "\n",
    "                if resampling_outlier_config['resampling']['do_resample'] == True:\n",
    "                    X_train_embeddings, y_train = augmenter.smote_resample(X_train_embeddings, y_train, n_neighbors=resampling_outlier_config['resampling']['n_neighbours'])\n",
    "                \n",
    "\n",
    "                unique_classes, counts = np.unique(y_train, return_counts=True)\n",
    "                class_sizes = dict(zip(unique_classes, counts))\n",
    "\n",
    "                # Encode labels for training and evaluation\n",
    "                label_encoder, y_train_encoded, y_eval_functions_encoded, y_eval_files_encoded = encode_labels(y_train, y_eval_function_content, y_eval_files_content)\n",
    "\n",
    "                for config in machine_learning_configurations:\n",
    "                    # Instantiate and train the classifier\n",
    "                    clf = Classifier(method=config[\"classifier_method\"], hyperparameters=config[\"hyperparameters\"])\n",
    "                    clf.train(X_train_embeddings, y_train_encoded)\n",
    "\n",
    "                    method_classifier = MethodClassifier(clf)\n",
    "                    X_eval_file_entropy, X_eval_file_mean_entropy = method_classifier.get_neighbor_entropy(X_train_embeddings, y_train_encoded, X_eval_files_content_embeddings)\n",
    "                    X_eval_function_entropy, X_eval_function_mean_entropy = method_classifier.get_neighbor_entropy(X_train_embeddings, y_train_encoded, X_eval_function_content_embeddings)\n",
    "\n",
    "                    X_eval_file_density, X_eval_file_mean_density = method_classifier.get_neighbor_density(X_train_embeddings, X_eval_files_content_embeddings)\n",
    "                    X_eval_function_density, X_eval_function_mean_density = method_classifier.get_neighbor_density(X_train_embeddings, X_eval_function_content_embeddings)\n",
    "\n",
    "                    X_eval_function_entropy = pd.DataFrame(X_eval_function_entropy, columns=['Entropy'])\n",
    "                    X_eval_function_density = pd.DataFrame(X_eval_function_density, columns=['Density'])\n",
    "                    \n",
    "                    X_eval_file_entropy = pd.DataFrame(X_eval_file_entropy, columns=['Entropy'])\n",
    "                    X_eval_file_density = pd.DataFrame(X_eval_file_density, columns=['Density'])\n",
    "                    \n",
    "                    if not os.path.exists('results.csv'):\n",
    "                        max_id = 0\n",
    "                    else:\n",
    "                        results_df = pd.read_csv('results.csv')\n",
    "                        max_id = results_df['ID'].max() if len(results_df) > 0 else 0\n",
    "                    id = max_id + 1\n",
    "                    top_n = 1\n",
    "                    compute_and_save_data_properties(id, \n",
    "                                                    X_train,\n",
    "                                                    X_train_embeddings,\n",
    "                                                    y_train_encoded, \n",
    "                                                    X_eval_function_content, \n",
    "                                                    X_eval_function_content_embeddings, \n",
    "                                                    y_eval_functions_encoded,                                                 \n",
    "                                                    training_directory, \n",
    "                                                    inputConfig['data_creation']['data_creation_method'])\n",
    "\n",
    "                    compute_and_save_data_properties(id+top_n, \n",
    "                                                    X_train,\n",
    "                                                    X_train_embeddings,\n",
    "                                                    y_train_encoded, \n",
    "                                                    X_eval_files_content, \n",
    "                                                    X_eval_files_content_embeddings, \n",
    "                                                    y_eval_files_encoded,                                                 \n",
    "                                                    training_directory, \n",
    "                                                    inputConfig['data_creation']['data_creation_method'])\n",
    "                    \n",
    "                    ## Predict functions\n",
    "                    for N in range(1,top_n+1):\n",
    "                        #with threadpool_limits(limits=1, user_api='blas'):\n",
    "                            # Your scikit-learn model prediction code here\n",
    "                        y_pred_topN_encoded, ids  = method_classifier.predict_method_content(X_eval_function_content_embeddings, function_ids, N=N)\n",
    "\n",
    "                        #X_eval_function_entropy = pd.DataFrame(X_eval_function_entropy, columns=['Entropy'])\n",
    "                        #X_eval_function_density = pd.DataFrame(X_eval_function_density, columns=['Density'])\n",
    "\n",
    "                        # Decode the predicted classes (top N)\n",
    "                        y_pred_topN = decode_labels(label_encoder, y_pred_topN_encoded)\n",
    "\n",
    "                        y_eval_temp_list = []\n",
    "                        \n",
    "                        # Initialize lists to store metrics\n",
    "                        lines_of_code_list = []\n",
    "                        non_generic_terms_list = []\n",
    "                        unique_non_generic_terms_list = []\n",
    "                        n_ext_includes_list = []\n",
    "                        n_comments_list = []\n",
    "                        n_nl_words_list = []\n",
    "\n",
    "                        # Loop over each method_id to gather metrics from df_src\n",
    "                        for method_id in ids:\n",
    "                            # Get the corresponding row in df_src based on method_id\n",
    "                            method_row = df_src_functions[df_src_functions['Id'] == method_id]\n",
    "                            \n",
    "                            # Extract Lines of Code and number of non-generic terms from df_src\n",
    "                            lines_of_code = method_row['Lines of code'].values[0]\n",
    "                            non_generic_terms_count = method_row['n_non_generic_terms'].values[0]\n",
    "                            unique_non_generic_terms_count = method_row['n_unique_non_generic_terms'].values[0]\n",
    "                            n_ext_includes = method_row['n_ext_includes'].values[0]\n",
    "                            n_comments = method_row['n_comments'].values[0]\n",
    "                            n_nl_words = method_row['n_NL_words'].values[0]\n",
    "\n",
    "                            # Append the values to the respective lists\n",
    "                            lines_of_code_list.append(lines_of_code)\n",
    "                            non_generic_terms_list.append(non_generic_terms_count)\n",
    "                            unique_non_generic_terms_list.append(unique_non_generic_terms_count)\n",
    "                            n_ext_includes_list.append(n_ext_includes)\n",
    "                            n_comments_list.append(n_comments)\n",
    "                            n_nl_words_list.append(n_nl_words)\n",
    "\n",
    "                            # Get the true class name\n",
    "                            class_name = method_row['Class'].values[0]\n",
    "                            y_eval_temp_list.append(class_name)\n",
    "\n",
    "                        # Convert lists to DataFrames (1D, since we only have one method name and one true class per method)\n",
    "                        y_eval_temp = pd.DataFrame(y_eval_temp_list, columns=['True Class'])\n",
    "\n",
    "                        # Decode the predicted classes (top N)\n",
    "                        y_pred_topN_df = pd.DataFrame(y_pred_topN, columns=[f'Predicted Class {i+1}' for i in range(N)])\n",
    "\n",
    "                        # Create DataFrames for the metrics\n",
    "                        lines_of_code_df = pd.DataFrame(lines_of_code_list, columns=['Lines of code'])\n",
    "                        non_generic_terms_df = pd.DataFrame(non_generic_terms_list, columns=['Non-generic terms'])\n",
    "                        unique_non_generic_terms_df = pd.DataFrame(non_generic_terms_list, columns=['Unique non-generic terms'])\n",
    "                        n_ext_includes_df = pd.DataFrame(n_ext_includes_list, columns =['n_ext_includes'])\n",
    "                        n_comments_df = pd.DataFrame(n_comments_list, columns =['n_comments'])\n",
    "                        n_nl_words_df = pd.DataFrame(n_nl_words_list, columns =['n_NL_words'])\n",
    "\n",
    "                        # Concatenate to form the final DataFrame\n",
    "                        predictions_top_N_df = pd.concat([y_pred_topN_df, y_eval_temp, X_eval_function_entropy, X_eval_function_density, lines_of_code_df, non_generic_terms_df, unique_non_generic_terms_df, n_ext_includes_df, n_comments_df,n_nl_words_df], axis=1)\n",
    "\n",
    "                        # Evaluate the top N predictions\n",
    "                        metrics_top_N, predictions_top_N_df = evaluate_topN_predictions(predictions_top_N_df, original_n_o_functions, N)\n",
    "\n",
    "                        # Save results\n",
    "                        save_results(datetime.datetime.now(), len(label_encoder.classes_)-1, inputConfig, resampling_outlier_config,\n",
    "                                    config['classifier_method'], config['hyperparameters'], metrics_top_N, training_directory,\n",
    "                                    src_directory, predictions_top_N_df, N, X_train_tokens, apply_stemming, apply_lemmitization, do_clean_code, do_remove_stopwords,\n",
    "                                    class_sizes, original_n_o_functions,n_LoC_functions,n_LoC_mean_functions,n_LoC_median_functions,n_LoC_std_functions, X_eval_function_mean_entropy, X_eval_function_mean_density, do_map_files=False, do_merge_training_data=do_merge_training_data)\n",
    "                        \n",
    "                    ## Predict files\n",
    "                    for N in range(1,top_n+1):\n",
    "                        #with threadpool_limits(limits=1, user_api='blas'):\n",
    "                            # Your scikit-learn model prediction code here\n",
    "                        y_pred_topN_encoded, ids = method_classifier.predict_method_content(X_eval_files_content_embeddings, file_ids, N=N)\n",
    "                        \n",
    "                        # Decode the predicted classes (top N)\n",
    "                        y_pred_topN = decode_labels(label_encoder, y_pred_topN_encoded)\n",
    "\n",
    "                        y_eval_temp_list = []\n",
    "                        \n",
    "                        # Initialize lists to store metrics\n",
    "                        lines_of_code_list = []\n",
    "                        non_generic_terms_list = []\n",
    "                        unique_non_generic_terms_list = []\n",
    "                        n_ext_includes_list = []\n",
    "                        n_comments_list = []\n",
    "                        n_nl_words_list = []\n",
    "\n",
    "                        # Loop over each method_id to gather metrics from df_src\n",
    "                        for method_id in ids:\n",
    "                            # Get the corresponding row in df_src based on method_id\n",
    "                            method_row = df_src_files[df_src_files['Id'] == method_id]\n",
    "                            \n",
    "                            # Extract Lines of Code and number of non-generic terms from df_src\n",
    "                            lines_of_code = method_row['Lines of code'].values[0]\n",
    "                            non_generic_terms_count = method_row['n_non_generic_terms'].values[0]\n",
    "                            unique_non_generic_terms_count = method_row['n_unique_non_generic_terms'].values[0]\n",
    "                            n_ext_includes = method_row['n_ext_includes'].values[0]\n",
    "                            n_comments = method_row['n_comments'].values[0]\n",
    "                            n_nl_words = method_row['n_NL_words'].values[0]\n",
    "\n",
    "                            # Append the values to the respective lists\n",
    "                            lines_of_code_list.append(lines_of_code)\n",
    "                            non_generic_terms_list.append(non_generic_terms_count)\n",
    "                            unique_non_generic_terms_list.append(unique_non_generic_terms_count)\n",
    "                            n_ext_includes_list.append(n_ext_includes)\n",
    "                            n_comments_list.append(n_comments)\n",
    "                            n_nl_words_list.append(n_nl_words)\n",
    "\n",
    "                            # Get the true class name\n",
    "                            class_name = method_row['Class'].values[0]\n",
    "                            y_eval_temp_list.append(class_name)\n",
    "\n",
    "                        # Convert lists to DataFrames (1D, since we only have one method name and one true class per method)\n",
    "                        y_eval_temp = pd.DataFrame(y_eval_temp_list, columns=['True Class'])\n",
    "\n",
    "                        # Decode the predicted classes (top N)\n",
    "                        y_pred_topN_df = pd.DataFrame(y_pred_topN, columns=[f'Predicted Class {i+1}' for i in range(N)])\n",
    "\n",
    "                        # Create DataFrames for the metrics\n",
    "                        lines_of_code_df = pd.DataFrame(lines_of_code_list, columns=['Lines of code'])\n",
    "                        non_generic_terms_df = pd.DataFrame(non_generic_terms_list, columns=['Non-generic terms'])\n",
    "                        unique_non_generic_terms_df = pd.DataFrame(unique_non_generic_terms_list, columns=['Unique non-generic terms'])\n",
    "                        n_ext_includes_df = pd.DataFrame(n_ext_includes_list, columns =['n_ext_includes'])\n",
    "                        n_comments_df = pd.DataFrame(n_comments_list, columns =['n_comments'])\n",
    "                        n_nl_words_df = pd.DataFrame(n_nl_words_list, columns =['n_NL_words'])\n",
    "\n",
    "                        # Concatenate to form the final DataFrame\n",
    "                        predictions_top_N_df = pd.concat([y_pred_topN_df, y_eval_temp, X_eval_file_entropy, X_eval_file_density, lines_of_code_df, non_generic_terms_df, unique_non_generic_terms_df, n_ext_includes_df, n_comments_df, n_nl_words_df], axis=1)\n",
    "\n",
    "                        # Evaluate the top N predictions\n",
    "                        metrics_top_N, predictions_top_N_df = evaluate_topN_predictions(predictions_top_N_df, original_n_o_files, N)\n",
    "\n",
    "                        # Save results\n",
    "                        save_results(datetime.datetime.now(), len(label_encoder.classes_)-1, inputConfig, resampling_outlier_config,\n",
    "                                    config['classifier_method'], config['hyperparameters'], metrics_top_N, training_directory,\n",
    "                                    src_directory, predictions_top_N_df, N, X_train_tokens, apply_stemming, apply_lemmitization, do_clean_code, do_remove_stopwords,\n",
    "                                    class_sizes,  \n",
    "                                    original_n_o_files,n_LoC_files,n_LoC_mean_files, n_LoC_median_files, n_LoC_std_files, X_eval_function_mean_entropy, X_eval_function_mean_density, do_map_files=True, do_merge_training_data=do_merge_training_data)\n",
    "\n",
    "            del X_train, y_train, X_train_embeddings\n",
    "\n",
    "        except MemoryError as e:\n",
    "            print('MemoryError:','inputconfig:',inputConfig, '. Directory: ', training_directory,'. Error: ',e)\n",
    "            continue\n",
    "        except ValueError as e:\n",
    "            print('ValueError:','inputconfig:',inputConfig, '. Directory: ', training_directory,'. Error: ',e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system:  ['./data/log4j/descriptions/level 0', './data/log4j/code', '.java', ['csv'], False, False, 'solr', True, True, True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching embeddings: 100%|██████████| 747/747 [00:11<00:00, 65.60it/s]\n",
      "Fetching embeddings: 100%|██████████| 3331/3331 [00:45<00:00, 73.96it/s]\n",
      "Fetching embeddings: 100%|██████████| 716/716 [01:00<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing iteration  1  of  1\n",
      "Running system:  ['./data/log4j/descriptions/level 1', './data/log4j/code', '.java', ['csv'], False, False, 'solr', True, True, True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching embeddings: 100%|██████████| 1876/1876 [00:26<00:00, 70.54it/s]\n",
      "Fetching embeddings: 100%|██████████| 3331/3331 [00:42<00:00, 78.88it/s]\n",
      "Fetching embeddings: 100%|██████████| 716/716 [00:58<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing iteration  1  of  1\n",
      "Running system:  ['./data/log4j/descriptions/level 2', './data/log4j/code', '.java', ['csv'], False, False, 'solr', True, True, True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching embeddings: 100%|██████████| 9961/9961 [02:07<00:00, 78.03it/s]\n",
      "Fetching embeddings: 100%|██████████| 3331/3331 [00:40<00:00, 81.69it/s] \n",
      "Fetching embeddings: 100%|██████████| 716/716 [00:58<00:00, 12.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing iteration  1  of  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "autoware_file = ['./data/autoware/descriptions/level 1', \"./data/autoware/code\", 'cpp', ['txt'], False,True,False,'autoware']\n",
    "autoware_file_lv2 = ['./data/autoware/descriptions/level 2', \"./data/autoware/code\", 'cpp', ['txt'], False,True,False,'autoware']\n",
    "\n",
    "opencv_file = ['./data/opencv/descriptions/level 1', \"./data/opencv/code\", 'cpp', ['csv'], True,True, False,'opencv']\n",
    "opencv_file_lv2 = ['./data/opencv/descriptions/level 2', \"./data/opencv/code\", 'cpp', ['csv'], True,True,False,'opencv']\n",
    "opencv_file_lv3 = ['./data/opencv/descriptions/level 3', \"./data/opencv/code\", 'cpp', ['csv'], True,True,False,'opencv']\n",
    "\n",
    "rtems_file_lv0 = ['./data/rtems/descriptions/level 0', \"./data/rtems/code\", 'c', ['csv'], False,True,False,'rtems']\n",
    "rtems_file = ['./data/rtems/descriptions/level 1', \"./data/rtems/code\", 'c', ['csv'], False,True,False,'rtems']\n",
    "rtems_file_lv2 = ['./data/rtems/descriptions/level 2', \"./data/rtems/code\", 'c', ['csv'], False,True,False,'rtems']\n",
    "rtems_file_lv3 = ['./data/rtems/descriptions/level 3', \"./data/rtems/code\", 'c', ['csv'], False,True,False,'rtems']\n",
    "\n",
    "teammates_file = ['./data/teammates/descriptions/level 1', \"./data/teammates/code\", 'java', ['txt'], False,True,False,'teammates']\n",
    "teammates_file_lv2 = ['./data/teammates/descriptions/level 2', \"./data/teammates/code\", 'java', ['csv','txt'], False,True,False,'teammates']\n",
    "teammates_file_lv3 = ['./data/teammates/descriptions/level 3', \"./data/teammates/code\", 'java', ['csv','txt'], False,True,False,'teammates']\n",
    "\"\"\"\n",
    "targetSystems = [autoware_lv0, autoware_lv1, opencv_lv0, opencv_lv1, opencv_lv2, rtems_lv0, rtems_lv1, rtems_lv2, rtems_lv3, teammates_lv0, teammates_lv1, teammates_lv2, solr_lv0, solr_lv1, solr_lv2, log4j_lv0, log4j_lv1, log4j_lv2] #  \n",
    "targetSystems = [log4j_lv0, log4j_lv1, log4j_lv2]\n",
    "\n",
    "targetSystems_test = [autoware_lv0, opencv_lv0, rtems_lv0, teammates_lv0]\n",
    "targetSystems_test_big = [opencv_lv2, rtems_lv3, teammates_lv2, autoware_lv1] #, autoware_lv1\n",
    "use_super_classes = True\n",
    "\n",
    "\n",
    "true_false_opts = [False, True]\n",
    "\n",
    "for system in targetSystems:\n",
    "    training_directory = system[0]\n",
    "    src_directory = system[1]\n",
    "    file_extension = system[2]\n",
    "    loading_method = system[3]\n",
    "    do_merge_text = system[4]\n",
    "    do_merge_training_data = False #system[5]\n",
    "    system_name = system[6]\n",
    "\n",
    "    do_stem = system[7]\n",
    "    do_lemmitize = system[8]\n",
    "    do_remove_stopwords = system[9]\n",
    "    do_clean_code = True\n",
    "\n",
    "    root = src_directory.split('/')[-1]\n",
    "    print('Running system: ', system)\n",
    "    #run_prompt_classification(training_directory, src_directory, root, file_extension, use_super_classes, loading_method, do_stem, do_lemmitize, False, do_merge_text, system_name, fileMapping)\n",
    "    run_classification(training_directory, src_directory, root, file_extension, use_super_classes, loading_method, \n",
    "                                do_stem, do_lemmitize, do_remove_stopwords, do_clean_code, do_merge_text, system_name, do_merge_training_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
